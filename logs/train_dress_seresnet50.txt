/usr/lib64/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
[2018-07-02 14:51:41,577] [train] [INFO] define model+
[2018-07-02 14:51:41,588] [pose_dataset] [INFO] dataflow img_path=/home/shy/projects/tf-openpose/data/
[2018-07-02 14:51:43,957] [pose_dataset] [INFO] /home/shy/projects/tf-openpose/data/train/train_bak.csv dataset 10181
[32m[0702 14:51:43 @parallel.py:178][0m [MultiProcessPrefetchData] Will fork a dataflow more than one times. This assumes the datapoints are i.i.d.
[32m[0702 14:51:44 @parallel.py:178][0m [MultiProcessPrefetchData] Will fork a dataflow more than one times. This assumes the datapoints are i.i.d.
[2018-07-02 14:51:44,622] [pose_dataset] [INFO] dataflow img_path=/home/shy/projects/tf-openpose/data/
[2018-07-02 14:51:45,994] [pose_dataset] [INFO] /home/shy/projects/tf-openpose/data/train/val_bak.csv dataset 1132
[32m[0702 14:51:45 @parallel.py:178][0m [MultiProcessPrefetchData] Will fork a dataflow more than one times. This assumes the datapoints are i.i.d.
[32m[0702 14:51:46 @parallel.py:178][0m [MultiProcessPrefetchData] Will fork a dataflow more than one times. This assumes the datapoints are i.i.d.
[2018-07-02 14:51:46,709] [train] [INFO] tensorboard val image: 12
[2018-07-02 14:51:46,710] [train] [INFO] Tensor("fifo_queue_Dequeue:0", shape=(16, 368, 368, 3), dtype=float32, device=/device:GPU:0)
[2018-07-02 14:51:46,711] [train] [INFO] Tensor("fifo_queue_Dequeue:1", shape=(16, 46, 46, 16), dtype=float32, device=/device:GPU:0)
[2018-07-02 14:51:46,711] [train] [INFO] Tensor("fifo_queue_Dequeue:2", shape=(16, 46, 46, 32), dtype=float32, device=/device:GPU:0)
[2018-07-02 14:53:25,321] [train] [INFO] define model-
2018-07-02 14:53:28.050469: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-07-02 14:53:28.050524: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-07-02 14:53:28.050535: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-07-02 14:53:28.050543: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-07-02 14:53:28.050551: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2018-07-02 14:53:36.575658: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: 
name: GeForce GTX 1080 Ti
major: 6 minor: 1 memoryClockRate (GHz) 1.582
pciBusID 0000:04:00.0
Total memory: 10.92GiB
Free memory: 10.76GiB
2018-07-02 14:53:36.750330: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x565b6060 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.
2018-07-02 14:53:36.751468: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 1 with properties: 
name: GeForce GTX 1080 Ti
major: 6 minor: 1 memoryClockRate (GHz) 1.582
pciBusID 0000:05:00.0
Total memory: 10.92GiB
Free memory: 10.76GiB
2018-07-02 14:53:36.926319: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x565d5c20 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.
2018-07-02 14:53:36.927573: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 2 with properties: 
name: GeForce GTX 1080 Ti
major: 6 minor: 1 memoryClockRate (GHz) 1.582
pciBusID 0000:06:00.0
Total memory: 10.92GiB
Free memory: 10.76GiB
2018-07-02 14:53:37.109973: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x566cd760 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.
2018-07-02 14:53:37.111120: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 3 with properties: 
name: GeForce GTX 1080 Ti
major: 6 minor: 1 memoryClockRate (GHz) 1.582
pciBusID 0000:07:00.0
Total memory: 10.92GiB
Free memory: 10.76GiB
2018-07-02 14:53:37.296165: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x75ad4c80 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.
2018-07-02 14:53:37.297537: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 4 with properties: 
name: GeForce GTX 1080 Ti
major: 6 minor: 1 memoryClockRate (GHz) 1.582
pciBusID 0000:08:00.0
Total memory: 10.92GiB
Free memory: 10.76GiB
2018-07-02 14:53:37.489523: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x56626fc0 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.
2018-07-02 14:53:37.490653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 5 with properties: 
name: GeForce GTX 1080 Ti
major: 6 minor: 1 memoryClockRate (GHz) 1.582
pciBusID 0000:0b:00.0
Total memory: 10.92GiB
Free memory: 10.76GiB
2018-07-02 14:53:37.688214: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x566c5d80 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.
2018-07-02 14:53:37.689532: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 6 with properties: 
name: GeForce GTX 1080 Ti
major: 6 minor: 1 memoryClockRate (GHz) 1.582
pciBusID 0000:0c:00.0
Total memory: 10.92GiB
Free memory: 10.76GiB
2018-07-02 14:53:37.904518: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x565bad40 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.
2018-07-02 14:53:37.905839: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 7 with properties: 
name: GeForce GTX 1080 Ti
major: 6 minor: 1 memoryClockRate (GHz) 1.582
pciBusID 0000:0d:00.0
Total memory: 10.92GiB
Free memory: 10.76GiB
2018-07-02 14:53:37.938739: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 1 2 3 4 5 6 7 
2018-07-02 14:53:37.938770: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y Y Y Y Y Y Y Y 
2018-07-02 14:53:37.938794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 1:   Y Y Y Y Y Y Y Y 
2018-07-02 14:53:37.938801: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 2:   Y Y Y Y Y Y Y Y 
2018-07-02 14:53:37.938818: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 3:   Y Y Y Y Y Y Y Y 
2018-07-02 14:53:37.938823: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 4:   Y Y Y Y Y Y Y Y 
2018-07-02 14:53:37.938829: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 5:   Y Y Y Y Y Y Y Y 
2018-07-02 14:53:37.938850: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 6:   Y Y Y Y Y Y Y Y 
2018-07-02 14:53:37.938856: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 7:   Y Y Y Y Y Y Y Y 
2018-07-02 14:53:37.938878: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:04:00.0)
2018-07-02 14:53:37.938887: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:05:00.0)
2018-07-02 14:53:37.938897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:2) -> (device: 2, name: GeForce GTX 1080 Ti, pci bus id: 0000:06:00.0)
2018-07-02 14:53:37.938903: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:3) -> (device: 3, name: GeForce GTX 1080 Ti, pci bus id: 0000:07:00.0)
2018-07-02 14:53:37.938919: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:4) -> (device: 4, name: GeForce GTX 1080 Ti, pci bus id: 0000:08:00.0)
2018-07-02 14:53:37.938926: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:5) -> (device: 5, name: GeForce GTX 1080 Ti, pci bus id: 0000:0b:00.0)
2018-07-02 14:53:37.938932: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:6) -> (device: 6, name: GeForce GTX 1080 Ti, pci bus id: 0000:0c:00.0)
2018-07-02 14:53:37.938937: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:7) -> (device: 7, name: GeForce GTX 1080 Ti, pci bus id: 0000:0d:00.0)
[2018-07-02 14:53:38,852] [train] [INFO] model weights initialization
[2018-07-02 14:54:24,737] [train] [INFO] Restore pretrained weights from /home/shy/projects/tf-openpose/models/numpy/se_resnet50.npy ...
conv4_1_3x3/bn/gamma   0 / 225
conv4_2_1x1_increase/bn/beta   1 / 225
conv2_1_1x1_increase/bn/gamma   2 / 225
conv4_1_1x1_proj/kernel   3 / 225
conv5_2_1x1_reduce/kernel   4 / 225
conv3_2_3x3/bn/gamma   5 / 225
conv2_3_3x3/bn/gamma   6 / 225
conv3_4_1x1_reduce/bn/beta   7 / 225
conv5_2_1x1_down/kernel   8 / 225
conv3_2_1x1_down/kernel   9 / 225
conv3_4_1x1_increase/bn/beta   10 / 225
conv2_3_3x3/bn/beta   11 / 225
conv5_1_1x1_down/bias   12 / 225
conv4_4_3x3/bn/beta   13 / 225
conv2_1_1x1_proj/bn/gamma   14 / 225
conv3_2_1x1_down/bias   15 / 225
conv4_6_1x1_up/kernel   16 / 225
conv4_4_1x1_reduce/bn/gamma   17 / 225
conv5_2_1x1_reduce/bn/gamma   18 / 225
conv3_4_3x3/kernel   19 / 225
conv1/7x7_s2/bn/gamma   20 / 225
conv4_6_1x1_up/bias   21 / 225
conv3_3_1x1_down/kernel   22 / 225
conv4_1_1x1_reduce/kernel   23 / 225
conv2_2_3x3/kernel   24 / 225
conv2_1_3x3/kernel   25 / 225
conv5_3_1x1_down/kernel   26 / 225
conv2_1_1x1_reduce/kernel   27 / 225
conv3_2_1x1_reduce/kernel   28 / 225
conv4_3_3x3/bn/gamma   29 / 225
conv4_2_1x1_up/bias   30 / 225
conv5_1_1x1_increase/kernel   31 / 225
dense/bias   32 / 225
conv4_4_1x1_reduce/bn/beta   33 / 225
conv3_1_1x1_proj/bn/gamma   34 / 225
conv5_1_1x1_proj/kernel   35 / 225
conv5_1_3x3/bn/beta   36 / 225
conv5_1_1x1_up/kernel   37 / 225
conv4_3_1x1_reduce/kernel   38 / 225
conv4_6_1x1_reduce/bn/beta   39 / 225
conv5_3_1x1_increase/bn/beta   40 / 225
conv4_4_1x1_increase/kernel   41 / 225
conv3_4_1x1_increase/kernel   42 / 225
conv3_3_1x1_down/bias   43 / 225
conv3_2_3x3/kernel   44 / 225
conv5_3_1x1_increase/kernel   45 / 225
conv5_3_3x3/bn/beta   46 / 225
conv5_3_1x1_reduce/bn/beta   47 / 225
conv4_6_1x1_increase/bn/beta   48 / 225
conv2_2_1x1_reduce/bn/beta   49 / 225
conv5_2_3x3/bn/gamma   50 / 225
conv2_1_1x1_increase/bn/beta   51 / 225
conv4_1_1x1_up/kernel   52 / 225
conv3_2_1x1_up/kernel   53 / 225
conv5_1_1x1_reduce/bn/beta   54 / 225
conv4_4_1x1_increase/bn/gamma   55 / 225
conv4_2_3x3/bn/gamma   56 / 225
conv4_3_1x1_down/kernel   57 / 225
conv4_1_3x3/bn/beta   58 / 225
conv3_4_1x1_increase/bn/gamma   59 / 225
conv3_3_1x1_reduce/bn/beta   60 / 225
conv4_1_3x3/kernel   61 / 225
conv4_3_3x3/bn/beta   62 / 225
conv4_1_1x1_proj/bn/beta   63 / 225
conv2_3_1x1_up/kernel   64 / 225
conv4_5_1x1_up/bias   65 / 225
conv4_2_1x1_up/kernel   66 / 225
conv4_1_1x1_increase/bn/gamma   67 / 225
conv3_4_3x3/bn/gamma   68 / 225
conv5_1_1x1_reduce/bn/gamma   69 / 225
conv3_1_3x3/bn/gamma   70 / 225
conv3_2_1x1_up/bias   71 / 225
conv2_1_1x1_down/bias   72 / 225
conv2_2_3x3/bn/gamma   73 / 225
conv2_1_1x1_up/kernel   74 / 225
conv4_2_1x1_reduce/bn/gamma   75 / 225
conv4_1_1x1_reduce/bn/beta   76 / 225
conv3_3_1x1_up/kernel   77 / 225
conv3_1_1x1_proj/bn/beta   78 / 225
conv5_1_3x3/bn/gamma   79 / 225
conv3_1_1x1_reduce/bn/beta   80 / 225
conv5_3_3x3/kernel   81 / 225
conv3_4_1x1_down/kernel   82 / 225
conv4_2_1x1_reduce/kernel   83 / 225
conv4_3_3x3/kernel   84 / 225
conv3_4_1x1_down/bias   85 / 225
conv2_3_1x1_reduce/bn/gamma   86 / 225
conv4_4_3x3/bn/gamma   87 / 225
conv1/7x7_s2/kernel   88 / 225
conv3_3_1x1_reduce/bn/gamma   89 / 225
conv4_1_1x1_up/bias   90 / 225
conv2_1_1x1_increase/kernel   91 / 225
conv5_1_1x1_down/kernel   92 / 225
conv4_5_3x3/bn/gamma   93 / 225
conv4_6_3x3/kernel   94 / 225
conv4_2_1x1_down/bias   95 / 225
conv4_3_1x1_increase/bn/gamma   96 / 225
conv4_2_3x3/kernel   97 / 225
conv4_2_1x1_reduce/bn/beta   98 / 225
conv3_3_1x1_reduce/kernel   99 / 225
conv4_4_1x1_reduce/kernel   100 / 225
conv5_3_1x1_reduce/bn/gamma   101 / 225
conv2_2_1x1_down/bias   102 / 225
conv4_6_1x1_reduce/kernel   103 / 225
conv2_3_1x1_reduce/bn/beta   104 / 225
conv3_1_1x1_increase/bn/beta   105 / 225
conv4_3_1x1_reduce/bn/beta   106 / 225
conv4_6_1x1_reduce/bn/gamma   107 / 225
conv3_1_1x1_down/kernel   108 / 225
conv3_3_1x1_increase/bn/gamma   109 / 225
conv5_3_1x1_increase/bn/gamma   110 / 225
conv2_1_3x3/bn/gamma   111 / 225
conv2_3_3x3/kernel   112 / 225
conv4_4_1x1_up/bias   113 / 225
conv4_2_1x1_increase/kernel   114 / 225
conv3_4_1x1_reduce/bn/gamma   115 / 225
conv2_2_3x3/bn/beta   11[2018-07-02 16:45:38,475] [train] [INFO] Restore pretrained weights...Done
[2018-07-02 16:45:38,476] [train] [INFO] prepare file writer
[2018-07-02 16:46:01,486] [train] [INFO] prepare coordinator
[2018-07-02 16:46:02,153] [train] [INFO] Training Started.
[2018-07-02 16:51:32,480] [train] [INFO] epoch=0.00 step=100, 4.8437 examples/sec lr=0.000100, loss=271.377, loss_ll=54.1829, loss_ll_paf=81.7759, loss_ll_heat=26.5899, q=376
[2018-07-02 16:54:38,116] [train] [INFO] epoch=0.00 step=200, 6.2020 examples/sec lr=0.000100, loss=237.537, loss_ll=47.4438, loss_ll_paf=72.4113, loss_ll_heat=22.4764, q=582
[2018-07-02 16:57:38,607] [train] [INFO] epoch=0.00 step=300, 6.8921 examples/sec lr=0.000100, loss=244.053, loss_ll=48.1472, loss_ll_paf=72.3627, loss_ll_heat=23.9317, q=785
[2018-07-02 17:00:46,276] [train] [INFO] epoch=0.00 step=400, 7.2388 examples/sec lr=0.000100, loss=208.601, loss_ll=41.2374, loss_ll_paf=62.555, loss_ll_heat=19.9199, q=990
[2018-07-02 17:03:15,121] [train] [INFO] epoch=0.00 step=500, 7.7447 examples/sec lr=0.000100, loss=194.591, loss_ll=37.5589, loss_ll_paf=56.6632, loss_ll_heat=18.4546, q=1000
[2018-07-02 17:05:42,564] [train] [INFO] epoch=0.00 step=600, 8.1328 examples/sec lr=0.000100, loss=157.041, loss_ll=29.884, loss_ll_paf=44.7761, loss_ll_heat=14.992, q=1000
[2018-07-02 17:08:06,871] [train] [INFO] epoch=0.00 step=700, 8.4546 examples/sec lr=0.000100, loss=170.976, loss_ll=32.5049, loss_ll_paf=48.6638, loss_ll_heat=16.346, q=1000
[2018-07-02 17:10:33,009] [train] [INFO] epoch=0.00 step=800, 8.7024 examples/sec lr=0.000100, loss=131.89, loss_ll=24.7217, loss_ll_paf=36.2207, loss_ll_heat=13.2228, q=1000
[2018-07-02 17:12:56,771] [train] [INFO] epoch=0.00 step=900, 8.9185 examples/sec lr=0.000100, loss=152.309, loss_ll=28.8797, loss_ll_paf=44.1957, loss_ll_heat=13.5637, q=1000
[2018-07-02 17:15:26,246] [train] [INFO] epoch=0.00 step=1000, 9.0698 examples/sec lr=0.000100, loss=127.175, loss_ll=23.8317, loss_ll_paf=34.5853, loss_ll_heat=13.0781, q=1000
[2018-07-02 17:18:26,249] [train] [INFO] epoch=0.00 step=1100, 9.0531 examples/sec lr=0.000100, loss=101.694, loss_ll=18.7718, loss_ll_paf=27.5361, loss_ll_heat=10.0075, q=1000
[2018-07-02 17:20:53,667] [train] [INFO] epoch=0.00 step=1200, 9.1800 examples/sec lr=0.000100, loss=117.511, loss_ll=21.8529, loss_ll_paf=33.7621, loss_ll_heat=9.94373, q=1000
[2018-07-02 17:23:18,126] [train] [INFO] epoch=0.00 step=1300, 9.3025 examples/sec lr=0.000100, loss=117.358, loss_ll=22.7267, loss_ll_paf=33.2211, loss_ll_heat=12.2324, q=1000
[2018-07-02 17:25:47,697] [train] [INFO] epoch=0.00 step=1400, 9.3899 examples/sec lr=0.000100, loss=104.241, loss_ll=19.5222, loss_ll_paf=29.3929, loss_ll_heat=9.65159, q=1000
[2018-07-02 17:28:12,402] [train] [INFO] epoch=0.00 step=1500, 9.4852 examples/sec lr=0.000100, loss=106.846, loss_ll=19.8554, loss_ll_paf=29.8812, loss_ll_heat=9.82956, q=1000
[2018-07-02 17:30:38,304] [train] [INFO] epoch=0.00 step=1600, 9.5660 examples/sec lr=0.000100, loss=99.0577, loss_ll=18.3348, loss_ll_paf=28.0269, loss_ll_heat=8.64272, q=1000
[2018-07-02 17:33:03,443] [train] [INFO] epoch=0.00 step=1700, 9.6410 examples/sec lr=0.000100, loss=90.6128, loss_ll=16.931, loss_ll_paf=25.0797, loss_ll_heat=8.78241, q=1000
[2018-07-02 17:35:33,125] [train] [INFO] epoch=0.00 step=1800, 9.6938 examples/sec lr=0.000100, loss=105.037, loss_ll=19.5661, loss_ll_paf=29.3506, loss_ll_heat=9.78156, q=1000
[2018-07-02 17:37:57,348] [train] [INFO] epoch=0.00 step=1900, 9.7586 examples/sec lr=0.000100, loss=78.3929, loss_ll=14.2251, loss_ll_paf=21.0678, loss_ll_heat=7.38233, q=1000
[2018-07-02 17:40:24,908] [train] [INFO] epoch=0.00 step=2000, 9.8077 examples/sec lr=0.000100, loss=111.994, loss_ll=20.8151, loss_ll_paf=32.4209, loss_ll_heat=9.20934, q=1000
[2018-07-02 17:43:02,038] [train] [INFO] epoch=0.00 step=2100, 9.8249 examples/sec lr=0.000100, loss=117.312, loss_ll=22.4153, loss_ll_paf=34.466, loss_ll_heat=10.3646, q=1000
[2018-07-02 17:45:31,522] [train] [INFO] epoch=0.00 step=2200, 9.8617 examples/sec lr=0.000100, loss=89.81, loss_ll=17.0522, loss_ll_paf=25.4038, loss_ll_heat=8.70049, q=1000
[2018-07-02 17:47:55,642] [train] [INFO] epoch=0.00 step=2300, 9.9098 examples/sec lr=0.000100, loss=94.9343, loss_ll=17.9876, loss_ll_paf=28.3348, loss_ll_heat=7.64042, q=1000
[2018-07-02 17:50:19,773] [train] [INFO] epoch=0.00 step=2400, 9.9543 examples/sec lr=0.000100, loss=98.8925, loss_ll=18.013, loss_ll_paf=28.3084, loss_ll_heat=7.71765, q=1000
[2018-07-02 17:52:44,050] [train] [INFO] epoch=0.00 step=2500, 9.9953 examples/sec lr=0.000100, loss=76.2159, loss_ll=13.9626, loss_ll_paf=19.9504, loss_ll_heat=7.97467, q=1000
[2018-07-02 17:55:13,440] [train] [INFO] epoch=0.00 step=2600, 10.0210 examples/sec lr=0.000100, loss=104.767, loss_ll=19.726, loss_ll_paf=30.8258, loss_ll_heat=8.62616, q=1000
[2018-07-02 17:57:38,942] [train] [INFO] epoch=0.00 step=2700, 10.0540 examples/sec lr=0.000100, loss=104.688, loss_ll=19.6732, loss_ll_paf=30.7606, loss_ll_heat=8.58572, q=1000
[2018-07-02 18:00:08,138] [train] [INFO] epoch=0.00 step=2800, 10.0765 examples/sec lr=0.000100, loss=82.3049, loss_ll=15.1053, loss_ll_paf=23.0127, loss_ll_heat=7.19794, q=1000
[2018-07-02 18:02:32,020] [train] [INFO] epoch=0.00 step=2900, 10.1092 examples/sec lr=0.000100, loss=71.3775, loss_ll=13.0461, loss_ll_paf=18.8309, loss_ll_heat=7.26128, q=1000
[2018-07-02 18:04:57,389] [train] [INFO] epoch=0.00 step=3000, 10.1368 examples/sec lr=0.000100, loss=68.642, loss_ll=12.8054, loss_ll_paf=18.4928, loss_ll_heat=7.1181, q=1000
[2018-07-02 18:07:32,080] [train] [INFO] epoch=0.00 step=3100, 10.1433 examples/sec lr=0.000100, loss=71.6359, loss_ll=12.662, loss_ll_paf=18.1254, loss_ll_heat=7.19853, q=1000
[2018-07-02 18:09:58,931] [train] [INFO] epoch=0.00 step=3200, 10.1652 examples/sec lr=0.000100, loss=77.715, loss_ll=14.3653, loss_ll_paf=20.9202, loss_ll_heat=7.8104, q=1000
[2018-07-02 18:12:22,892] [train] [INFO] epoch=0.00 step=3300, 10.1916 examples/sec lr=0.000100, loss=68.5932, loss_ll=12.2509, loss_ll_paf=17.7699, loss_ll_heat=6.73182, q=1000
[2018-07-02 18:14:47,963] [train] [INFO] epoch=0.00 step=3400, 10.2144 examples/sec lr=0.000100, loss=65.8323, loss_ll=11.7271, loss_ll_paf=16.9146, loss_ll_heat=6.53953, q=1000
[2018-07-02 18:17:12,398] [train] [INFO] epoch=0.00 step=3500, 10.2372 examples/sec lr=0.000100, loss=68.9849, loss_ll=12.7125, loss_ll_paf=17.5605, loss_ll_heat=7.8644, q=1000
[2018-07-02 18:19:36,248] [train] [INFO] epoch=0.00 step=3600, 10.2599 examples/sec lr=0.000100, loss=75.7777, loss_ll=13.2613, loss_ll_paf=19.3732, loss_ll_heat=7.14948, q=1000
[2018-07-02 18:22:01,599] [train] [INFO] epoch=0.00 step=3700, 10.2788 examples/sec lr=0.000100, loss=65.7364, loss_ll=11.7445, loss_ll_paf=16.9323, loss_ll_heat=6.55667, q=1000
[2018-07-02 18:24:26,902] [train] [INFO] epoch=0.00 step=3800, 10.2968 examples/sec lr=0.000100, loss=75.4701, loss_ll=13.8949, loss_ll_paf=20.0725, loss_ll_heat=7.71727, q=1000
[2018-07-02 18:26:53,277] [train] [INFO] epoch=0.00 step=3900, 10.3121 examples/sec lr=0.000100, loss=92.6578, loss_ll=16.3066, loss_ll_paf=25.4967, loss_ll_heat=7.1166, q=1000
[2018-07-02 18:29:18,102] [train] [INFO] epoch=0.00 step=4000, 10.3293 examples/sec lr=0.000100, loss=59.7769, loss_ll=10.9818, loss_ll_paf=16.4948, loss_ll_heat=5.46881, q=1000
[2018-07-02 18:31:51,637] [train] [INFO] epoch=0.00 step=4100, 10.3316 examples/sec lr=0.000100, loss=53.9046, loss_ll=10.0952, loss_ll_paf=13.7704, loss_ll_heat=6.41997, q=1000
[2018-07-02 18:34:15,242] [train] [INFO] epoch=0.00 step=4200, 10.3495 examples/sec lr=0.000100, loss=71.0644, loss_ll=13.391, loss_ll_paf=19.8178, loss_ll_heat=6.96414, q=1000
[2018-07-02 18:36:38,566] [train] [INFO] epoch=0.00 step=4300, 10.3671 examples/sec lr=0.000100, loss=74.3622, loss_ll=14.2474, loss_ll_paf=22.0686, loss_ll_heat=6.42629, q=1000
[2018-07-02 18:39:02,192] [train] [INFO] epoch=0.00 step=4400, 10.3834 examples/sec lr=0.000100, loss=61.2633, loss_ll=10.9675, loss_ll_paf=16.3693, loss_ll_heat=5.56575, q=1000
[2018-07-02 18:41:26,992] [train] [INFO] epoch=0.00 step=4500, 10.3974 examples/sec lr=0.000100, loss=84.3593, loss_ll=15.87, loss_ll_paf=24.3193, loss_ll_heat=7.42062, q=1000
[2018-07-02 18:43:49,528] [train] [INFO] epoch=0.00 step=4600, 10.4141 examples/sec lr=0.000100, loss=66.2192, loss_ll=11.9909, loss_ll_paf=16.4086, loss_ll_heat=7.57323, q=1000
[2018-07-02 18:46:14,686] [train] [INFO] epoch=0.00 step=4700, 10.4263 examples/sec lr=0.000100, loss=68.3156, loss_ll=12.4989, loss_ll_paf=18.2426, loss_ll_heat=6.75515, q=1000
[2018-07-02 18:48:41,170] [train] [INFO] epoch=0.00 step=4800, 10.4362 examples/sec lr=0.000100, loss=57.9614, loss_ll=10.88, loss_ll_paf=15.7971, loss_ll_heat=5.96299, q=1000
[2018-07-02 18:51:07,150] [train] [INFO] epoch=0.00 step=4900, 10.4464 examples/sec lr=0.000100, loss=64.0909, loss_ll=12.2291, loss_ll_paf=18.2473, loss_ll_heat=6.21079, q=1000
[2018-07-02 18:53:32,208] [train] [INFO] epoch=0.00 step=5000, 10.4574 examples/sec lr=0.000100, loss=74.4324, loss_ll=13.7879, loss_ll_paf=21.1908, loss_ll_heat=6.38499, q=1000
[2018-07-02 18:56:09,075] [train] [INFO] epoch=0.00 step=5100, 10.4523 examples/sec lr=0.000100, loss=88.32, loss_ll=16.1744, loss_ll_paf=24.8971, loss_ll_heat=7.45176, q=1000
[2018-07-02 18:58:32,705] [train] [INFO] epoch=0.00 step=5200, 10.4647 examples/sec lr=0.000100, loss=78.5206, loss_ll=14.403, loss_ll_paf=21.3499, loss_ll_heat=7.45607, q=1000
[2018-07-02 19:00:57,149] [train] [INFO] epoch=0.00 step=5300, 10.4756 examples/sec lr=0.000100, loss=63.9868, loss_ll=11.9406, loss_ll_paf=16.8319, loss_ll_heat=7.04921, q=1000
[2018-07-02 19:03:20,800] [train] [INFO] epoch=0.00 step=5400, 10.4872 examples/sec lr=0.000100, loss=65.126, loss_ll=11.9736, loss_ll_paf=16.6474, loss_ll_heat=7.29987, q=1000
[2018-07-02 19:05:42,600] [train] [INFO] epoch=0.00 step=5500, 10.5006 examples/sec lr=0.000100, loss=60.7347, loss_ll=10.5659, loss_ll_paf=15.6871, loss_ll_heat=5.44479, q=1000
[2018-07-02 19:08:08,676] [train] [INFO] epoch=0.00 step=5600, 10.5084 examples/sec lr=0.000100, loss=61.1548, loss_ll=11.1294, loss_ll_paf=15.2093, loss_ll_heat=7.04945, q=1000
[2018-07-02 19:10:31,700] [train] [INFO] epoch=0.00 step=5700, 10.5196 examples/sec lr=0.000100, loss=68.0041, loss_ll=12.1792, loss_ll_paf=18.4279, loss_ll_heat=5.9304, q=1000
[2018-07-02 19:12:53,685] [train] [INFO] epoch=0.00 step=5800, 10.5317 examples/sec lr=0.000100, loss=84.957, loss_ll=16.4279, loss_ll_paf=23.7677, loss_ll_heat=9.08806, q=1000
[2018-07-02 19:15:19,864] [train] [INFO] epoch=0.00 step=5900, 10.5384 examples/sec lr=0.000100, loss=72.7297, loss_ll=13.1025, loss_ll_paf=19.2726, loss_ll_heat=6.93243, q=1000
[2018-07-02 19:17:45,995] [train] [INFO] epoch=0.00 step=6000, 10.5450 examples/sec lr=0.000100, loss=87.3548, loss_ll=16.3005, loss_ll_paf=24.3559, loss_ll_heat=8.24519, q=1000
[2018-07-02 19:20:24,072] [train] [INFO] epoch=0.00 step=6100, 10.5378 examples/sec lr=0.000100, loss=65.6013, loss_ll=12.1325, loss_ll_paf=17.6772, loss_ll_heat=6.58776, q=1000
[2018-07-02 19:22:49,648] [train] [INFO] epoch=0.00 step=6200, 10.5448 examples/sec lr=0.000100, loss=84.8347, loss_ll=15.8554, loss_ll_paf=23.7115, loss_ll_heat=7.9993, q=1000
[2018-07-02 19:25:14,437] [train] [INFO] epoch=0.00 step=6300, 10.5525 examples/sec lr=0.000100, loss=56.0822, loss_ll=10.3111, loss_ll_paf=14.432, loss_ll_heat=6.19019, q=1000
[2018-07-02 19:27:37,344] [train] [INFO] epoch=0.00 step=6400, 10.5619 examples/sec lr=0.000100, loss=64.2583, loss_ll=12.1246, loss_ll_paf=18.0837, loss_ll_heat=6.1655, q=1000
[2018-07-02 19:29:59,430] [train] [INFO] epoch=0.00 step=6500, 10.5720 examples/sec lr=0.000100, loss=60.7571, loss_ll=11.3645, loss_ll_paf=16.1244, loss_ll_heat=6.60471, q=1000
[2018-07-02 19:32:23,096] [train] [INFO] epoch=0.00 step=6600, 10.5802 examples/sec lr=0.000100, loss=55.8048, loss_ll=10.0731, loss_ll_paf=14.1409, loss_ll_heat=6.0053, q=1000
[2018-07-02 19:34:49,025] [train] [INFO] epoch=0.00 step=6700, 10.5857 examples/sec lr=0.000100, loss=55.8396, loss_ll=10.27, loss_ll_paf=15.4144, loss_ll_heat=5.12551, q=1000
[2018-07-02 19:37:13,820] [train] [INFO] epoch=0.00 step=6800, 10.5922 examples/sec lr=0.000100, loss=72.814, loss_ll=13.1388, loss_ll_paf=19.102, loss_ll_heat=7.17551, q=1000
[2018-07-02 19:39:38,020] [train] [INFO] epoch=0.00 step=6900, 10.5992 examples/sec lr=0.000100, loss=50.1074, loss_ll=9.4122, loss_ll_paf=12.8059, loss_ll_heat=6.01853, q=1000
[2018-07-02 19:42:00,633] [train] [INFO] epoch=0.00 step=7000, 10.6076 examples/sec lr=0.000100, loss=48.3634, loss_ll=8.77224, loss_ll_paf=11.9054, loss_ll_heat=5.63903, q=1000
[2018-07-02 19:44:38,769] [train] [INFO] epoch=0.00 step=7100, 10.6004 examples/sec lr=0.000100, loss=74.8124, loss_ll=13.9953, loss_ll_paf=21.6293, loss_ll_heat=6.36132, q=1000
[2018-07-02 19:47:00,366] [train] [INFO] epoch=0.00 step=7200, 10.6095 examples/sec lr=0.000100, loss=47.1003, loss_ll=8.44985, loss_ll_paf=11.6382, loss_ll_heat=5.26152, q=1000
[2018-07-02 19:49:23,771] [train] [INFO] epoch=0.00 step=7300, 10.6166 examples/sec lr=0.000100, loss=51.4859, loss_ll=9.68206, loss_ll_paf=14.293, loss_ll_heat=5.07112, q=1000
[2018-07-02 19:51:50,152] [train] [INFO] epoch=0.00 step=7400, 10.6207 examples/sec lr=0.000100, loss=72.4315, loss_ll=13.4012, loss_ll_paf=19.2808, loss_ll_heat=7.52154, q=1000
[2018-07-02 19:54:13,163] [train] [INFO] epoch=0.00 step=7500, 10.6279 examples/sec lr=0.000100, loss=57.3034, loss_ll=10.3071, loss_ll_paf=15.4206, loss_ll_heat=5.19366, q=1000
[2018-07-02 19:56:34,984] [train] [INFO] epoch=0.00 step=7600, 10.6360 examples/sec lr=0.000100, loss=53.8216, loss_ll=9.73048, loss_ll_paf=14.5567, loss_ll_heat=4.90426, q=1000
[2018-07-02 19:58:57,822] [train] [INFO] epoch=1.00 step=7700, 10.6430 examples/sec lr=0.000100, loss=56.1075, loss_ll=10.0538, loss_ll_paf=14.0639, loss_ll_heat=6.04362, q=1000
[2018-07-02 20:01:19,891] [train] [INFO] epoch=1.00 step=7800, 10.6505 examples/sec lr=0.000100, loss=61.5844, loss_ll=11.0858, loss_ll_paf=16.8927, loss_ll_heat=5.27893, q=1000
[2018-07-02 20:03:46,139] [train] [INFO] epoch=1.00 step=7900, 10.6541 examples/sec lr=0.000100, loss=34.438, loss_ll=6.07487, loss_ll_paf=7.62626, loss_ll_heat=4.52348, q=1000
[2018-07-02 20:06:11,896] [train] [INFO] epoch=1.00 step=8000, 10.6580 examples/sec lr=0.000100, loss=74.42, loss_ll=13.6834, loss_ll_paf=20.3148, loss_ll_heat=7.05198, q=1000
[2018-07-02 20:08:47,348] [train] [INFO] epoch=1.00 step=8100, 10.6533 examples/sec lr=0.000100, loss=62.7303, loss_ll=11.1943, loss_ll_paf=16.21, loss_ll_heat=6.17869, q=1000
[2018-07-02 20:11:12,661] [train] [INFO] epoch=1.00 step=8200, 10.6576 examples/sec lr=0.000100, loss=63.6168, loss_ll=11.3588, loss_ll_paf=16.0889, loss_ll_heat=6.62871, q=1000
[2018-07-02 20:13:36,823] [train] [INFO] epoch=1.00 step=8300, 10.6627 examples/sec lr=0.000100, loss=52.1787, loss_ll=9.59374, loss_ll_paf=13.7964, loss_ll_heat=5.39104, q=1000
[2018-07-02 20:15:59,311] [train] [INFO] epoch=1.00 step=8400, 10.6691 examples/sec lr=0.000100, loss=58.0184, loss_ll=11.0838, loss_ll_paf=16.4811, loss_ll_heat=5.68641, q=1000
[2018-07-02 20:18:21,518] [train] [INFO] epoch=1.00 step=8500, 10.6756 examples/sec lr=0.000100, loss=63.783, loss_ll=12.1782, loss_ll_paf=17.6573, loss_ll_heat=6.69898, q=1000
[2018-07-02 20:20:44,913] [train] [INFO] epoch=1.00 step=8600, 10.6809 examples/sec lr=0.000100, loss=52.0307, loss_ll=9.55886, loss_ll_paf=13.6106, loss_ll_heat=5.50715, q=1000
[2018-07-02 20:23:07,996] [train] [INFO] epoch=1.00 step=8700, 10.6865 examples/sec lr=0.000100, loss=50.82, loss_ll=9.15105, loss_ll_paf=13.4511, loss_ll_heat=4.85103, q=1000
[2018-07-02 20:25:33,744] [train] [INFO] epoch=1.00 step=8800, 10.6897 examples/sec lr=0.000100, loss=55.5929, loss_ll=10.4001, loss_ll_paf=15.0672, loss_ll_heat=5.73304, q=1000
[2018-07-02 20:27:55,915] [train] [INFO] epoch=1.00 step=8900, 10.6957 examples/sec lr=0.000100, loss=48.7663, loss_ll=9.30561, loss_ll_paf=13.1186, loss_ll_heat=5.49266, q=1000
[2018-07-02 20:30:17,241] [train] [INFO] epoch=1.00 step=9000, 10.7023 examples/sec lr=0.000100, loss=54.5865, loss_ll=9.52687, loss_ll_paf=13.3051, loss_ll_heat=5.74866, q=1000
[2018-07-02 20:32:55,697] [train] [INFO] epoch=1.00 step=9100, 10.6952 examples/sec lr=0.000100, loss=52.3781, loss_ll=9.56121, loss_ll_paf=14.2664, loss_ll_heat=4.85606, q=1000
[2018-07-02 20:35:21,261] [train] [INFO] epoch=1.00 step=9200, 10.6984 examples/sec lr=0.000100, loss=56.2819, loss_ll=10.641, loss_ll_paf=15.158, loss_ll_heat=6.12413, q=1000
[2018-07-02 20:37:43,934] [train] [INFO] epoch=1.00 step=9300, 10.7037 examples/sec lr=0.000100, loss=67.1991, loss_ll=12.3648, loss_ll_paf=18.6826, loss_ll_heat=6.04707, q=1000
[2018-07-02 20:40:09,607] [train] [INFO] epoch=1.00 step=9400, 10.7066 examples/sec lr=0.000100, loss=47.136, loss_ll=8.70535, loss_ll_paf=10.8956, loss_ll_heat=6.51509, q=1000
[2018-07-02 20:42:31,974] [train] [INFO] epoch=1.00 step=9500, 10.7119 examples/sec lr=0.000100, loss=41.7053, loss_ll=7.77652, loss_ll_paf=11.0468, loss_ll_heat=4.50628, q=1000
[2018-07-02 20:44:58,177] [train] [INFO] epoch=1.00 step=9600, 10.7143 examples/sec lr=0.000100, loss=45.4602, loss_ll=8.16589, loss_ll_paf=11.38, loss_ll_heat=4.95174, q=1000
[2018-07-02 20:47:20,685] [train] [INFO] epoch=1.00 step=9700, 10.7193 examples/sec lr=0.000100, loss=53.6582, loss_ll=9.89302, loss_ll_paf=13.7129, loss_ll_heat=6.07313, q=1000
[2018-07-02 20:49:42,860] [train] [INFO] epoch=1.00 step=9800, 10.7245 examples/sec lr=0.000100, loss=44.9659, loss_ll=8.26883, loss_ll_paf=10.534, loss_ll_heat=6.00361, q=1000
[2018-07-02 20:52:07,319] [train] [INFO] epoch=1.00 step=9900, 10.7280 examples/sec lr=0.000100, loss=36.961, loss_ll=6.37601, loss_ll_paf=9.02823, loss_ll_heat=3.7238, q=1000
[2018-07-02 20:54:33,071] [train] [INFO] epoch=1.00 step=10000, 10.7304 examples/sec lr=0.000100, loss=55.2422, loss_ll=10.5832, loss_ll_paf=15.1694, loss_ll_heat=5.99707, q=1000
[2018-07-02 20:57:07,208] [train] [INFO] epoch=1.00 step=10100, 10.7268 examples/sec lr=0.000100, loss=44.6656, loss_ll=8.15796, loss_ll_paf=11.2671, loss_ll_heat=5.04884, q=1000
[2018-07-02 20:59:28,580] [train] [INFO] epoch=1.00 step=10200, 10.7323 examples/sec lr=0.000100, loss=56.628, loss_ll=10.3736, loss_ll_paf=14.722, loss_ll_heat=6.02519, q=1000
[2018-07-02 21:01:52,434] [train] [INFO] epoch=1.00 step=10300, 10.7360 examples/sec lr=0.000100, loss=57.8054, loss_ll=10.8066, loss_ll_paf=15.2485, loss_ll_heat=6.36468, q=1000
[2018-07-02 21:04:15,121] [train] [INFO] epoch=1.00 step=10400, 10.7404 examples/sec lr=0.000100, loss=46.8512, loss_ll=8.22719, loss_ll_paf=11.4919, loss_ll_heat=4.96253, q=1000
[2018-07-02 21:06:38,454] [train] [INFO] epoch=1.00 step=10500, 10.7442 examples/sec lr=0.000100, loss=49.4263, loss_ll=9.00207, loss_ll_paf=11.6038, loss_ll_heat=6.40036, q=1000
[2018-07-02 21:08:59,617] [train] [INFO] epoch=1.00 step=10600, 10.7495 examples/sec lr=0.000100, loss=41.1325, loss_ll=7.17091, loss_ll_paf=9.4073, loss_ll_heat=4.93451, q=1000
[2018-07-02 21:11:24,437] [train] [INFO] epoch=1.00 step=10700, 10.7522 examples/sec lr=0.000100, loss=57.7835, loss_ll=10.5238, loss_ll_paf=14.8376, loss_ll_heat=6.2099, q=1000
[2018-07-02 21:13:47,672] [train] [INFO] epoch=1.00 step=10800, 10.7560 examples/sec lr=0.000100, loss=54.2135, loss_ll=10.2182, loss_ll_paf=15.0124, loss_ll_heat=5.42407, q=1000
[2018-07-02 21:16:10,232] [train] [INFO] epoch=1.00 step=10900, 10.7601 examples/sec lr=0.000100, loss=48.0036, loss_ll=8.83575, loss_ll_paf=13.343, loss_ll_heat=4.32852, q=1000
[2018-07-02 21:18:31,497] [train] [INFO] epoch=1.00 step=11000, 10.7650 examples/sec lr=0.000100, loss=61.6169, loss_ll=11.4198, loss_ll_paf=16.7766, loss_ll_heat=6.06298, q=1000
[2018-07-02 21:21:05,498] [train] [INFO] epoch=1.00 step=11100, 10.7615 examples/sec lr=0.000100, loss=48.8185, loss_ll=9.28603, loss_ll_paf=14.1699, loss_ll_heat=4.40216, q=1000
[2018-07-02 21:23:25,419] [train] [INFO] epoch=1.00 step=11200, 10.7671 examples/sec lr=0.000100, loss=72.672, loss_ll=13.5048, loss_ll_paf=19.5257, loss_ll_heat=7.48404, q=1000
[2018-07-02 21:25:45,617] [train] [INFO] epoch=1.00 step=11300, 10.7725 examples/sec lr=0.000100, loss=51.8291, loss_ll=9.41637, loss_ll_paf=12.5588, loss_ll_heat=6.27393, q=1000
[2018-07-02 21:28:09,021] [train] [INFO] epoch=1.00 step=11400, 10.7758 examples/sec lr=0.000100, loss=52.7901, loss_ll=9.8589, loss_ll_paf=14.5352, loss_ll_heat=5.18263, q=1000
[2018-07-02 21:30:30,792] [train] [INFO] epoch=1.00 step=11500, 10.7800 examples/sec lr=0.000100, loss=57.0304, loss_ll=10.4202, loss_ll_paf=15.2472, loss_ll_heat=5.59322, q=1000
[2018-07-02 21:32:54,877] [train] [INFO] epoch=1.00 step=11600, 10.7827 examples/sec lr=0.000100, loss=78.0327, loss_ll=15.0135, loss_ll_paf=23.6251, loss_ll_heat=6.40187, q=1000
[2018-07-02 21:35:17,564] [train] [INFO] epoch=1.00 step=11700, 10.7863 examples/sec lr=0.000100, loss=55.1937, loss_ll=9.95705, loss_ll_paf=13.7601, loss_ll_heat=6.15401, q=1000
[2018-07-02 21:37:41,137] [train] [INFO] epoch=1.00 step=11800, 10.7892 examples/sec lr=0.000100, loss=58.7455, loss_ll=11.0135, loss_ll_paf=16.1067, loss_ll_heat=5.92028, q=1000
[2018-07-02 21:40:04,057] [train] [INFO] epoch=1.00 step=11900, 10.7925 examples/sec lr=0.000100, loss=61.1232, loss_ll=11.2669, loss_ll_paf=16.4128, loss_ll_heat=6.12105, q=1000
[2018-07-02 21:42:25,782] [train] [INFO] epoch=1.00 step=12000, 10.7964 examples/sec lr=0.000100, loss=48.9654, loss_ll=9.38418, loss_ll_paf=14.709, loss_ll_heat=4.05932, q=1000
[2018-07-02 21:44:56,650] [train] [INFO] epoch=1.00 step=12100, 10.7948 examples/sec lr=0.000100, loss=64.9744, loss_ll=12.1181, loss_ll_paf=17.4402, loss_ll_heat=6.79605, q=1000
[2018-07-02 21:47:20,405] [train] [INFO] epoch=1.00 step=12200, 10.7975 examples/sec lr=0.000100, loss=46.6516, loss_ll=8.38853, loss_ll_paf=12.6135, loss_ll_heat=4.16357, q=1000
[2018-07-02 21:49:42,146] [train] [INFO] epoch=1.00 step=12300, 10.8013 examples/sec lr=0.000100, loss=49.7832, loss_ll=9.40796, loss_ll_paf=13.4423, loss_ll_heat=5.37358, q=1000
[2018-07-02 21:52:04,034] [train] [INFO] epoch=1.00 step=12400, 10.8050 examples/sec lr=0.000100, loss=49.6522, loss_ll=9.45583, loss_ll_paf=14.1693, loss_ll_heat=4.74234, q=1000
[2018-07-02 21:54:25,311] [train] [INFO] epoch=1.00 step=12500, 10.8090 examples/sec lr=0.000100, loss=52.8132, loss_ll=9.72486, loss_ll_paf=14.1478, loss_ll_heat=5.30194, q=1000
[2018-07-02 21:56:46,244] [train] [INFO] epoch=1.00 step=12600, 10.8131 examples/sec lr=0.000100, loss=52.1774, loss_ll=9.55864, loss_ll_paf=13.2094, loss_ll_heat=5.90784, q=1000
[2018-07-02 21:59:07,566] [train] [INFO] epoch=1.00 step=12700, 10.8169 examples/sec lr=0.000100, loss=45.5076, loss_ll=8.79784, loss_ll_paf=12.416, loss_ll_heat=5.17964, q=1000
[2018-07-02 22:01:30,411] [train] [INFO] epoch=1.00 step=12800, 10.8198 examples/sec lr=0.000100, loss=61.5717, loss_ll=11.5567, loss_ll_paf=17.2852, loss_ll_heat=5.82825, q=1000
[2018-07-02 22:03:50,831] [train] [INFO] epoch=1.00 step=12900, 10.8240 examples/sec lr=0.000100, loss=40.8653, loss_ll=6.83039, loss_ll_paf=9.64333, loss_ll_heat=4.01746, q=1000
[2018-07-02 22:06:10,412] [train] [INFO] epoch=1.00 step=13000, 10.8287 examples/sec lr=0.000100, loss=49.7842, loss_ll=9.43287, loss_ll_paf=13.6764, loss_ll_heat=5.18931, q=1000
[2018-07-02 22:08:41,848] [train] [INFO] epoch=1.00 step=13100, 10.8266 examples/sec lr=0.000100, loss=55.0164, loss_ll=10.1026, loss_ll_paf=14.2896, loss_ll_heat=5.91572, q=1000
[2018-07-02 22:11:02,850] [train] [INFO] epoch=1.00 step=13200, 10.8304 examples/sec lr=0.000100, loss=58.9529, loss_ll=10.7311, loss_ll_paf=15.321, loss_ll_heat=6.14114, q=1000
[2018-07-02 22:13:23,331] [train] [INFO] epoch=1.00 step=13300, 10.8344 examples/sec lr=0.000100, loss=62.0836, loss_ll=11.1958, loss_ll_paf=16.5658, loss_ll_heat=5.82568, q=1000
[2018-07-02 22:15:45,314] [train] [INFO] epoch=1.00 step=13400, 10.8375 examples/sec lr=0.000100, loss=42.184, loss_ll=8.1489, loss_ll_paf=10.6172, loss_ll_heat=5.68062, q=1000
[2018-07-02 22:18:07,352] [train] [INFO] epoch=1.00 step=13500, 10.8405 examples/sec lr=0.000100, loss=56.3802, loss_ll=10.8655, loss_ll_paf=16.6022, loss_ll_heat=5.12883, q=1000
[2018-07-02 22:20:28,586] [train] [INFO] epoch=1.00 step=13600, 10.8440 examples/sec lr=0.000100, loss=39.0387, loss_ll=7.1946, loss_ll_paf=8.85729, loss_ll_heat=5.53191, q=1000
[2018-07-02 22:22:50,880] [train] [INFO] epoch=1.00 step=13700, 10.8468 examples/sec lr=0.000100, loss=30.173, loss_ll=5.36708, loss_ll_paf=7.06906, loss_ll_heat=3.6651, q=1000
[2018-07-02 22:25:13,477] [train] [INFO] epoch=1.00 step=13800, 10.8494 examples/sec lr=0.000100, loss=60.3491, loss_ll=11.2205, loss_ll_paf=15.8982, loss_ll_heat=6.54273, q=1000
[2018-07-02 22:27:33,351] [train] [INFO] epoch=1.00 step=13900, 10.8534 examples/sec lr=0.000100, loss=40.8691, loss_ll=7.96564, loss_ll_paf=12.0741, loss_ll_heat=3.85721, q=1000
[2018-07-02 22:29:54,018] [train] [INFO] epoch=1.00 step=14000, 10.8570 examples/sec lr=0.000100, loss=48.0475, loss_ll=8.98167, loss_ll_paf=12.654, loss_ll_heat=5.30937, q=1000
[2018-07-02 22:32:27,257] [train] [INFO] epoch=1.00 step=14100, 10.8539 examples/sec lr=0.000100, loss=50.7657, loss_ll=9.54644, loss_ll_paf=14.3034, loss_ll_heat=4.78946, q=1000
[2018-07-02 22:34:49,498] [train] [INFO] epoch=1.00 step=14200, 10.8566 examples/sec lr=0.000100, loss=61.8975, loss_ll=11.3853, loss_ll_paf=17.0386, loss_ll_heat=5.73199, q=1000
[2018-07-02 22:37:13,088] [train] [INFO] epoch=1.00 step=14300, 10.8586 examples/sec lr=0.000100, loss=67.9505, loss_ll=12.2887, loss_ll_paf=19.1097, loss_ll_heat=5.46774, q=1000
[2018-07-02 22:39:36,659] [train] [INFO] epoch=1.00 step=14400, 10.8605 examples/sec lr=0.000100, loss=67.7077, loss_ll=12.476, loss_ll_paf=19.5925, loss_ll_heat=5.3596, q=1000
[2018-07-02 22:41:57,191] [train] [INFO] epoch=1.00 step=14500, 10.8639 examples/sec lr=0.000100, loss=38.0999, loss_ll=7.14567, loss_ll_paf=10.1001, loss_ll_heat=4.19121, q=1000
[2018-07-02 22:44:18,913] [train] [INFO] epoch=1.00 step=14600, 10.8668 examples/sec lr=0.000100, loss=51.2764, loss_ll=9.42407, loss_ll_paf=13.1497, loss_ll_heat=5.6984, q=1000
[2018-07-02 22:46:41,765] [train] [INFO] epoch=1.00 step=14700, 10.8690 examples/sec lr=0.000100, loss=42.4441, loss_ll=7.45157, loss_ll_paf=10.7747, loss_ll_heat=4.12842, q=1000
[2018-07-02 22:49:02,567] [train] [INFO] epoch=1.00 step=14800, 10.8722 examples/sec lr=0.000100, loss=53.7519, loss_ll=10.2155, loss_ll_paf=14.6911, loss_ll_heat=5.73995, q=1000
[2018-07-02 22:51:23,784] [train] [INFO] epoch=1.00 step=14900, 10.8751 examples/sec lr=0.000100, loss=43.0772, loss_ll=7.66819, loss_ll_paf=11.1717, loss_ll_heat=4.16468, q=1000
[2018-07-02 22:53:49,099] [train] [INFO] epoch=1.00 step=15000, 10.8760 examples/sec lr=0.000100, loss=47.2381, loss_ll=8.90763, loss_ll_paf=12.3019, loss_ll_heat=5.51338, q=1000
[2018-07-02 22:56:21,278] [train] [INFO] epoch=1.00 step=15100, 10.8735 examples/sec lr=0.000100, loss=45.5054, loss_ll=8.13076, loss_ll_paf=11.0508, loss_ll_heat=5.21073, q=1000
[2018-07-02 22:58:42,602] [train] [INFO] epoch=1.00 step=15200, 10.8763 examples/sec lr=0.000100, loss=44.7868, loss_ll=7.90562, loss_ll_paf=11.4139, loss_ll_heat=4.39731, q=1000
[2018-07-02 23:01:02,772] [train] [INFO] epoch=2.00 step=15300, 10.8797 examples/sec lr=0.000100, loss=48.057, loss_ll=9.05886, loss_ll_paf=12.2255, loss_ll_heat=5.89224, q=1000
[2018-07-02 23:03:28,143] [train] [INFO] epoch=2.00 step=15400, 10.8805 examples/sec lr=0.000100, loss=39.6463, loss_ll=6.80081, loss_ll_paf=9.38795, loss_ll_heat=4.21367, q=1000
[2018-07-02 23:05:50,019] [train] [INFO] epoch=2.00 step=15500, 10.8830 examples/sec lr=0.000100, loss=34.6665, loss_ll=6.07992, loss_ll_paf=7.64488, loss_ll_heat=4.51497, q=1000
[2018-07-02 23:08:10,231] [train] [INFO] epoch=2.00 step=15600, 10.8862 examples/sec lr=0.000100, loss=51.1615, loss_ll=9.71708, loss_ll_paf=14.9691, loss_ll_heat=4.46502, q=1000
[2018-07-02 23:10:31,626] [train] [INFO] epoch=2.00 step=15700, 10.8889 examples/sec lr=0.000100, loss=59.5072, loss_ll=10.9735, loss_ll_paf=16.3234, loss_ll_heat=5.62357, q=1000
[2018-07-02 23:12:52,767] [train] [INFO] epoch=2.00 step=15800, 10.8916 examples/sec lr=0.000100, loss=37.1868, loss_ll=6.7455, loss_ll_paf=9.31786, loss_ll_heat=4.17314, q=1000
[2018-07-02 23:15:12,123] [train] [INFO] epoch=2.00 step=15900, 10.8951 examples/sec lr=0.000100, loss=49.4024, loss_ll=9.11071, loss_ll_paf=13.7313, loss_ll_heat=4.49015, q=1000
[2018-07-02 23:17:32,975] [train] [INFO] epoch=2.00 step=16000, 10.8979 examples/sec lr=0.000100, loss=47.3776, loss_ll=9.02016, loss_ll_paf=13.2662, loss_ll_heat=4.77408, q=1000
[2018-07-02 23:20:03,543] [train] [INFO] epoch=2.00 step=16100, 10.8961 examples/sec lr=0.000100, loss=47.1293, loss_ll=8.43337, loss_ll_paf=12.7858, loss_ll_heat=4.0809, q=1000
[2018-07-02 23:22:25,230] [train] [INFO] epoch=2.00 step=16200, 10.8985 examples/sec lr=0.000100, loss=65.8606, loss_ll=11.9191, loss_ll_paf=18.0673, loss_ll_heat=5.77096, q=1000
[2018-07-02 23:24:46,893] [train] [INFO] epoch=2.00 step=16300, 10.9009 examples/sec lr=0.000100, loss=53.9873, loss_ll=10.1178, loss_ll_paf=15.0865, loss_ll_heat=5.14917, q=1000
[2018-07-02 23:27:07,439] [train] [INFO] epoch=2.00 step=16400, 10.9037 examples/sec lr=0.000100, loss=58.307, loss_ll=10.6532, loss_ll_paf=14.6721, loss_ll_heat=6.63417, q=1000
[2018-07-02 23:29:30,107] [train] [INFO] epoch=2.00 step=16500, 10.9055 examples/sec lr=0.000100, loss=44.6234, loss_ll=8.26028, loss_ll_paf=11.9557, loss_ll_heat=4.56489, q=1000
[2018-07-02 23:31:50,446] [train] [INFO] epoch=2.00 step=16600, 10.9084 examples/sec lr=0.000100, loss=58.758, loss_ll=10.6718, loss_ll_paf=15.8702, loss_ll_heat=5.47332, q=1000
[2018-07-02 23:34:11,881] [train] [INFO] epoch=2.00 step=16700, 10.9107 examples/sec lr=0.000100, loss=49.9936, loss_ll=9.53668, loss_ll_paf=14.9413, loss_ll_heat=4.13206, q=1000
[2018-07-02 23:36:33,559] [train] [INFO] epoch=2.00 step=16800, 10.9129 examples/sec lr=0.000100, loss=45.6149, loss_ll=8.28522, loss_ll_paf=10.9588, loss_ll_heat=5.61162, q=1000
[2018-07-02 23:38:55,209] [train] [INFO] epoch=2.00 step=16900, 10.9151 examples/sec lr=0.000100, loss=51.9834, loss_ll=10.1101, loss_ll_paf=15.0187, loss_ll_heat=5.20151, q=1000
[2018-07-02 23:41:15,571] [train] [INFO] epoch=2.00 step=17000, 10.9178 examples/sec lr=0.000100, loss=57.711, loss_ll=10.9137, loss_ll_paf=15.5402, loss_ll_heat=6.28723, q=1000
[2018-07-02 23:43:47,477] [train] [INFO] epoch=2.00 step=17100, 10.9155 examples/sec lr=0.000100, loss=39.8014, loss_ll=7.24462, loss_ll_paf=10.4082, loss_ll_heat=4.08102, q=1000
[2018-07-02 23:46:07,726] [train] [INFO] epoch=2.00 step=17200, 10.9182 examples/sec lr=0.000100, loss=39.5246, loss_ll=7.14508, loss_ll_paf=10.0516, loss_ll_heat=4.23853, q=1000
[2018-07-02 23:48:27,361] [train] [INFO] epoch=2.00 step=17300, 10.9212 examples/sec lr=0.000100, loss=35.03, loss_ll=6.3079, loss_ll_paf=8.65776, loss_ll_heat=3.95804, q=1000
[2018-07-02 23:50:48,651] [train] [INFO] epoch=2.00 step=17400, 10.9234 examples/sec lr=0.000100, loss=48.4324, loss_ll=9.18556, loss_ll_paf=14.2893, loss_ll_heat=4.08184, q=1000
[2018-07-02 23:53:10,436] [train] [INFO] epoch=2.00 step=17500, 10.9254 examples/sec lr=0.000100, loss=61.9203, loss_ll=11.8536, loss_ll_paf=17.3734, loss_ll_heat=6.33373, q=1000
[2018-07-02 23:55:33,192] [train] [INFO] epoch=2.00 step=17600, 10.9270 examples/sec lr=0.000100, loss=48.8568, loss_ll=8.93552, loss_ll_paf=11.7601, loss_ll_heat=6.11096, q=1000
[2018-07-02 23:57:54,661] [train] [INFO] epoch=2.00 step=17700, 10.9291 examples/sec lr=0.000100, loss=60.3065, loss_ll=11.8618, loss_ll_paf=18.6142, loss_ll_heat=5.10939, q=1000
[2018-07-03 00:00:14,960] [train] [INFO] epoch=2.00 step=17800, 10.9316 examples/sec lr=0.000100, loss=43.5219, loss_ll=8.2951, loss_ll_paf=12.0739, loss_ll_heat=4.51628, q=1000
[2018-07-03 00:02:37,367] [train] [INFO] epoch=2.00 step=17900, 10.9333 examples/sec lr=0.000100, loss=45.3669, loss_ll=8.14885, loss_ll_paf=11.3644, loss_ll_heat=4.93327, q=1000
[2018-07-03 00:04:59,158] [train] [INFO] epoch=2.00 step=18000, 10.9352 examples/sec lr=0.000100, loss=54.7656, loss_ll=10.0206, loss_ll_paf=15.125, loss_ll_heat=4.9162, q=1000
[2018-07-03 00:07:31,015] [train] [INFO] epoch=2.00 step=18100, 10.9329 examples/sec lr=0.000100, loss=46.0762, loss_ll=8.71551, loss_ll_paf=13.0196, loss_ll_heat=4.41139, q=1000
[2018-07-03 00:09:52,188] [train] [INFO] epoch=2.00 step=18200, 10.9350 examples/sec lr=0.000100, loss=47.0578, loss_ll=8.73134, loss_ll_paf=12.8137, loss_ll_heat=4.64904, q=1000
[2018-07-03 00:12:14,036] [train] [INFO] epoch=2.00 step=18300, 10.9368 examples/sec lr=0.000100, loss=58.3746, loss_ll=10.4796, loss_ll_paf=15.4694, loss_ll_heat=5.48982, q=1000
[2018-07-03 00:14:34,026] [train] [INFO] epoch=2.00 step=18400, 10.9394 examples/sec lr=0.000100, loss=55.2206, loss_ll=10.0025, loss_ll_paf=13.7975, loss_ll_heat=6.20754, q=1000
[2018-07-03 00:16:54,018] [train] [INFO] epoch=2.00 step=18500, 10.9419 examples/sec lr=0.000100, loss=52.896, loss_ll=9.6675, loss_ll_paf=13.3007, loss_ll_heat=6.03432, q=1000
[2018-07-03 00:19:16,928] [train] [INFO] epoch=2.00 step=18600, 10.9433 examples/sec lr=0.000100, loss=55.0651, loss_ll=10.2376, loss_ll_paf=13.861, loss_ll_heat=6.61413, q=1000
[2018-07-03 00:21:39,541] [train] [INFO] epoch=2.00 step=18700, 10.9447 examples/sec lr=0.000100, loss=63.2952, loss_ll=11.6298, loss_ll_paf=17.1183, loss_ll_heat=6.1412, q=1000
[2018-07-03 00:23:59,584] [train] [INFO] epoch=2.00 step=18800, 10.9472 examples/sec lr=0.000100, loss=47.3354, loss_ll=8.79677, loss_ll_paf=13.1504, loss_ll_heat=4.44314, q=1000
[2018-07-03 00:26:22,689] [train] [INFO] epoch=2.00 step=18900, 10.9484 examples/sec lr=0.000100, loss=66.98, loss_ll=12.5902, loss_ll_paf=18.8149, loss_ll_heat=6.36552, q=1000
[2018-07-03 00:28:42,568] [train] [INFO] epoch=2.00 step=19000, 10.9508 examples/sec lr=0.000100, loss=52.8143, loss_ll=9.62937, loss_ll_paf=15.0635, loss_ll_heat=4.19527, q=1000
[2018-07-03 00:31:14,634] [train] [INFO] epoch=2.00 step=19100, 10.9485 examples/sec lr=0.000100, loss=41.8939, loss_ll=7.7443, loss_ll_paf=10.9003, loss_ll_heat=4.58833, q=1000
[2018-07-03 00:33:35,864] [train] [INFO] epoch=2.00 step=19200, 10.9504 examples/sec lr=0.000100, loss=60.6625, loss_ll=11.1739, loss_ll_paf=16.1117, loss_ll_heat=6.23608, q=1000
[2018-07-03 00:35:57,414] [train] [INFO] epoch=2.00 step=19300, 10.9522 examples/sec lr=0.000100, loss=51.9884, loss_ll=9.9746, loss_ll_paf=14.9058, loss_ll_heat=5.04336, q=1000
[2018-07-03 00:38:19,253] [train] [INFO] epoch=2.00 step=19400, 10.9538 examples/sec lr=0.000100, loss=51.0816, loss_ll=9.39657, loss_ll_paf=13.5393, loss_ll_heat=5.25386, q=1000
[2018-07-03 00:40:41,489] [train] [INFO] epoch=2.00 step=19500, 10.9553 examples/sec lr=0.000100, loss=44.85, loss_ll=8.69625, loss_ll_paf=12.336, loss_ll_heat=5.05649, q=1000
[2018-07-03 00:43:02,066] [train] [INFO] epoch=2.00 step=19600, 10.9574 examples/sec lr=0.000100, loss=55.786, loss_ll=11.0079, loss_ll_paf=16.8345, loss_ll_heat=5.18137, q=1000
[2018-07-03 00:45:24,229] [train] [INFO] epoch=2.00 step=19700, 10.9589 examples/sec lr=0.000100, loss=56.7041, loss_ll=10.8672, loss_ll_paf=16.55, loss_ll_heat=5.18428, q=1000
[2018-07-03 00:47:46,718] [train] [INFO] epoch=2.00 step=19800, 10.9602 examples/sec lr=0.000100, loss=51.2402, loss_ll=9.41074, loss_ll_paf=14.1031, loss_ll_heat=4.71841, q=1000
[2018-07-03 00:50:10,385] [train] [INFO] epoch=2.00 step=19900, 10.9611 examples/sec lr=0.000100, loss=51.6683, loss_ll=9.80735, loss_ll_paf=14.273, loss_ll_heat=5.34174, q=1000
[2018-07-03 00:52:32,725] [train] [INFO] epoch=2.00 step=20000, 10.9624 examples/sec lr=0.000100, loss=53.4997, loss_ll=9.92843, loss_ll_paf=14.9099, loss_ll_heat=4.94691, q=1000
[2018-07-03 00:55:05,785] [train] [INFO] epoch=2.00 step=20100, 10.9598 examples/sec lr=0.000100, loss=42.398, loss_ll=7.56287, loss_ll_paf=10.3004, loss_ll_heat=4.82534, q=1000
[2018-07-03 00:57:27,345] [train] [INFO] epoch=2.00 step=20200, 10.9614 examples/sec lr=0.000100, loss=64.827, loss_ll=12.0717, loss_ll_paf=18.9093, loss_ll_heat=5.23417, q=1000
[2018-07-03 00:59:49,599] [train] [INFO] epoch=2.00 step=20300, 10.9628 examples/sec lr=0.000100, loss=53.8374, loss_ll=9.95076, loss_ll_paf=14.3244, loss_ll_heat=5.57711, q=1000
[2018-07-03 01:02:13,309] [train] [INFO] epoch=2.00 step=20400, 10.9636 examples/sec lr=0.000100, loss=50.6869, loss_ll=8.93644, loss_ll_paf=13.5441, loss_ll_heat=4.3288, q=1000
[2018-07-03 01:04:35,793] [train] [INFO] epoch=2.00 step=20500, 10.9649 examples/sec lr=0.000100, loss=40.0499, loss_ll=7.18264, loss_ll_paf=10.1006, loss_ll_heat=4.2647, q=1000
[2018-07-03 01:06:58,112] [train] [INFO] epoch=2.00 step=20600, 10.9662 examples/sec lr=0.000100, loss=58.3398, loss_ll=10.7226, loss_ll_paf=15.0553, loss_ll_heat=6.38981, q=1000
[2018-07-03 01:09:22,467] [train] [INFO] epoch=2.00 step=20700, 10.9668 examples/sec lr=0.000100, loss=46.0984, loss_ll=8.29491, loss_ll_paf=11.9441, loss_ll_heat=4.64566, q=1000
[2018-07-03 01:11:44,698] [train] [INFO] epoch=2.00 step=20800, 10.9681 examples/sec lr=0.000100, loss=42.8325, loss_ll=7.52409, loss_ll_paf=10.9122, loss_ll_heat=4.13596, q=1000
[2018-07-03 01:14:05,758] [train] [INFO] epoch=2.00 step=20900, 10.9698 examples/sec lr=0.000100, loss=43.6959, loss_ll=7.95394, loss_ll_paf=11.4139, loss_ll_heat=4.49399, q=1000
[2018-07-03 01:16:28,701] [train] [INFO] epoch=2.00 step=21000, 10.9709 examples/sec lr=0.000100, loss=52.6472, loss_ll=9.7348, loss_ll_paf=13.7877, loss_ll_heat=5.68186, q=1000
[2018-07-03 01:19:02,646] [train] [INFO] epoch=2.00 step=21100, 10.9680 examples/sec lr=0.000100, loss=39.2204, loss_ll=7.09431, loss_ll_paf=9.91581, loss_ll_heat=4.27282, q=1000
[2018-07-03 01:21:25,301] [train] [INFO] epoch=2.00 step=21200, 10.9691 examples/sec lr=0.000100, loss=50.8338, loss_ll=9.64014, loss_ll_paf=14.827, loss_ll_heat=4.45328, q=1000
[2018-07-03 01:23:47,426] [train] [INFO] epoch=2.00 step=21300, 10.9705 examples/sec lr=0.000100, loss=36.5154, loss_ll=6.51256, loss_ll_paf=8.61904, loss_ll_heat=4.40607, q=1000
[2018-07-03 01:26:10,832] [train] [INFO] epoch=2.00 step=21400, 10.9713 examples/sec lr=0.000100, loss=41.9828, loss_ll=7.76955, loss_ll_paf=10.9286, loss_ll_heat=4.61049, q=1000
[2018-07-03 01:28:31,215] [train] [INFO] epoch=2.00 step=21500, 10.9732 examples/sec lr=0.000100, loss=45.7505, loss_ll=8.47515, loss_ll_paf=12.5168, loss_ll_heat=4.43351, q=1000
[2018-07-03 01:30:53,293] [train] [INFO] epoch=2.00 step=21600, 10.9745 examples/sec lr=0.000100, loss=48.9521, loss_ll=9.41246, loss_ll_paf=14.3878, loss_ll_heat=4.43707, q=1000
[2018-07-03 01:33:16,848] [train] [INFO] epoch=2.00 step=21700, 10.9753 examples/sec lr=0.000100, loss=32.1925, loss_ll=5.63762, loss_ll_paf=6.94103, loss_ll_heat=4.33421, q=1000
[2018-07-03 01:35:38,179] [train] [INFO] epoch=2.00 step=21800, 10.9768 examples/sec lr=0.000100, loss=47.8929, loss_ll=8.6444, loss_ll_paf=12.1364, loss_ll_heat=5.15242, q=1000
[2018-07-03 01:38:00,402] [train] [INFO] epoch=2.00 step=21900, 10.9780 examples/sec lr=0.000100, loss=58.3198, loss_ll=10.5883, loss_ll_paf=15.7782, loss_ll_heat=5.39842, q=1000
[2018-07-03 01:40:23,541] [train] [INFO] epoch=2.00 step=22000, 10.9789 examples/sec lr=0.000100, loss=54.8911, loss_ll=10.0125, loss_ll_paf=14.4292, loss_ll_heat=5.59574, q=1000
[2018-07-03 01:42:56,505] [train] [INFO] epoch=2.00 step=22100, 10.9765 examples/sec lr=0.000100, loss=53.4178, loss_ll=9.87825, loss_ll_paf=13.9806, loss_ll_heat=5.77586, q=1000
[2018-07-03 01:45:19,656] [train] [INFO] epoch=2.00 step=22200, 10.9774 examples/sec lr=0.000100, loss=51.6678, loss_ll=9.96433, loss_ll_paf=13.8656, loss_ll_heat=6.06302, q=1000
[2018-07-03 01:47:42,091] [train] [INFO] epoch=2.00 step=22300, 10.9785 examples/sec lr=0.000100, loss=43.2399, loss_ll=7.85574, loss_ll_paf=11.8442, loss_ll_heat=3.8673, q=1000
[2018-07-03 01:50:01,844] [train] [INFO] epoch=2.00 step=22400, 10.9805 examples/sec lr=0.000100, loss=48.053, loss_ll=8.81776, loss_ll_paf=13.5285, loss_ll_heat=4.107, q=1000
[2018-07-03 01:52:23,714] [train] [INFO] epoch=2.00 step=22500, 10.9818 examples/sec lr=0.000100, loss=54.9872, loss_ll=10.377, loss_ll_paf=15.6273, loss_ll_heat=5.12679, q=1000
[2018-07-03 01:54:44,092] [train] [INFO] epoch=2.00 step=22600, 10.9836 examples/sec lr=0.000100, loss=46.1881, loss_ll=8.67057, loss_ll_paf=12.6099, loss_ll_heat=4.73128, q=1000
[2018-07-03 01:57:07,215] [train] [INFO] epoch=2.00 step=22700, 10.9844 examples/sec lr=0.000100, loss=41.0589, loss_ll=7.11905, loss_ll_paf=10.2286, loss_ll_heat=4.00954, q=1000
[2018-07-03 01:59:26,924] [train] [INFO] epoch=2.00 step=22800, 10.9864 examples/sec lr=0.000100, loss=44.3208, loss_ll=7.61635, loss_ll_paf=10.6546, loss_ll_heat=4.5781, q=1000
[2018-07-03 02:01:51,712] [train] [INFO] epoch=3.00 step=22900, 10.9867 examples/sec lr=0.000100, loss=34.6728, loss_ll=6.48583, loss_ll_paf=8.89581, loss_ll_heat=4.07586, q=1000
[2018-07-03 02:04:14,325] [train] [INFO] epoch=3.00 step=23000, 10.9876 examples/sec lr=0.000100, loss=53.8041, loss_ll=10.3066, loss_ll_paf=15.6744, loss_ll_heat=4.93891, q=1000
[2018-07-03 02:06:50,271] [train] [INFO] epoch=3.00 step=23100, 10.9843 examples/sec lr=0.000100, loss=26.1919, loss_ll=5.06352, loss_ll_paf=6.72594, loss_ll_heat=3.4011, q=1000
[2018-07-03 02:09:13,629] [train] [INFO] epoch=3.00 step=23200, 10.9850 examples/sec lr=0.000100, loss=44.2872, loss_ll=7.84008, loss_ll_paf=10.614, loss_ll_heat=5.06615, q=1000
[2018-07-03 02:11:36,112] [train] [INFO] epoch=3.00 step=23300, 10.9860 examples/sec lr=0.000100, loss=65.1439, loss_ll=11.8017, loss_ll_paf=18.8941, loss_ll_heat=4.70937, q=1000
[2018-07-03 02:13:59,255] [train] [INFO] epoch=3.00 step=23400, 10.9869 examples/sec lr=0.000100, loss=58.6838, loss_ll=11.1605, loss_ll_paf=16.9136, loss_ll_heat=5.40745, q=1000
[2018-07-03 02:16:21,320] [train] [INFO] epoch=3.00 step=23500, 10.9880 examples/sec lr=0.000100, loss=60.0163, loss_ll=11.2105, loss_ll_paf=16.9312, loss_ll_heat=5.48984, q=1000
[2018-07-03 02:18:45,694] [train] [INFO] epoch=3.00 step=23600, 10.9884 examples/sec lr=0.000100, loss=60.7754, loss_ll=11.7114, loss_ll_paf=17.8312, loss_ll_heat=5.59162, q=1000
[2018-07-03 02:21:09,511] [train] [INFO] epoch=3.00 step=23700, 10.9890 examples/sec lr=0.000100, loss=43.678, loss_ll=7.76616, loss_ll_paf=10.5441, loss_ll_heat=4.9882, q=1000
[2018-07-03 02:23:33,136] [train] [INFO] epoch=3.00 step=23800, 10.9896 examples/sec lr=0.000100, loss=107.456, loss_ll=20.9615, loss_ll_paf=33.3153, loss_ll_heat=8.60775, q=1000
[2018-07-03 02:25:57,826] [train] [INFO] epoch=3.00 step=23900, 10.9899 examples/sec lr=0.000100, loss=40.2362, loss_ll=7.12309, loss_ll_paf=9.74324, loss_ll_heat=4.50294, q=1000
[2018-07-03 02:28:22,900] [train] [INFO] epoch=3.00 step=24000, 10.9900 examples/sec lr=0.000100, loss=66.5056, loss_ll=12.7, loss_ll_paf=18.9559, loss_ll_heat=6.44419, q=1000
[2018-07-03 02:30:56,541] [train] [INFO] epoch=3.00 step=24100, 10.9875 examples/sec lr=0.000100, loss=47.5923, loss_ll=8.69891, loss_ll_paf=12.5512, loss_ll_heat=4.8466, q=1000
[2018-07-03 02:33:19,782] [train] [INFO] epoch=3.00 step=24200, 10.9883 examples/sec lr=0.000100, loss=51.0789, loss_ll=9.1761, loss_ll_paf=12.7414, loss_ll_heat=5.6108, q=1000
[2018-07-03 02:35:43,386] [train] [INFO] epoch=3.00 step=24300, 10.9889 examples/sec lr=0.000100, loss=51.1946, loss_ll=9.49836, loss_ll_paf=14.0394, loss_ll_heat=4.95728, q=1000
[2018-07-03 02:38:06,396] [train] [INFO] epoch=3.00 step=24400, 10.9897 examples/sec lr=0.000100, loss=43.2613, loss_ll=7.68258, loss_ll_paf=10.4208, loss_ll_heat=4.94435, q=1000
[2018-07-03 02:40:29,733] [train] [INFO] epoch=3.00 step=24500, 10.9904 examples/sec lr=0.000100, loss=44.3206, loss_ll=7.71065, loss_ll_paf=11.0973, loss_ll_heat=4.32404, q=1000
[2018-07-03 02:42:57,181] [train] [INFO] epoch=3.00 step=24600, 10.9898 examples/sec lr=0.000100, loss=40.2669, loss_ll=7.37692, loss_ll_paf=9.62383, loss_ll_heat=5.13002, q=1000
[2018-07-03 02:45:23,734] [train] [INFO] epoch=3.00 step=24700, 10.9895 examples/sec lr=0.000100, loss=88.5482, loss_ll=16.6489, loss_ll_paf=26.7222, loss_ll_heat=6.57548, q=1000
[2018-07-03 02:47:50,832] [train] [INFO] epoch=3.00 step=24800, 10.9890 examples/sec lr=0.000100, loss=35.1834, loss_ll=6.62239, loss_ll_paf=10.0978, loss_ll_heat=3.14699, q=1000
[2018-07-03 02:50:15,646] [train] [INFO] epoch=3.00 step=24900, 10.9893 examples/sec lr=0.000100, loss=38.4331, loss_ll=6.63352, loss_ll_paf=9.22715, loss_ll_heat=4.0399, q=1000
[2018-07-03 02:52:41,361] [train] [INFO] epoch=3.00 step=25000, 10.9893 examples/sec lr=0.000100, loss=49.3258, loss_ll=8.74433, loss_ll_paf=13.1723, loss_ll_heat=4.31634, q=1000
[2018-07-03 02:55:17,861] [train] [INFO] epoch=3.00 step=25100, 10.9860 examples/sec lr=0.000100, loss=55.2835, loss_ll=9.56587, loss_ll_paf=13.5842, loss_ll_heat=5.5475, q=1000
[2018-07-03 02:57:42,996] [train] [INFO] epoch=3.00 step=25200, 10.9861 examples/sec lr=0.000100, loss=44.4584, loss_ll=8.23383, loss_ll_paf=12.0941, loss_ll_heat=4.37352, q=1000
[2018-07-03 03:00:09,370] [train] [INFO] epoch=3.00 step=25300, 10.9859 examples/sec lr=0.000100, loss=52.7799, loss_ll=9.50006, loss_ll_paf=13.4588, loss_ll_heat=5.54136, q=1000
[2018-07-03 03:02:35,380] [train] [INFO] epoch=3.00 step=25400, 10.9858 examples/sec lr=0.000100, loss=43.7795, loss_ll=8.1121, loss_ll_paf=11.6061, loss_ll_heat=4.61811, q=1000
[2018-07-03 03:04:59,309] [train] [INFO] epoch=3.00 step=25500, 10.9863 examples/sec lr=0.000100, loss=40.2199, loss_ll=7.26515, loss_ll_paf=10.2622, loss_ll_heat=4.26805, q=1000
[2018-07-03 03:07:26,626] [train] [INFO] epoch=3.00 step=25600, 10.9858 examples/sec lr=0.000100, loss=37.3002, loss_ll=6.78408, loss_ll_paf=9.37216, loss_ll_heat=4.19601, q=1000
[2018-07-03 03:09:53,258] [train] [INFO] epoch=3.00 step=25700, 10.9855 examples/sec lr=0.000100, loss=41.2703, loss_ll=7.61519, loss_ll_paf=10.4591, loss_ll_heat=4.77125, q=1000
[2018-07-03 03:12:19,320] [train] [INFO] epoch=3.00 step=25800, 10.9854 examples/sec lr=0.000100, loss=39.6109, loss_ll=7.49595, loss_ll_paf=10.4432, loss_ll_heat=4.5487, q=1000
[2018-07-03 03:14:44,438] [train] [INFO] epoch=3.00 step=25900, 10.9855 examples/sec lr=0.000100, loss=38.59, loss_ll=6.99831, loss_ll_paf=9.24394, loss_ll_heat=4.75268, q=1000
[2018-07-03 03:17:09,336] [train] [INFO] epoch=3.00 step=26000, 10.9858 examples/sec lr=0.000100, loss=82.5503, loss_ll=15.2899, loss_ll_paf=23.8138, loss_ll_heat=6.76607, q=1000
[2018-07-03 03:19:44,950] [train] [INFO] epoch=3.00 step=26100, 10.9829 examples/sec lr=0.000100, loss=39.8495, loss_ll=7.35351, loss_ll_paf=10.5386, loss_ll_heat=4.16844, q=1000
[2018-07-03 03:22:13,385] [train] [INFO] epoch=3.00 step=26200, 10.9821 examples/sec lr=0.000100, loss=37.9538, loss_ll=7.08799, loss_ll_paf=10.0672, loss_ll_heat=4.10879, q=1000
[2018-07-03 03:24:38,767] [train] [INFO] epoch=3.00 step=26300, 10.9822 examples/sec lr=0.000100, loss=29.5843, loss_ll=5.37089, loss_ll_paf=7.01362, loss_ll_heat=3.72816, q=1000
[2018-07-03 03:27:06,086] [train] [INFO] epoch=3.00 step=26400, 10.9817 examples/sec lr=0.000100, loss=56.5071, loss_ll=10.5044, loss_ll_paf=17.0122, loss_ll_heat=3.9966, q=1000
[2018-07-03 03:29:30,188] [train] [INFO] epoch=3.00 step=26500, 10.9822 examples/sec lr=0.000100, loss=30.1114, loss_ll=5.47206, loss_ll_paf=6.62317, loss_ll_heat=4.32094, q=1000
[2018-07-03 03:31:56,694] [train] [INFO] epoch=3.00 step=26600, 10.9819 examples/sec lr=0.000100, loss=52.7867, loss_ll=9.45152, loss_ll_paf=13.4036, loss_ll_heat=5.49942, q=1000
[2018-07-03 03:34:24,996] [train] [INFO] epoch=3.00 step=26700, 10.9812 examples/sec lr=0.000100, loss=42.0797, loss_ll=7.56133, loss_ll_paf=10.7871, loss_ll_heat=4.3356, q=1000
[2018-07-03 03:36:53,164] [train] [INFO] epoch=3.00 step=26800, 10.9805 examples/sec lr=0.000100, loss=36.1909, loss_ll=6.40059, loss_ll_paf=9.28934, loss_ll_heat=3.51183, q=1000
[2018-07-03 03:39:17,924] [train] [INFO] epoch=3.00 step=26900, 10.9808 examples/sec lr=0.000100, loss=34.7167, loss_ll=6.17906, loss_ll_paf=8.7864, loss_ll_heat=3.57172, q=1000
[2018-07-03 03:41:44,585] [train] [INFO] epoch=3.00 step=27000, 10.9805 examples/sec lr=0.000100, loss=35.3016, loss_ll=6.00176, loss_ll_paf=8.63472, loss_ll_heat=3.3688, q=1000
[2018-07-03 03:44:22,568] [train] [INFO] epoch=3.00 step=27100, 10.9771 examples/sec lr=0.000100, loss=36.957, loss_ll=6.74034, loss_ll_paf=9.37134, loss_ll_heat=4.10935, q=1000
[2018-07-03 03:46:50,861] [train] [INFO] epoch=3.00 step=27200, 10.9764 examples/sec lr=0.000100, loss=36.9314, loss_ll=7.15916, loss_ll_paf=9.92734, loss_ll_heat=4.39097, q=1000
[2018-07-03 03:49:17,494] [train] [INFO] epoch=3.00 step=27300, 10.9762 examples/sec lr=0.000100, loss=44.2677, loss_ll=8.33136, loss_ll_paf=11.736, loss_ll_heat=4.92676, q=1000
[2018-07-03 03:51:45,563] [train] [INFO] epoch=3.00 step=27400, 10.9755 examples/sec lr=0.000100, loss=58.453, loss_ll=11.0206, loss_ll_paf=16.5656, loss_ll_heat=5.4756, q=1000
[2018-07-03 03:54:10,739] [train] [INFO] epoch=3.00 step=27500, 10.9757 examples/sec lr=0.000100, loss=40.2195, loss_ll=7.2079, loss_ll_paf=9.7365, loss_ll_heat=4.6793, q=1000
[2018-07-03 03:56:36,053] [train] [INFO] epoch=3.00 step=27600, 10.9758 examples/sec lr=0.000100, loss=37.9978, loss_ll=7.35284, loss_ll_paf=10.0081, loss_ll_heat=4.69754, q=1000
[2018-07-03 03:59:02,704] [train] [INFO] epoch=3.00 step=27700, 10.9756 examples/sec lr=0.000100, loss=37.602, loss_ll=7.25232, loss_ll_paf=10.9812, loss_ll_heat=3.52345, q=1000
[2018-07-03 04:01:30,768] [train] [INFO] epoch=3.00 step=27800, 10.9750 examples/sec lr=0.000100, loss=57.1963, loss_ll=11.1019, loss_ll_paf=15.6499, loss_ll_heat=6.55403, q=1000
[2018-07-03 04:03:56,618] [train] [INFO] epoch=3.00 step=27900, 10.9749 examples/sec lr=0.000100, loss=51.5396, loss_ll=9.53847, loss_ll_paf=13.801, loss_ll_heat=5.2759, q=1000
[2018-07-03 04:06:23,742] [train] [INFO] epoch=3.00 step=28000, 10.9746 examples/sec lr=0.000100, loss=40.4905, loss_ll=7.21818, loss_ll_paf=9.9799, loss_ll_heat=4.45646, q=1000
[2018-07-03 04:09:02,860] [train] [INFO] epoch=3.00 step=28100, 10.9710 examples/sec lr=0.000100, loss=29.9589, loss_ll=5.3923, loss_ll_paf=7.6105, loss_ll_heat=3.1741, q=1000
[2018-07-03 04:11:28,453] [train] [INFO] epoch=3.00 step=28200, 10.9711 examples/sec lr=0.000100, loss=40.3047, loss_ll=7.66531, loss_ll_paf=10.2257, loss_ll_heat=5.10494, q=1000
[2018-07-03 04:13:55,837] [train] [INFO] epoch=3.00 step=28300, 10.9707 examples/sec lr=0.000100, loss=33.4647, loss_ll=6.29584, loss_ll_paf=8.63407, loss_ll_heat=3.95761, q=1000
[2018-07-03 04:16:22,400] [train] [INFO] epoch=3.00 step=28400, 10.9705 examples/sec lr=0.000100, loss=63.1646, loss_ll=10.9117, loss_ll_paf=17.3661, loss_ll_heat=4.45729, q=1000
[2018-07-03 04:18:50,620] [train] [INFO] epoch=3.00 step=28500, 10.9699 examples/sec lr=0.000100, loss=60.0691, loss_ll=10.4085, loss_ll_paf=15.5001, loss_ll_heat=5.31688, q=1000
[2018-07-03 04:21:18,414] [train] [INFO] epoch=3.00 step=28600, 10.9693 examples/sec lr=0.000100, loss=28.8631, loss_ll=5.43331, loss_ll_paf=7.52863, loss_ll_heat=3.33799, q=1000
[2018-07-03 04:23:46,882] [train] [INFO] epoch=3.00 step=28700, 10.9687 examples/sec lr=0.000100, loss=54.548, loss_ll=10.7128, loss_ll_paf=16.72, loss_ll_heat=4.70566, q=1000
[2018-07-03 04:26:14,694] [train] [INFO] epoch=3.00 step=28800, 10.9682 examples/sec lr=0.000100, loss=49.7534, loss_ll=9.3015, loss_ll_paf=14.03, loss_ll_heat=4.57305, q=1000
[2018-07-03 04:28:42,931] [train] [INFO] epoch=3.00 step=28900, 10.9675 examples/sec lr=0.000100, loss=33.2466, loss_ll=5.93276, loss_ll_paf=8.43074, loss_ll_heat=3.43477, q=1000
[2018-07-03 04:31:10,561] [train] [INFO] epoch=3.00 step=29000, 10.9671 examples/sec lr=0.000100, loss=42.4627, loss_ll=7.47621, loss_ll_paf=11.1627, loss_ll_heat=3.78968, q=1000
[2018-07-03 04:33:49,907] [train] [INFO] epoch=3.00 step=29100, 10.9636 examples/sec lr=0.000100, loss=54.9877, loss_ll=9.99465, loss_ll_paf=15.349, loss_ll_heat=4.6403, q=1000
[2018-07-03 04:36:17,663] [train] [INFO] epoch=3.00 step=29200, 10.9631 examples/sec lr=0.000100, loss=31.8218, loss_ll=5.84394, loss_ll_paf=7.85002, loss_ll_heat=3.83786, q=1000
[2018-07-03 04:38:45,221] [train] [INFO] epoch=3.00 step=29300, 10.9627 examples/sec lr=0.000100, loss=39.8349, loss_ll=7.10619, loss_ll_paf=10.5083, loss_ll_heat=3.70407, q=1000
[2018-07-03 04:41:12,762] [train] [INFO] epoch=3.00 step=29400, 10.9623 examples/sec lr=0.000100, loss=45.8484, loss_ll=8.44533, loss_ll_paf=12.0287, loss_ll_heat=4.86201, q=1000
[2018-07-03 04:43:40,631] [train] [INFO] epoch=3.00 step=29500, 10.9618 examples/sec lr=0.000100, loss=41.9145, loss_ll=7.77664, loss_ll_paf=11.2108, loss_ll_heat=4.34245, q=1000
[2018-07-03 04:46:07,105] [train] [INFO] epoch=3.00 step=29600, 10.9617 examples/sec lr=0.000100, loss=44.843, loss_ll=7.95734, loss_ll_paf=11.0756, loss_ll_heat=4.83911, q=1000
[2018-07-03 04:48:34,703] [train] [INFO] epoch=3.00 step=29700, 10.9613 examples/sec lr=0.000100, loss=40.4234, loss_ll=7.29609, loss_ll_paf=10.1598, loss_ll_heat=4.43241, q=1000
[2018-07-03 04:51:02,065] [train] [INFO] epoch=3.00 step=29800, 10.9609 examples/sec lr=0.000100, loss=46.1293, loss_ll=8.78228, loss_ll_paf=13.7009, loss_ll_heat=3.86362, q=1000
[2018-07-03 04:53:30,991] [train] [INFO] epoch=3.00 step=29900, 10.9602 examples/sec lr=0.000100, loss=51.0556, loss_ll=9.44122, loss_ll_paf=13.5079, loss_ll_heat=5.37458, q=1000
[2018-07-03 04:56:00,037] [train] [INFO] epoch=3.00 step=30000, 10.9594 examples/sec lr=0.000033, loss=31.4893, loss_ll=5.9602, loss_ll_paf=8.29536, loss_ll_heat=3.62503, q=1000
[2018-07-03 04:58:40,887] [train] [INFO] epoch=3.00 step=30100, 10.9557 examples/sec lr=0.000033, loss=47.7304, loss_ll=8.93601, loss_ll_paf=13.5898, loss_ll_heat=4.28217, q=1000
[2018-07-03 05:01:08,512] [train] [INFO] epoch=3.00 step=30200, 10.9553 examples/sec lr=0.000033, loss=36.2456, loss_ll=6.23357, loss_ll_paf=8.69908, loss_ll_heat=3.76807, q=1000
[2018-07-03 05:03:36,560] [train] [INFO] epoch=3.00 step=30300, 10.9548 examples/sec lr=0.000033, loss=27.6203, loss_ll=4.85365, loss_ll_paf=6.65269, loss_ll_heat=3.0546, q=1000
[2018-07-03 05:06:04,026] [train] [INFO] epoch=3.00 step=30400, 10.9545 examples/sec lr=0.000033, loss=39.3605, loss_ll=7.22452, loss_ll_paf=10.2927, loss_ll_heat=4.15636, q=1000
[2018-07-03 05:08:32,422] [train] [INFO] epoch=4.00 step=30500, 10.9539 examples/sec lr=0.000033, loss=45.8848, loss_ll=8.42523, loss_ll_paf=12.7313, loss_ll_heat=4.11922, q=1000
[2018-07-03 05:10:59,138] [train] [INFO] epoch=4.00 step=30600, 10.9538 examples/sec lr=0.000033, loss=47.0414, loss_ll=8.61586, loss_ll_paf=12.0336, loss_ll_heat=5.19816, q=1000
[2018-07-03 05:13:25,429] [train] [INFO] epoch=4.00 step=30700, 10.9537 examples/sec lr=0.000033, loss=36.5472, loss_ll=6.73324, loss_ll_paf=9.65833, loss_ll_heat=3.80814, q=1000
[2018-07-03 05:15:50,859] [train] [INFO] epoch=4.00 step=30800, 10.9539 examples/sec lr=0.000033, loss=34.9759, loss_ll=6.45042, loss_ll_paf=9.28959, loss_ll_heat=3.61124, q=1000
[2018-07-03 05:18:18,708] [train] [INFO] epoch=4.00 step=30900, 10.9534 examples/sec lr=0.000033, loss=39.0658, loss_ll=7.52454, loss_ll_paf=10.5066, loss_ll_heat=4.54249, q=1000
[2018-07-03 05:20:42,914] [train] [INFO] epoch=4.00 step=31000, 10.9539 examples/sec lr=0.000033, loss=42.2529, loss_ll=7.87895, loss_ll_paf=11.9085, loss_ll_heat=3.8494, q=1000
[2018-07-03 05:23:19,875] [train] [INFO] epoch=4.00 step=31100, 10.9513 examples/sec lr=0.000033, loss=45.007, loss_ll=8.27243, loss_ll_paf=12.4102, loss_ll_heat=4.13466, q=1000
[2018-07-03 05:25:46,938] [train] [INFO] epoch=4.00 step=31200, 10.9510 examples/sec lr=0.000033, loss=33.3415, loss_ll=6.2178, loss_ll_paf=9.06332, loss_ll_heat=3.37228, q=1000
[2018-07-03 05:28:14,333] [train] [INFO] epoch=4.00 step=31300, 10.9507 examples/sec lr=0.000033, loss=30.917, loss_ll=5.68192, loss_ll_paf=7.6219, loss_ll_heat=3.74194, q=1000
[2018-07-03 05:30:40,921] [train] [INFO] epoch=4.00 step=31400, 10.9506 examples/sec lr=0.000033, loss=33.4018, loss_ll=6.10037, loss_ll_paf=8.86849, loss_ll_heat=3.33225, q=1000
[2018-07-03 05:33:07,005] [train] [INFO] epoch=4.00 step=31500, 10.9506 examples/sec lr=0.000033, loss=38.1198, loss_ll=7.10879, loss_ll_paf=9.41597, loss_ll_heat=4.80162, q=1000
[2018-07-03 05:35:33,085] [train] [INFO] epoch=4.00 step=31600, 10.9506 examples/sec lr=0.000033, loss=28.8046, loss_ll=5.42006, loss_ll_paf=7.72119, loss_ll_heat=3.11894, q=1000
[2018-07-03 05:37:59,671] [train] [INFO] epoch=4.00 step=31700, 10.9505 examples/sec lr=0.000033, loss=31.1542, loss_ll=5.80574, loss_ll_paf=7.76002, loss_ll_heat=3.85145, q=1000
[2018-07-03 05:40:26,923] [train] [INFO] epoch=4.00 step=31800, 10.9502 examples/sec lr=0.000033, loss=49.5499, loss_ll=9.01529, loss_ll_paf=13.5122, loss_ll_heat=4.5184, q=1000
[2018-07-03 05:42:53,786] [train] [INFO] epoch=4.00 step=31900, 10.9501 examples/sec lr=0.000033, loss=31.0623, loss_ll=5.72036, loss_ll_paf=7.84858, loss_ll_heat=3.59214, q=1000
[2018-07-03 05:45:19,683] [train] [INFO] epoch=4.00 step=32000, 10.9501 examples/sec lr=0.000033, loss=41.1767, loss_ll=7.58982, loss_ll_paf=10.4613, loss_ll_heat=4.71832, q=1000
[2018-07-03 05:47:59,473] [train] [INFO] epoch=4.00 step=32100, 10.9469 examples/sec lr=0.000033, loss=24.2863, loss_ll=4.39425, loss_ll_paf=5.78785, loss_ll_heat=3.00065, q=1000
[2018-07-03 05:50:27,662] [train] [INFO] epoch=4.00 step=32200, 10.9464 examples/sec lr=0.000033, loss=33.9274, loss_ll=6.10333, loss_ll_paf=7.70899, loss_ll_heat=4.49767, q=1000
[2018-07-03 05:52:56,673] [train] [INFO] epoch=4.00 step=32300, 10.9458 examples/sec lr=0.000033, loss=33.5915, loss_ll=6.2987, loss_ll_paf=8.40939, loss_ll_heat=4.18801, q=1000
[2018-07-03 05:55:25,001] [train] [INFO] epoch=4.00 step=32400, 10.9453 examples/sec lr=0.000033, loss=30.2538, loss_ll=5.37925, loss_ll_paf=6.99423, loss_ll_heat=3.76427, q=1000
[2018-07-03 05:57:52,624] [train] [INFO] epoch=4.00 step=32500, 10.9450 examples/sec lr=0.000033, loss=58.8007, loss_ll=11.1906, loss_ll_paf=16.9281, loss_ll_heat=5.45303, q=1000
[2018-07-03 06:00:17,848] [train] [INFO] epoch=4.00 step=32600, 10.9452 examples/sec lr=0.000033, loss=32.8367, loss_ll=5.88722, loss_ll_paf=8.23242, loss_ll_heat=3.54201, q=1000
[2018-07-03 06:02:46,609] [train] [INFO] epoch=4.00 step=32700, 10.9446 examples/sec lr=0.000033, loss=25.0084, loss_ll=4.39501, loss_ll_paf=5.63862, loss_ll_heat=3.1514, q=1000
[2018-07-03 06:05:14,873] [train] [INFO] epoch=4.00 step=32800, 10.9441 examples/sec lr=0.000033, loss=50.3428, loss_ll=9.04027, loss_ll_paf=13.1878, loss_ll_heat=4.89277, q=1000
[2018-07-03 06:07:43,050] [train] [INFO] epoch=4.00 step=32900, 10.9437 examples/sec lr=0.000033, loss=29.3844, loss_ll=5.52637, loss_ll_paf=7.50659, loss_ll_heat=3.54616, q=1000
[2018-07-03 06:10:12,239] [train] [INFO] epoch=4.00 step=33000, 10.9430 examples/sec lr=0.000033, loss=28.3439, loss_ll=5.13342, loss_ll_paf=6.9683, loss_ll_heat=3.29853, q=1000
[2018-07-03 06:12:51,248] [train] [INFO] epoch=4.00 step=33100, 10.9401 examples/sec lr=0.000033, loss=31.4266, loss_ll=5.7986, loss_ll_paf=6.9872, loss_ll_heat=4.61001, q=1000
[2018-07-03 06:15:17,074] [train] [INFO] epoch=4.00 step=33200, 10.9402 examples/sec lr=0.000033, loss=24.0326, loss_ll=4.28078, loss_ll_paf=5.73719, loss_ll_heat=2.82437, q=1000
[2018-07-03 06:17:45,692] [train] [INFO] epoch=4.00 step=33300, 10.9397 examples/sec lr=0.000033, loss=25.9194, loss_ll=4.78789, loss_ll_paf=6.24365, loss_ll_heat=3.33213, q=1000
[2018-07-03 06:20:11,118] [train] [INFO] epoch=4.00 step=33400, 10.9398 examples/sec lr=0.000033, loss=33.7672, loss_ll=6.15591, loss_ll_paf=7.97927, loss_ll_heat=4.33256, q=1000
[2018-07-03 06:22:39,058] [train] [INFO] epoch=4.00 step=33500, 10.9395 examples/sec lr=0.000033, loss=45.1488, loss_ll=7.6612, loss_ll_paf=11.9325, loss_ll_heat=3.38987, q=1000
[2018-07-03 06:25:07,841] [train] [INFO] epoch=4.00 step=33600, 10.9389 examples/sec lr=0.000033, loss=29.9412, loss_ll=5.55345, loss_ll_paf=7.06876, loss_ll_heat=4.03814, q=1000
[2018-07-03 06:27:36,821] [train] [INFO] epoch=4.00 step=33700, 10.9383 examples/sec lr=0.000033, loss=32.3904, loss_ll=5.65754, loss_ll_paf=7.77189, loss_ll_heat=3.5432, q=1000
[2018-07-03 06:30:04,717] [train] [INFO] epoch=4.00 step=33800, 10.9379 examples/sec lr=0.000033, loss=41.3615, loss_ll=7.7677, loss_ll_paf=11.9148, loss_ll_heat=3.62061, q=1000
[2018-07-03 06:32:31,538] [train] [INFO] epoch=4.00 step=33900, 10.9378 examples/sec lr=0.000033, loss=33.422, loss_ll=6.11844, loss_ll_paf=8.85861, loss_ll_heat=3.37828, q=1000
[2018-07-03 06:34:57,094] [train] [INFO] epoch=4.00 step=34000, 10.9380 examples/sec lr=0.000033, loss=40.6804, loss_ll=7.43992, loss_ll_paf=10.2861, loss_ll_heat=4.5937, q=1000
[2018-07-03 06:37:35,416] [train] [INFO] epoch=4.00 step=34100, 10.9353 examples/sec lr=0.000033, loss=32.3938, loss_ll=5.98119, loss_ll_paf=8.10519, loss_ll_heat=3.85719, q=1000
[2018-07-03 06:40:02,682] [train] [INFO] epoch=4.00 step=34200, 10.9351 examples/sec lr=0.000033, loss=34.5462, loss_ll=6.26256, loss_ll_paf=9.14175, loss_ll_heat=3.38337, q=1000
[2018-07-03 06:42:31,693] [train] [INFO] epoch=4.00 step=34300, 10.9345 examples/sec lr=0.000033, loss=29.0073, loss_ll=5.03049, loss_ll_paf=6.25763, loss_ll_heat=3.80336, q=1000
[2018-07-03 06:44:57,208] [train] [INFO] epoch=4.00 step=34400, 10.9347 examples/sec lr=0.000033, loss=40.0062, loss_ll=7.40274, loss_ll_paf=11.0731, loss_ll_heat=3.73239, q=1000
[2018-07-03 06:47:25,752] [train] [INFO] epoch=4.00 step=34500, 10.9342 examples/sec lr=0.000033, loss=37.6393, loss_ll=6.44357, loss_ll_paf=9.13244, loss_ll_heat=3.7547, q=1000
[2018-07-03 06:49:55,160] [train] [INFO] epoch=4.00 step=34600, 10.9336 examples/sec lr=0.000033, loss=29.1643, loss_ll=5.33261, loss_ll_paf=7.42567, loss_ll_heat=3.23954, q=1000
[2018-07-03 06:52:23,768] [train] [INFO] epoch=4.00 step=34700, 10.9331 examples/sec lr=0.000033, loss=32.2306, loss_ll=6.09326, loss_ll_paf=8.75741, loss_ll_heat=3.42912, q=1000
[2018-07-03 06:54:50,374] [train] [INFO] epoch=4.00 step=34800, 10.9330 examples/sec lr=0.000033, loss=28.426, loss_ll=5.2677, loss_ll_paf=7.36075, loss_ll_heat=3.17465, q=1000
[2018-07-03 06:57:18,005] [train] [INFO] epoch=4.00 step=34900, 10.9328 examples/sec lr=0.000033, loss=35.4379, loss_ll=6.26733, loss_ll_paf=8.95634, loss_ll_heat=3.57834, q=1000
[2018-07-03 06:59:45,076] [train] [INFO] epoch=4.00 step=35000, 10.9326 examples/sec lr=0.000033, loss=28.2079, loss_ll=4.99752, loss_ll_paf=6.31844, loss_ll_heat=3.67659, q=1000
[2018-07-03 07:02:23,396] [train] [INFO] epoch=4.00 step=35100, 10.9301 examples/sec lr=0.000033, loss=31.6529, loss_ll=5.77744, loss_ll_paf=7.24111, loss_ll_heat=4.31377, q=1000
[2018-07-03 07:04:50,947] [train] [INFO] epoch=4.00 step=35200, 10.9298 examples/sec lr=0.000033, loss=39.0128, loss_ll=6.90643, loss_ll_paf=9.1003, loss_ll_heat=4.71256, q=1000
[2018-07-03 07:07:19,741] [train] [INFO] epoch=4.00 step=35300, 10.9293 examples/sec lr=0.000033, loss=30.4537, loss_ll=5.32366, loss_ll_paf=6.81905, loss_ll_heat=3.82827, q=1000
[2018-07-03 07:09:42,668] [train] [INFO] epoch=4.00 step=35400, 10.9300 examples/sec lr=0.000033, loss=39.5933, loss_ll=7.27531, loss_ll_paf=10.4499, loss_ll_heat=4.10067, q=1000
[2018-07-03 07:12:09,375] [train] [INFO] epoch=4.00 step=35500, 10.9300 examples/sec lr=0.000033, loss=33.8512, loss_ll=6.30496, loss_ll_paf=8.47817, loss_ll_heat=4.13176, q=1000
[2018-07-03 07:14:37,287] [train] [INFO] epoch=4.00 step=35600, 10.9296 examples/sec lr=0.000033, loss=25.9125, loss_ll=4.60668, loss_ll_paf=6.06059, loss_ll_heat=3.15277, q=1000
[2018-07-03 07:17:06,608] [train] [INFO] epoch=4.00 step=35700, 10.9290 examples/sec lr=0.000033, loss=33.2238, loss_ll=6.10833, loss_ll_paf=8.78561, loss_ll_heat=3.43104, q=1000
[2018-07-03 07:19:32,461] [train] [INFO] epoch=4.00 step=35800, 10.9291 examples/sec lr=0.000033, loss=20.8127, loss_ll=3.65772, loss_ll_paf=4.20007, loss_ll_heat=3.11538, q=1000
[2018-07-03 07:21:59,333] [train] [INFO] epoch=4.00 step=35900, 10.9290 examples/sec lr=0.000033, loss=34.5512, loss_ll=6.0666, loss_ll_paf=8.03035, loss_ll_heat=4.10284, q=1000
[2018-07-03 07:24:24,038] [train] [INFO] epoch=4.00 step=36000, 10.9294 examples/sec lr=0.000033, loss=35.185, loss_ll=6.47375, loss_ll_paf=8.98287, loss_ll_heat=3.96464, q=1000
[2018-07-03 07:27:02,869] [train] [INFO] epoch=4.00 step=36100, 10.9268 examples/sec lr=0.000033, loss=23.4096, loss_ll=4.21452, loss_ll_paf=5.69026, loss_ll_heat=2.73877, q=1000
[2018-07-03 07:29:27,372] [train] [INFO] epoch=4.00 step=36200, 10.9272 examples/sec lr=0.000033, loss=34.5059, loss_ll=6.43208, loss_ll_paf=9.7964, loss_ll_heat=3.06776, q=1000
[2018-07-03 07:31:53,897] [train] [INFO] epoch=4.00 step=36300, 10.9272 examples/sec lr=0.000033, loss=35.175, loss_ll=6.61517, loss_ll_paf=9.42678, loss_ll_heat=3.80356, q=1000
[2018-07-03 07:34:20,318] [train] [INFO] epoch=4.00 step=36400, 10.9272 examples/sec lr=0.000033, loss=36.8556, loss_ll=6.72158, loss_ll_paf=10.0758, loss_ll_heat=3.36736, q=1000
[2018-07-03 07:36:48,749] [train] [INFO] epoch=4.00 step=36500, 10.9268 examples/sec lr=0.000033, loss=29.8847, loss_ll=4.83738, loss_ll_paf=6.32822, loss_ll_heat=3.34653, q=1000
[2018-07-03 07:39:15,296] [train] [INFO] epoch=4.00 step=36600, 10.9268 examples/sec lr=0.000033, loss=30.8743, loss_ll=5.62158, loss_ll_paf=7.85421, loss_ll_heat=3.38895, q=1000
[2018-07-03 07:41:42,703] [train] [INFO] epoch=4.00 step=36700, 10.9266 examples/sec lr=0.000033, loss=29.2352, loss_ll=5.41204, loss_ll_paf=7.38442, loss_ll_heat=3.43967, q=1000
[2018-07-03 07:44:10,644] [train] [INFO] epoch=4.00 step=36800, 10.9263 examples/sec lr=0.000033, loss=29.2556, loss_ll=5.35847, loss_ll_paf=7.5992, loss_ll_heat=3.11773, q=1000
[2018-07-03 07:46:38,450] [train] [INFO] epoch=4.00 step=36900, 10.9260 examples/sec lr=0.000033, loss=38.7192, loss_ll=7.03205, loss_ll_paf=9.72248, loss_ll_heat=4.34162, q=1000
[2018-07-03 07:49:05,712] [train] [INFO] epoch=4.00 step=37000, 10.9258 examples/sec lr=0.000033, loss=40.5986, loss_ll=7.60427, loss_ll_paf=10.9453, loss_ll_heat=4.26325, q=1000
[2018-07-03 07:51:45,456] [train] [INFO] epoch=4.00 step=37100, 10.9231 examples/sec lr=0.000033, loss=30.1486, loss_ll=5.32613, loss_ll_paf=7.33946, loss_ll_heat=3.31279, q=1000
[2018-07-03 07:54:12,608] [train] [INFO] epoch=4.00 step=37200, 10.9230 examples/sec lr=0.000033, loss=38.6193, loss_ll=7.25479, loss_ll_paf=10.2822, loss_ll_heat=4.22733, q=1000
[2018-07-03 07:56:41,226] [train] [INFO] epoch=4.00 step=37300, 10.9226 examples/sec lr=0.000033, loss=30.4468, loss_ll=4.89649, loss_ll_paf=6.76095, loss_ll_heat=3.03203, q=1000
[2018-07-03 07:59:09,531] [train] [INFO] epoch=4.00 step=37400, 10.9222 examples/sec lr=0.000033, loss=33.1056, loss_ll=6.18759, loss_ll_paf=8.6641, loss_ll_heat=3.71108, q=1000
[2018-07-03 08:01:37,200] [train] [INFO] epoch=4.00 step=37500, 10.9220 examples/sec lr=0.000033, loss=26.4575, loss_ll=4.78651, loss_ll_paf=6.92679, loss_ll_heat=2.64624, q=1000
[2018-07-03 08:04:03,111] [train] [INFO] epoch=4.00 step=37600, 10.9221 examples/sec lr=0.000033, loss=28.954, loss_ll=5.04696, loss_ll_paf=6.90905, loss_ll_heat=3.18486, q=1000
[2018-07-03 08:06:31,488] [train] [INFO] epoch=4.00 step=37700, 10.9217 examples/sec lr=0.000033, loss=40.2916, loss_ll=7.42796, loss_ll_paf=11.858, loss_ll_heat=2.99788, q=1000
[2018-07-03 08:08:59,901] [train] [INFO] epoch=4.00 step=37800, 10.9214 examples/sec lr=0.000033, loss=27.4959, loss_ll=5.22926, loss_ll_paf=6.91483, loss_ll_heat=3.5437, q=1000
[2018-07-03 08:11:29,020] [train] [INFO] epoch=4.00 step=37900, 10.9208 examples/sec lr=0.000033, loss=29.3984, loss_ll=5.17505, loss_ll_paf=7.37101, loss_ll_heat=2.97909, q=1000
[2018-07-03 08:13:55,296] [train] [INFO] epoch=4.00 step=38000, 10.9209 examples/sec lr=0.000033, loss=37.4658, loss_ll=7.07797, loss_ll_paf=10.65, loss_ll_heat=3.50597, q=1000
[2018-07-03 08:16:31,086] [train] [INFO] epoch=5.00 step=38100, 10.9191 examples/sec lr=0.000033, loss=27.1699, loss_ll=4.50768, loss_ll_paf=5.71721, loss_ll_heat=3.29815, q=1000
[2018-07-03 08:18:59,295] [train] [INFO] epoch=5.00 step=38200, 10.9187 examples/sec lr=0.000033, loss=28.9524, loss_ll=5.29282, loss_ll_paf=7.51471, loss_ll_heat=3.07092, q=1000
[2018-07-03 08:21:26,083] [train] [INFO] epoch=5.00 step=38300, 10.9187 examples/sec lr=0.000033, loss=35.8382, loss_ll=6.60884, loss_ll_paf=9.02483, loss_ll_heat=4.19286, q=1000
[2018-07-03 08:23:52,188] [train] [INFO] epoch=5.00 step=38400, 10.9188 examples/sec lr=0.000033, loss=46.453, loss_ll=8.6452, loss_ll_paf=13.3193, loss_ll_heat=3.97107, q=1000
[2018-07-03 08:26:18,988] [train] [INFO] epoch=5.00 step=38500, 10.9187 examples/sec lr=0.000033, loss=30.4127, loss_ll=5.58464, loss_ll_paf=7.94636, loss_ll_heat=3.22293, q=1000
[2018-07-03 08:28:46,413] [train] [INFO] epoch=5.00 step=38600, 10.9186 examples/sec lr=0.000033, loss=32.4115, loss_ll=5.62836, loss_ll_paf=8.43846, loss_ll_heat=2.81825, q=1000
[2018-07-03 08:31:11,450] [train] [INFO] epoch=5.00 step=38700, 10.9188 examples/sec lr=0.000033, loss=37.5887, loss_ll=6.71436, loss_ll_paf=9.34077, loss_ll_heat=4.08795, q=1000
[2018-07-03 08:33:36,517] [train] [INFO] epoch=5.00 step=38800, 10.9191 examples/sec lr=0.000033, loss=47.5045, loss_ll=8.87649, loss_ll_paf=13.3646, loss_ll_heat=4.38841, q=1000
[2018-07-03 08:36:04,325] [train] [INFO] epoch=5.00 step=38900, 10.9189 examples/sec lr=0.000033, loss=23.052, loss_ll=4.19628, loss_ll_paf=5.74878, loss_ll_heat=2.64378, q=1000
[2018-07-03 08:38:30,347] [train] [INFO] epoch=5.00 step=39000, 10.9190 examples/sec lr=0.000033, loss=37.9285, loss_ll=7.08332, loss_ll_paf=10.3643, loss_ll_heat=3.80233, q=1000
[2018-07-03 08:41:08,034] [train] [INFO] epoch=5.00 step=39100, 10.9169 examples/sec lr=0.000033, loss=20.7704, loss_ll=3.76818, loss_ll_paf=5.10002, loss_ll_heat=2.43634, q=1000
[2018-07-03 08:43:31,928] [train] [INFO] epoch=5.00 step=39200, 10.9174 examples/sec lr=0.000033, loss=31.3884, loss_ll=5.94916, loss_ll_paf=9.02488, loss_ll_heat=2.87345, q=1000
[2018-07-03 08:45:57,694] [train] [INFO] epoch=5.00 step=39300, 10.9175 examples/sec lr=0.000033, loss=38.8994, loss_ll=6.94829, loss_ll_paf=9.6852, loss_ll_heat=4.21138, q=1000
[2018-07-03 08:48:30,701] [train] [INFO] epoch=5.00 step=39400, 10.9163 examples/sec lr=0.000033, loss=32.9121, loss_ll=5.86043, loss_ll_paf=7.68008, loss_ll_heat=4.04078, q=1000
[2018-07-03 08:50:56,579] [train] [INFO] epoch=5.00 step=39500, 10.9164 examples/sec lr=0.000033, loss=27.6777, loss_ll=5.01062, loss_ll_paf=6.76568, loss_ll_heat=3.25556, q=1000
[2018-07-03 08:53:26,075] [train] [INFO] epoch=5.00 step=39600, 10.9159 examples/sec lr=0.000033, loss=26.3783, loss_ll=4.85237, loss_ll_paf=6.62296, loss_ll_heat=3.08178, q=1000
[2018-07-03 08:55:52,506] [train] [INFO] epoch=5.00 step=39700, 10.9159 examples/sec lr=0.000033, loss=25.32, loss_ll=4.5191, loss_ll_paf=6.64857, loss_ll_heat=2.38964, q=1000
[2018-07-03 08:58:19,965] [train] [INFO] epoch=5.00 step=39800, 10.9157 examples/sec lr=0.000033, loss=30.1389, loss_ll=5.36274, loss_ll_paf=7.029, loss_ll_heat=3.69647, q=1000
[2018-07-03 09:00:44,470] [train] [INFO] epoch=5.00 step=39900, 10.9161 examples/sec lr=0.000033, loss=30.4676, loss_ll=5.57367, loss_ll_paf=7.54204, loss_ll_heat=3.60531, q=1000
[2018-07-03 09:03:11,448] [train] [INFO] epoch=5.00 step=40000, 10.9160 examples/sec lr=0.000033, loss=44.376, loss_ll=8.25074, loss_ll_paf=13.1316, loss_ll_heat=3.36992, q=1000
[2018-07-03 09:05:49,857] [train] [INFO] epoch=5.00 step=40100, 10.9138 examples/sec lr=0.000033, loss=21.4464, loss_ll=3.95125, loss_ll_paf=5.05322, loss_ll_heat=2.84927, q=1000
[2018-07-03 09:08:12,932] [train] [INFO] epoch=5.00 step=40200, 10.9145 examples/sec lr=0.000033, loss=35.0911, loss_ll=6.23111, loss_ll_paf=9.2685, loss_ll_heat=3.19371, q=1000
[2018-07-03 09:10:40,779] [train] [INFO] epoch=5.00 step=40300, 10.9143 examples/sec lr=0.000033, loss=32.2819, loss_ll=5.87357, loss_ll_paf=8.3289, loss_ll_heat=3.41825, q=1000
[2018-07-03 09:13:06,501] [train] [INFO] epoch=5.00 step=40400, 10.9144 examples/sec lr=0.000033, loss=29.2512, loss_ll=5.30213, loss_ll_paf=7.12513, loss_ll_heat=3.47914, q=1000
[2018-07-03 09:15:34,532] [train] [INFO] epoch=5.00 step=40500, 10.9142 examples/sec lr=0.000033, loss=33.4426, loss_ll=6.15741, loss_ll_paf=8.86121, loss_ll_heat=3.45361, q=1000
[2018-07-03 09:18:01,794] [train] [INFO] epoch=5.00 step=40600, 10.9140 examples/sec lr=0.000033, loss=31.529, loss_ll=5.85536, loss_ll_paf=9.27184, loss_ll_heat=2.43888, q=1000
[2018-07-03 09:20:29,864] [train] [INFO] epoch=5.00 step=40700, 10.9138 examples/sec lr=0.000033, loss=34.545, loss_ll=6.29679, loss_ll_paf=8.99667, loss_ll_heat=3.5969, q=1000
[2018-07-03 09:22:56,943] [train] [INFO] epoch=5.00 step=40800, 10.9137 examples/sec lr=0.000033, loss=35.6414, loss_ll=6.6455, loss_ll_paf=9.29644, loss_ll_heat=3.99455, q=1000
[2018-07-03 09:25:22,503] [train] [INFO] epoch=5.00 step=40900, 10.9139 examples/sec lr=0.000033, loss=26.847, loss_ll=4.53396, loss_ll_paf=6.79936, loss_ll_heat=2.26857, q=1000
[2018-07-03 09:27:47,663] [train] [INFO] epoch=5.00 step=41000, 10.9141 examples/sec lr=0.000033, loss=28.4926, loss_ll=4.69762, loss_ll_paf=6.79458, loss_ll_heat=2.60066, q=1000
[2018-07-03 09:30:26,449] [train] [INFO] epoch=5.00 step=41100, 10.9119 examples/sec lr=0.000033, loss=21.5377, loss_ll=3.78658, loss_ll_paf=5.15266, loss_ll_heat=2.42051, q=1000
[2018-07-03 09:32:53,076] [train] [INFO] epoch=5.00 step=41200, 10.9119 examples/sec lr=0.000033, loss=29.1512, loss_ll=5.15967, loss_ll_paf=7.52388, loss_ll_heat=2.79547, q=1000
[2018-07-03 09:35:19,552] [train] [INFO] epoch=5.00 step=41300, 10.9120 examples/sec lr=0.000033, loss=27.8016, loss_ll=4.84607, loss_ll_paf=7.11487, loss_ll_heat=2.57727, q=1000
[2018-07-03 09:37:48,080] [train] [INFO] epoch=5.00 step=41400, 10.9116 examples/sec lr=0.000033, loss=46.1899, loss_ll=8.46741, loss_ll_paf=12.7242, loss_ll_heat=4.21061, q=1000
[2018-07-03 09:40:14,172] [train] [INFO] epoch=5.00 step=41500, 10.9117 examples/sec lr=0.000033, loss=24.4343, loss_ll=4.33111, loss_ll_paf=5.66356, loss_ll_heat=2.99866, q=1000
[2018-07-03 09:42:40,797] [train] [INFO] epoch=5.00 step=41600, 10.9117 examples/sec lr=0.000033, loss=28.9264, loss_ll=5.23478, loss_ll_paf=7.23898, loss_ll_heat=3.23058, q=1000
[2018-07-03 09:45:08,473] [train] [INFO] epoch=5.00 step=41700, 10.9115 examples/sec lr=0.000033, loss=35.3539, loss_ll=6.45252, loss_ll_paf=8.74651, loss_ll_heat=4.15852, q=1000
[2018-07-03 09:47:38,772] [train] [INFO] epoch=5.00 step=41800, 10.9109 examples/sec lr=0.000033, loss=18.3315, loss_ll=3.47064, loss_ll_paf=4.41092, loss_ll_heat=2.53035, q=1000
[2018-07-03 09:50:05,289] [train] [INFO] epoch=5.00 step=41900, 10.9109 examples/sec lr=0.000033, loss=40.6451, loss_ll=8.05671, loss_ll_paf=12.2552, loss_ll_heat=3.8582, q=1000
[2018-07-03 09:52:30,931] [train] [INFO] epoch=5.00 step=42000, 10.9111 examples/sec lr=0.000033, loss=30.6359, loss_ll=5.53681, loss_ll_paf=7.72017, loss_ll_heat=3.35345, q=1000
[2018-07-03 09:55:09,618] [train] [INFO] epoch=5.00 step=42100, 10.9090 examples/sec lr=0.000033, loss=42.6597, loss_ll=8.01168, loss_ll_paf=11.7905, loss_ll_heat=4.23285, q=1000
[2018-07-03 09:57:34,898] [train] [INFO] epoch=5.00 step=42200, 10.9092 examples/sec lr=0.000033, loss=36.6802, loss_ll=6.54389, loss_ll_paf=9.29297, loss_ll_heat=3.79481, q=1000
[2018-07-03 10:00:04,375] [train] [INFO] epoch=5.00 step=42300, 10.9087 examples/sec lr=0.000033, loss=33.2414, loss_ll=6.33581, loss_ll_paf=9.78775, loss_ll_heat=2.88388, q=1000
[2018-07-03 10:02:33,427] [train] [INFO] epoch=5.00 step=42400, 10.9083 examples/sec lr=0.000033, loss=35.0942, loss_ll=6.49193, loss_ll_paf=9.8378, loss_ll_heat=3.14606, q=1000
[2018-07-03 10:04:58,745] [train] [INFO] epoch=5.00 step=42500, 10.9085 examples/sec lr=0.000033, loss=30.2266, loss_ll=5.30434, loss_ll_paf=7.50809, loss_ll_heat=3.10058, q=1000
[2018-07-03 10:07:25,592] [train] [INFO] epoch=5.00 step=42600, 10.9085 examples/sec lr=0.000033, loss=31.993, loss_ll=5.44529, loss_ll_paf=7.50624, loss_ll_heat=3.38434, q=1000
[2018-07-03 10:09:52,848] [train] [INFO] epoch=5.00 step=42700, 10.9084 examples/sec lr=0.000033, loss=28.7466, loss_ll=5.13981, loss_ll_paf=7.10907, loss_ll_heat=3.17056, q=1000
[2018-07-03 10:12:17,018] [train] [INFO] epoch=5.00 step=42800, 10.9088 examples/sec lr=0.000033, loss=31.5242, loss_ll=5.49285, loss_ll_paf=7.99542, loss_ll_heat=2.99028, q=1000
[2018-07-03 10:14:42,197] [train] [INFO] epoch=5.00 step=42900, 10.9091 examples/sec lr=0.000033, loss=24.5281, loss_ll=4.31328, loss_ll_paf=5.97696, loss_ll_heat=2.64961, q=1000
[2018-07-03 10:17:07,769] [train] [INFO] epoch=5.00 step=43000, 10.9093 examples/sec lr=0.000033, loss=46.3663, loss_ll=8.75364, loss_ll_paf=12.5704, loss_ll_heat=4.93688, q=1000
[2018-07-03 10:19:45,168] [train] [INFO] epoch=5.00 step=43100, 10.9074 examples/sec lr=0.000033, loss=30.9911, loss_ll=5.77704, loss_ll_paf=8.51655, loss_ll_heat=3.03753, q=1000
[2018-07-03 10:22:10,115] [train] [INFO] epoch=5.00 step=43200, 10.9077 examples/sec lr=0.000033, loss=25.897, loss_ll=4.45098, loss_ll_paf=6.05098, loss_ll_heat=2.85098, q=1000
[2018-07-03 10:24:37,773] [train] [INFO] epoch=5.00 step=43300, 10.9076 examples/sec lr=0.000033, loss=34.9026, loss_ll=6.65834, loss_ll_paf=9.79926, loss_ll_heat=3.51741, q=1000
[2018-07-03 10:27:03,168] [train] [INFO] epoch=5.00 step=43400, 10.9078 examples/sec lr=0.000033, loss=38.3173, loss_ll=7.33626, loss_ll_paf=12.0759, loss_ll_heat=2.59663, q=1000
[2018-07-03 10:29:29,961] [train] [INFO] epoch=5.00 step=43500, 10.9078 examples/sec lr=0.000033, loss=39.9091, loss_ll=7.39599, loss_ll_paf=11.6606, loss_ll_heat=3.13141, q=1000
[2018-07-03 10:31:56,926] [train] [INFO] epoch=5.00 step=43600, 10.9077 examples/sec lr=0.000033, loss=24.8521, loss_ll=4.55403, loss_ll_paf=6.06431, loss_ll_heat=3.04375, q=1000
[2018-07-03 10:34:22,769] [train] [INFO] epoch=5.00 step=43700, 10.9079 examples/sec lr=0.000033, loss=38.5674, loss_ll=7.04251, loss_ll_paf=10.486, loss_ll_heat=3.59899, q=1000
[2018-07-03 10:36:51,130] [train] [INFO] epoch=5.00 step=43800, 10.9076 examples/sec lr=0.000033, loss=25.8345, loss_ll=4.63832, loss_ll_paf=6.00943, loss_ll_heat=3.26721, q=1000
[2018-07-03 10:39:18,543] [train] [INFO] epoch=5.00 step=43900, 10.9074 examples/sec lr=0.000033, loss=29.5807, loss_ll=4.92781, loss_ll_paf=6.17282, loss_ll_heat=3.6828, q=1000
[2018-07-03 10:41:46,288] [train] [INFO] epoch=5.00 step=44000, 10.9073 examples/sec lr=0.000033, loss=32.9651, loss_ll=6.03183, loss_ll_paf=8.37269, loss_ll_heat=3.69098, q=1000
[2018-07-03 10:44:23,639] [train] [INFO] epoch=5.00 step=44100, 10.9055 examples/sec lr=0.000033, loss=38.1366, loss_ll=6.96421, loss_ll_paf=9.67518, loss_ll_heat=4.25323, q=1000
[2018-07-03 10:46:51,199] [train] [INFO] epoch=5.00 step=44200, 10.9053 examples/sec lr=0.000033, loss=21.1097, loss_ll=3.99378, loss_ll_paf=5.40722, loss_ll_heat=2.58035, q=1000
[2018-07-03 10:49:17,848] [train] [INFO] epoch=5.00 step=44300, 10.9053 examples/sec lr=0.000033, loss=37.6398, loss_ll=6.64785, loss_ll_paf=9.97093, loss_ll_heat=3.32476, q=1000
[2018-07-03 10:51:42,328] [train] [INFO] epoch=5.00 step=44400, 10.9057 examples/sec lr=0.000033, loss=29.7616, loss_ll=5.56638, loss_ll_paf=7.95192, loss_ll_heat=3.18084, q=1000
[2018-07-03 10:54:07,795] [train] [INFO] epoch=5.00 step=44500, 10.9059 examples/sec lr=0.000033, loss=32.469, loss_ll=6.00921, loss_ll_paf=8.60812, loss_ll_heat=3.4103, q=1000
[2018-07-03 10:56:34,174] [train] [INFO] epoch=5.00 step=44600, 10.9060 examples/sec lr=0.000033, loss=22.8323, loss_ll=4.23125, loss_ll_paf=5.82061, loss_ll_heat=2.64189, q=1000
[2018-07-03 10:59:00,162] [train] [INFO] epoch=5.00 step=44700, 10.9061 examples/sec lr=0.000033, loss=38.133, loss_ll=7.01144, loss_ll_paf=10.0044, loss_ll_heat=4.01852, q=1000
[2018-07-03 11:01:28,122] [train] [INFO] epoch=5.00 step=44800, 10.9059 examples/sec lr=0.000033, loss=29.135, loss_ll=5.35238, loss_ll_paf=7.0811, loss_ll_heat=3.62367, q=1000
[2018-07-03 11:03:55,752] [train] [INFO] epoch=5.00 step=44900, 10.9057 examples/sec lr=0.000033, loss=32.0542, loss_ll=5.74747, loss_ll_paf=8.58095, loss_ll_heat=2.91398, q=1000
[2018-07-03 11:06:20,026] [train] [INFO] epoch=5.00 step=45000, 10.9061 examples/sec lr=0.000033, loss=41.232, loss_ll=7.80614, loss_ll_paf=11.0106, loss_ll_heat=4.60172, q=1000
[2018-07-03 11:08:57,418] [train] [INFO] epoch=5.00 step=45100, 10.9044 examples/sec lr=0.000033, loss=21.0192, loss_ll=3.84096, loss_ll_paf=5.22204, loss_ll_heat=2.45988, q=1000
[2018-07-03 11:11:24,783] [train] [INFO] epoch=5.00 step=45200, 10.9043 examples/sec lr=0.000033, loss=49.7133, loss_ll=9.02635, loss_ll_paf=13.9135, loss_ll_heat=4.13918, q=1000
[2018-07-03 11:13:52,092] [train] [INFO] epoch=5.00 step=45300, 10.9042 examples/sec lr=0.000033, loss=41.1512, loss_ll=7.33869, loss_ll_paf=10.9785, loss_ll_heat=3.6989, q=1000
[2018-07-03 11:16:16,800] [train] [INFO] epoch=5.00 step=45400, 10.9045 examples/sec lr=0.000033, loss=35.4904, loss_ll=6.45411, loss_ll_paf=8.89738, loss_ll_heat=4.01083, q=1000
[2018-07-03 11:18:47,617] [train] [INFO] epoch=5.00 step=45500, 10.9038 examples/sec lr=0.000033, loss=35.4696, loss_ll=6.77995, loss_ll_paf=10.1845, loss_ll_heat=3.37542, q=1000
[2018-07-03 11:21:14,626] [train] [INFO] epoch=5.00 step=45600, 10.9038 examples/sec lr=0.000033, loss=33.3681, loss_ll=6.10235, loss_ll_paf=8.61075, loss_ll_heat=3.59396, q=1000
[2018-07-03 11:23:43,703] [train] [INFO] epoch=6.00 step=45700, 10.9034 examples/sec lr=0.000033, loss=32.0855, loss_ll=5.61241, loss_ll_paf=8.10284, loss_ll_heat=3.12199, q=1000
[2018-07-03 11:26:17,663] [train] [INFO] epoch=6.00 step=45800, 10.9022 examples/sec lr=0.000033, loss=36.3425, loss_ll=6.46399, loss_ll_paf=9.31608, loss_ll_heat=3.61191, q=1000
[2018-07-03 11:28:47,812] [train] [INFO] epoch=6.00 step=45900, 10.9017 examples/sec lr=0.000033, loss=29.5837, loss_ll=5.28788, loss_ll_paf=7.48667, loss_ll_heat=3.08908, q=1000
[2018-07-03 11:31:15,038] [train] [INFO] epoch=6.00 step=46000, 10.9016 examples/sec lr=0.000033, loss=25.41, loss_ll=4.48294, loss_ll_paf=6.13387, loss_ll_heat=2.832, q=1000
[2018-07-03 11:33:51,842] [train] [INFO] epoch=6.00 step=46100, 10.9000 examples/sec lr=0.000033, loss=31.2961, loss_ll=5.91069, loss_ll_paf=8.63423, loss_ll_heat=3.18714, q=1000
[2018-07-03 11:36:16,765] [train] [INFO] epoch=6.00 step=46200, 10.9003 examples/sec lr=0.000033, loss=35.4049, loss_ll=6.59343, loss_ll_paf=8.74605, loss_ll_heat=4.44082, q=1000
[2018-07-03 11:38:43,896] [train] [INFO] epoch=6.00 step=46300, 10.9003 examples/sec lr=0.000033, loss=42.1123, loss_ll=7.65487, loss_ll_paf=10.7921, loss_ll_heat=4.5176, q=1000
[2018-07-03 11:41:08,968] [train] [INFO] epoch=6.00 step=46400, 10.9005 examples/sec lr=0.000033, loss=13.1901, loss_ll=2.44533, loss_ll_paf=3.21448, loss_ll_heat=1.67618, q=1000
[2018-07-03 11:43:36,610] [train] [INFO] epoch=6.00 step=46500, 10.9004 examples/sec lr=0.000033, loss=30.1676, loss_ll=5.29737, loss_ll_paf=6.88387, loss_ll_heat=3.71087, q=1000
[2018-07-03 11:46:03,075] [train] [INFO] epoch=6.00 step=46600, 10.9004 examples/sec lr=0.000033, loss=24.2023, loss_ll=4.50504, loss_ll_paf=6.62721, loss_ll_heat=2.38287, q=1000
[2018-07-03 11:48:29,679] [train] [INFO] epoch=6.00 step=46700, 10.9005 examples/sec lr=0.000033, loss=38.2758, loss_ll=7.02975, loss_ll_paf=9.92382, loss_ll_heat=4.13568, q=1000
[2018-07-03 11:50:53,067] [train] [INFO] epoch=6.00 step=46800, 10.9010 examples/sec lr=0.000033, loss=35.1781, loss_ll=6.45896, loss_ll_paf=9.6896, loss_ll_heat=3.22832, q=1000
[2018-07-03 11:53:19,919] [train] [INFO] epoch=6.00 step=46900, 10.9010 examples/sec lr=0.000033, loss=26.1886, loss_ll=4.62955, loss_ll_paf=6.2415, loss_ll_heat=3.01761, q=1000
[2018-07-03 11:55:45,951] [train] [INFO] epoch=6.00 step=47000, 10.9011 examples/sec lr=0.000033, loss=27.577, loss_ll=5.04953, loss_ll_paf=7.04559, loss_ll_heat=3.05348, q=1000
[2018-07-03 11:58:23,155] [train] [INFO] epoch=6.00 step=47100, 10.8995 examples/sec lr=0.000033, loss=22.0192, loss_ll=4.09657, loss_ll_paf=5.51269, loss_ll_heat=2.68044, q=1000
[2018-07-03 12:00:50,026] [train] [INFO] epoch=6.00 step=47200, 10.8995 examples/sec lr=0.000033, loss=33.1371, loss_ll=5.66346, loss_ll_paf=7.87982, loss_ll_heat=3.44711, q=1000
[2018-07-03 12:03:17,241] [train] [INFO] epoch=6.00 step=47300, 10.8994 examples/sec lr=0.000033, loss=26.009, loss_ll=4.82103, loss_ll_paf=6.77382, loss_ll_heat=2.86824, q=1000
[2018-07-03 12:05:44,373] [train] [INFO] epoch=6.00 step=47400, 10.8993 examples/sec lr=0.000033, loss=22.1139, loss_ll=3.99618, loss_ll_paf=5.32431, loss_ll_heat=2.66805, q=1000
[2018-07-03 12:08:11,854] [train] [INFO] epoch=6.00 step=47500, 10.8992 examples/sec lr=0.000033, loss=22.1542, loss_ll=3.98655, loss_ll_paf=5.24068, loss_ll_heat=2.73242, q=1000
[2018-07-03 12:10:36,680] [train] [INFO] epoch=6.00 step=47600, 10.8995 examples/sec lr=0.000033, loss=28.7138, loss_ll=5.31735, loss_ll_paf=7.81774, loss_ll_heat=2.81695, q=1000
[2018-07-03 12:13:03,114] [train] [INFO] epoch=6.00 step=47700, 10.8996 examples/sec lr=0.000033, loss=29.7978, loss_ll=5.56034, loss_ll_paf=8.13029, loss_ll_heat=2.99039, q=1000
[2018-07-03 12:15:29,865] [train] [INFO] epoch=6.00 step=47800, 10.8996 examples/sec lr=0.000033, loss=31.3405, loss_ll=5.13926, loss_ll_paf=6.97485, loss_ll_heat=3.30367, q=1000
[2018-07-03 12:17:54,591] [train] [INFO] epoch=6.00 step=47900, 10.8999 examples/sec lr=0.000033, loss=28.5041, loss_ll=5.01694, loss_ll_paf=7.16079, loss_ll_heat=2.87309, q=1000
[2018-07-03 12:20:20,066] [train] [INFO] epoch=6.00 step=48000, 10.9001 examples/sec lr=0.000033, loss=31.997, loss_ll=5.82019, loss_ll_paf=8.09796, loss_ll_heat=3.54241, q=1000
[2018-07-03 12:22:58,542] [train] [INFO] epoch=6.00 step=48100, 10.8983 examples/sec lr=0.000033, loss=31.8976, loss_ll=5.81711, loss_ll_paf=8.79418, loss_ll_heat=2.84004, q=1000
[2018-07-03 12:25:24,062] [train] [INFO] epoch=6.00 step=48200, 10.8985 examples/sec lr=0.000033, loss=30.4776, loss_ll=5.66083, loss_ll_paf=7.99236, loss_ll_heat=3.3293, q=1000
[2018-07-03 12:27:47,367] [train] [INFO] epoch=6.00 step=48300, 10.8991 examples/sec lr=0.000033, loss=31.897, loss_ll=6.09553, loss_ll_paf=8.57726, loss_ll_heat=3.6138, q=1000
[2018-07-03 12:30:13,538] [train] [INFO] epoch=6.00 step=48400, 10.8992 examples/sec lr=0.000033, loss=35.5475, loss_ll=6.85165, loss_ll_paf=9.56467, loss_ll_heat=4.13864, q=1000
[2018-07-03 12:32:39,435] [train] [INFO] epoch=6.00 step=48500, 10.8993 examples/sec lr=0.000033, loss=29.0574, loss_ll=5.31539, loss_ll_paf=7.2519, loss_ll_heat=3.37887, q=1000
[2018-07-03 12:35:01,754] [train] [INFO] epoch=6.00 step=48600, 10.9000 examples/sec lr=0.000033, loss=29.5344, loss_ll=5.61975, loss_ll_paf=7.82258, loss_ll_heat=3.41692, q=1000
[2018-07-03 12:37:26,045] [train] [INFO] epoch=6.00 step=48700, 10.9004 examples/sec lr=0.000033, loss=26.7579, loss_ll=4.87072, loss_ll_paf=6.89418, loss_ll_heat=2.84725, q=1000
[2018-07-03 12:39:51,533] [train] [INFO] epoch=6.00 step=48800, 10.9006 examples/sec lr=0.000033, loss=33.6864, loss_ll=6.33717, loss_ll_paf=10.1043, loss_ll_heat=2.57007, q=1000
[2018-07-03 12:42:25,048] [train] [INFO] epoch=6.00 step=48900, 10.8995 examples/sec lr=0.000033, loss=30.7978, loss_ll=5.5172, loss_ll_paf=7.77675, loss_ll_heat=3.25765, q=1000
[2018-07-03 12:45:00,044] [train] [INFO] epoch=6.00 step=49000, 10.8983 examples/sec lr=0.000033, loss=34.8239, loss_ll=6.25338, loss_ll_paf=9.24082, loss_ll_heat=3.26593, q=1000
[2018-07-03 12:47:47,283] [train] [INFO] epoch=6.00 step=49100, 10.8952 examples/sec lr=0.000033, loss=20.3157, loss_ll=3.68184, loss_ll_paf=4.27726, loss_ll_heat=3.08641, q=1000
[2018-07-03 12:50:20,859] [train] [INFO] epoch=6.00 step=49200, 10.8942 examples/sec lr=0.000033, loss=28.3294, loss_ll=5.04303, loss_ll_paf=7.66679, loss_ll_heat=2.41927, q=1000
[2018-07-03 12:52:55,237] [train] [INFO] epoch=6.00 step=49300, 10.8931 examples/sec lr=0.000033, loss=31.6016, loss_ll=5.67448, loss_ll_paf=8.44287, loss_ll_heat=2.90609, q=1000
[2018-07-03 12:55:28,851] [train] [INFO] epoch=6.00 step=49400, 10.8920 examples/sec lr=0.000033, loss=30.8515, loss_ll=5.63507, loss_ll_paf=8.80161, loss_ll_heat=2.46854, q=1000
[2018-07-03 12:58:00,530] [train] [INFO] epoch=6.00 step=49500, 10.8913 examples/sec lr=0.000033, loss=39.8457, loss_ll=7.46441, loss_ll_paf=11.0934, loss_ll_heat=3.83542, q=1000
[2018-07-03 13:00:33,034] [train] [INFO] epoch=6.00 step=49600, 10.8905 examples/sec lr=0.000033, loss=32.6538, loss_ll=6.11616, loss_ll_paf=8.83669, loss_ll_heat=3.39564, q=1000
[2018-07-03 13:03:06,499] [train] [INFO] epoch=6.00 step=49700, 10.8895 examples/sec lr=0.000033, loss=33.0432, loss_ll=6.32465, loss_ll_paf=9.17425, loss_ll_heat=3.47504, q=1000
[2018-07-03 13:05:40,387] [train] [INFO] epoch=6.00 step=49800, 10.8885 examples/sec lr=0.000033, loss=28.401, loss_ll=4.90944, loss_ll_paf=7.16443, loss_ll_heat=2.65446, q=1000
[2018-07-03 13:08:12,626] [train] [INFO] epoch=6.00 step=49900, 10.8877 examples/sec lr=0.000033, loss=30.8436, loss_ll=5.49933, loss_ll_paf=7.62775, loss_ll_heat=3.37092, q=1000
[2018-07-03 13:10:44,946] [train] [INFO] epoch=6.00 step=50000, 10.8869 examples/sec lr=0.000033, loss=22.4184, loss_ll=4.05175, loss_ll_paf=5.33462, loss_ll_heat=2.76888, q=1000
[2018-07-03 13:13:30,417] [train] [INFO] epoch=6.00 step=50100, 10.8842 examples/sec lr=0.000033, loss=34.6172, loss_ll=6.26352, loss_ll_paf=9.49066, loss_ll_heat=3.03637, q=1000
[2018-07-03 13:16:01,733] [train] [INFO] epoch=6.00 step=50200, 10.8835 examples/sec lr=0.000033, loss=28.8537, loss_ll=5.20647, loss_ll_paf=7.36376, loss_ll_heat=3.04919, q=1000
[2018-07-03 13:18:32,841] [train] [INFO] epoch=6.00 step=50300, 10.8829 examples/sec lr=0.000033, loss=28.5857, loss_ll=5.14075, loss_ll_paf=7.10634, loss_ll_heat=3.17516, q=1000
[2018-07-03 13:21:06,042] [train] [INFO] epoch=6.00 step=50400, 10.8820 examples/sec lr=0.000033, loss=24.8208, loss_ll=4.73391, loss_ll_paf=6.05478, loss_ll_heat=3.41303, q=1000
[2018-07-03 13:23:42,234] [train] [INFO] epoch=6.00 step=50500, 10.8807 examples/sec lr=0.000033, loss=25.2429, loss_ll=4.5109, loss_ll_paf=5.91032, loss_ll_heat=3.11148, q=1000
[2018-07-03 13:26:17,061] [train] [INFO] epoch=6.00 step=50600, 10.8795 examples/sec lr=0.000033, loss=37.6628, loss_ll=6.78625, loss_ll_paf=10.7894, loss_ll_heat=2.78313, q=1000
[2018-07-03 13:28:53,492] [train] [INFO] epoch=6.00 step=50700, 10.8782 examples/sec lr=0.000033, loss=28.9681, loss_ll=5.17678, loss_ll_paf=6.99448, loss_ll_heat=3.35908, q=1000
[2018-07-03 13:31:28,220] [train] [INFO] epoch=6.00 step=50800, 10.8771 examples/sec lr=0.000033, loss=31.2491, loss_ll=6.00687, loss_ll_paf=9.91946, loss_ll_heat=2.09428, q=1000
[2018-07-03 13:34:01,595] [train] [INFO] epoch=6.00 step=50900, 10.8761 examples/sec lr=0.000033, loss=27.034, loss_ll=4.84165, loss_ll_paf=6.64952, loss_ll_heat=3.03379, q=1000
[2018-07-03 13:36:34,789] [train] [INFO] epoch=6.00 step=51000, 10.8753 examples/sec lr=0.000033, loss=25.6149, loss_ll=4.78849, loss_ll_paf=6.33108, loss_ll_heat=3.24589, q=1000
[2018-07-03 13:39:22,966] [train] [INFO] epoch=6.00 step=51100, 10.8722 examples/sec lr=0.000033, loss=15.6523, loss_ll=2.75648, loss_ll_paf=3.41864, loss_ll_heat=2.09432, q=1000
[2018-07-03 13:41:53,075] [train] [INFO] epoch=6.00 step=51200, 10.8718 examples/sec lr=0.000033, loss=33.8319, loss_ll=6.1738, loss_ll_paf=9.02952, loss_ll_heat=3.31808, q=1000
[2018-07-03 13:44:24,731] [train] [INFO] epoch=6.00 step=51300, 10.8712 examples/sec lr=0.000033, loss=27.9724, loss_ll=4.9542, loss_ll_paf=7.10314, loss_ll_heat=2.80527, q=1000
[2018-07-03 13:46:55,312] [train] [INFO] epoch=6.00 step=51400, 10.8707 examples/sec lr=0.000033, loss=30.0765, loss_ll=5.34521, loss_ll_paf=7.62608, loss_ll_heat=3.06434, q=1000
[2018-07-03 13:49:27,724] [train] [INFO] epoch=6.00 step=51500, 10.8699 examples/sec lr=0.000033, loss=35.0475, loss_ll=6.07578, loss_ll_paf=8.35417, loss_ll_heat=3.79738, q=1000
[2018-07-03 13:52:02,031] [train] [INFO] epoch=6.00 step=51600, 10.8689 examples/sec lr=0.000033, loss=33.5598, loss_ll=6.30819, loss_ll_paf=9.24917, loss_ll_heat=3.36721, q=1000
[2018-07-03 13:54:37,257] [train] [INFO] epoch=6.00 step=51700, 10.8678 examples/sec lr=0.000033, loss=25.8557, loss_ll=4.86648, loss_ll_paf=7.10982, loss_ll_heat=2.62315, q=1000
[2018-07-03 13:57:10,044] [train] [INFO] epoch=6.00 step=51800, 10.8670 examples/sec lr=0.000033, loss=31.0233, loss_ll=5.32631, loss_ll_paf=7.08539, loss_ll_heat=3.56722, q=1000
[2018-07-03 13:59:42,319] [train] [INFO] epoch=6.00 step=51900, 10.8662 examples/sec lr=0.000033, loss=27.2168, loss_ll=4.81992, loss_ll_paf=7.15528, loss_ll_heat=2.48455, q=1000
[2018-07-03 14:02:11,773] [train] [INFO] epoch=6.00 step=52000, 10.8659 examples/sec lr=0.000033, loss=30.9984, loss_ll=5.52586, loss_ll_paf=7.35837, loss_ll_heat=3.69335, q=1000
[2018-07-03 14:04:55,437] [train] [INFO] epoch=6.00 step=52100, 10.8636 examples/sec lr=0.000033, loss=21.7373, loss_ll=3.97161, loss_ll_paf=5.34096, loss_ll_heat=2.60225, q=1000
[2018-07-03 14:07:26,730] [train] [INFO] epoch=6.00 step=52200, 10.8630 examples/sec lr=0.000033, loss=33.9122, loss_ll=6.54392, loss_ll_paf=10.5033, loss_ll_heat=2.5845, q=1000
[2018-07-03 14:09:56,658] [train] [INFO] epoch=6.00 step=52300, 10.8627 examples/sec lr=0.000033, loss=23.0779, loss_ll=4.08109, loss_ll_paf=5.70939, loss_ll_heat=2.45279, q=1000
[2018-07-03 14:12:26,713] [train] [INFO] epoch=6.00 step=52400, 10.8623 examples/sec lr=0.000033, loss=25.6808, loss_ll=4.79089, loss_ll_paf=6.91467, loss_ll_heat=2.6671, q=1000
[2018-07-03 14:14:57,461] [train] [INFO] epoch=6.00 step=52500, 10.8618 examples/sec lr=0.000033, loss=28.8192, loss_ll=5.09572, loss_ll_paf=7.42173, loss_ll_heat=2.76971, q=1000
[2018-07-03 14:17:26,936] [train] [INFO] epoch=6.00 step=52600, 10.8615 examples/sec lr=0.000033, loss=34.3735, loss_ll=6.31046, loss_ll_paf=8.96568, loss_ll_heat=3.65524, q=1000
[2018-07-03 14:19:57,446] [train] [INFO] epoch=6.00 step=52700, 10.8610 examples/sec lr=0.000033, loss=31.4033, loss_ll=5.71906, loss_ll_paf=8.80414, loss_ll_heat=2.63399, q=1000
[2018-07-03 14:22:28,357] [train] [INFO] epoch=6.00 step=52800, 10.8605 examples/sec lr=0.000033, loss=25.0174, loss_ll=4.66846, loss_ll_paf=6.01273, loss_ll_heat=3.32419, q=1000
[2018-07-03 14:25:02,527] [train] [INFO] epoch=6.00 step=52900, 10.8596 examples/sec lr=0.000033, loss=25.8468, loss_ll=4.48274, loss_ll_paf=5.63529, loss_ll_heat=3.33019, q=1000
[2018-07-03 14:27:34,249] [train] [INFO] epoch=6.00 step=53000, 10.8590 examples/sec lr=0.000033, loss=25.1275, loss_ll=4.40395, loss_ll_paf=6.60744, loss_ll_heat=2.20046, q=1000
[2018-07-03 14:30:25,083] [train] [INFO] epoch=6.00 step=53100, 10.8557 examples/sec lr=0.000033, loss=21.8171, loss_ll=3.49952, loss_ll_paf=4.8521, loss_ll_heat=2.14695, q=1000
[2018-07-03 14:32:58,741] [train] [INFO] epoch=6.00 step=53200, 10.8548 examples/sec lr=0.000033, loss=34.8154, loss_ll=6.34812, loss_ll_paf=9.48197, loss_ll_heat=3.21426, q=1000
[2018-07-03 14:35:34,335] [train] [INFO] epoch=7.00 step=53300, 10.8537 examples/sec lr=0.000033, loss=29.7731, loss_ll=5.3525, loss_ll_paf=7.73915, loss_ll_heat=2.96584, q=1000
[2018-07-03 14:38:06,126] [train] [INFO] epoch=7.00 step=53400, 10.8531 examples/sec lr=0.000033, loss=37.8259, loss_ll=7.07799, loss_ll_paf=11.0469, loss_ll_heat=3.10909, q=1000
[2018-07-03 14:40:39,888] [train] [INFO] epoch=7.00 step=53500, 10.8522 examples/sec lr=0.000033, loss=34.3294, loss_ll=5.55762, loss_ll_paf=7.85739, loss_ll_heat=3.25785, q=1000
[2018-07-03 14:43:14,178] [train] [INFO] epoch=7.00 step=53600, 10.8513 examples/sec lr=0.000033, loss=28.1701, loss_ll=5.13218, loss_ll_paf=7.11843, loss_ll_heat=3.14594, q=1000
[2018-07-03 14:45:49,902] [train] [INFO] epoch=7.00 step=53700, 10.8502 examples/sec lr=0.000033, loss=32.0423, loss_ll=5.72352, loss_ll_paf=8.15416, loss_ll_heat=3.29288, q=1000
[2018-07-03 14:48:15,146] [train] [INFO] epoch=7.00 step=53800, 10.8505 examples/sec lr=0.000033, loss=20.9028, loss_ll=3.94929, loss_ll_paf=5.32487, loss_ll_heat=2.57371, q=1000
[2018-07-03 14:50:39,458] [train] [INFO] epoch=7.00 step=53900, 10.8509 examples/sec lr=0.000033, loss=32.8508, loss_ll=6.15233, loss_ll_paf=9.71145, loss_ll_heat=2.5932, q=1000
[2018-07-03 14:53:04,398] [train] [INFO] epoch=7.00 step=54000, 10.8512 examples/sec lr=0.000033, loss=34.5761, loss_ll=6.09398, loss_ll_paf=8.93596, loss_ll_heat=3.252, q=1000
[2018-07-03 14:55:40,950] [train] [INFO] epoch=7.00 step=54100, 10.8500 examples/sec lr=0.000033, loss=26.3381, loss_ll=4.83725, loss_ll_paf=6.73431, loss_ll_heat=2.94019, q=1000
[2018-07-03 14:58:05,422] [train] [INFO] epoch=7.00 step=54200, 10.8504 examples/sec lr=0.000033, loss=22.9701, loss_ll=4.19261, loss_ll_paf=5.82703, loss_ll_heat=2.55819, q=1000
[2018-07-03 15:00:33,590] [train] [INFO] epoch=7.00 step=54300, 10.8503 examples/sec lr=0.000033, loss=44.1538, loss_ll=7.76518, loss_ll_paf=12.2621, loss_ll_heat=3.26831, q=1000
[2018-07-03 15:02:58,132] [train] [INFO] epoch=7.00 step=54400, 10.8507 examples/sec lr=0.000033, loss=28.7536, loss_ll=4.96525, loss_ll_paf=7.29335, loss_ll_heat=2.63715, q=1000
[2018-07-03 15:05:23,087] [train] [INFO] epoch=7.00 step=54500, 10.8510 examples/sec lr=0.000033, loss=33.5718, loss_ll=6.47433, loss_ll_paf=9.55463, loss_ll_heat=3.39402, q=1000
[2018-07-03 15:07:47,012] [train] [INFO] epoch=7.00 step=54600, 10.8515 examples/sec lr=0.000033, loss=26.6481, loss_ll=4.74789, loss_ll_paf=6.71071, loss_ll_heat=2.78507, q=1000
[2018-07-03 15:10:15,317] [train] [INFO] epoch=7.00 step=54700, 10.8514 examples/sec lr=0.000033, loss=31.5228, loss_ll=5.69435, loss_ll_paf=8.41309, loss_ll_heat=2.97561, q=1000
[2018-07-03 15:12:39,655] [train] [INFO] epoch=7.00 step=54800, 10.8518 examples/sec lr=0.000033, loss=27.9505, loss_ll=5.23561, loss_ll_paf=7.61332, loss_ll_heat=2.85791, q=1000
[2018-07-03 15:15:03,907] [train] [INFO] epoch=7.00 step=54900, 10.8522 examples/sec lr=0.000033, loss=21.641, loss_ll=3.91485, loss_ll_paf=5.78734, loss_ll_heat=2.04235, q=1000
[2018-07-03 15:17:30,077] [train] [INFO] epoch=7.00 step=55000, 10.8524 examples/sec lr=0.000033, loss=24.8889, loss_ll=4.73028, loss_ll_paf=6.62334, loss_ll_heat=2.83721, q=1000
[2018-07-03 15:20:08,582] [train] [INFO] epoch=7.00 step=55100, 10.8509 examples/sec lr=0.000033, loss=31.4018, loss_ll=5.87964, loss_ll_paf=8.51797, loss_ll_heat=3.24131, q=1000
[2018-07-03 15:22:32,622] [train] [INFO] epoch=7.00 step=55200, 10.8514 examples/sec lr=0.000033, loss=25.1466, loss_ll=4.41526, loss_ll_paf=6.48339, loss_ll_heat=2.34714, q=1000
[2018-07-03 15:24:57,440] [train] [INFO] epoch=7.00 step=55300, 10.8517 examples/sec lr=0.000033, loss=21.9037, loss_ll=4.10404, loss_ll_paf=4.97381, loss_ll_heat=3.23427, q=1000
[2018-07-03 15:27:22,670] [train] [INFO] epoch=7.00 step=55400, 10.8520 examples/sec lr=0.000033, loss=25.9896, loss_ll=4.69458, loss_ll_paf=6.4521, loss_ll_heat=2.93707, q=1000
[2018-07-03 15:29:48,088] [train] [INFO] epoch=7.00 step=55500, 10.8523 examples/sec lr=0.000033, loss=30.0365, loss_ll=5.62267, loss_ll_paf=8.4927, loss_ll_heat=2.75264, q=1000
[2018-07-03 15:32:14,985] [train] [INFO] epoch=7.00 step=55600, 10.8524 examples/sec lr=0.000033, loss=22.6222, loss_ll=3.89405, loss_ll_paf=5.45975, loss_ll_heat=2.32834, q=1000
[2018-07-03 15:34:39,245] [train] [INFO] epoch=7.00 step=55700, 10.8528 examples/sec lr=0.000033, loss=30.5693, loss_ll=5.40917, loss_ll_paf=7.84363, loss_ll_heat=2.97471, q=1000
[2018-07-03 15:37:04,216] [train] [INFO] epoch=7.00 step=55800, 10.8531 examples/sec lr=0.000033, loss=34.4738, loss_ll=6.40993, loss_ll_paf=9.42317, loss_ll_heat=3.39669, q=1000
[2018-07-03 15:39:30,615] [train] [INFO] epoch=7.00 step=55900, 10.8533 examples/sec lr=0.000033, loss=31.345, loss_ll=6.02855, loss_ll_paf=8.6715, loss_ll_heat=3.3856, q=1000
[2018-07-03 15:41:55,045] [train] [INFO] epoch=7.00 step=56000, 10.8536 examples/sec lr=0.000033, loss=27.3519, loss_ll=4.82632, loss_ll_paf=7.21677, loss_ll_heat=2.43588, q=1000
[2018-07-03 15:44:31,956] [train] [INFO] epoch=7.00 step=56100, 10.8524 examples/sec lr=0.000033, loss=30.3551, loss_ll=5.67591, loss_ll_paf=8.19909, loss_ll_heat=3.15272, q=1000
[2018-07-03 15:46:58,584] [train] [INFO] epoch=7.00 step=56200, 10.8525 examples/sec lr=0.000033, loss=30.133, loss_ll=5.59299, loss_ll_paf=7.98096, loss_ll_heat=3.20503, q=1000
[2018-07-03 15:49:25,169] [train] [INFO] epoch=7.00 step=56300, 10.8526 examples/sec lr=0.000033, loss=24.4144, loss_ll=4.48763, loss_ll_paf=6.40443, loss_ll_heat=2.57082, q=1000
[2018-07-03 15:51:50,593] [train] [INFO] epoch=7.00 step=56400, 10.8529 examples/sec lr=0.000033, loss=28.1846, loss_ll=5.4267, loss_ll_paf=7.94235, loss_ll_heat=2.91104, q=1000
[2018-07-03 15:54:17,546] [train] [INFO] epoch=7.00 step=56500, 10.8529 examples/sec lr=0.000033, loss=30.2922, loss_ll=5.26596, loss_ll_paf=7.75666, loss_ll_heat=2.77526, q=1000
[2018-07-03 15:56:44,008] [train] [INFO] epoch=7.00 step=56600, 10.8531 examples/sec lr=0.000033, loss=39.2236, loss_ll=6.73563, loss_ll_paf=10.8154, loss_ll_heat=2.65586, q=1000
[2018-07-03 15:59:08,803] [train] [INFO] epoch=7.00 step=56700, 10.8534 examples/sec lr=0.000033, loss=25.418, loss_ll=4.67432, loss_ll_paf=6.95887, loss_ll_heat=2.38976, q=1000
[2018-07-03 16:01:34,657] [train] [INFO] epoch=7.00 step=56800, 10.8536 examples/sec lr=0.000033, loss=23.5223, loss_ll=4.20406, loss_ll_paf=5.44801, loss_ll_heat=2.9601, q=1000
[2018-07-03 16:04:04,382] [train] [INFO] epoch=7.00 step=56900, 10.8533 examples/sec lr=0.000033, loss=20.8544, loss_ll=3.47776, loss_ll_paf=4.58058, loss_ll_heat=2.37493, q=1000
[2018-07-03 16:06:29,780] [train] [INFO] epoch=7.00 step=57000, 10.8536 examples/sec lr=0.000033, loss=27.6029, loss_ll=4.98388, loss_ll_paf=7.25959, loss_ll_heat=2.70817, q=1000
[2018-07-03 16:09:08,973] [train] [INFO] epoch=7.00 step=57100, 10.8521 examples/sec lr=0.000033, loss=26.9959, loss_ll=4.67669, loss_ll_paf=6.41719, loss_ll_heat=2.93618, q=1000
[2018-07-03 16:11:33,239] [train] [INFO] epoch=7.00 step=57200, 10.8525 examples/sec lr=0.000033, loss=39.0146, loss_ll=7.32626, loss_ll_paf=10.6074, loss_ll_heat=4.04516, q=1000
[2018-07-03 16:13:58,215] [train] [INFO] epoch=7.00 step=57300, 10.8528 examples/sec lr=0.000033, loss=32.963, loss_ll=5.72864, loss_ll_paf=8.25456, loss_ll_heat=3.20272, q=1000
[2018-07-03 16:16:25,151] [train] [INFO] epoch=7.00 step=57400, 10.8528 examples/sec lr=0.000033, loss=36.6292, loss_ll=6.44613, loss_ll_paf=8.53661, loss_ll_heat=4.35565, q=1000
[2018-07-03 16:18:49,275] [train] [INFO] epoch=7.00 step=57500, 10.8533 examples/sec lr=0.000033, loss=26.0902, loss_ll=4.96306, loss_ll_paf=6.74463, loss_ll_heat=3.1815, q=1000
[2018-07-03 16:21:12,804] [train] [INFO] epoch=7.00 step=57600, 10.8538 examples/sec lr=0.000033, loss=23.1993, loss_ll=4.1138, loss_ll_paf=4.98281, loss_ll_heat=3.24479, q=1000
[2018-07-03 16:23:38,956] [train] [INFO] epoch=7.00 step=57700, 10.8539 examples/sec lr=0.000033, loss=34.8837, loss_ll=6.22896, loss_ll_paf=9.49012, loss_ll_heat=2.96781, q=1000
[2018-07-03 16:26:07,610] [train] [INFO] epoch=7.00 step=57800, 10.8538 examples/sec lr=0.000033, loss=28.2502, loss_ll=4.77376, loss_ll_paf=6.66174, loss_ll_heat=2.88577, q=1000
[2018-07-03 16:28:34,237] [train] [INFO] epoch=7.00 step=57900, 10.8539 examples/sec lr=0.000033, loss=40.342, loss_ll=7.25401, loss_ll_paf=10.7408, loss_ll_heat=3.76722, q=1000
[2018-07-03 16:31:01,089] [train] [INFO] epoch=7.00 step=58000, 10.8539 examples/sec lr=0.000033, loss=25.7463, loss_ll=4.9366, loss_ll_paf=6.9903, loss_ll_heat=2.8829, q=1000
[2018-07-03 16:33:37,223] [train] [INFO] epoch=7.00 step=58100, 10.8528 examples/sec lr=0.000033, loss=27.0042, loss_ll=5.03646, loss_ll_paf=7.48194, loss_ll_heat=2.59099, q=1000
[2018-07-03 16:36:04,079] [train] [INFO] epoch=7.00 step=58200, 10.8529 examples/sec lr=0.000033, loss=18.8755, loss_ll=3.23371, loss_ll_paf=4.41582, loss_ll_heat=2.0516, q=1000
[2018-07-03 16:38:31,950] [train] [INFO] epoch=7.00 step=58300, 10.8528 examples/sec lr=0.000033, loss=35.2804, loss_ll=6.52892, loss_ll_paf=9.93796, loss_ll_heat=3.11988, q=1000
[2018-07-03 16:40:56,614] [train] [INFO] epoch=7.00 step=58400, 10.8532 examples/sec lr=0.000033, loss=39.5253, loss_ll=7.2745, loss_ll_paf=11.87, loss_ll_heat=2.67901, q=1000
[2018-07-03 16:43:20,559] [train] [INFO] epoch=7.00 step=58500, 10.8536 examples/sec lr=0.000033, loss=25.4122, loss_ll=4.31227, loss_ll_paf=6.08238, loss_ll_heat=2.54217, q=1000
[2018-07-03 16:45:46,480] [train] [INFO] epoch=7.00 step=58600, 10.8538 examples/sec lr=0.000033, loss=26.7603, loss_ll=4.81436, loss_ll_paf=6.62923, loss_ll_heat=2.9995, q=1000
[2018-07-03 16:48:10,051] [train] [INFO] epoch=7.00 step=58700, 10.8543 examples/sec lr=0.000033, loss=29.5537, loss_ll=5.45205, loss_ll_paf=8.075, loss_ll_heat=2.82911, q=1000
[2018-07-03 16:50:38,159] [train] [INFO] epoch=7.00 step=58800, 10.8542 examples/sec lr=0.000033, loss=26.4724, loss_ll=4.55719, loss_ll_paf=6.475, loss_ll_heat=2.63938, q=1000
[2018-07-03 16:53:01,428] [train] [INFO] epoch=7.00 step=58900, 10.8547 examples/sec lr=0.000033, loss=26.1292, loss_ll=4.7503, loss_ll_paf=6.95854, loss_ll_heat=2.54207, q=1000
[2018-07-03 16:55:28,931] [train] [INFO] epoch=7.00 step=59000, 10.8547 examples/sec lr=0.000033, loss=14.3282, loss_ll=2.51481, loss_ll_paf=2.96226, loss_ll_heat=2.06737, q=1000
[2018-07-03 16:58:05,812] [train] [INFO] epoch=7.00 step=59100, 10.8535 examples/sec lr=0.000033, loss=37.2979, loss_ll=7.12978, loss_ll_paf=10.7638, loss_ll_heat=3.49578, q=1000
[2018-07-03 17:00:33,065] [train] [INFO] epoch=7.00 step=59200, 10.8536 examples/sec lr=0.000033, loss=25.6282, loss_ll=4.71351, loss_ll_paf=6.0637, loss_ll_heat=3.36332, q=1000
[2018-07-03 17:02:58,063] [train] [INFO] epoch=7.00 step=59300, 10.8539 examples/sec lr=0.000033, loss=27.2618, loss_ll=4.68534, loss_ll_paf=6.67306, loss_ll_heat=2.69761, q=1000
[2018-07-03 17:05:24,429] [train] [INFO] epoch=7.00 step=59400, 10.8540 examples/sec lr=0.000033, loss=35.2458, loss_ll=6.65634, loss_ll_paf=9.68199, loss_ll_heat=3.63069, q=1000
[2018-07-03 17:07:50,814] [train] [INFO] epoch=7.00 step=59500, 10.8541 examples/sec lr=0.000033, loss=29.9082, loss_ll=5.25132, loss_ll_paf=7.25366, loss_ll_heat=3.24899, q=1000
[2018-07-03 17:10:16,559] [train] [INFO] epoch=7.00 step=59600, 10.8543 examples/sec lr=0.000033, loss=32.1914, loss_ll=5.56156, loss_ll_paf=8.20915, loss_ll_heat=2.91397, q=1000
[2018-07-03 17:12:41,744] [train] [INFO] epoch=7.00 step=59700, 10.8546 examples/sec lr=0.000033, loss=26.5934, loss_ll=4.75076, loss_ll_paf=6.7314, loss_ll_heat=2.77011, q=1000
[2018-07-03 17:15:08,201] [train] [INFO] epoch=7.00 step=59800, 10.8547 examples/sec lr=0.000033, loss=27.6476, loss_ll=5.01503, loss_ll_paf=7.39579, loss_ll_heat=2.63426, q=1000
[2018-07-03 17:17:34,651] [train] [INFO] epoch=7.00 step=59900, 10.8548 examples/sec lr=0.000033, loss=18.7736, loss_ll=3.16447, loss_ll_paf=4.40912, loss_ll_heat=1.91981, q=1000
[2018-07-03 17:20:02,349] [train] [INFO] epoch=7.00 step=60000, 10.8548 examples/sec lr=0.000011, loss=30.4866, loss_ll=5.54641, loss_ll_paf=7.87374, loss_ll_heat=3.21907, q=1000
[2018-07-03 17:22:43,669] [train] [INFO] epoch=7.00 step=60100, 10.8531 examples/sec lr=0.000011, loss=20.0584, loss_ll=3.68847, loss_ll_paf=5.47168, loss_ll_heat=1.90527, q=1000
[2018-07-03 17:25:08,481] [train] [INFO] epoch=7.00 step=60200, 10.8534 examples/sec lr=0.000011, loss=27.9544, loss_ll=5.15782, loss_ll_paf=7.8181, loss_ll_heat=2.49754, q=1000
[2018-07-03 17:27:36,063] [train] [INFO] epoch=7.00 step=60300, 10.8534 examples/sec lr=0.000011, loss=28.2916, loss_ll=5.28746, loss_ll_paf=8.32995, loss_ll_heat=2.24497, q=1000
[2018-07-03 17:30:03,101] [train] [INFO] epoch=7.00 step=60400, 10.8534 examples/sec lr=0.000011, loss=20.7239, loss_ll=3.59267, loss_ll_paf=4.9026, loss_ll_heat=2.28274, q=1000
[2018-07-03 17:32:27,458] [train] [INFO] epoch=7.00 step=60500, 10.8538 examples/sec lr=0.000011, loss=23.3521, loss_ll=4.04157, loss_ll_paf=5.52566, loss_ll_heat=2.55747, q=1000
[2018-07-03 17:34:52,440] [train] [INFO] epoch=7.00 step=60600, 10.8541 examples/sec lr=0.000011, loss=30.1926, loss_ll=5.62849, loss_ll_paf=8.78314, loss_ll_heat=2.47385, q=1000
[2018-07-03 17:37:15,571] [train] [INFO] epoch=7.00 step=60700, 10.8546 examples/sec lr=0.000011, loss=27.4562, loss_ll=4.56092, loss_ll_paf=5.94881, loss_ll_heat=3.17303, q=1000
[2018-07-03 17:39:40,393] [train] [INFO] epoch=7.00 step=60800, 10.8549 examples/sec lr=0.000011, loss=27.9009, loss_ll=4.8731, loss_ll_paf=7.14557, loss_ll_heat=2.60064, q=1000
[2018-07-03 17:42:04,935] [train] [INFO] epoch=8.00 step=60900, 10.8553 examples/sec lr=0.000011, loss=29.8996, loss_ll=5.09124, loss_ll_paf=8.09256, loss_ll_heat=2.08992, q=1000
[2018-07-03 17:44:31,880] [train] [INFO] epoch=8.00 step=61000, 10.8553 examples/sec lr=0.000011, loss=33.9012, loss_ll=5.83291, loss_ll_paf=8.66846, loss_ll_heat=2.99736, q=1000
[2018-07-03 17:47:10,314] [train] [INFO] epoch=8.00 step=61100, 10.8540 examples/sec lr=0.000011, loss=30.5658, loss_ll=5.47464, loss_ll_paf=8.12585, loss_ll_heat=2.82343, q=1000
[2018-07-03 17:49:35,100] [train] [INFO] epoch=8.00 step=61200, 10.8543 examples/sec lr=0.000011, loss=25.7797, loss_ll=4.52945, loss_ll_paf=6.25481, loss_ll_heat=2.80409, q=1000
[2018-07-03 17:51:59,765] [train] [INFO] epoch=8.00 step=61300, 10.8546 examples/sec lr=0.000011, loss=27.4847, loss_ll=4.97792, loss_ll_paf=7.11605, loss_ll_heat=2.83978, q=1000
[2018-07-03 17:54:25,661] [train] [INFO] epoch=8.00 step=61400, 10.8548 examples/sec lr=0.000011, loss=24.8315, loss_ll=4.63561, loss_ll_paf=6.68219, loss_ll_heat=2.58904, q=1000
[2018-07-03 17:56:51,657] [train] [INFO] epoch=8.00 step=61500, 10.8550 examples/sec lr=0.000011, loss=21.8914, loss_ll=4.13707, loss_ll_paf=5.60562, loss_ll_heat=2.66853, q=1000
[2018-07-03 17:59:19,666] [train] [INFO] epoch=8.00 step=61600, 10.8549 examples/sec lr=0.000011, loss=35.3359, loss_ll=6.36891, loss_ll_paf=9.40059, loss_ll_heat=3.33723, q=1000
[2018-07-03 18:01:47,418] [train] [INFO] epoch=8.00 step=61700, 10.8549 examples/sec lr=0.000011, loss=16.0415, loss_ll=2.86856, loss_ll_paf=3.61437, loss_ll_heat=2.12276, q=1000
[2018-07-03 18:04:13,945] [train] [INFO] epoch=8.00 step=61800, 10.8550 examples/sec lr=0.000011, loss=26.7191, loss_ll=4.94496, loss_ll_paf=7.72035, loss_ll_heat=2.16957, q=1000
[2018-07-03 18:06:40,294] [train] [INFO] epoch=8.00 step=61900, 10.8551 examples/sec lr=0.000011, loss=24.9202, loss_ll=4.29465, loss_ll_paf=5.88663, loss_ll_heat=2.70267, q=1000
[2018-07-03 18:09:07,677] [train] [INFO] epoch=8.00 step=62000, 10.8551 examples/sec lr=0.000011, loss=26.1394, loss_ll=4.65479, loss_ll_paf=6.54346, loss_ll_heat=2.76612, q=1000
[2018-07-03 18:11:44,737] [train] [INFO] epoch=8.00 step=62100, 10.8540 examples/sec lr=0.000011, loss=19.728, loss_ll=3.70541, loss_ll_paf=5.67903, loss_ll_heat=1.7318, q=1000
[2018-07-03 18:14:10,156] [train] [INFO] epoch=8.00 step=62200, 10.8542 examples/sec lr=0.000011, loss=19.2086, loss_ll=3.52614, loss_ll_paf=5.0611, loss_ll_heat=1.99118, q=1000
[2018-07-03 18:16:36,558] [train] [INFO] epoch=8.00 step=62300, 10.8543 examples/sec lr=0.000011, loss=21.3859, loss_ll=3.73672, loss_ll_paf=4.76098, loss_ll_heat=2.71245, q=1000
[2018-07-03 18:19:01,440] [train] [INFO] epoch=8.00 step=62400, 10.8546 examples/sec lr=0.000011, loss=26.3877, loss_ll=4.67592, loss_ll_paf=6.80786, loss_ll_heat=2.54398, q=1000
[2018-07-03 18:21:26,370] [train] [INFO] epoch=8.00 step=62500, 10.8549 examples/sec lr=0.000011, loss=26.8691, loss_ll=4.86417, loss_ll_paf=7.36154, loss_ll_heat=2.36681, q=1000
[2018-07-03 18:23:52,959] [train] [INFO] epoch=8.00 step=62600, 10.8550 examples/sec lr=0.000011, loss=26.4393, loss_ll=4.44161, loss_ll_paf=6.53957, loss_ll_heat=2.34364, q=1000
[2018-07-03 18:26:19,047] [train] [INFO] epoch=8.00 step=62700, 10.8552 examples/sec lr=0.000011, loss=32.6305, loss_ll=6.04211, loss_ll_paf=8.53875, loss_ll_heat=3.54546, q=1000
[2018-07-03 18:28:45,330] [train] [INFO] epoch=8.00 step=62800, 10.8553 examples/sec lr=0.000011, loss=19.1209, loss_ll=3.18557, loss_ll_paf=3.83592, loss_ll_heat=2.53523, q=1000
[2018-07-03 18:31:10,894] [train] [INFO] epoch=8.00 step=62900, 10.8555 examples/sec lr=0.000011, loss=34.0569, loss_ll=6.29613, loss_ll_paf=9.34546, loss_ll_heat=3.24681, q=1000
[2018-07-03 18:33:35,456] [train] [INFO] epoch=8.00 step=63000, 10.8558 examples/sec lr=0.000011, loss=33.3435, loss_ll=6.06042, loss_ll_paf=8.8395, loss_ll_heat=3.28134, q=1000
[2018-07-03 18:36:14,542] [train] [INFO] epoch=8.00 step=63100, 10.8545 examples/sec lr=0.000011, loss=30.679, loss_ll=5.39473, loss_ll_paf=7.82376, loss_ll_heat=2.9657, q=1000
[2018-07-03 18:38:41,953] [train] [INFO] epoch=8.00 step=63200, 10.8545 examples/sec lr=0.000011, loss=19.5098, loss_ll=3.31596, loss_ll_paf=4.53818, loss_ll_heat=2.09374, q=1000
[2018-07-03 18:41:07,279] [train] [INFO] epoch=8.00 step=63300, 10.8547 examples/sec lr=0.000011, loss=26.0752, loss_ll=4.7271, loss_ll_paf=5.76397, loss_ll_heat=3.69023, q=1000
[2018-07-03 18:43:34,673] [train] [INFO] epoch=8.00 step=63400, 10.8547 examples/sec lr=0.000011, loss=34.558, loss_ll=6.59909, loss_ll_paf=9.90715, loss_ll_heat=3.29103, q=1000
[2018-07-03 18:46:00,612] [train] [INFO] epoch=8.00 step=63500, 10.8549 examples/sec lr=0.000011, loss=26.6414, loss_ll=4.86835, loss_ll_paf=6.70207, loss_ll_heat=3.03464, q=1000
[2018-07-03 18:48:26,884] [train] [INFO] epoch=8.00 step=63600, 10.8550 examples/sec lr=0.000011, loss=22.1586, loss_ll=4.09371, loss_ll_paf=5.80838, loss_ll_heat=2.37905, q=1000
[2018-07-03 18:50:52,751] [train] [INFO] epoch=8.00 step=63700, 10.8552 examples/sec lr=0.000011, loss=29.3875, loss_ll=5.55522, loss_ll_paf=8.09646, loss_ll_heat=3.01398, q=1000
[2018-07-03 18:53:20,381] [train] [INFO] epoch=8.00 step=63800, 10.8552 examples/sec lr=0.000011, loss=21.4596, loss_ll=3.86162, loss_ll_paf=4.94517, loss_ll_heat=2.77808, q=1000
[2018-07-03 18:55:46,047] [train] [INFO] epoch=8.00 step=63900, 10.8554 examples/sec lr=0.000011, loss=25.1812, loss_ll=4.67423, loss_ll_paf=6.53598, loss_ll_heat=2.81248, q=1000
[2018-07-03 18:58:13,332] [train] [INFO] epoch=8.00 step=64000, 10.8554 examples/sec lr=0.000011, loss=14.8513, loss_ll=2.54266, loss_ll_paf=3.14929, loss_ll_heat=1.93604, q=1000
[2018-07-03 19:00:55,469] [train] [INFO] epoch=8.00 step=64100, 10.8537 examples/sec lr=0.000011, loss=27.6739, loss_ll=5.46905, loss_ll_paf=8.30377, loss_ll_heat=2.63434, q=1000
[2018-07-03 19:03:30,983] [train] [INFO] epoch=8.00 step=64200, 10.8527 examples/sec lr=0.000011, loss=29.1839, loss_ll=4.78805, loss_ll_paf=6.75784, loss_ll_heat=2.81826, q=1000
[2018-07-03 19:06:07,030] [train] [INFO] epoch=8.00 step=64300, 10.8518 examples/sec lr=0.000011, loss=35.1898, loss_ll=6.42886, loss_ll_paf=10.3431, loss_ll_heat=2.51462, q=1000
[2018-07-03 19:08:42,027] [train] [INFO] epoch=8.00 step=64400, 10.8509 examples/sec lr=0.000011, loss=28.8491, loss_ll=5.09796, loss_ll_paf=7.43901, loss_ll_heat=2.75691, q=1000
[2018-07-03 19:11:16,554] [train] [INFO] epoch=8.00 step=64500, 10.8501 examples/sec lr=0.000011, loss=25.6996, loss_ll=4.51685, loss_ll_paf=6.74251, loss_ll_heat=2.2912, q=1000
[2018-07-03 19:13:51,163] [train] [INFO] epoch=8.00 step=64600, 10.8493 examples/sec lr=0.000011, loss=31.3726, loss_ll=5.54795, loss_ll_paf=7.87821, loss_ll_heat=3.2177, q=1000
[2018-07-03 19:16:25,539] [train] [INFO] epoch=8.00 step=64700, 10.8485 examples/sec lr=0.000011, loss=30.9118, loss_ll=5.58949, loss_ll_paf=8.03278, loss_ll_heat=3.14621, q=1000
[2018-07-03 19:19:00,241] [train] [INFO] epoch=8.00 step=64800, 10.8477 examples/sec lr=0.000011, loss=23.4636, loss_ll=3.92389, loss_ll_paf=5.4279, loss_ll_heat=2.41987, q=1000
[2018-07-03 19:21:35,987] [train] [INFO] epoch=8.00 step=64900, 10.8467 examples/sec lr=0.000011, loss=19.691, loss_ll=3.44396, loss_ll_paf=4.82127, loss_ll_heat=2.06665, q=1000
[2018-07-03 19:24:12,137] [train] [INFO] epoch=8.00 step=65000, 10.8458 examples/sec lr=0.000011, loss=26.5184, loss_ll=4.78349, loss_ll_paf=7.15954, loss_ll_heat=2.40744, q=1000
[2018-07-03 19:27:00,687] [train] [INFO] epoch=8.00 step=65100, 10.8434 examples/sec lr=0.000011, loss=28.5445, loss_ll=5.10822, loss_ll_paf=7.52339, loss_ll_heat=2.69306, q=1000
[2018-07-03 19:29:29,933] [train] [INFO] epoch=8.00 step=65200, 10.8432 examples/sec lr=0.000011, loss=29.0904, loss_ll=5.11588, loss_ll_paf=7.87726, loss_ll_heat=2.35449, q=1000
[2018-07-03 19:31:56,510] [train] [INFO] epoch=8.00 step=65300, 10.8433 examples/sec lr=0.000011, loss=27.7183, loss_ll=4.80351, loss_ll_paf=7.74601, loss_ll_heat=1.86101, q=1000
[2018-07-03 19:34:19,613] [train] [INFO] epoch=8.00 step=65400, 10.8438 examples/sec lr=0.000011, loss=18.9957, loss_ll=3.45798, loss_ll_paf=4.80652, loss_ll_heat=2.10943, q=1000
[2018-07-03 19:36:46,822] [train] [INFO] epoch=8.00 step=65500, 10.8438 examples/sec lr=0.000011, loss=29.8418, loss_ll=5.53358, loss_ll_paf=8.15758, loss_ll_heat=2.90958, q=1000
[2018-07-03 19:39:14,550] [train] [INFO] epoch=8.00 step=65600, 10.8438 examples/sec lr=0.000011, loss=23.7032, loss_ll=4.09721, loss_ll_paf=5.64054, loss_ll_heat=2.55387, q=1000
[2018-07-03 19:41:41,968] [train] [INFO] epoch=8.00 step=65700, 10.8438 examples/sec lr=0.000011, loss=21.0035, loss_ll=3.7136, loss_ll_paf=5.57044, loss_ll_heat=1.85677, q=1000
[2018-07-03 19:44:08,792] [train] [INFO] epoch=8.00 step=65800, 10.8439 examples/sec lr=0.000011, loss=17.9712, loss_ll=2.89662, loss_ll_paf=3.89471, loss_ll_heat=1.89852, q=1000
[2018-07-03 19:46:36,485] [train] [INFO] epoch=8.00 step=65900, 10.8439 examples/sec lr=0.000011, loss=33.8931, loss_ll=6.10567, loss_ll_paf=9.42468, loss_ll_heat=2.78666, q=1000
[2018-07-03 19:49:02,087] [train] [INFO] epoch=8.00 step=66000, 10.8441 examples/sec lr=0.000011, loss=29.7188, loss_ll=5.45457, loss_ll_paf=7.99237, loss_ll_heat=2.91677, q=1000
[2018-07-03 19:51:38,529] [train] [INFO] epoch=8.00 step=66100, 10.8431 examples/sec lr=0.000011, loss=23.3507, loss_ll=3.93211, loss_ll_paf=5.71706, loss_ll_heat=2.14717, q=1000
[2018-07-03 19:54:10,556] [train] [INFO] epoch=8.00 step=66200, 10.8426 examples/sec lr=0.000011, loss=19.9668, loss_ll=3.42619, loss_ll_paf=4.37663, loss_ll_heat=2.47575, q=1000
[2018-07-03 19:56:43,417] [train] [INFO] epoch=8.00 step=66300, 10.8421 examples/sec lr=0.000011, loss=19.9044, loss_ll=3.64863, loss_ll_paf=4.88678, loss_ll_heat=2.41048, q=1000
[2018-07-03 19:59:17,123] [train] [INFO] epoch=8.00 step=66400, 10.8414 examples/sec lr=0.000011, loss=21.9298, loss_ll=3.75824, loss_ll_paf=4.93312, loss_ll_heat=2.58336, q=1000
[2018-07-03 20:01:51,371] [train] [INFO] epoch=8.00 step=66500, 10.8406 examples/sec lr=0.000011, loss=14.2251, loss_ll=2.50643, loss_ll_paf=3.32081, loss_ll_heat=1.69204, q=1000
[2018-07-03 20:04:23,078] [train] [INFO] epoch=8.00 step=66600, 10.8402 examples/sec lr=0.000011, loss=29.7656, loss_ll=5.65913, loss_ll_paf=8.85464, loss_ll_heat=2.46361, q=1000
[2018-07-03 20:06:48,476] [train] [INFO] epoch=8.00 step=66700, 10.8404 examples/sec lr=0.000011, loss=23.4481, loss_ll=4.25116, loss_ll_paf=5.80456, loss_ll_heat=2.69775, q=1000
[2018-07-03 20:09:15,283] [train] [INFO] epoch=8.00 step=66800, 10.8405 examples/sec lr=0.000011, loss=24.7925, loss_ll=4.40541, loss_ll_paf=5.43843, loss_ll_heat=3.37239, q=1000
[2018-07-03 20:11:41,366] [train] [INFO] epoch=8.00 step=66900, 10.8407 examples/sec lr=0.000011, loss=23.369, loss_ll=4.07152, loss_ll_paf=6.20053, loss_ll_heat=1.94251, q=1000
[2018-07-03 20:14:06,336] [train] [INFO] epoch=8.00 step=67000, 10.8410 examples/sec lr=0.000011, loss=26.4857, loss_ll=4.92524, loss_ll_paf=7.5805, loss_ll_heat=2.26998, q=1000
[2018-07-03 20:16:43,986] [train] [INFO] epoch=8.00 step=67100, 10.8399 examples/sec lr=0.000011, loss=26.6685, loss_ll=4.43138, loss_ll_paf=6.35964, loss_ll_heat=2.50312, q=1000
[2018-07-03 20:19:11,287] [train] [INFO] epoch=8.00 step=67200, 10.8399 examples/sec lr=0.000011, loss=24.1183, loss_ll=4.05989, loss_ll_paf=5.52476, loss_ll_heat=2.59503, q=1000
[2018-07-03 20:21:38,872] [train] [INFO] epoch=8.00 step=67300, 10.8399 examples/sec lr=0.000011, loss=21.3078, loss_ll=3.88308, loss_ll_paf=5.83869, loss_ll_heat=1.92746, q=1000
[2018-07-03 20:24:06,461] [train] [INFO] epoch=8.00 step=67400, 10.8399 examples/sec lr=0.000011, loss=20.6064, loss_ll=3.74655, loss_ll_paf=4.99753, loss_ll_heat=2.49557, q=1000
[2018-07-03 20:26:32,856] [train] [INFO] epoch=8.00 step=67500, 10.8400 examples/sec lr=0.000011, loss=27.4611, loss_ll=4.81821, loss_ll_paf=6.45928, loss_ll_heat=3.17713, q=1000
[2018-07-03 20:28:59,413] [train] [INFO] epoch=8.00 step=67600, 10.8401 examples/sec lr=0.000011, loss=23.0964, loss_ll=4.03028, loss_ll_paf=5.61354, loss_ll_heat=2.44702, q=1000
[2018-07-03 20:31:26,254] [train] [INFO] epoch=8.00 step=67700, 10.8402 examples/sec lr=0.000011, loss=25.0166, loss_ll=4.32012, loss_ll_paf=6.0996, loss_ll_heat=2.54063, q=1000
[2018-07-03 20:33:53,251] [train] [INFO] epoch=8.00 step=67800, 10.8403 examples/sec lr=0.000011, loss=30.6845, loss_ll=5.759, loss_ll_paf=8.42232, loss_ll_heat=3.09569, q=1000
[2018-07-03 20:36:20,287] [train] [INFO] epoch=8.00 step=67900, 10.8404 examples/sec lr=0.000011, loss=18.026, loss_ll=3.23474, loss_ll_paf=4.60188, loss_ll_heat=1.86759, q=1000
[2018-07-03 20:38:47,318] [train] [INFO] epoch=8.00 step=68000, 10.8404 examples/sec lr=0.000011, loss=34.1949, loss_ll=6.19469, loss_ll_paf=10.0452, loss_ll_heat=2.34421, q=1000
[2018-07-03 20:41:24,096] [train] [INFO] epoch=8.00 step=68100, 10.8394 examples/sec lr=0.000011, loss=27.6718, loss_ll=4.95618, loss_ll_paf=7.42459, loss_ll_heat=2.48777, q=1000
[2018-07-03 20:43:51,664] [train] [INFO] epoch=8.00 step=68200, 10.8394 examples/sec lr=0.000011, loss=20.4986, loss_ll=3.46635, loss_ll_paf=4.65633, loss_ll_heat=2.27637, q=1000
[2018-07-03 20:46:15,609] [train] [INFO] epoch=8.00 step=68300, 10.8398 examples/sec lr=0.000011, loss=21.7653, loss_ll=3.7463, loss_ll_paf=5.4859, loss_ll_heat=2.00671, q=1000
[2018-07-03 20:48:42,782] [train] [INFO] epoch=8.00 step=68400, 10.8399 examples/sec lr=0.000011, loss=33.702, loss_ll=6.12245, loss_ll_paf=9.06174, loss_ll_heat=3.18316, q=1000
[2018-07-03 20:51:12,809] [train] [INFO] epoch=9.00 step=68500, 10.8396 examples/sec lr=0.000011, loss=34.4329, loss_ll=6.24354, loss_ll_paf=9.97553, loss_ll_heat=2.51155, q=1000
[2018-07-03 20:53:40,622] [train] [INFO] epoch=9.00 step=68600, 10.8396 examples/sec lr=0.000011, loss=25.4516, loss_ll=4.54332, loss_ll_paf=6.89208, loss_ll_heat=2.19456, q=1000
[2018-07-03 20:56:07,480] [train] [INFO] epoch=9.00 step=68700, 10.8397 examples/sec lr=0.000011, loss=18.579, loss_ll=3.16664, loss_ll_paf=3.63699, loss_ll_heat=2.69629, q=1000
[2018-07-03 20:58:32,904] [train] [INFO] epoch=9.00 step=68800, 10.8399 examples/sec lr=0.000011, loss=19.1264, loss_ll=3.27137, loss_ll_paf=4.58519, loss_ll_heat=1.95756, q=1000
[2018-07-03 21:00:59,341] [train] [INFO] epoch=9.00 step=68900, 10.8400 examples/sec lr=0.000011, loss=22.857, loss_ll=3.88301, loss_ll_paf=5.61909, loss_ll_heat=2.14693, q=1000
[2018-07-03 21:03:25,798] [train] [INFO] epoch=9.00 step=69000, 10.8401 examples/sec lr=0.000011, loss=17.2136, loss_ll=3.0432, loss_ll_paf=4.02893, loss_ll_heat=2.05748, q=1000
[2018-07-03 21:06:04,525] [train] [INFO] epoch=9.00 step=69100, 10.8390 examples/sec lr=0.000011, loss=19.5068, loss_ll=3.48273, loss_ll_paf=5.03925, loss_ll_heat=1.92621, q=1000
[2018-07-03 21:08:32,441] [train] [INFO] epoch=9.00 step=69200, 10.8389 examples/sec lr=0.000011, loss=28.907, loss_ll=5.30907, loss_ll_paf=7.59518, loss_ll_heat=3.02295, q=1000
[2018-07-03 21:10:59,488] [train] [INFO] epoch=9.00 step=69300, 10.8390 examples/sec lr=0.000011, loss=23.3635, loss_ll=4.09618, loss_ll_paf=5.57658, loss_ll_heat=2.61578, q=1000
[2018-07-03 21:13:26,000] [train] [INFO] epoch=9.00 step=69400, 10.8391 examples/sec lr=0.000011, loss=13.9348, loss_ll=2.47233, loss_ll_paf=2.98608, loss_ll_heat=1.95858, q=1000
[2018-07-03 21:15:52,969] [train] [INFO] epoch=9.00 step=69500, 10.8392 examples/sec lr=0.000011, loss=17.2419, loss_ll=2.95713, loss_ll_paf=4.05139, loss_ll_heat=1.86286, q=1000
[2018-07-03 21:18:18,226] [train] [INFO] epoch=9.00 step=69600, 10.8394 examples/sec lr=0.000011, loss=27.0823, loss_ll=4.88391, loss_ll_paf=7.43829, loss_ll_heat=2.32953, q=1000
[2018-07-03 21:20:44,020] [train] [INFO] epoch=9.00 step=69700, 10.8396 examples/sec lr=0.000011, loss=22.6022, loss_ll=4.01097, loss_ll_paf=5.77777, loss_ll_heat=2.24418, q=1000
[2018-07-03 21:23:09,995] [train] [INFO] epoch=9.00 step=69800, 10.8398 examples/sec lr=0.000011, loss=32.4078, loss_ll=6.17239, loss_ll_paf=9.28144, loss_ll_heat=3.06334, q=1000
[2018-07-03 21:25:36,048] [train] [INFO] epoch=9.00 step=69900, 10.8400 examples/sec lr=0.000011, loss=19.0309, loss_ll=3.1781, loss_ll_paf=4.27785, loss_ll_heat=2.07834, q=1000
[2018-07-03 21:28:02,977] [train] [INFO] epoch=9.00 step=70000, 10.8400 examples/sec lr=0.000011, loss=28.0003, loss_ll=4.89444, loss_ll_paf=6.75919, loss_ll_heat=3.0297, q=1000
[2018-07-03 21:30:42,705] [train] [INFO] epoch=9.00 step=70100, 10.8388 examples/sec lr=0.000011, loss=22.4823, loss_ll=4.11909, loss_ll_paf=6.31798, loss_ll_heat=1.9202, q=1000
[2018-07-03 21:33:09,644] [train] [INFO] epoch=9.00 step=70200, 10.8388 examples/sec lr=0.000011, loss=22.7265, loss_ll=3.93614, loss_ll_paf=5.69724, loss_ll_heat=2.17504, q=1000
[2018-07-03 21:35:37,232] [train] [INFO] epoch=9.00 step=70300, 10.8388 examples/sec lr=0.000011, loss=23.1509, loss_ll=4.28001, loss_ll_paf=6.34505, loss_ll_heat=2.21497, q=1000
[2018-07-03 21:38:02,256] [train] [INFO] epoch=9.00 step=70400, 10.8391 examples/sec lr=0.000011, loss=22.4513, loss_ll=4.09306, loss_ll_paf=5.51836, loss_ll_heat=2.66776, q=1000
[2018-07-03 21:40:29,904] [train] [INFO] epoch=9.00 step=70500, 10.8391 examples/sec lr=0.000011, loss=16.0711, loss_ll=2.79296, loss_ll_paf=3.5148, loss_ll_heat=2.07112, q=1000
[2018-07-03 21:42:55,519] [train] [INFO] epoch=9.00 step=70600, 10.8393 examples/sec lr=0.000011, loss=26.321, loss_ll=4.82543, loss_ll_paf=6.8136, loss_ll_heat=2.83726, q=1000
[2018-07-03 21:45:22,824] [train] [INFO] epoch=9.00 step=70700, 10.8393 examples/sec lr=0.000011, loss=21.8557, loss_ll=3.80504, loss_ll_paf=4.9832, loss_ll_heat=2.62687, q=1000
[2018-07-03 21:47:50,701] [train] [INFO] epoch=9.00 step=70800, 10.8393 examples/sec lr=0.000011, loss=26.0419, loss_ll=4.78334, loss_ll_paf=6.91595, loss_ll_heat=2.65073, q=1000
[2018-07-03 21:50:17,197] [train] [INFO] epoch=9.00 step=70900, 10.8394 examples/sec lr=0.000011, loss=18.9934, loss_ll=3.34593, loss_ll_paf=4.7389, loss_ll_heat=1.95297, q=1000
[2018-07-03 21:52:43,267] [train] [INFO] epoch=9.00 step=71000, 10.8396 examples/sec lr=0.000011, loss=25.0328, loss_ll=4.64161, loss_ll_paf=5.92405, loss_ll_heat=3.35917, q=1000
[2018-07-03 21:55:21,241] [train] [INFO] epoch=9.00 step=71100, 10.8385 examples/sec lr=0.000011, loss=26.3913, loss_ll=4.98112, loss_ll_paf=7.12071, loss_ll_heat=2.84154, q=1000
[2018-07-03 21:57:48,286] [train] [INFO] epoch=9.00 step=71200, 10.8386 examples/sec lr=0.000011, loss=21.7211, loss_ll=3.66783, loss_ll_paf=4.85376, loss_ll_heat=2.48191, q=1000
[2018-07-03 22:00:15,608] [train] [INFO] epoch=9.00 step=71300, 10.8386 examples/sec lr=0.000011, loss=12.9172, loss_ll=2.12901, loss_ll_paf=2.98686, loss_ll_heat=1.27117, q=1000
[2018-07-03 22:02:40,559] [train] [INFO] epoch=9.00 step=71400, 10.8389 examples/sec lr=0.000011, loss=17.9866, loss_ll=3.15678, loss_ll_paf=4.36032, loss_ll_heat=1.95325, q=1000
[2018-07-03 22:05:06,616] [train] [INFO] epoch=9.00 step=71500, 10.8390 examples/sec lr=0.000011, loss=23.3184, loss_ll=4.09214, loss_ll_paf=6.11829, loss_ll_heat=2.066, q=1000
[2018-07-03 22:07:34,078] [train] [INFO] epoch=9.00 step=71600, 10.8390 examples/sec lr=0.000011, loss=33.9115, loss_ll=6.16613, loss_ll_paf=8.91452, loss_ll_heat=3.41774, q=1000
[2018-07-03 22:10:01,891] [train] [INFO] epoch=9.00 step=71700, 10.8390 examples/sec lr=0.000011, loss=25.6291, loss_ll=4.67261, loss_ll_paf=7.01456, loss_ll_heat=2.33067, q=1000
[2018-07-03 22:12:29,921] [train] [INFO] epoch=9.00 step=71800, 10.8390 examples/sec lr=0.000011, loss=23.1425, loss_ll=3.96032, loss_ll_paf=5.63023, loss_ll_heat=2.29041, q=1000
[2018-07-03 22:14:56,492] [train] [INFO] epoch=9.00 step=71900, 10.8391 examples/sec lr=0.000011, loss=17.6068, loss_ll=3.22926, loss_ll_paf=4.83539, loss_ll_heat=1.62313, q=1000
[2018-07-03 22:17:24,850] [train] [INFO] epoch=9.00 step=72000, 10.8390 examples/sec lr=0.000011, loss=17.4437, loss_ll=3.19981, loss_ll_paf=3.83382, loss_ll_heat=2.56579, q=1000
[2018-07-03 22:20:03,423] [train] [INFO] epoch=9.00 step=72100, 10.8379 examples/sec lr=0.000011, loss=17.5554, loss_ll=3.17241, loss_ll_paf=4.15925, loss_ll_heat=2.18558, q=1000
[2018-07-03 22:22:30,527] [train] [INFO] epoch=9.00 step=72200, 10.8380 examples/sec lr=0.000011, loss=31.9056, loss_ll=5.35853, loss_ll_paf=8.12579, loss_ll_heat=2.59127, q=1000
[2018-07-03 22:24:56,410] [train] [INFO] epoch=9.00 step=72300, 10.8381 examples/sec lr=0.000011, loss=30.8629, loss_ll=5.66498, loss_ll_paf=8.36819, loss_ll_heat=2.96176, q=1000
[2018-07-03 22:27:23,152] [train] [INFO] epoch=9.00 step=72400, 10.8382 examples/sec lr=0.000011, loss=23.3493, loss_ll=3.93531, loss_ll_paf=6.17096, loss_ll_heat=1.69965, q=1000
[2018-07-03 22:29:49,281] [train] [INFO] epoch=9.00 step=72500, 10.8384 examples/sec lr=0.000011, loss=18.5133, loss_ll=2.99664, loss_ll_paf=3.73011, loss_ll_heat=2.26316, q=1000
[2018-07-03 22:32:15,477] [train] [INFO] epoch=9.00 step=72600, 10.8385 examples/sec lr=0.000011, loss=26.4797, loss_ll=4.98522, loss_ll_paf=7.84455, loss_ll_heat=2.12589, q=1000
[2018-07-03 22:34:40,785] [train] [INFO] epoch=9.00 step=72700, 10.8388 examples/sec lr=0.000011, loss=27.9613, loss_ll=5.45006, loss_ll_paf=8.08512, loss_ll_heat=2.81501, q=1000
[2018-07-03 22:37:06,558] [train] [INFO] epoch=9.00 step=72800, 10.8389 examples/sec lr=0.000011, loss=35.2526, loss_ll=6.24082, loss_ll_paf=9.52092, loss_ll_heat=2.96072, q=1000
[2018-07-03 22:39:33,304] [train] [INFO] epoch=9.00 step=72900, 10.8390 examples/sec lr=0.000011, loss=20.3806, loss_ll=3.70712, loss_ll_paf=5.31886, loss_ll_heat=2.09538, q=1000
[2018-07-03 22:42:01,379] [train] [INFO] epoch=9.00 step=73000, 10.8390 examples/sec lr=0.000011, loss=17.9201, loss_ll=3.15874, loss_ll_paf=4.0314, loss_ll_heat=2.28609, q=1000
[2018-07-03 22:44:39,583] [train] [INFO] epoch=9.00 step=73100, 10.8379 examples/sec lr=0.000011, loss=21.9408, loss_ll=3.89054, loss_ll_paf=5.40392, loss_ll_heat=2.37716, q=1000
[2018-07-03 22:47:05,980] [train] [INFO] epoch=9.00 step=73200, 10.8380 examples/sec lr=0.000011, loss=21.6815, loss_ll=4.18597, loss_ll_paf=6.31345, loss_ll_heat=2.05849, q=1000
[2018-07-03 22:49:34,004] [train] [INFO] epoch=9.00 step=73300, 10.8380 examples/sec lr=0.000011, loss=21.2326, loss_ll=3.52473, loss_ll_paf=4.75132, loss_ll_heat=2.29815, q=1000
[2018-07-03 22:52:00,494] [train] [INFO] epoch=9.00 step=73400, 10.8381 examples/sec lr=0.000011, loss=28.7863, loss_ll=5.18763, loss_ll_paf=8.03485, loss_ll_heat=2.3404, q=1000
[2018-07-03 22:54:24,857] [train] [INFO] epoch=9.00 step=73500, 10.8384 examples/sec lr=0.000011, loss=20.0902, loss_ll=3.28737, loss_ll_paf=4.68281, loss_ll_heat=1.89192, q=1000
[2018-07-03 22:56:50,774] [train] [INFO] epoch=9.00 step=73600, 10.8386 examples/sec lr=0.000011, loss=25.8936, loss_ll=4.43775, loss_ll_paf=6.30287, loss_ll_heat=2.57263, q=1000
[2018-07-03 22:59:19,236] [train] [INFO] epoch=9.00 step=73700, 10.8385 examples/sec lr=0.000011, loss=39.5224, loss_ll=7.51745, loss_ll_paf=11.8672, loss_ll_heat=3.16768, q=1000
[2018-07-03 23:01:46,586] [train] [INFO] epoch=9.00 step=73800, 10.8386 examples/sec lr=0.000011, loss=23.2198, loss_ll=4.42849, loss_ll_paf=6.38387, loss_ll_heat=2.47312, q=1000
[2018-07-03 23:04:11,595] [train] [INFO] epoch=9.00 step=73900, 10.8388 examples/sec lr=0.000011, loss=23.4992, loss_ll=4.24981, loss_ll_paf=6.23613, loss_ll_heat=2.26348, q=1000
[2018-07-03 23:06:36,470] [train] [INFO] epoch=9.00 step=74000, 10.8391 examples/sec lr=0.000011, loss=22.9753, loss_ll=4.04494, loss_ll_paf=5.50913, loss_ll_heat=2.58076, q=1000
[2018-07-03 23:09:13,743] [train] [INFO] epoch=9.00 step=74100, 10.8381 examples/sec lr=0.000011, loss=30.6546, loss_ll=5.54515, loss_ll_paf=7.59622, loss_ll_heat=3.49407, q=1000
[2018-07-03 23:11:40,824] [train] [INFO] epoch=9.00 step=74200, 10.8382 examples/sec lr=0.000011, loss=16.9235, loss_ll=2.88639, loss_ll_paf=3.92688, loss_ll_heat=1.8459, q=1000
[2018-07-03 23:14:06,212] [train] [INFO] epoch=9.00 step=74300, 10.8384 examples/sec lr=0.000011, loss=28.647, loss_ll=5.05119, loss_ll_paf=7.38462, loss_ll_heat=2.71775, q=1000
[2018-07-03 23:16:31,044] [train] [INFO] epoch=9.00 step=74400, 10.8387 examples/sec lr=0.000011, loss=18.1317, loss_ll=3.23744, loss_ll_paf=4.67712, loss_ll_heat=1.79775, q=1000
[2018-07-03 23:18:58,304] [train] [INFO] epoch=9.00 step=74500, 10.8387 examples/sec lr=0.000011, loss=22.6417, loss_ll=3.97371, loss_ll_paf=5.52534, loss_ll_heat=2.42209, q=1000
[2018-07-03 23:21:25,745] [train] [INFO] epoch=9.00 step=74600, 10.8387 examples/sec lr=0.000011, loss=33.9776, loss_ll=5.54662, loss_ll_paf=8.2454, loss_ll_heat=2.84784, q=1000
[2018-07-03 23:23:51,302] [train] [INFO] epoch=9.00 step=74700, 10.8389 examples/sec lr=0.000011, loss=26.2524, loss_ll=4.71437, loss_ll_paf=7.14114, loss_ll_heat=2.28761, q=1000
[2018-07-03 23:26:19,248] [train] [INFO] epoch=9.00 step=74800, 10.8389 examples/sec lr=0.000011, loss=23.9624, loss_ll=4.05287, loss_ll_paf=5.82927, loss_ll_heat=2.27646, q=1000
[2018-07-03 23:28:45,251] [train] [INFO] epoch=9.00 step=74900, 10.8391 examples/sec lr=0.000011, loss=19.3639, loss_ll=3.33003, loss_ll_paf=4.77038, loss_ll_heat=1.88968, q=1000
[2018-07-03 23:31:11,313] [train] [INFO] epoch=9.00 step=75000, 10.8392 examples/sec lr=0.000011, loss=20.8251, loss_ll=3.76303, loss_ll_paf=5.24952, loss_ll_heat=2.27655, q=1000
[2018-07-03 23:33:48,128] [train] [INFO] epoch=9.00 step=75100, 10.8383 examples/sec lr=0.000011, loss=29.8496, loss_ll=5.58977, loss_ll_paf=8.5309, loss_ll_heat=2.64865, q=1000
[2018-07-03 23:36:14,289] [train] [INFO] epoch=9.00 step=75200, 10.8385 examples/sec lr=0.000011, loss=31.702, loss_ll=5.91535, loss_ll_paf=9.45244, loss_ll_heat=2.37826, q=1000
[2018-07-03 23:38:41,291] [train] [INFO] epoch=9.00 step=75300, 10.8385 examples/sec lr=0.000011, loss=21.9341, loss_ll=4.10182, loss_ll_paf=5.82876, loss_ll_heat=2.37488, q=1000
[2018-07-03 23:41:08,636] [train] [INFO] epoch=9.00 step=75400, 10.8385 examples/sec lr=0.000011, loss=17.9898, loss_ll=3.23807, loss_ll_paf=4.57287, loss_ll_heat=1.90327, q=1000
[2018-07-03 23:43:34,672] [train] [INFO] epoch=9.00 step=75500, 10.8387 examples/sec lr=0.000011, loss=24.6594, loss_ll=4.29222, loss_ll_paf=6.08279, loss_ll_heat=2.50166, q=1000
[2018-07-03 23:46:02,534] [train] [INFO] epoch=9.00 step=75600, 10.8387 examples/sec lr=0.000011, loss=22.2521, loss_ll=3.92845, loss_ll_paf=5.28375, loss_ll_heat=2.57316, q=1000
[2018-07-03 23:48:28,098] [train] [INFO] epoch=9.00 step=75700, 10.8389 examples/sec lr=0.000011, loss=20.2795, loss_ll=3.33222, loss_ll_paf=4.67224, loss_ll_heat=1.99219, q=1000
[2018-07-03 23:50:55,506] [train] [INFO] epoch=9.00 step=75800, 10.8389 examples/sec lr=0.000011, loss=24.8825, loss_ll=4.67927, loss_ll_paf=6.64797, loss_ll_heat=2.71057, q=1000
[2018-07-03 23:53:22,273] [train] [INFO] epoch=9.00 step=75900, 10.8390 examples/sec lr=0.000011, loss=25.2804, loss_ll=4.40164, loss_ll_paf=6.30229, loss_ll_heat=2.50099, q=1000
[2018-07-03 23:55:50,229] [train] [INFO] epoch=9.00 step=76000, 10.8389 examples/sec lr=0.000011, loss=32.6107, loss_ll=5.89353, loss_ll_paf=8.87872, loss_ll_heat=2.90833, q=1000
[2018-07-03 23:58:31,140] [train] [INFO] epoch=10.00 step=76100, 10.8377 examples/sec lr=0.000011, loss=16.1015, loss_ll=2.85906, loss_ll_paf=3.88923, loss_ll_heat=1.82889, q=1000
[2018-07-04 00:00:58,017] [train] [INFO] epoch=10.00 step=76200, 10.8377 examples/sec lr=0.000011, loss=13.6212, loss_ll=2.3901, loss_ll_paf=3.11943, loss_ll_heat=1.66077, q=1000
[2018-07-04 00:03:23,685] [train] [INFO] epoch=10.00 step=76300, 10.8379 examples/sec lr=0.000011, loss=14.7299, loss_ll=2.44972, loss_ll_paf=3.29499, loss_ll_heat=1.60445, q=1000
[2018-07-04 00:05:50,342] [train] [INFO] epoch=10.00 step=76400, 10.8380 examples/sec lr=0.000011, loss=31.3087, loss_ll=5.90812, loss_ll_paf=9.10071, loss_ll_heat=2.71553, q=1000
[2018-07-04 00:08:15,603] [train] [INFO] epoch=10.00 step=76500, 10.8382 examples/sec lr=0.000011, loss=30.2965, loss_ll=5.46909, loss_ll_paf=7.83957, loss_ll_heat=3.09861, q=1000
[2018-07-04 00:10:43,506] [train] [INFO] epoch=10.00 step=76600, 10.8382 examples/sec lr=0.000011, loss=19.3594, loss_ll=3.42138, loss_ll_paf=4.40246, loss_ll_heat=2.4403, q=1000
[2018-07-04 00:13:08,540] [train] [INFO] epoch=10.00 step=76700, 10.8385 examples/sec lr=0.000011, loss=24.7424, loss_ll=4.3852, loss_ll_paf=6.39296, loss_ll_heat=2.37745, q=1000
[2018-07-04 00:15:38,899] [train] [INFO] epoch=10.00 step=76800, 10.8382 examples/sec lr=0.000011, loss=18.6481, loss_ll=3.10835, loss_ll_paf=4.39828, loss_ll_heat=1.81842, q=1000
[2018-07-04 00:18:05,574] [train] [INFO] epoch=10.00 step=76900, 10.8383 examples/sec lr=0.000011, loss=18.4403, loss_ll=3.29242, loss_ll_paf=4.02306, loss_ll_heat=2.56177, q=1000
[2018-07-04 00:20:31,222] [train] [INFO] epoch=10.00 step=77000, 10.8385 examples/sec lr=0.000011, loss=29.1336, loss_ll=5.58232, loss_ll_paf=8.3127, loss_ll_heat=2.85194, q=1000
[2018-07-04 00:23:11,506] [train] [INFO] epoch=10.00 step=77100, 10.8373 examples/sec lr=0.000011, loss=18.2593, loss_ll=3.25479, loss_ll_paf=4.68457, loss_ll_heat=1.82501, q=1000
[2018-07-04 00:25:39,394] [train] [INFO] epoch=10.00 step=77200, 10.8373 examples/sec lr=0.000011, loss=19.1991, loss_ll=3.46207, loss_ll_paf=4.56424, loss_ll_heat=2.35991, q=1000
[2018-07-04 00:28:07,997] [train] [INFO] epoch=10.00 step=77300, 10.8372 examples/sec lr=0.000011, loss=24.6929, loss_ll=4.68069, loss_ll_paf=6.94855, loss_ll_heat=2.41283, q=1000
[2018-07-04 00:30:33,685] [train] [INFO] epoch=10.00 step=77400, 10.8373 examples/sec lr=0.000011, loss=19.4915, loss_ll=3.52556, loss_ll_paf=4.94049, loss_ll_heat=2.11063, q=1000
[2018-07-04 00:32:59,852] [train] [INFO] epoch=10.00 step=77500, 10.8375 examples/sec lr=0.000011, loss=17.892, loss_ll=3.25188, loss_ll_paf=4.28143, loss_ll_heat=2.22232, q=1000
[2018-07-04 00:35:24,894] [train] [INFO] epoch=10.00 step=77600, 10.8377 examples/sec lr=0.000011, loss=33.0957, loss_ll=5.72628, loss_ll_paf=8.84211, loss_ll_heat=2.61044, q=1000
[2018-07-04 00:37:50,797] [train] [INFO] epoch=10.00 step=77700, 10.8379 examples/sec lr=0.000011, loss=28.3408, loss_ll=5.39474, loss_ll_paf=8.49994, loss_ll_heat=2.28955, q=1000
[2018-07-04 00:40:16,323] [train] [INFO] epoch=10.00 step=77800, 10.8381 examples/sec lr=0.000011, loss=19.2694, loss_ll=3.24523, loss_ll_paf=4.41757, loss_ll_heat=2.07288, q=1000
[2018-07-04 00:42:42,357] [train] [INFO] epoch=10.00 step=77900, 10.8382 examples/sec lr=0.000011, loss=16.8307, loss_ll=2.83595, loss_ll_paf=4.35384, loss_ll_heat=1.31806, q=1000
[2018-07-04 00:45:11,951] [train] [INFO] epoch=10.00 step=78000, 10.8381 examples/sec lr=0.000011, loss=16.6101, loss_ll=2.83227, loss_ll_paf=3.31584, loss_ll_heat=2.3487, q=1000
[2018-07-04 00:47:51,035] [train] [INFO] epoch=10.00 step=78100, 10.8370 examples/sec lr=0.000011, loss=24.619, loss_ll=4.67732, loss_ll_paf=6.86816, loss_ll_heat=2.48648, q=1000
[2018-07-04 00:50:16,921] [train] [INFO] epoch=10.00 step=78200, 10.8371 examples/sec lr=0.000011, loss=26.4261, loss_ll=4.68437, loss_ll_paf=7.02561, loss_ll_heat=2.34312, q=1000
[2018-07-04 00:52:43,642] [train] [INFO] epoch=10.00 step=78300, 10.8372 examples/sec lr=0.000011, loss=19.9433, loss_ll=3.64904, loss_ll_paf=5.28384, loss_ll_heat=2.01423, q=1000
[2018-07-04 00:55:12,101] [train] [INFO] epoch=10.00 step=78400, 10.8372 examples/sec lr=0.000011, loss=18.8533, loss_ll=3.00919, loss_ll_paf=3.99376, loss_ll_heat=2.02462, q=1000
[2018-07-04 00:57:38,002] [train] [INFO] epoch=10.00 step=78500, 10.8373 examples/sec lr=0.000011, loss=24.9542, loss_ll=4.51257, loss_ll_paf=6.792, loss_ll_heat=2.23314, q=1000
[2018-07-04 01:00:06,910] [train] [INFO] epoch=10.00 step=78600, 10.8372 examples/sec lr=0.000011, loss=23.7358, loss_ll=4.25382, loss_ll_paf=5.86564, loss_ll_heat=2.642, q=1000
[2018-07-04 01:02:34,974] [train] [INFO] epoch=10.00 step=78700, 10.8372 examples/sec lr=0.000011, loss=19.5346, loss_ll=3.49805, loss_ll_paf=4.64652, loss_ll_heat=2.34958, q=1000
[2018-07-04 01:05:00,269] [train] [INFO] epoch=10.00 step=78800, 10.8374 examples/sec lr=0.000011, loss=20.6621, loss_ll=3.67124, loss_ll_paf=5.63837, loss_ll_heat=1.70411, q=1000
[2018-07-04 01:07:27,489] [train] [INFO] epoch=10.00 step=78900, 10.8374 examples/sec lr=0.000011, loss=17.8709, loss_ll=3.07447, loss_ll_paf=3.96604, loss_ll_heat=2.1829, q=1000
[2018-07-04 01:09:54,237] [train] [INFO] epoch=10.00 step=79000, 10.8375 examples/sec lr=0.000011, loss=33.7709, loss_ll=6.10593, loss_ll_paf=9.8875, loss_ll_heat=2.32436, q=1000
[2018-07-04 01:12:34,479] [train] [INFO] epoch=10.00 step=79100, 10.8363 examples/sec lr=0.000011, loss=20.5532, loss_ll=3.58619, loss_ll_paf=4.6954, loss_ll_heat=2.47699, q=1000
[2018-07-04 01:15:02,474] [train] [INFO] epoch=10.00 step=79200, 10.8363 examples/sec lr=0.000011, loss=23.4361, loss_ll=4.16606, loss_ll_paf=5.98127, loss_ll_heat=2.35086, q=1000
[2018-07-04 01:17:30,245] [train] [INFO] epoch=10.00 step=79300, 10.8363 examples/sec lr=0.000011, loss=19.4906, loss_ll=3.50001, loss_ll_paf=5.38108, loss_ll_heat=1.61894, q=1000
[2018-07-04 01:19:57,518] [train] [INFO] epoch=10.00 step=79400, 10.8363 examples/sec lr=0.000011, loss=15.9402, loss_ll=2.7879, loss_ll_paf=3.92352, loss_ll_heat=1.65227, q=1000
[2018-07-04 01:22:23,005] [train] [INFO] epoch=10.00 step=79500, 10.8365 examples/sec lr=0.000011, loss=22.2576, loss_ll=4.28841, loss_ll_paf=5.86356, loss_ll_heat=2.71327, q=1000
[2018-07-04 01:24:48,115] [train] [INFO] epoch=10.00 step=79600, 10.8368 examples/sec lr=0.000011, loss=19.3053, loss_ll=3.57181, loss_ll_paf=4.61312, loss_ll_heat=2.53051, q=1000
[2018-07-04 01:27:14,924] [train] [INFO] epoch=10.00 step=79700, 10.8368 examples/sec lr=0.000011, loss=14.9051, loss_ll=2.63361, loss_ll_paf=3.71827, loss_ll_heat=1.54894, q=1000
[2018-07-04 01:29:41,474] [train] [INFO] epoch=10.00 step=79800, 10.8369 examples/sec lr=0.000011, loss=20.1266, loss_ll=3.43378, loss_ll_paf=4.5318, loss_ll_heat=2.33577, q=1000
[2018-07-04 01:32:08,639] [train] [INFO] epoch=10.00 step=79900, 10.8370 examples/sec lr=0.000011, loss=21.1423, loss_ll=3.63828, loss_ll_paf=5.0271, loss_ll_heat=2.24946, q=1000
[2018-07-04 01:34:36,397] [train] [INFO] epoch=10.00 step=80000, 10.8370 examples/sec lr=0.000011, loss=24.4663, loss_ll=4.64444, loss_ll_paf=6.97346, loss_ll_heat=2.31543, q=1000
[2018-07-04 01:37:14,769] [train] [INFO] epoch=10.00 step=80100, 10.8360 examples/sec lr=0.000011, loss=17.4675, loss_ll=2.74911, loss_ll_paf=3.65748, loss_ll_heat=1.84075, q=1000
[2018-07-04 01:39:41,774] [train] [INFO] epoch=10.00 step=80200, 10.8360 examples/sec lr=0.000011, loss=24.0401, loss_ll=4.21375, loss_ll_paf=5.83046, loss_ll_heat=2.59705, q=1000
[2018-07-04 01:42:08,489] [train] [INFO] epoch=10.00 step=80300, 10.8361 examples/sec lr=0.000011, loss=21.6265, loss_ll=3.90067, loss_ll_paf=5.34815, loss_ll_heat=2.45318, q=1000
[2018-07-04 01:44:35,893] [train] [INFO] epoch=10.00 step=80400, 10.8362 examples/sec lr=0.000011, loss=18.1173, loss_ll=3.32734, loss_ll_paf=4.75128, loss_ll_heat=1.90339, q=1000
[2018-07-04 01:47:02,580] [train] [INFO] epoch=10.00 step=80500, 10.8362 examples/sec lr=0.000011, loss=26.2266, loss_ll=4.72282, loss_ll_paf=7.27695, loss_ll_heat=2.16868, q=1000
[2018-07-04 01:49:29,004] [train] [INFO] epoch=10.00 step=80600, 10.8364 examples/sec lr=0.000011, loss=26.0315, loss_ll=4.95004, loss_ll_paf=7.04784, loss_ll_heat=2.85223, q=1000
[2018-07-04 01:51:56,119] [train] [INFO] epoch=10.00 step=80700, 10.8364 examples/sec lr=0.000011, loss=23.4442, loss_ll=4.44371, loss_ll_paf=6.67377, loss_ll_heat=2.21364, q=1000
[2018-07-04 01:54:23,848] [train] [INFO] epoch=10.00 step=80800, 10.8364 examples/sec lr=0.000011, loss=18.272, loss_ll=3.35321, loss_ll_paf=4.85943, loss_ll_heat=1.847, q=1000
[2018-07-04 01:56:49,829] [train] [INFO] epoch=10.00 step=80900, 10.8365 examples/sec lr=0.000011, loss=25.4227, loss_ll=4.48201, loss_ll_paf=6.53321, loss_ll_heat=2.43082, q=1000
[2018-07-04 01:59:22,317] [train] [INFO] epoch=10.00 step=81000, 10.8361 examples/sec lr=0.000011, loss=18.5338, loss_ll=3.25143, loss_ll_paf=3.95259, loss_ll_heat=2.55028, q=1000
[2018-07-04 02:01:57,856] [train] [INFO] epoch=10.00 step=81100, 10.8354 examples/sec lr=0.000011, loss=20.2897, loss_ll=3.65159, loss_ll_paf=4.97689, loss_ll_heat=2.32628, q=1000
[2018-07-04 02:04:24,645] [train] [INFO] epoch=10.00 step=81200, 10.8355 examples/sec lr=0.000011, loss=18.893, loss_ll=3.30093, loss_ll_paf=4.77771, loss_ll_heat=1.82415, q=1000
[2018-07-04 02:06:50,148] [train] [INFO] epoch=10.00 step=81300, 10.8357 examples/sec lr=0.000011, loss=32.3262, loss_ll=5.86252, loss_ll_paf=8.95498, loss_ll_heat=2.77007, q=1000
[2018-07-04 02:09:17,344] [train] [INFO] epoch=10.00 step=81400, 10.8357 examples/sec lr=0.000011, loss=25.8674, loss_ll=4.51832, loss_ll_paf=6.47789, loss_ll_heat=2.55875, q=1000
[2018-07-04 02:11:44,644] [train] [INFO] epoch=10.00 step=81500, 10.8357 examples/sec lr=0.000011, loss=17.6656, loss_ll=3.11514, loss_ll_paf=4.44982, loss_ll_heat=1.78046, q=1000
[2018-07-04 02:14:12,439] [train] [INFO] epoch=10.00 step=81600, 10.8357 examples/sec lr=0.000011, loss=21.6018, loss_ll=4.11387, loss_ll_paf=6.2592, loss_ll_heat=1.96854, q=1000
[2018-07-04 02:16:36,950] [train] [INFO] epoch=10.00 step=81700, 10.8360 examples/sec lr=0.000011, loss=20.1604, loss_ll=3.46745, loss_ll_paf=4.67145, loss_ll_heat=2.26346, q=1000
[2018-07-04 02:19:03,712] [train] [INFO] epoch=10.00 step=81800, 10.8361 examples/sec lr=0.000011, loss=23.4922, loss_ll=3.91167, loss_ll_paf=5.68455, loss_ll_heat=2.13879, q=1000
[2018-07-04 02:21:30,669] [train] [INFO] epoch=10.00 step=81900, 10.8362 examples/sec lr=0.000011, loss=17.2377, loss_ll=3.14626, loss_ll_paf=4.72257, loss_ll_heat=1.56995, q=1000
[2018-07-04 02:23:58,816] [train] [INFO] epoch=10.00 step=82000, 10.8361 examples/sec lr=0.000011, loss=17.7243, loss_ll=3.08989, loss_ll_paf=4.01919, loss_ll_heat=2.16058, q=1000
[2018-07-04 02:26:37,639] [train] [INFO] epoch=10.00 step=82100, 10.8351 examples/sec lr=0.000011, loss=21.4674, loss_ll=3.69061, loss_ll_paf=4.99522, loss_ll_heat=2.386, q=1000
[2018-07-04 02:29:03,309] [train] [INFO] epoch=10.00 step=82200, 10.8353 examples/sec lr=0.000011, loss=20.3099, loss_ll=3.60142, loss_ll_paf=4.76463, loss_ll_heat=2.4382, q=1000
[2018-07-04 02:31:29,522] [train] [INFO] epoch=10.00 step=82300, 10.8354 examples/sec lr=0.000011, loss=22.2109, loss_ll=3.94818, loss_ll_paf=6.14606, loss_ll_heat=1.75029, q=1000
[2018-07-04 02:33:57,907] [train] [INFO] epoch=10.00 step=82400, 10.8354 examples/sec lr=0.000011, loss=17.9297, loss_ll=3.2035, loss_ll_paf=4.54048, loss_ll_heat=1.86652, q=1000
[2018-07-04 02:36:20,730] [train] [INFO] epoch=10.00 step=82500, 10.8358 examples/sec lr=0.000011, loss=19.0386, loss_ll=3.56981, loss_ll_paf=5.07974, loss_ll_heat=2.05988, q=1000
[2018-07-04 02:38:47,182] [train] [INFO] epoch=10.00 step=82600, 10.8359 examples/sec lr=0.000011, loss=31.6326, loss_ll=5.62856, loss_ll_paf=8.39556, loss_ll_heat=2.86157, q=1000
[2018-07-04 02:41:13,559] [train] [INFO] epoch=10.00 step=82700, 10.8360 examples/sec lr=0.000011, loss=27.2654, loss_ll=4.83056, loss_ll_paf=7.41858, loss_ll_heat=2.24254, q=1000
[2018-07-04 02:43:40,125] [train] [INFO] epoch=10.00 step=82800, 10.8361 examples/sec lr=0.000011, loss=19.6725, loss_ll=3.68547, loss_ll_paf=5.05057, loss_ll_heat=2.32037, q=1000
[2018-07-04 02:46:06,829] [train] [INFO] epoch=10.00 step=82900, 10.8362 examples/sec lr=0.000011, loss=35.6783, loss_ll=6.57667, loss_ll_paf=11.0072, loss_ll_heat=2.14617, q=1000
[2018-07-04 02:48:34,607] [train] [INFO] epoch=10.00 step=83000, 10.8362 examples/sec lr=0.000011, loss=21.2649, loss_ll=3.72191, loss_ll_paf=5.09123, loss_ll_heat=2.35259, q=1000
[2018-07-04 02:51:12,899] [train] [INFO] epoch=10.00 step=83100, 10.8352 examples/sec lr=0.000011, loss=26.0875, loss_ll=4.66062, loss_ll_paf=6.63142, loss_ll_heat=2.68982, q=1000
[2018-07-04 02:53:38,896] [train] [INFO] epoch=10.00 step=83200, 10.8354 examples/sec lr=0.000011, loss=25.8529, loss_ll=4.79646, loss_ll_paf=7.40687, loss_ll_heat=2.18605, q=1000
[2018-07-04 02:56:07,336] [train] [INFO] epoch=10.00 step=83300, 10.8353 examples/sec lr=0.000011, loss=14.7023, loss_ll=2.65375, loss_ll_paf=3.16317, loss_ll_heat=2.14432, q=1000
[2018-07-04 02:58:33,752] [train] [INFO] epoch=10.00 step=83400, 10.8354 examples/sec lr=0.000011, loss=19.2461, loss_ll=3.12677, loss_ll_paf=3.96058, loss_ll_heat=2.29295, q=1000
[2018-07-04 03:01:00,332] [train] [INFO] epoch=10.00 step=83500, 10.8355 examples/sec lr=0.000011, loss=27.889, loss_ll=5.14236, loss_ll_paf=7.64652, loss_ll_heat=2.63819, q=1000
[2018-07-04 03:03:27,961] [train] [INFO] epoch=10.00 step=83600, 10.8355 examples/sec lr=0.000011, loss=25.3253, loss_ll=4.60787, loss_ll_paf=6.92904, loss_ll_heat=2.28671, q=1000
[2018-07-04 03:05:54,252] [train] [INFO] epoch=11.00 step=83700, 10.8356 examples/sec lr=0.000011, loss=17.0939, loss_ll=2.83333, loss_ll_paf=4.17212, loss_ll_heat=1.49454, q=1000
[2018-07-04 03:08:22,664] [train] [INFO] epoch=11.00 step=83800, 10.8356 examples/sec lr=0.000011, loss=23.7064, loss_ll=4.22157, loss_ll_paf=6.34225, loss_ll_heat=2.10089, q=1000
[2018-07-04 03:10:49,757] [train] [INFO] epoch=11.00 step=83900, 10.8356 examples/sec lr=0.000011, loss=17.8668, loss_ll=3.4065, loss_ll_paf=4.46971, loss_ll_heat=2.34329, q=1000
[2018-07-04 03:13:14,351] [train] [INFO] epoch=11.00 step=84000, 10.8359 examples/sec lr=0.000011, loss=16.2735, loss_ll=2.75628, loss_ll_paf=3.85602, loss_ll_heat=1.65654, q=1000
[2018-07-04 03:15:50,366] [train] [INFO] epoch=11.00 step=84100, 10.8352 examples/sec lr=0.000011, loss=26.7713, loss_ll=4.60908, loss_ll_paf=7.01165, loss_ll_heat=2.20651, q=1000
[2018-07-04 03:18:18,441] [train] [INFO] epoch=11.00 step=84200, 10.8351 examples/sec lr=0.000011, loss=19.8452, loss_ll=3.35883, loss_ll_paf=4.73225, loss_ll_heat=1.98542, q=1000
[2018-07-04 03:20:48,527] [train] [INFO] epoch=11.00 step=84300, 10.8349 examples/sec lr=0.000011, loss=37.4875, loss_ll=6.84677, loss_ll_paf=10.286, loss_ll_heat=3.40756, q=1000
[2018-07-04 03:23:15,015] [train] [INFO] epoch=11.00 step=84400, 10.8350 examples/sec lr=0.000011, loss=30.0382, loss_ll=5.49952, loss_ll_paf=8.2895, loss_ll_heat=2.70954, q=1000
[2018-07-04 03:25:42,225] [train] [INFO] epoch=11.00 step=84500, 10.8351 examples/sec lr=0.000011, loss=20.6184, loss_ll=3.88629, loss_ll_paf=5.7034, loss_ll_heat=2.06918, q=1000
[2018-07-04 03:28:07,607] [train] [INFO] epoch=11.00 step=84600, 10.8353 examples/sec lr=0.000011, loss=24.1208, loss_ll=4.14166, loss_ll_paf=5.76, loss_ll_heat=2.52332, q=1000
[2018-07-04 03:30:33,796] [train] [INFO] epoch=11.00 step=84700, 10.8354 examples/sec lr=0.000011, loss=37.3768, loss_ll=7.06003, loss_ll_paf=10.7721, loss_ll_heat=3.34792, q=1000
[2018-07-04 03:33:02,998] [train] [INFO] epoch=11.00 step=84800, 10.8353 examples/sec lr=0.000011, loss=26.4593, loss_ll=4.73581, loss_ll_paf=6.81314, loss_ll_heat=2.65849, q=1000
[2018-07-04 03:35:32,279] [train] [INFO] epoch=11.00 step=84900, 10.8351 examples/sec lr=0.000011, loss=32.7926, loss_ll=5.94346, loss_ll_paf=9.19201, loss_ll_heat=2.69491, q=1000
[2018-07-04 03:38:01,906] [train] [INFO] epoch=11.00 step=85000, 10.8349 examples/sec lr=0.000011, loss=26.8682, loss_ll=5.00454, loss_ll_paf=8.11952, loss_ll_heat=1.88956, q=1000
[2018-07-04 03:40:38,938] [train] [INFO] epoch=11.00 step=85100, 10.8341 examples/sec lr=0.000011, loss=20.1971, loss_ll=3.77533, loss_ll_paf=5.28743, loss_ll_heat=2.26324, q=1000
[2018-07-04 03:43:05,340] [train] [INFO] epoch=11.00 step=85200, 10.8343 examples/sec lr=0.000011, loss=27.8779, loss_ll=5.23388, loss_ll_paf=7.62615, loss_ll_heat=2.8416, q=1000
[2018-07-04 03:45:34,271] [train] [INFO] epoch=11.00 step=85300, 10.8341 examples/sec lr=0.000011, loss=18.0632, loss_ll=3.249, loss_ll_paf=4.76626, loss_ll_heat=1.73175, q=1000
[2018-07-04 03:48:04,463] [train] [INFO] epoch=11.00 step=85400, 10.8339 examples/sec lr=0.000011, loss=26.8458, loss_ll=4.7537, loss_ll_paf=6.56169, loss_ll_heat=2.94571, q=1000
[2018-07-04 03:50:32,319] [train] [INFO] epoch=11.00 step=85500, 10.8339 examples/sec lr=0.000011, loss=20.6223, loss_ll=3.57651, loss_ll_paf=5.05184, loss_ll_heat=2.10119, q=1000
[2018-07-04 03:53:01,763] [train] [INFO] epoch=11.00 step=85600, 10.8338 examples/sec lr=0.000011, loss=22.9276, loss_ll=3.9709, loss_ll_paf=5.76796, loss_ll_heat=2.17383, q=1000
[2018-07-04 03:55:28,839] [train] [INFO] epoch=11.00 step=85700, 10.8338 examples/sec lr=0.000011, loss=18.4892, loss_ll=3.14323, loss_ll_paf=4.15386, loss_ll_heat=2.13259, q=1000
[2018-07-04 03:57:56,536] [train] [INFO] epoch=11.00 step=85800, 10.8338 examples/sec lr=0.000011, loss=27.8955, loss_ll=4.65342, loss_ll_paf=7.39176, loss_ll_heat=1.91507, q=1000
[2018-07-04 04:00:24,778] [train] [INFO] epoch=11.00 step=85900, 10.8338 examples/sec lr=0.000011, loss=17.2143, loss_ll=2.8737, loss_ll_paf=3.86085, loss_ll_heat=1.88655, q=1000
[2018-07-04 04:02:52,979] [train] [INFO] epoch=11.00 step=86000, 10.8337 examples/sec lr=0.000011, loss=14.2935, loss_ll=2.57726, loss_ll_paf=3.31305, loss_ll_heat=1.84146, q=1000
[2018-07-04 04:05:33,537] [train] [INFO] epoch=11.00 step=86100, 10.8326 examples/sec lr=0.000011, loss=18.7771, loss_ll=3.28643, loss_ll_paf=4.61576, loss_ll_heat=1.95709, q=1000
[2018-07-04 04:08:04,049] [train] [INFO] epoch=11.00 step=86200, 10.8324 examples/sec lr=0.000011, loss=18.9314, loss_ll=3.26319, loss_ll_paf=4.67012, loss_ll_heat=1.85627, q=1000
[2018-07-04 04:10:32,924] [train] [INFO] epoch=11.00 step=86300, 10.8323 examples/sec lr=0.000011, loss=28.1728, loss_ll=5.20834, loss_ll_paf=7.65075, loss_ll_heat=2.76592, q=1000
[2018-07-04 04:13:02,090] [train] [INFO] epoch=11.00 step=86400, 10.8322 examples/sec lr=0.000011, loss=16.9376, loss_ll=3.21641, loss_ll_paf=4.25017, loss_ll_heat=2.18266, q=1000
[2018-07-04 04:15:28,843] [train] [INFO] epoch=11.00 step=86500, 10.8322 examples/sec lr=0.000011, loss=18.686, loss_ll=3.30619, loss_ll_paf=4.50482, loss_ll_heat=2.10756, q=1000
[2018-07-04 04:17:56,714] [train] [INFO] epoch=11.00 step=86600, 10.8322 examples/sec lr=0.000011, loss=18.8926, loss_ll=3.54156, loss_ll_paf=5.27911, loss_ll_heat=1.80401, q=1000
[2018-07-04 04:20:29,196] [train] [INFO] epoch=11.00 step=86700, 10.8318 examples/sec lr=0.000011, loss=27.5929, loss_ll=4.97479, loss_ll_paf=7.41551, loss_ll_heat=2.53408, q=1000
[2018-07-04 04:23:01,144] [train] [INFO] epoch=11.00 step=86800, 10.8315 examples/sec lr=0.000011, loss=17.4965, loss_ll=3.08728, loss_ll_paf=4.30308, loss_ll_heat=1.87148, q=1000
[2018-07-04 04:25:30,617] [train] [INFO] epoch=11.00 step=86900, 10.8313 examples/sec lr=0.000011, loss=24.313, loss_ll=4.3444, loss_ll_paf=6.2805, loss_ll_heat=2.4083, q=1000
[2018-07-04 04:28:01,328] [train] [INFO] epoch=11.00 step=87000, 10.8311 examples/sec lr=0.000011, loss=22.3562, loss_ll=3.69453, loss_ll_paf=5.10782, loss_ll_heat=2.28124, q=1000
[2018-07-04 04:30:40,806] [train] [INFO] epoch=11.00 step=87100, 10.8301 examples/sec lr=0.000011, loss=29.8859, loss_ll=5.17748, loss_ll_paf=8.17984, loss_ll_heat=2.17512, q=1000
[2018-07-04 04:33:08,262] [train] [INFO] epoch=11.00 step=87200, 10.8301 examples/sec lr=0.000011, loss=36.4906, loss_ll=6.82951, loss_ll_paf=10.9196, loss_ll_heat=2.73941, q=1000
[2018-07-04 04:35:35,159] [train] [INFO] epoch=11.00 step=87300, 10.8302 examples/sec lr=0.000011, loss=18.7378, loss_ll=3.27501, loss_ll_paf=4.63805, loss_ll_heat=1.91197, q=1000
[2018-07-04 04:38:03,534] [train] [INFO] epoch=11.00 step=87400, 10.8301 examples/sec lr=0.000011, loss=14.3438, loss_ll=2.42269, loss_ll_paf=3.19922, loss_ll_heat=1.64617, q=1000
[2018-07-04 04:40:30,361] [train] [INFO] epoch=11.00 step=87500, 10.8302 examples/sec lr=0.000011, loss=16.8462, loss_ll=3.07002, loss_ll_paf=4.1046, loss_ll_heat=2.03544, q=1000
[2018-07-04 04:42:59,315] [train] [INFO] epoch=11.00 step=87600, 10.8301 examples/sec lr=0.000011, loss=19.0754, loss_ll=3.44585, loss_ll_paf=4.86822, loss_ll_heat=2.02348, q=1000
[2018-07-04 04:45:24,992] [train] [INFO] epoch=11.00 step=87700, 10.8303 examples/sec lr=0.000011, loss=29.5337, loss_ll=5.57663, loss_ll_paf=9.03777, loss_ll_heat=2.11548, q=1000
[2018-07-04 04:47:55,083] [train] [INFO] epoch=11.00 step=87800, 10.8301 examples/sec lr=0.000011, loss=24.4309, loss_ll=4.10099, loss_ll_paf=6.10301, loss_ll_heat=2.09897, q=1000
[2018-07-04 04:50:24,685] [train] [INFO] epoch=11.00 step=87900, 10.8299 examples/sec lr=0.000011, loss=20.9521, loss_ll=3.73036, loss_ll_paf=5.37254, loss_ll_heat=2.08817, q=1000
[2018-07-04 04:52:54,458] [train] [INFO] epoch=11.00 step=88000, 10.8297 examples/sec lr=0.000011, loss=18.6884, loss_ll=3.39895, loss_ll_paf=4.7172, loss_ll_heat=2.0807, q=1000
[2018-07-04 04:55:36,863] [train] [INFO] epoch=11.00 step=88100, 10.8285 examples/sec lr=0.000011, loss=26.6392, loss_ll=4.94196, loss_ll_paf=7.02905, loss_ll_heat=2.85486, q=1000
[2018-07-04 04:58:04,148] [train] [INFO] epoch=11.00 step=88200, 10.8286 examples/sec lr=0.000011, loss=30.5864, loss_ll=5.19461, loss_ll_paf=8.25464, loss_ll_heat=2.13458, q=1000
[2018-07-04 05:00:32,190] [train] [INFO] epoch=11.00 step=88300, 10.8285 examples/sec lr=0.000011, loss=24.3239, loss_ll=3.84729, loss_ll_paf=5.65198, loss_ll_heat=2.04261, q=1000
[2018-07-04 05:02:59,145] [train] [INFO] epoch=11.00 step=88400, 10.8286 examples/sec lr=0.000011, loss=26.2721, loss_ll=4.94926, loss_ll_paf=8.09501, loss_ll_heat=1.80351, q=1000
[2018-07-04 05:05:26,391] [train] [INFO] epoch=11.00 step=88500, 10.8286 examples/sec lr=0.000011, loss=23.0244, loss_ll=3.85273, loss_ll_paf=5.48259, loss_ll_heat=2.22286, q=1000
[2018-07-04 05:07:57,688] [train] [INFO] epoch=11.00 step=88600, 10.8284 examples/sec lr=0.000011, loss=24.5816, loss_ll=4.26554, loss_ll_paf=6.05582, loss_ll_heat=2.47526, q=1000
[2018-07-04 05:10:24,854] [train] [INFO] epoch=11.00 step=88700, 10.8284 examples/sec lr=0.000011, loss=27.6604, loss_ll=4.51177, loss_ll_paf=6.62901, loss_ll_heat=2.39452, q=1000
[2018-07-04 05:12:54,399] [train] [INFO] epoch=11.00 step=88800, 10.8283 examples/sec lr=0.000011, loss=21.21, loss_ll=3.87627, loss_ll_paf=5.22473, loss_ll_heat=2.52781, q=1000
[2018-07-04 05:15:24,045] [train] [INFO] epoch=11.00 step=88900, 10.8281 examples/sec lr=0.000011, loss=24.1881, loss_ll=4.24926, loss_ll_paf=5.69012, loss_ll_heat=2.80839, q=1000
[2018-07-04 05:17:54,341] [train] [INFO] epoch=11.00 step=89000, 10.8279 examples/sec lr=0.000011, loss=19.3921, loss_ll=3.29213, loss_ll_paf=4.23192, loss_ll_heat=2.35233, q=1000
[2018-07-04 05:20:33,976] [train] [INFO] epoch=11.00 step=89100, 10.8269 examples/sec lr=0.000011, loss=22.2125, loss_ll=3.92605, loss_ll_paf=5.31425, loss_ll_heat=2.53786, q=1000
[2018-07-04 05:23:03,366] [train] [INFO] epoch=11.00 step=89200, 10.8268 examples/sec lr=0.000011, loss=13.7915, loss_ll=2.31912, loss_ll_paf=2.79504, loss_ll_heat=1.84319, q=1000
[2018-07-04 05:25:32,271] [train] [INFO] epoch=11.00 step=89300, 10.8267 examples/sec lr=0.000011, loss=25.9693, loss_ll=4.63922, loss_ll_paf=7.15374, loss_ll_heat=2.12471, q=1000
[2018-07-04 05:28:02,743] [train] [INFO] epoch=11.00 step=89400, 10.8265 examples/sec lr=0.000011, loss=21.5268, loss_ll=4.0606, loss_ll_paf=6.16964, loss_ll_heat=1.95155, q=1000
[2018-07-04 05:30:30,758] [train] [INFO] epoch=11.00 step=89500, 10.8265 examples/sec lr=0.000011, loss=16.3375, loss_ll=2.92574, loss_ll_paf=3.48869, loss_ll_heat=2.36279, q=1000
[2018-07-04 05:33:00,811] [train] [INFO] epoch=11.00 step=89600, 10.8263 examples/sec lr=0.000011, loss=30.4445, loss_ll=5.57566, loss_ll_paf=8.23579, loss_ll_heat=2.91554, q=1000
[2018-07-04 05:35:28,726] [train] [INFO] epoch=11.00 step=89700, 10.8263 examples/sec lr=0.000011, loss=32.7147, loss_ll=5.99062, loss_ll_paf=9.46227, loss_ll_heat=2.51897, q=1000
[2018-07-04 05:37:59,533] [train] [INFO] epoch=11.00 step=89800, 10.8260 examples/sec lr=0.000011, loss=22.9138, loss_ll=4.04602, loss_ll_paf=5.42257, loss_ll_heat=2.66947, q=1000
[2018-07-04 05:40:27,439] [train] [INFO] epoch=11.00 step=89900, 10.8260 examples/sec lr=0.000011, loss=18.0805, loss_ll=3.38324, loss_ll_paf=4.36194, loss_ll_heat=2.40454, q=1000
[2018-07-04 05:42:57,129] [train] [INFO] epoch=11.00 step=90000, 10.8258 examples/sec lr=0.000004, loss=18.0947, loss_ll=3.18017, loss_ll_paf=4.23322, loss_ll_heat=2.12712, q=1000
[2018-07-04 05:45:36,706] [train] [INFO] epoch=11.00 step=90100, 10.8249 examples/sec lr=0.000004, loss=29.3292, loss_ll=5.11996, loss_ll_paf=7.43396, loss_ll_heat=2.80597, q=1000
[2018-07-04 05:48:06,510] [train] [INFO] epoch=11.00 step=90200, 10.8247 examples/sec lr=0.000004, loss=45.812, loss_ll=8.33803, loss_ll_paf=12.4642, loss_ll_heat=4.21182, q=1000
[2018-07-04 05:50:37,363] [train] [INFO] epoch=11.00 step=90300, 10.8245 examples/sec lr=0.000004, loss=15.7808, loss_ll=2.663, loss_ll_paf=3.08043, loss_ll_heat=2.24557, q=1000
[2018-07-04 05:53:07,205] [train] [INFO] epoch=11.00 step=90400, 10.8243 examples/sec lr=0.000004, loss=40.494, loss_ll=7.43391, loss_ll_paf=11.6978, loss_ll_heat=3.17005, q=1000
[2018-07-04 05:55:33,182] [train] [INFO] epoch=11.00 step=90500, 10.8245 examples/sec lr=0.000004, loss=19.4062, loss_ll=3.52257, loss_ll_paf=4.59152, loss_ll_heat=2.45361, q=1000
[2018-07-04 05:58:03,303] [train] [INFO] epoch=11.00 step=90600, 10.8243 examples/sec lr=0.000004, loss=20.1517, loss_ll=3.62469, loss_ll_paf=4.40961, loss_ll_heat=2.83977, q=1000
[2018-07-04 06:00:34,247] [train] [INFO] epoch=11.00 step=90700, 10.8240 examples/sec lr=0.000004, loss=22.6428, loss_ll=4.08557, loss_ll_paf=5.47031, loss_ll_heat=2.70083, q=1000
[2018-07-04 06:03:04,797] [train] [INFO] epoch=11.00 step=90800, 10.8238 examples/sec lr=0.000004, loss=18.3484, loss_ll=3.47424, loss_ll_paf=4.91891, loss_ll_heat=2.02956, q=1000
[2018-07-04 06:05:35,659] [train] [INFO] epoch=11.00 step=90900, 10.8236 examples/sec lr=0.000004, loss=18.864, loss_ll=3.37554, loss_ll_paf=5.01216, loss_ll_heat=1.73892, q=1000
[2018-07-04 06:08:05,823] [train] [INFO] epoch=11.00 step=91000, 10.8234 examples/sec lr=0.000004, loss=20.4639, loss_ll=3.81747, loss_ll_paf=5.53931, loss_ll_heat=2.09562, q=1000
[2018-07-04 06:10:46,645] [train] [INFO] epoch=11.00 step=91100, 10.8223 examples/sec lr=0.000004, loss=18.7135, loss_ll=3.39989, loss_ll_paf=4.7363, loss_ll_heat=2.06349, q=1000
[2018-07-04 06:13:15,886] [train] [INFO] epoch=11.00 step=91200, 10.8222 examples/sec lr=0.000004, loss=21.533, loss_ll=3.68391, loss_ll_paf=5.30096, loss_ll_heat=2.06686, q=1000
[2018-07-04 06:15:45,289] [train] [INFO] epoch=11.00 step=91300, 10.8221 examples/sec lr=0.000004, loss=24.8466, loss_ll=4.57588, loss_ll_paf=6.50019, loss_ll_heat=2.65158, q=1000
[2018-07-04 06:18:15,655] [train] [INFO] epoch=12.00 step=91400, 10.8219 examples/sec lr=0.000004, loss=21.6264, loss_ll=3.68473, loss_ll_paf=5.11835, loss_ll_heat=2.25112, q=1000
[2018-07-04 06:20:44,637] [train] [INFO] epoch=12.00 step=91500, 10.8218 examples/sec lr=0.000004, loss=15.8468, loss_ll=2.85231, loss_ll_paf=3.67257, loss_ll_heat=2.03205, q=1000
[2018-07-04 06:23:16,933] [train] [INFO] epoch=12.00 step=91600, 10.8214 examples/sec lr=0.000004, loss=22.4124, loss_ll=3.88166, loss_ll_paf=5.13742, loss_ll_heat=2.62589, q=1000
[2018-07-04 06:25:46,454] [train] [INFO] epoch=12.00 step=91700, 10.8213 examples/sec lr=0.000004, loss=21.6246, loss_ll=3.70847, loss_ll_paf=5.06648, loss_ll_heat=2.35046, q=1000
[2018-07-04 06:28:18,483] [train] [INFO] epoch=12.00 step=91800, 10.8210 examples/sec lr=0.000004, loss=17.7756, loss_ll=3.09961, loss_ll_paf=4.42478, loss_ll_heat=1.77444, q=1000
[2018-07-04 06:30:48,539] [train] [INFO] epoch=12.00 step=91900, 10.8208 examples/sec lr=0.000004, loss=24.5596, loss_ll=4.53399, loss_ll_paf=6.5282, loss_ll_heat=2.53977, q=1000
[2018-07-04 06:33:19,175] [train] [INFO] epoch=12.00 step=92000, 10.8206 examples/sec lr=0.000004, loss=19.58, loss_ll=3.38328, loss_ll_paf=4.98101, loss_ll_heat=1.78555, q=1000
[2018-07-04 06:36:00,933] [train] [INFO] epoch=12.00 step=92100, 10.8195 examples/sec lr=0.000004, loss=23.3469, loss_ll=4.21798, loss_ll_paf=5.86363, loss_ll_heat=2.57233, q=1000
[2018-07-04 06:38:30,321] [train] [INFO] epoch=12.00 step=92200, 10.8194 examples/sec lr=0.000004, loss=13.5798, loss_ll=2.4424, loss_ll_paf=3.10752, loss_ll_heat=1.77728, q=1000
[2018-07-04 06:41:00,819] [train] [INFO] epoch=12.00 step=92300, 10.8192 examples/sec lr=0.000004, loss=20.196, loss_ll=3.52138, loss_ll_paf=5.1064, loss_ll_heat=1.93636, q=1000
[2018-07-04 06:43:30,375] [train] [INFO] epoch=12.00 step=92400, 10.8190 examples/sec lr=0.000004, loss=26.624, loss_ll=4.96376, loss_ll_paf=7.25127, loss_ll_heat=2.67624, q=1000
[2018-07-04 06:45:57,721] [train] [INFO] epoch=12.00 step=92500, 10.8191 examples/sec lr=0.000004, loss=22.667, loss_ll=4.27155, loss_ll_paf=6.3326, loss_ll_heat=2.21051, q=1000
[2018-07-04 06:48:29,726] [train] [INFO] epoch=12.00 step=92600, 10.8187 examples/sec lr=0.000004, loss=21.4069, loss_ll=3.49007, loss_ll_paf=4.90881, loss_ll_heat=2.07134, q=1000
[2018-07-04 06:50:59,965] [train] [INFO] epoch=12.00 step=92700, 10.8186 examples/sec lr=0.000004, loss=24.0307, loss_ll=4.09688, loss_ll_paf=6.52713, loss_ll_heat=1.66662, q=1000
[2018-07-04 06:53:30,327] [train] [INFO] epoch=12.00 step=92800, 10.8184 examples/sec lr=0.000004, loss=25.7984, loss_ll=4.60887, loss_ll_paf=7.09815, loss_ll_heat=2.11958, q=1000
[2018-07-04 06:55:59,546] [train] [INFO] epoch=12.00 step=92900, 10.8183 examples/sec lr=0.000004, loss=23.2159, loss_ll=4.20948, loss_ll_paf=6.36457, loss_ll_heat=2.0544, q=1000
[2018-07-04 06:58:30,775] [train] [INFO] epoch=12.00 step=93000, 10.8180 examples/sec lr=0.000004, loss=17.9327, loss_ll=3.29437, loss_ll_paf=4.33565, loss_ll_heat=2.25308, q=1000
[2018-07-04 07:01:12,736] [train] [INFO] epoch=12.00 step=93100, 10.8169 examples/sec lr=0.000004, loss=20.6082, loss_ll=3.74254, loss_ll_paf=5.81073, loss_ll_heat=1.67436, q=1000
[2018-07-04 07:03:42,462] [train] [INFO] epoch=12.00 step=93200, 10.8167 examples/sec lr=0.000004, loss=15.3944, loss_ll=2.32755, loss_ll_paf=3.07611, loss_ll_heat=1.57899, q=1000
[2018-07-04 07:06:13,500] [train] [INFO] epoch=12.00 step=93300, 10.8165 examples/sec lr=0.000004, loss=24.6099, loss_ll=4.38586, loss_ll_paf=6.46475, loss_ll_heat=2.30697, q=1000
[2018-07-04 07:08:44,227] [train] [INFO] epoch=12.00 step=93400, 10.8163 examples/sec lr=0.000004, loss=25.1665, loss_ll=4.55312, loss_ll_paf=5.78935, loss_ll_heat=3.31688, q=1000
[2018-07-04 07:11:13,851] [train] [INFO] epoch=12.00 step=93500, 10.8161 examples/sec lr=0.000004, loss=18.113, loss_ll=3.28876, loss_ll_paf=4.75391, loss_ll_heat=1.82361, q=1000
[2018-07-04 07:13:43,585] [train] [INFO] epoch=12.00 step=93600, 10.8160 examples/sec lr=0.000004, loss=14.4228, loss_ll=2.54753, loss_ll_paf=3.21693, loss_ll_heat=1.87813, q=1000
[2018-07-04 07:16:15,758] [train] [INFO] epoch=12.00 step=93700, 10.8157 examples/sec lr=0.000004, loss=25.3432, loss_ll=4.99595, loss_ll_paf=6.92937, loss_ll_heat=3.06253, q=1000
[2018-07-04 07:18:46,865] [train] [INFO] epoch=12.00 step=93800, 10.8154 examples/sec lr=0.000004, loss=30.3795, loss_ll=5.63674, loss_ll_paf=8.68176, loss_ll_heat=2.59172, q=1000
[2018-07-04 07:21:17,415] [train] [INFO] epoch=12.00 step=93900, 10.8152 examples/sec lr=0.000004, loss=16.8372, loss_ll=2.90411, loss_ll_paf=3.69304, loss_ll_heat=2.11519, q=1000
[2018-07-04 07:23:47,977] [train] [INFO] epoch=12.00 step=94000, 10.8150 examples/sec lr=0.000004, loss=17.0269, loss_ll=3.10028, loss_ll_paf=4.20838, loss_ll_heat=1.99218, q=1000
[2018-07-04 07:26:33,832] [train] [INFO] epoch=12.00 step=94100, 10.8136 examples/sec lr=0.000004, loss=23.7143, loss_ll=4.3519, loss_ll_paf=6.56752, loss_ll_heat=2.13627, q=1000
[2018-07-04 07:29:06,806] [train] [INFO] epoch=12.00 step=94200, 10.8132 examples/sec lr=0.000004, loss=17.1928, loss_ll=3.0022, loss_ll_paf=3.95435, loss_ll_heat=2.05006, q=1000
[2018-07-04 07:31:37,627] [train] [INFO] epoch=12.00 step=94300, 10.8130 examples/sec lr=0.000004, loss=27.6681, loss_ll=5.31782, loss_ll_paf=8.52704, loss_ll_heat=2.10859, q=1000
[2018-07-04 07:34:10,898] [train] [INFO] epoch=12.00 step=94400, 10.8126 examples/sec lr=0.000004, loss=16.4314, loss_ll=2.86233, loss_ll_paf=3.92528, loss_ll_heat=1.79938, q=1000
[2018-07-04 07:36:41,528] [train] [INFO] epoch=12.00 step=94500, 10.8124 examples/sec lr=0.000004, loss=26.6478, loss_ll=4.64994, loss_ll_paf=7.2606, loss_ll_heat=2.03927, q=1000
[2018-07-04 07:39:15,565] [train] [INFO] epoch=12.00 step=94600, 10.8119 examples/sec lr=0.000004, loss=29.653, loss_ll=5.53633, loss_ll_paf=8.32608, loss_ll_heat=2.74659, q=1000
[2018-07-04 07:41:46,681] [train] [INFO] epoch=12.00 step=94700, 10.8117 examples/sec lr=0.000004, loss=21.2805, loss_ll=3.80563, loss_ll_paf=5.76702, loss_ll_heat=1.84423, q=1000
[2018-07-04 07:44:17,411] [train] [INFO] epoch=12.00 step=94800, 10.8115 examples/sec lr=0.000004, loss=19.2133, loss_ll=3.51492, loss_ll_paf=5.36455, loss_ll_heat=1.66529, q=1000
[2018-07-04 07:46:46,355] [train] [INFO] epoch=12.00 step=94900, 10.8114 examples/sec lr=0.000004, loss=18.6959, loss_ll=3.23059, loss_ll_paf=4.48583, loss_ll_heat=1.97535, q=1000
[2018-07-04 07:49:17,363] [train] [INFO] epoch=12.00 step=95000, 10.8112 examples/sec lr=0.000004, loss=19.4282, loss_ll=3.50863, loss_ll_paf=4.70929, loss_ll_heat=2.30796, q=1000
[2018-07-04 07:52:02,172] [train] [INFO] epoch=12.00 step=95100, 10.8099 examples/sec lr=0.000004, loss=26.2045, loss_ll=4.77051, loss_ll_paf=7.46157, loss_ll_heat=2.07945, q=1000
[2018-07-04 07:54:36,402] [train] [INFO] epoch=12.00 step=95200, 10.8094 examples/sec lr=0.000004, loss=28.4704, loss_ll=5.16462, loss_ll_paf=8.13119, loss_ll_heat=2.19805, q=1000
[2018-07-04 07:57:06,167] [train] [INFO] epoch=12.00 step=95300, 10.8093 examples/sec lr=0.000004, loss=38.0983, loss_ll=6.93058, loss_ll_paf=11.5748, loss_ll_heat=2.28633, q=1000
[2018-07-04 07:59:37,180] [train] [INFO] epoch=12.00 step=95400, 10.8090 examples/sec lr=0.000004, loss=23.6418, loss_ll=4.08288, loss_ll_paf=6.00687, loss_ll_heat=2.15888, q=1000
[2018-07-04 08:02:07,851] [train] [INFO] epoch=12.00 step=95500, 10.8088 examples/sec lr=0.000004, loss=18.9019, loss_ll=3.16549, loss_ll_paf=4.02991, loss_ll_heat=2.30106, q=1000
[2018-07-04 08:04:41,860] [train] [INFO] epoch=12.00 step=95600, 10.8084 examples/sec lr=0.000004, loss=17.8325, loss_ll=3.22458, loss_ll_paf=4.94608, loss_ll_heat=1.50309, q=1000
[2018-07-04 08:07:13,696] [train] [INFO] epoch=12.00 step=95700, 10.8081 examples/sec lr=0.000004, loss=20.2875, loss_ll=3.5349, loss_ll_paf=5.18957, loss_ll_heat=1.88022, q=1000
[2018-07-04 08:09:46,124] [train] [INFO] epoch=12.00 step=95800, 10.8078 examples/sec lr=0.000004, loss=21.1479, loss_ll=3.76176, loss_ll_paf=5.1689, loss_ll_heat=2.35461, q=1000
[2018-07-04 08:12:18,184] [train] [INFO] epoch=12.00 step=95900, 10.8075 examples/sec lr=0.000004, loss=23.7031, loss_ll=4.25673, loss_ll_paf=6.40022, loss_ll_heat=2.11323, q=1000
[2018-07-04 08:14:49,856] [train] [INFO] epoch=12.00 step=96000, 10.8072 examples/sec lr=0.000004, loss=13.0753, loss_ll=2.06635, loss_ll_paf=2.54046, loss_ll_heat=1.59223, q=1000
[2018-07-04 08:17:31,810] [train] [INFO] epoch=12.00 step=96100, 10.8061 examples/sec lr=0.000004, loss=20.123, loss_ll=3.58494, loss_ll_paf=5.36598, loss_ll_heat=1.8039, q=1000
[2018-07-04 08:20:04,063] [train] [INFO] epoch=12.00 step=96200, 10.8058 examples/sec lr=0.000004, loss=23.2235, loss_ll=3.79095, loss_ll_paf=5.59773, loss_ll_heat=1.98418, q=1000
[2018-07-04 08:22:35,016] [train] [INFO] epoch=12.00 step=96300, 10.8056 examples/sec lr=0.000004, loss=25.3936, loss_ll=4.52245, loss_ll_paf=6.56304, loss_ll_heat=2.48187, q=1000
[2018-07-04 08:25:07,887] [train] [INFO] epoch=12.00 step=96400, 10.8052 examples/sec lr=0.000004, loss=18.8492, loss_ll=3.33098, loss_ll_paf=4.71872, loss_ll_heat=1.94324, q=1000
[2018-07-04 08:27:39,523] [train] [INFO] epoch=12.00 step=96500, 10.8050 examples/sec lr=0.000004, loss=29.5273, loss_ll=5.35026, loss_ll_paf=8.66244, loss_ll_heat=2.03808, q=1000
[2018-07-04 08:30:12,476] [train] [INFO] epoch=12.00 step=96600, 10.8046 examples/sec lr=0.000004, loss=17.2367, loss_ll=2.77013, loss_ll_paf=3.70214, loss_ll_heat=1.83813, q=1000
[2018-07-04 08:32:43,636] [train] [INFO] epoch=12.00 step=96700, 10.8044 examples/sec lr=0.000004, loss=30.866, loss_ll=5.36547, loss_ll_paf=7.68094, loss_ll_heat=3.04999, q=1000
[2018-07-04 08:35:18,186] [train] [INFO] epoch=12.00 step=96800, 10.8039 examples/sec lr=0.000004, loss=19.3267, loss_ll=3.50077, loss_ll_paf=4.98717, loss_ll_heat=2.01436, q=1000
[2018-07-04 08:37:48,145] [train] [INFO] epoch=12.00 step=96900, 10.8037 examples/sec lr=0.000004, loss=23.1023, loss_ll=4.08336, loss_ll_paf=6.16324, loss_ll_heat=2.00349, q=1000
[2018-07-04 08:40:22,686] [train] [INFO] epoch=12.00 step=97000, 10.8032 examples/sec lr=0.000004, loss=22.2043, loss_ll=3.75394, loss_ll_paf=5.40133, loss_ll_heat=2.10654, q=1000
[2018-07-04 08:43:08,621] [train] [INFO] epoch=12.00 step=97100, 10.8019 examples/sec lr=0.000004, loss=18.8985, loss_ll=3.20793, loss_ll_paf=4.65647, loss_ll_heat=1.75939, q=1000
[2018-07-04 08:45:41,851] [train] [INFO] epoch=12.00 step=97200, 10.8015 examples/sec lr=0.000004, loss=19.535, loss_ll=3.3439, loss_ll_paf=4.76077, loss_ll_heat=1.92704, q=1000
[2018-07-04 08:48:13,817] [train] [INFO] epoch=12.00 step=97300, 10.8012 examples/sec lr=0.000004, loss=25.1739, loss_ll=4.63189, loss_ll_paf=7.34411, loss_ll_heat=1.91966, q=1000
[2018-07-04 08:50:45,458] [train] [INFO] epoch=12.00 step=97400, 10.8010 examples/sec lr=0.000004, loss=16.5474, loss_ll=3.10772, loss_ll_paf=4.12845, loss_ll_heat=2.08698, q=1000
[2018-07-04 08:53:15,488] [train] [INFO] epoch=12.00 step=97500, 10.8008 examples/sec lr=0.000004, loss=26.1674, loss_ll=4.82352, loss_ll_paf=6.85168, loss_ll_heat=2.79536, q=1000
[2018-07-04 08:55:49,044] [train] [INFO] epoch=12.00 step=97600, 10.8004 examples/sec lr=0.000004, loss=21.8717, loss_ll=4.21684, loss_ll_paf=6.7694, loss_ll_heat=1.66428, q=1000
[2018-07-04 08:58:20,454] [train] [INFO] epoch=12.00 step=97700, 10.8002 examples/sec lr=0.000004, loss=26.5317, loss_ll=4.86098, loss_ll_paf=6.90551, loss_ll_heat=2.81646, q=1000
[2018-07-04 09:00:53,414] [train] [INFO] epoch=12.00 step=97800, 10.7998 examples/sec lr=0.000004, loss=31.2475, loss_ll=5.7703, loss_ll_paf=9.16389, loss_ll_heat=2.3767, q=1000
[2018-07-04 09:03:24,252] [train] [INFO] epoch=12.00 step=97900, 10.7996 examples/sec lr=0.000004, loss=19.2315, loss_ll=3.47511, loss_ll_paf=4.61462, loss_ll_heat=2.33561, q=1000
[2018-07-04 09:05:57,289] [train] [INFO] epoch=12.00 step=98000, 10.7993 examples/sec lr=0.000004, loss=20.9427, loss_ll=3.98673, loss_ll_paf=5.39864, loss_ll_heat=2.57482, q=1000
[2018-07-04 09:08:39,260] [train] [INFO] epoch=12.00 step=98100, 10.7982 examples/sec lr=0.000004, loss=18.6282, loss_ll=3.24957, loss_ll_paf=4.48667, loss_ll_heat=2.01247, q=1000
[2018-07-04 09:11:11,214] [train] [INFO] epoch=12.00 step=98200, 10.7980 examples/sec lr=0.000004, loss=19.2563, loss_ll=3.56079, loss_ll_paf=5.04103, loss_ll_heat=2.08056, q=1000
[2018-07-04 09:13:40,155] [train] [INFO] epoch=12.00 step=98300, 10.7979 examples/sec lr=0.000004, loss=24.3951, loss_ll=4.10106, loss_ll_paf=5.9276, loss_ll_heat=2.27452, q=1000
[2018-07-04 09:16:12,850] [train] [INFO] epoch=12.00 step=98400, 10.7976 examples/sec lr=0.000004, loss=22.4534, loss_ll=3.8455, loss_ll_paf=5.40083, loss_ll_heat=2.29016, q=1000
[2018-07-04 09:18:44,585] [train] [INFO] epoch=12.00 step=98500, 10.7973 examples/sec lr=0.000004, loss=22.0651, loss_ll=3.71537, loss_ll_paf=5.75489, loss_ll_heat=1.67585, q=1000
[2018-07-04 09:21:17,853] [train] [INFO] epoch=12.00 step=98600, 10.7969 examples/sec lr=0.000004, loss=17.3279, loss_ll=3.10462, loss_ll_paf=4.39918, loss_ll_heat=1.81005, q=1000
[2018-07-04 09:23:49,472] [train] [INFO] epoch=12.00 step=98700, 10.7967 examples/sec lr=0.000004, loss=22.6265, loss_ll=4.43714, loss_ll_paf=6.36029, loss_ll_heat=2.51398, q=1000
[2018-07-04 09:26:24,303] [train] [INFO] epoch=12.00 step=98800, 10.7962 examples/sec lr=0.000004, loss=19.1156, loss_ll=3.43119, loss_ll_paf=5.09109, loss_ll_heat=1.77129, q=1000
[2018-07-04 09:28:57,713] [train] [INFO] epoch=12.00 step=98900, 10.7958 examples/sec lr=0.000004, loss=22.4755, loss_ll=4.25098, loss_ll_paf=6.04248, loss_ll_heat=2.45947, q=1000
[2018-07-04 09:31:30,140] [train] [INFO] epoch=13.00 step=99000, 10.7955 examples/sec lr=0.000004, loss=22.7737, loss_ll=3.97637, loss_ll_paf=5.28794, loss_ll_heat=2.66479, q=1000
[2018-07-04 09:34:16,692] [train] [INFO] epoch=13.00 step=99100, 10.7941 examples/sec lr=0.000004, loss=26.5363, loss_ll=4.9243, loss_ll_paf=7.51922, loss_ll_heat=2.32938, q=1000
[2018-07-04 09:36:50,158] [train] [INFO] epoch=13.00 step=99200, 10.7938 examples/sec lr=0.000004, loss=34.3762, loss_ll=6.67167, loss_ll_paf=10.2941, loss_ll_heat=3.04927, q=1000
[2018-07-04 09:39:23,339] [train] [INFO] epoch=13.00 step=99300, 10.7934 examples/sec lr=0.000004, loss=26.3771, loss_ll=4.59876, loss_ll_paf=7.0109, loss_ll_heat=2.18662, q=1000
[2018-07-04 09:41:56,890] [train] [INFO] epoch=13.00 step=99400, 10.7930 examples/sec lr=0.000004, loss=24.4206, loss_ll=4.42592, loss_ll_paf=7.25726, loss_ll_heat=1.59458, q=1000
[2018-07-04 09:44:29,201] [train] [INFO] epoch=13.00 step=99500, 10.7927 examples/sec lr=0.000004, loss=21.1628, loss_ll=3.88774, loss_ll_paf=5.58183, loss_ll_heat=2.19364, q=1000
[2018-07-04 09:47:03,175] [train] [INFO] epoch=13.00 step=99600, 10.7923 examples/sec lr=0.000004, loss=17.2362, loss_ll=2.88055, loss_ll_paf=3.82588, loss_ll_heat=1.93522, q=1000
[2018-07-04 09:49:34,999] [train] [INFO] epoch=13.00 step=99700, 10.7920 examples/sec lr=0.000004, loss=23.51, loss_ll=4.15639, loss_ll_paf=5.96302, loss_ll_heat=2.34975, q=1000
[2018-07-04 09:52:08,468] [train] [INFO] epoch=13.00 step=99800, 10.7916 examples/sec lr=0.000004, loss=17.5567, loss_ll=3.11215, loss_ll_paf=4.40308, loss_ll_heat=1.82122, q=1000
[2018-07-04 09:54:41,616] [train] [INFO] epoch=13.00 step=99900, 10.7913 examples/sec lr=0.000004, loss=19.308, loss_ll=3.46927, loss_ll_paf=5.45963, loss_ll_heat=1.47891, q=1000
[2018-07-04 09:57:15,243] [train] [INFO] epoch=13.00 step=100000, 10.7909 examples/sec lr=0.000004, loss=28.7114, loss_ll=5.14512, loss_ll_paf=7.70235, loss_ll_heat=2.58789, q=1000
[2018-07-04 10:00:01,512] [train] [INFO] epoch=13.00 step=100100, 10.7896 examples/sec lr=0.000004, loss=19.9567, loss_ll=3.61622, loss_ll_paf=5.54892, loss_ll_heat=1.68353, q=1000
[2018-07-04 10:02:34,293] [train] [INFO] epoch=13.00 step=100200, 10.7893 examples/sec lr=0.000004, loss=14.0262, loss_ll=2.28391, loss_ll_paf=3.10606, loss_ll_heat=1.46177, q=1000
[2018-07-04 10:05:07,740] [train] [INFO] epoch=13.00 step=100300, 10.7889 examples/sec lr=0.000004, loss=18.3109, loss_ll=3.34866, loss_ll_paf=4.43568, loss_ll_heat=2.26164, q=1000
[2018-07-04 10:07:41,248] [train] [INFO] epoch=13.00 step=100400, 10.7885 examples/sec lr=0.000004, loss=35.6893, loss_ll=6.61025, loss_ll_paf=10.5461, loss_ll_heat=2.67441, q=1000
[2018-07-04 10:10:14,497] [train] [INFO] epoch=13.00 step=100500, 10.7882 examples/sec lr=0.000004, loss=15.8975, loss_ll=2.89135, loss_ll_paf=3.96791, loss_ll_heat=1.81478, q=1000
[2018-07-04 10:12:47,203] [train] [INFO] epoch=13.00 step=100600, 10.7878 examples/sec lr=0.000004, loss=29.2961, loss_ll=5.19098, loss_ll_paf=8.04398, loss_ll_heat=2.33799, q=1000
[2018-07-04 10:15:19,955] [train] [INFO] epoch=13.00 step=100700, 10.7875 examples/sec lr=0.000004, loss=16.5802, loss_ll=2.92588, loss_ll_paf=4.26778, loss_ll_heat=1.58397, q=1000
[2018-07-04 10:17:52,353] [train] [INFO] epoch=13.00 step=100800, 10.7872 examples/sec lr=0.000004, loss=21.3132, loss_ll=3.8358, loss_ll_paf=5.84899, loss_ll_heat=1.82261, q=1000
[2018-07-04 10:20:24,657] [train] [INFO] epoch=13.00 step=100900, 10.7869 examples/sec lr=0.000004, loss=24.7026, loss_ll=4.38651, loss_ll_paf=6.95006, loss_ll_heat=1.82296, q=1000
[2018-07-04 10:22:53,616] [train] [INFO] epoch=13.00 step=101000, 10.7869 examples/sec lr=0.000004, loss=16.7529, loss_ll=2.96278, loss_ll_paf=3.86476, loss_ll_heat=2.06079, q=1000
[2018-07-04 10:25:42,554] [train] [INFO] epoch=13.00 step=101100, 10.7854 examples/sec lr=0.000004, loss=17.7007, loss_ll=3.09049, loss_ll_paf=4.49904, loss_ll_heat=1.68193, q=1000
[2018-07-04 10:28:13,863] [train] [INFO] epoch=13.00 step=101200, 10.7852 examples/sec lr=0.000004, loss=23.0997, loss_ll=4.39332, loss_ll_paf=7.03091, loss_ll_heat=1.75573, q=1000
[2018-07-04 10:30:47,372] [train] [INFO] epoch=13.00 step=101300, 10.7848 examples/sec lr=0.000004, loss=18.0691, loss_ll=3.20313, loss_ll_paf=3.71555, loss_ll_heat=2.6907, q=1000
[2018-07-04 10:33:21,337] [train] [INFO] epoch=13.00 step=101400, 10.7844 examples/sec lr=0.000004, loss=15.3949, loss_ll=2.58792, loss_ll_paf=3.50692, loss_ll_heat=1.66892, q=1000
[2018-07-04 10:35:50,998] [train] [INFO] epoch=13.00 step=101500, 10.7843 examples/sec lr=0.000004, loss=18.4659, loss_ll=3.12805, loss_ll_paf=4.43053, loss_ll_heat=1.82557, q=1000
[2018-07-04 10:38:23,410] [train] [INFO] epoch=13.00 step=101600, 10.7840 examples/sec lr=0.000004, loss=11.6049, loss_ll=1.96485, loss_ll_paf=2.46086, loss_ll_heat=1.46884, q=1000
[2018-07-04 10:40:55,103] [train] [INFO] epoch=13.00 step=101700, 10.7838 examples/sec lr=0.000004, loss=26.3275, loss_ll=4.66103, loss_ll_paf=6.56781, loss_ll_heat=2.75424, q=1000
[2018-07-04 10:43:27,797] [train] [INFO] epoch=13.00 step=101800, 10.7835 examples/sec lr=0.000004, loss=20.4258, loss_ll=3.7586, loss_ll_paf=5.17061, loss_ll_heat=2.3466, q=1000
[2018-07-04 10:46:00,464] [train] [INFO] epoch=13.00 step=101900, 10.7832 examples/sec lr=0.000004, loss=15.232, loss_ll=2.67272, loss_ll_paf=3.1764, loss_ll_heat=2.16904, q=1000
[2018-07-04 10:48:37,262] [train] [INFO] epoch=13.00 step=102000, 10.7826 examples/sec lr=0.000004, loss=44.7803, loss_ll=8.39961, loss_ll_paf=13.7789, loss_ll_heat=3.02032, q=1000
[2018-07-04 10:51:24,219] [train] [INFO] epoch=13.00 step=102100, 10.7813 examples/sec lr=0.000004, loss=17.1888, loss_ll=3.09173, loss_ll_paf=4.33894, loss_ll_heat=1.84452, q=1000
[2018-07-04 10:53:56,481] [train] [INFO] epoch=13.00 step=102200, 10.7810 examples/sec lr=0.000004, loss=35.7848, loss_ll=6.55479, loss_ll_paf=10.4279, loss_ll_heat=2.68163, q=1000
[2018-07-04 10:56:26,345] [train] [INFO] epoch=13.00 step=102300, 10.7809 examples/sec lr=0.000004, loss=16.2181, loss_ll=2.7322, loss_ll_paf=3.72777, loss_ll_heat=1.73662, q=1000
[2018-07-04 10:59:01,110] [train] [INFO] epoch=13.00 step=102400, 10.7804 examples/sec lr=0.000004, loss=29.5724, loss_ll=5.58773, loss_ll_paf=8.91427, loss_ll_heat=2.26119, q=1000
[2018-07-04 11:01:36,831] [train] [INFO] epoch=13.00 step=102500, 10.7799 examples/sec lr=0.000004, loss=20.4751, loss_ll=3.72599, loss_ll_paf=5.65738, loss_ll_heat=1.7946, q=1000
[2018-07-04 11:04:13,284] [train] [INFO] epoch=13.00 step=102600, 10.7794 examples/sec lr=0.000004, loss=17.2704, loss_ll=3.02585, loss_ll_paf=4.29682, loss_ll_heat=1.75487, q=1000
[2018-07-04 11:06:50,031] [train] [INFO] epoch=13.00 step=102700, 10.7788 examples/sec lr=0.000004, loss=26.2323, loss_ll=4.12431, loss_ll_paf=5.42888, loss_ll_heat=2.81975, q=1000
[2018-07-04 11:09:31,100] [train] [INFO] epoch=13.00 step=102800, 10.7779 examples/sec lr=0.000004, loss=16.8767, loss_ll=3.07704, loss_ll_paf=4.12821, loss_ll_heat=2.02587, q=1000
[2018-07-04 11:12:16,248] [train] [INFO] epoch=13.00 step=102900, 10.7767 examples/sec lr=0.000004, loss=17.5925, loss_ll=2.97135, loss_ll_paf=4.16744, loss_ll_heat=1.77526, q=1000
[2018-07-04 11:15:01,186] [train] [INFO] epoch=13.00 step=103000, 10.7755 examples/sec lr=0.000004, loss=22.1465, loss_ll=3.8911, loss_ll_paf=5.81601, loss_ll_heat=1.96619, q=1000
[2018-07-04 11:18:01,115] [train] [INFO] epoch=13.00 step=103100, 10.7733 examples/sec lr=0.000004, loss=24.1116, loss_ll=4.27323, loss_ll_paf=6.53754, loss_ll_heat=2.00892, q=1000
[2018-07-04 11:20:44,887] [train] [INFO] epoch=13.00 step=103200, 10.7723 examples/sec lr=0.000004, loss=19.6704, loss_ll=3.56194, loss_ll_paf=5.03196, loss_ll_heat=2.09192, q=1000
[2018-07-04 11:23:30,124] [train] [INFO] epoch=13.00 step=103300, 10.7711 examples/sec lr=0.000004, loss=19.2392, loss_ll=3.27884, loss_ll_paf=4.40504, loss_ll_heat=2.15265, q=1000
[2018-07-04 11:26:13,532] [train] [INFO] epoch=13.00 step=103400, 10.7700 examples/sec lr=0.000004, loss=21.779, loss_ll=3.88772, loss_ll_paf=5.65975, loss_ll_heat=2.11569, q=1000
[2018-07-04 11:28:56,057] [train] [INFO] epoch=13.00 step=103500, 10.7691 examples/sec lr=0.000004, loss=25.0151, loss_ll=4.01058, loss_ll_paf=6.19179, loss_ll_heat=1.82938, q=1000
[2018-07-04 11:31:38,350] [train] [INFO] epoch=13.00 step=103600, 10.7681 examples/sec lr=0.000004, loss=18.9868, loss_ll=3.4844, loss_ll_paf=4.76129, loss_ll_heat=2.20751, q=1000
[2018-07-04 11:34:22,041] [train] [INFO] epoch=13.00 step=103700, 10.7670 examples/sec lr=0.000004, loss=22.1063, loss_ll=3.94882, loss_ll_paf=5.9163, loss_ll_heat=1.98134, q=1000
[2018-07-04 11:37:01,537] [train] [INFO] epoch=13.00 step=103800, 10.7663 examples/sec lr=0.000004, loss=27.4059, loss_ll=5.06779, loss_ll_paf=7.72362, loss_ll_heat=2.41196, q=1000
[2018-07-04 11:39:43,222] [train] [INFO] epoch=13.00 step=103900, 10.7654 examples/sec lr=0.000004, loss=28.4897, loss_ll=5.07259, loss_ll_paf=7.64463, loss_ll_heat=2.50056, q=1000
[2018-07-04 11:42:25,264] [train] [INFO] epoch=13.00 step=104000, 10.7644 examples/sec lr=0.000004, loss=20.3438, loss_ll=3.41638, loss_ll_paf=4.91942, loss_ll_heat=1.91334, q=1000
[2018-07-04 11:45:25,512] [train] [INFO] epoch=13.00 step=104100, 10.7622 examples/sec lr=0.000004, loss=21.3014, loss_ll=3.90763, loss_ll_paf=5.8803, loss_ll_heat=1.93495, q=1000
[2018-07-04 11:48:07,969] [train] [INFO] epoch=13.00 step=104200, 10.7613 examples/sec lr=0.000004, loss=17.3126, loss_ll=2.89902, loss_ll_paf=4.12637, loss_ll_heat=1.67167, q=1000
[2018-07-04 11:50:49,622] [train] [INFO] epoch=13.00 step=104300, 10.7604 examples/sec lr=0.000004, loss=24.6216, loss_ll=4.46761, loss_ll_paf=6.8084, loss_ll_heat=2.12682, q=1000
[2018-07-04 11:53:30,835] [train] [INFO] epoch=13.00 step=104400, 10.7595 examples/sec lr=0.000004, loss=15.2229, loss_ll=2.74917, loss_ll_paf=3.35053, loss_ll_heat=2.14781, q=1000
[2018-07-04 11:56:13,500] [train] [INFO] epoch=13.00 step=104500, 10.7585 examples/sec lr=0.000004, loss=16.4598, loss_ll=2.83816, loss_ll_paf=4.21997, loss_ll_heat=1.45636, q=1000
[2018-07-04 11:58:55,356] [train] [INFO] epoch=13.00 step=104600, 10.7576 examples/sec lr=0.000004, loss=22.4525, loss_ll=3.97411, loss_ll_paf=6.26455, loss_ll_heat=1.68367, q=1000
[2018-07-04 12:01:38,648] [train] [INFO] epoch=13.00 step=104700, 10.7566 examples/sec lr=0.000004, loss=15.1927, loss_ll=2.81781, loss_ll_paf=3.56736, loss_ll_heat=2.06826, q=1000
[2018-07-04 12:04:20,535] [train] [INFO] epoch=13.00 step=104800, 10.7557 examples/sec lr=0.000004, loss=23.7187, loss_ll=4.52758, loss_ll_paf=6.66891, loss_ll_heat=2.38625, q=1000
[2018-07-04 12:07:00,623] [train] [INFO] epoch=13.00 step=104900, 10.7549 examples/sec lr=0.000004, loss=27.4804, loss_ll=4.89138, loss_ll_paf=7.46159, loss_ll_heat=2.32117, q=1000
[2018-07-04 12:09:45,025] [train] [INFO] epoch=13.00 step=105000, 10.7539 examples/sec lr=0.000004, loss=19.8924, loss_ll=3.57815, loss_ll_paf=5.49632, loss_ll_heat=1.65998, q=1000
[2018-07-04 12:12:40,861] [train] [INFO] epoch=13.00 step=105100, 10.7520 examples/sec lr=0.000004, loss=30.151, loss_ll=5.2383, loss_ll_paf=8.36236, loss_ll_heat=2.11425, q=1000
[2018-07-04 12:15:25,599] [train] [INFO] epoch=13.00 step=105200, 10.7509 examples/sec lr=0.000004, loss=17.2226, loss_ll=2.93404, loss_ll_paf=3.90268, loss_ll_heat=1.9654, q=1000
[2018-07-04 12:18:10,662] [train] [INFO] epoch=13.00 step=105300, 10.7498 examples/sec lr=0.000004, loss=15.4676, loss_ll=2.75192, loss_ll_paf=3.27718, loss_ll_heat=2.22666, q=1000
[2018-07-04 12:20:53,679] [train] [INFO] epoch=13.00 step=105400, 10.7488 examples/sec lr=0.000004, loss=24.2182, loss_ll=4.19877, loss_ll_paf=6.29937, loss_ll_heat=2.09816, q=1000
[2018-07-04 12:23:38,583] [train] [INFO] epoch=13.00 step=105500, 10.7477 examples/sec lr=0.000004, loss=18.0911, loss_ll=3.33591, loss_ll_paf=5.08199, loss_ll_heat=1.58983, q=1000
[2018-07-04 12:26:22,132] [train] [INFO] epoch=13.00 step=105600, 10.7467 examples/sec lr=0.000004, loss=15.2487, loss_ll=2.70851, loss_ll_paf=3.77193, loss_ll_heat=1.64508, q=1000
[2018-07-04 12:29:04,326] [train] [INFO] epoch=13.00 step=105700, 10.7458 examples/sec lr=0.000004, loss=14.137, loss_ll=2.48745, loss_ll_paf=3.02047, loss_ll_heat=1.95443, q=1000
[2018-07-04 12:31:47,910] [train] [INFO] epoch=13.00 step=105800, 10.7448 examples/sec lr=0.000004, loss=17.3069, loss_ll=2.9082, loss_ll_paf=3.77365, loss_ll_heat=2.04276, q=1000
[2018-07-04 12:34:28,345] [train] [INFO] epoch=13.00 step=105900, 10.7440 examples/sec lr=0.000004, loss=16.0886, loss_ll=2.77593, loss_ll_paf=3.47337, loss_ll_heat=2.0785, q=1000
[2018-07-04 12:37:13,223] [train] [INFO] epoch=13.00 step=106000, 10.7429 examples/sec lr=0.000004, loss=18.0272, loss_ll=3.26088, loss_ll_paf=4.40573, loss_ll_heat=2.11603, q=1000
[2018-07-04 12:40:10,241] [train] [INFO] epoch=13.00 step=106100, 10.7410 examples/sec lr=0.000004, loss=26.7018, loss_ll=4.87745, loss_ll_paf=8.0198, loss_ll_heat=1.73509, q=1000
[2018-07-04 12:42:52,125] [train] [INFO] epoch=13.00 step=106200, 10.7402 examples/sec lr=0.000004, loss=26.6084, loss_ll=4.71658, loss_ll_paf=6.77927, loss_ll_heat=2.6539, q=1000
[2018-07-04 12:45:33,033] [train] [INFO] epoch=13.00 step=106300, 10.7393 examples/sec lr=0.000004, loss=23.6198, loss_ll=4.22315, loss_ll_paf=5.62068, loss_ll_heat=2.82562, q=1000
[2018-07-04 12:48:20,585] [train] [INFO] epoch=13.00 step=106400, 10.7381 examples/sec lr=0.000004, loss=28.9044, loss_ll=5.41497, loss_ll_paf=8.61231, loss_ll_heat=2.21763, q=1000
[2018-07-04 12:51:06,349] [train] [INFO] epoch=13.00 step=106500, 10.7370 examples/sec lr=0.000004, loss=24.507, loss_ll=4.20835, loss_ll_paf=5.94757, loss_ll_heat=2.46914, q=1000
[2018-07-04 12:53:47,149] [train] [INFO] epoch=14.00 step=106600, 10.7362 examples/sec lr=0.000004, loss=21.3092, loss_ll=3.80295, loss_ll_paf=5.63523, loss_ll_heat=1.97068, q=1000
[2018-07-04 12:56:27,432] [train] [INFO] epoch=14.00 step=106700, 10.7354 examples/sec lr=0.000004, loss=28.4965, loss_ll=4.82857, loss_ll_paf=7.56906, loss_ll_heat=2.08808, q=1000
[2018-07-04 12:59:06,176] [train] [INFO] epoch=14.00 step=106800, 10.7347 examples/sec lr=0.000004, loss=19.0843, loss_ll=3.66256, loss_ll_paf=5.08105, loss_ll_heat=2.24406, q=1000
[2018-07-04 13:01:44,819] [train] [INFO] epoch=14.00 step=106900, 10.7341 examples/sec lr=0.000004, loss=26.4937, loss_ll=5.06857, loss_ll_paf=7.44043, loss_ll_heat=2.6967, q=1000
[2018-07-04 13:04:20,751] [train] [INFO] epoch=14.00 step=107000, 10.7336 examples/sec lr=0.000004, loss=13.0683, loss_ll=2.3585, loss_ll_paf=3.08189, loss_ll_heat=1.63511, q=1000
[2018-07-04 13:07:14,049] [train] [INFO] epoch=14.00 step=107100, 10.7320 examples/sec lr=0.000004, loss=20.5834, loss_ll=3.56305, loss_ll_paf=5.17948, loss_ll_heat=1.94662, q=1000
[2018-07-04 13:09:53,783] [train] [INFO] epoch=14.00 step=107200, 10.7313 examples/sec lr=0.000004, loss=18.6128, loss_ll=3.4197, loss_ll_paf=5.09189, loss_ll_heat=1.74751, q=1000
[2018-07-04 13:12:33,764] [train] [INFO] epoch=14.00 step=107300, 10.7306 examples/sec lr=0.000004, loss=19.7031, loss_ll=3.56843, loss_ll_paf=5.01354, loss_ll_heat=2.12331, q=1000
[2018-07-04 13:15:13,325] [train] [INFO] epoch=14.00 step=107400, 10.7299 examples/sec lr=0.000004, loss=23.737, loss_ll=3.92511, loss_ll_paf=5.51823, loss_ll_heat=2.33199, q=1000
[2018-07-04 13:17:53,435] [train] [INFO] epoch=14.00 step=107500, 10.7291 examples/sec lr=0.000004, loss=23.3649, loss_ll=4.07968, loss_ll_paf=6.09694, loss_ll_heat=2.06242, q=1000
[2018-07-04 13:20:34,658] [train] [INFO] epoch=14.00 step=107600, 10.7283 examples/sec lr=0.000004, loss=30.7738, loss_ll=5.45676, loss_ll_paf=9.06611, loss_ll_heat=1.84742, q=1000
[2018-07-04 13:23:14,919] [train] [INFO] epoch=14.00 step=107700, 10.7276 examples/sec lr=0.000004, loss=23.0122, loss_ll=4.29243, loss_ll_paf=6.23436, loss_ll_heat=2.3505, q=1000
[2018-07-04 13:25:54,206] [train] [INFO] epoch=14.00 step=107800, 10.7269 examples/sec lr=0.000004, loss=20.0455, loss_ll=3.13804, loss_ll_paf=4.22423, loss_ll_heat=2.05184, q=1000
[2018-07-04 13:28:34,597] [train] [INFO] epoch=14.00 step=107900, 10.7261 examples/sec lr=0.000004, loss=21.758, loss_ll=3.74579, loss_ll_paf=5.3464, loss_ll_heat=2.14519, q=1000
[2018-07-04 13:31:11,544] [train] [INFO] epoch=14.00 step=108000, 10.7256 examples/sec lr=0.000004, loss=28.2442, loss_ll=5.26333, loss_ll_paf=8.46646, loss_ll_heat=2.06019, q=1000
[2018-07-04 13:34:04,922] [train] [INFO] epoch=14.00 step=108100, 10.7240 examples/sec lr=0.000004, loss=23.6835, loss_ll=4.46133, loss_ll_paf=6.54259, loss_ll_heat=2.38007, q=1000
[2018-07-04 13:36:45,795] [train] [INFO] epoch=14.00 step=108200, 10.7232 examples/sec lr=0.000004, loss=29.8227, loss_ll=5.41862, loss_ll_paf=8.86388, loss_ll_heat=1.97336, q=1000
[2018-07-04 13:39:25,951] [train] [INFO] epoch=14.00 step=108300, 10.7225 examples/sec lr=0.000004, loss=23.4651, loss_ll=4.35143, loss_ll_paf=6.68921, loss_ll_heat=2.01364, q=1000
[2018-07-04 13:42:06,188] [train] [INFO] epoch=14.00 step=108400, 10.7218 examples/sec lr=0.000004, loss=24.2248, loss_ll=4.28256, loss_ll_paf=6.24788, loss_ll_heat=2.31723, q=1000
[2018-07-04 13:44:43,165] [train] [INFO] epoch=14.00 step=108500, 10.7213 examples/sec lr=0.000004, loss=18.5009, loss_ll=3.38021, loss_ll_paf=4.34306, loss_ll_heat=2.41736, q=1000
[2018-07-04 13:47:21,671] [train] [INFO] epoch=14.00 step=108600, 10.7207 examples/sec lr=0.000004, loss=20.0872, loss_ll=3.87459, loss_ll_paf=5.84288, loss_ll_heat=1.9063, q=1000
[2018-07-04 13:50:00,577] [train] [INFO] epoch=14.00 step=108700, 10.7200 examples/sec lr=0.000004, loss=19.1257, loss_ll=3.49492, loss_ll_paf=5.08612, loss_ll_heat=1.90372, q=1000
[2018-07-04 13:52:41,965] [train] [INFO] epoch=14.00 step=108800, 10.7192 examples/sec lr=0.000004, loss=20.1854, loss_ll=3.6941, loss_ll_paf=5.04054, loss_ll_heat=2.34765, q=1000
[2018-07-04 13:55:21,483] [train] [INFO] epoch=14.00 step=108900, 10.7185 examples/sec lr=0.000004, loss=21.8019, loss_ll=3.6733, loss_ll_paf=5.18964, loss_ll_heat=2.15696, q=1000
[2018-07-04 13:58:04,622] [train] [INFO] epoch=14.00 step=109000, 10.7176 examples/sec lr=0.000004, loss=26.4472, loss_ll=4.59041, loss_ll_paf=6.97925, loss_ll_heat=2.20157, q=1000
[2018-07-04 14:00:59,007] [train] [INFO] epoch=14.00 step=109100, 10.7160 examples/sec lr=0.000004, loss=18.3244, loss_ll=3.13958, loss_ll_paf=4.24145, loss_ll_heat=2.03771, q=1000
[2018-07-04 14:03:38,062] [train] [INFO] epoch=14.00 step=109200, 10.7153 examples/sec lr=0.000004, loss=19.0381, loss_ll=3.44359, loss_ll_paf=4.56697, loss_ll_heat=2.32021, q=1000
[2018-07-04 14:06:17,911] [train] [INFO] epoch=14.00 step=109300, 10.7147 examples/sec lr=0.000004, loss=16.2678, loss_ll=2.86204, loss_ll_paf=3.582, loss_ll_heat=2.14209, q=1000
[2018-07-04 14:08:56,540] [train] [INFO] epoch=14.00 step=109400, 10.7140 examples/sec lr=0.000004, loss=22.154, loss_ll=3.94443, loss_ll_paf=5.91869, loss_ll_heat=1.97017, q=1000
[2018-07-04 14:11:35,571] [train] [INFO] epoch=14.00 step=109500, 10.7134 examples/sec lr=0.000004, loss=25.4753, loss_ll=4.7552, loss_ll_paf=6.84067, loss_ll_heat=2.66972, q=1000
[2018-07-04 14:14:15,071] [train] [INFO] epoch=14.00 step=109600, 10.7127 examples/sec lr=0.000004, loss=22.5986, loss_ll=4.30634, loss_ll_paf=6.20191, loss_ll_heat=2.41077, q=1000
[2018-07-04 14:16:53,900] [train] [INFO] epoch=14.00 step=109700, 10.7121 examples/sec lr=0.000004, loss=18.0807, loss_ll=3.28284, loss_ll_paf=4.39952, loss_ll_heat=2.16617, q=1000
[2018-07-04 14:19:33,503] [train] [INFO] epoch=14.00 step=109800, 10.7115 examples/sec lr=0.000004, loss=23.1919, loss_ll=4.16371, loss_ll_paf=6.33712, loss_ll_heat=1.99029, q=1000
[2018-07-04 14:22:13,244] [train] [INFO] epoch=14.00 step=109900, 10.7108 examples/sec lr=0.000004, loss=18.8777, loss_ll=3.20566, loss_ll_paf=4.64213, loss_ll_heat=1.76919, q=1000
[2018-07-04 14:24:51,766] [train] [INFO] epoch=14.00 step=110000, 10.7102 examples/sec lr=0.000004, loss=23.1453, loss_ll=3.92499, loss_ll_paf=5.80517, loss_ll_heat=2.04481, q=1000
[2018-07-04 14:27:42,937] [train] [INFO] epoch=14.00 step=110100, 10.7088 examples/sec lr=0.000004, loss=26.972, loss_ll=4.52698, loss_ll_paf=6.52591, loss_ll_heat=2.52805, q=1000
[2018-07-04 14:30:21,304] [train] [INFO] epoch=14.00 step=110200, 10.7082 examples/sec lr=0.000004, loss=33.6907, loss_ll=5.94725, loss_ll_paf=8.62164, loss_ll_heat=3.27285, q=1000
[2018-07-04 14:32:59,575] [train] [INFO] epoch=14.00 step=110300, 10.7076 examples/sec lr=0.000004, loss=16.7753, loss_ll=2.85806, loss_ll_paf=3.95373, loss_ll_heat=1.7624, q=1000
[2018-07-04 14:35:38,104] [train] [INFO] epoch=14.00 step=110400, 10.7070 examples/sec lr=0.000004, loss=15.2494, loss_ll=2.74279, loss_ll_paf=3.64856, loss_ll_heat=1.83702, q=1000
[2018-07-04 14:38:16,582] [train] [INFO] epoch=14.00 step=110500, 10.7064 examples/sec lr=0.000004, loss=20.2102, loss_ll=3.50404, loss_ll_paf=5.28138, loss_ll_heat=1.7267, q=1000
[2018-07-04 14:40:55,994] [train] [INFO] epoch=14.00 step=110600, 10.7058 examples/sec lr=0.000004, loss=20.1511, loss_ll=3.70409, loss_ll_paf=5.51764, loss_ll_heat=1.89055, q=1000
[2018-07-04 14:43:34,201] [train] [INFO] epoch=14.00 step=110700, 10.7052 examples/sec lr=0.000004, loss=15.4791, loss_ll=2.6206, loss_ll_paf=3.41919, loss_ll_heat=1.82201, q=1000
[2018-07-04 14:46:13,253] [train] [INFO] epoch=14.00 step=110800, 10.7046 examples/sec lr=0.000004, loss=20.0722, loss_ll=3.31964, loss_ll_paf=4.66954, loss_ll_heat=1.96973, q=1000
[2018-07-04 14:48:51,035] [train] [INFO] epoch=14.00 step=110900, 10.7041 examples/sec lr=0.000004, loss=17.8297, loss_ll=3.01622, loss_ll_paf=3.92806, loss_ll_heat=2.10438, q=1000
[2018-07-04 14:51:30,643] [train] [INFO] epoch=14.00 step=111000, 10.7034 examples/sec lr=0.000004, loss=19.6774, loss_ll=3.15602, loss_ll_paf=4.9965, loss_ll_heat=1.31554, q=1000
[2018-07-04 14:54:23,626] [train] [INFO] epoch=14.00 step=111100, 10.7019 examples/sec lr=0.000004, loss=19.2628, loss_ll=3.32688, loss_ll_paf=4.89142, loss_ll_heat=1.76234, q=1000
[2018-07-04 14:57:05,427] [train] [INFO] epoch=14.00 step=111200, 10.7011 examples/sec lr=0.000004, loss=16.9917, loss_ll=2.86121, loss_ll_paf=3.93896, loss_ll_heat=1.78346, q=1000
[2018-07-04 14:59:41,885] [train] [INFO] epoch=14.00 step=111300, 10.7007 examples/sec lr=0.000004, loss=26.6323, loss_ll=4.77229, loss_ll_paf=7.4491, loss_ll_heat=2.09548, q=1000
[2018-07-04 15:02:20,798] [train] [INFO] epoch=14.00 step=111400, 10.7001 examples/sec lr=0.000004, loss=16.4562, loss_ll=2.94561, loss_ll_paf=3.87257, loss_ll_heat=2.01865, q=1000
[2018-07-04 15:04:58,937] [train] [INFO] epoch=14.00 step=111500, 10.6995 examples/sec lr=0.000004, loss=25.5072, loss_ll=4.85909, loss_ll_paf=7.18245, loss_ll_heat=2.53573, q=1000
[2018-07-04 15:07:46,883] [train] [INFO] epoch=14.00 step=111600, 10.6983 examples/sec lr=0.000004, loss=16.0456, loss_ll=2.6488, loss_ll_paf=3.53551, loss_ll_heat=1.7621, q=1000
[2018-07-04 15:10:32,738] [train] [INFO] epoch=14.00 step=111700, 10.6973 examples/sec lr=0.000004, loss=30.115, loss_ll=5.95253, loss_ll_paf=9.622, loss_ll_heat=2.28306, q=1000
[2018-07-04 15:13:18,224] [train] [INFO] epoch=14.00 step=111800, 10.6963 examples/sec lr=0.000004, loss=22.617, loss_ll=3.70792, loss_ll_paf=5.3794, loss_ll_heat=2.03644, q=1000
[2018-07-04 15:16:06,034] [train] [INFO] epoch=14.00 step=111900, 10.6951 examples/sec lr=0.000004, loss=28.0217, loss_ll=5.20842, loss_ll_paf=7.03163, loss_ll_heat=3.38521, q=1000
[2018-07-04 15:18:56,125] [train] [INFO] epoch=14.00 step=112000, 10.6938 examples/sec lr=0.000004, loss=34.5485, loss_ll=6.17097, loss_ll_paf=9.07815, loss_ll_heat=3.26379, q=1000
[2018-07-04 15:21:59,934] [train] [INFO] epoch=14.00 step=112100, 10.6916 examples/sec lr=0.000004, loss=19.7263, loss_ll=3.52475, loss_ll_paf=5.27891, loss_ll_heat=1.7706, q=1000
[2018-07-04 15:24:46,547] [train] [INFO] epoch=14.00 step=112200, 10.6905 examples/sec lr=0.000004, loss=18.8454, loss_ll=3.3152, loss_ll_paf=4.34589, loss_ll_heat=2.28451, q=1000
[2018-07-04 15:27:32,977] [train] [INFO] epoch=14.00 step=112300, 10.6895 examples/sec lr=0.000004, loss=28.4573, loss_ll=4.94748, loss_ll_paf=8.03055, loss_ll_heat=1.86442, q=1000
[2018-07-04 15:30:18,776] [train] [INFO] epoch=14.00 step=112400, 10.6884 examples/sec lr=0.000004, loss=18.6837, loss_ll=3.5313, loss_ll_paf=5.12017, loss_ll_heat=1.94244, q=1000
[2018-07-04 15:33:03,621] [train] [INFO] epoch=14.00 step=112500, 10.6875 examples/sec lr=0.000004, loss=22.1594, loss_ll=3.90611, loss_ll_paf=5.66549, loss_ll_heat=2.14673, q=1000
[2018-07-04 15:35:50,620] [train] [INFO] epoch=14.00 step=112600, 10.6864 examples/sec lr=0.000004, loss=24.4631, loss_ll=4.26065, loss_ll_paf=6.04038, loss_ll_heat=2.48093, q=1000
[2018-07-04 15:38:37,925] [train] [INFO] epoch=14.00 step=112700, 10.6853 examples/sec lr=0.000004, loss=20.4522, loss_ll=3.38571, loss_ll_paf=5.1724, loss_ll_heat=1.59903, q=1000
[2018-07-04 15:41:23,255] [train] [INFO] epoch=14.00 step=112800, 10.6843 examples/sec lr=0.000004, loss=29.06, loss_ll=5.00703, loss_ll_paf=7.42846, loss_ll_heat=2.5856, q=1000
[2018-07-04 15:44:10,143] [train] [INFO] epoch=14.00 step=112900, 10.6832 examples/sec lr=0.000004, loss=21.3916, loss_ll=3.75401, loss_ll_paf=5.94631, loss_ll_heat=1.56172, q=1000
[2018-07-04 15:46:56,246] [train] [INFO] epoch=14.00 step=113000, 10.6822 examples/sec lr=0.000004, loss=19.548, loss_ll=3.30024, loss_ll_paf=4.40328, loss_ll_heat=2.1972, q=1000
[2018-07-04 15:49:59,312] [train] [INFO] epoch=14.00 step=113100, 10.6801 examples/sec lr=0.000004, loss=16.2793, loss_ll=2.75313, loss_ll_paf=3.8847, loss_ll_heat=1.62156, q=1000
[2018-07-04 15:52:44,258] [train] [INFO] epoch=14.00 step=113200, 10.6791 examples/sec lr=0.000004, loss=21.9968, loss_ll=3.56393, loss_ll_paf=4.90236, loss_ll_heat=2.22549, q=1000
[2018-07-04 15:55:28,685] [train] [INFO] epoch=14.00 step=113300, 10.6782 examples/sec lr=0.000004, loss=26.5543, loss_ll=4.93543, loss_ll_paf=7.44231, loss_ll_heat=2.42855, q=1000
[2018-07-04 15:58:15,051] [train] [INFO] epoch=14.00 step=113400, 10.6772 examples/sec lr=0.000004, loss=23.0827, loss_ll=4.12882, loss_ll_paf=6.31248, loss_ll_heat=1.94515, q=1000
[2018-07-04 16:01:02,075] [train] [INFO] epoch=14.00 step=113500, 10.6761 examples/sec lr=0.000004, loss=23.3958, loss_ll=4.2965, loss_ll_paf=6.2789, loss_ll_heat=2.3141, q=1000
[2018-07-04 16:03:46,969] [train] [INFO] epoch=14.00 step=113600, 10.6751 examples/sec lr=0.000004, loss=28.4047, loss_ll=5.17362, loss_ll_paf=7.54152, loss_ll_heat=2.80572, q=1000
[2018-07-04 16:06:33,138] [train] [INFO] epoch=14.00 step=113700, 10.6741 examples/sec lr=0.000004, loss=27.9644, loss_ll=5.01027, loss_ll_paf=7.00232, loss_ll_heat=3.01822, q=1000
[2018-07-04 16:09:18,326] [train] [INFO] epoch=14.00 step=113800, 10.6732 examples/sec lr=0.000004, loss=16.2222, loss_ll=2.63094, loss_ll_paf=3.50532, loss_ll_heat=1.75655, q=1000
[2018-07-04 16:12:04,874] [train] [INFO] epoch=14.00 step=113900, 10.6721 examples/sec lr=0.000004, loss=17.8149, loss_ll=3.2048, loss_ll_paf=4.64298, loss_ll_heat=1.76662, q=1000
[2018-07-04 16:14:50,839] [train] [INFO] epoch=14.00 step=114000, 10.6711 examples/sec lr=0.000004, loss=19.3954, loss_ll=3.5105, loss_ll_paf=4.84692, loss_ll_heat=2.17407, q=1000
[2018-07-04 16:17:54,252] [train] [INFO] epoch=14.00 step=114100, 10.6690 examples/sec lr=0.000004, loss=21.3561, loss_ll=3.90875, loss_ll_paf=6.38712, loss_ll_heat=1.43037, q=1000
[2018-07-04 16:20:49,486] [train] [INFO] epoch=15.00 step=114200, 10.6675 examples/sec lr=0.000004, loss=20.7592, loss_ll=3.73529, loss_ll_paf=5.19383, loss_ll_heat=2.27676, q=1000
[2018-07-04 16:23:33,353] [train] [INFO] epoch=15.00 step=114300, 10.6666 examples/sec lr=0.000004, loss=16.8953, loss_ll=2.97631, loss_ll_paf=4.07868, loss_ll_heat=1.87395, q=1000
[2018-07-04 16:26:20,084] [train] [INFO] epoch=15.00 step=114400, 10.6656 examples/sec lr=0.000004, loss=20.041, loss_ll=3.36987, loss_ll_paf=4.62405, loss_ll_heat=2.11569, q=1000
[2018-07-04 16:29:07,785] [train] [INFO] epoch=15.00 step=114500, 10.6645 examples/sec lr=0.000004, loss=31.4658, loss_ll=5.34572, loss_ll_paf=7.74184, loss_ll_heat=2.9496, q=1000
[2018-07-04 16:31:53,194] [train] [INFO] epoch=15.00 step=114600, 10.6635 examples/sec lr=0.000004, loss=21.3173, loss_ll=3.81625, loss_ll_paf=5.50926, loss_ll_heat=2.12323, q=1000
[2018-07-04 16:34:38,028] [train] [INFO] epoch=15.00 step=114700, 10.6626 examples/sec lr=0.000004, loss=24.3902, loss_ll=4.64453, loss_ll_paf=6.88426, loss_ll_heat=2.40481, q=1000
[2018-07-04 16:37:23,923] [train] [INFO] epoch=15.00 step=114800, 10.6616 examples/sec lr=0.000004, loss=22.0716, loss_ll=4.06967, loss_ll_paf=6.15847, loss_ll_heat=1.98087, q=1000
[2018-07-04 16:40:09,732] [train] [INFO] epoch=15.00 step=114900, 10.6606 examples/sec lr=0.000004, loss=21.8989, loss_ll=3.52698, loss_ll_paf=4.71848, loss_ll_heat=2.33547, q=1000
[2018-07-04 16:42:56,780] [train] [INFO] epoch=15.00 step=115000, 10.6596 examples/sec lr=0.000004, loss=15.9611, loss_ll=2.96649, loss_ll_paf=3.9797, loss_ll_heat=1.95328, q=1000
[2018-07-04 16:46:00,191] [train] [INFO] epoch=15.00 step=115100, 10.6575 examples/sec lr=0.000004, loss=19.4001, loss_ll=3.16683, loss_ll_paf=4.44494, loss_ll_heat=1.88871, q=1000
[2018-07-04 16:48:47,743] [train] [INFO] epoch=15.00 step=115200, 10.6565 examples/sec lr=0.000004, loss=25.9126, loss_ll=4.65683, loss_ll_paf=6.98405, loss_ll_heat=2.32962, q=1000
[2018-07-04 16:51:31,649] [train] [INFO] epoch=15.00 step=115300, 10.6556 examples/sec lr=0.000004, loss=20.8859, loss_ll=3.63625, loss_ll_paf=5.30263, loss_ll_heat=1.96987, q=1000
[2018-07-04 16:54:16,380] [train] [INFO] epoch=15.00 step=115400, 10.6547 examples/sec lr=0.000004, loss=22.1492, loss_ll=4.01149, loss_ll_paf=5.99103, loss_ll_heat=2.03195, q=1000
[2018-07-04 16:57:02,459] [train] [INFO] epoch=15.00 step=115500, 10.6537 examples/sec lr=0.000004, loss=24.0499, loss_ll=4.39704, loss_ll_paf=6.33134, loss_ll_heat=2.46274, q=1000
[2018-07-04 16:59:48,460] [train] [INFO] epoch=15.00 step=115600, 10.6528 examples/sec lr=0.000004, loss=25.2994, loss_ll=4.44688, loss_ll_paf=6.82047, loss_ll_heat=2.0733, q=1000
[2018-07-04 17:02:33,943] [train] [INFO] epoch=15.00 step=115700, 10.6518 examples/sec lr=0.000004, loss=22.1301, loss_ll=3.84266, loss_ll_paf=5.33333, loss_ll_heat=2.35199, q=1000
[2018-07-04 17:05:20,309] [train] [INFO] epoch=15.00 step=115800, 10.6508 examples/sec lr=0.000004, loss=20.965, loss_ll=3.66303, loss_ll_paf=5.35888, loss_ll_heat=1.96719, q=1000
[2018-07-04 17:08:03,201] [train] [INFO] epoch=15.00 step=115900, 10.6501 examples/sec lr=0.000004, loss=15.8964, loss_ll=2.4722, loss_ll_paf=3.64304, loss_ll_heat=1.30137, q=1000
[2018-07-04 17:10:46,484] [train] [INFO] epoch=15.00 step=116000, 10.6493 examples/sec lr=0.000004, loss=26.5876, loss_ll=5.02039, loss_ll_paf=6.4694, loss_ll_heat=3.57137, q=1000
[2018-07-04 17:13:48,967] [train] [INFO] epoch=15.00 step=116100, 10.6473 examples/sec lr=0.000004, loss=13.8997, loss_ll=2.26896, loss_ll_paf=3.16004, loss_ll_heat=1.37787, q=1000
[2018-07-04 17:16:33,943] [train] [INFO] epoch=15.00 step=116200, 10.6464 examples/sec lr=0.000004, loss=16.8249, loss_ll=2.8594, loss_ll_paf=4.07245, loss_ll_heat=1.64635, q=1000
[2018-07-04 17:19:17,576] [train] [INFO] epoch=15.00 step=116300, 10.6456 examples/sec lr=0.000004, loss=20.8448, loss_ll=3.49519, loss_ll_paf=4.7651, loss_ll_heat=2.22528, q=1000
[2018-07-04 17:22:03,416] [train] [INFO] epoch=15.00 step=116400, 10.6446 examples/sec lr=0.000004, loss=27.4909, loss_ll=5.30105, loss_ll_paf=9.02376, loss_ll_heat=1.57833, q=1000
[2018-07-04 17:24:45,326] [train] [INFO] epoch=15.00 step=116500, 10.6439 examples/sec lr=0.000004, loss=26.6913, loss_ll=4.49859, loss_ll_paf=6.61329, loss_ll_heat=2.38389, q=1000
[2018-07-04 17:27:28,855] [train] [INFO] epoch=15.00 step=116600, 10.6431 examples/sec lr=0.000004, loss=25.1719, loss_ll=4.47124, loss_ll_paf=6.6829, loss_ll_heat=2.25958, q=1000
[2018-07-04 17:30:12,671] [train] [INFO] epoch=15.00 step=116700, 10.6423 examples/sec lr=0.000004, loss=17.3826, loss_ll=2.83953, loss_ll_paf=3.75395, loss_ll_heat=1.9251, q=1000
[2018-07-04 17:32:59,118] [train] [INFO] epoch=15.00 step=116800, 10.6413 examples/sec lr=0.000004, loss=23.7738, loss_ll=4.17242, loss_ll_paf=5.99513, loss_ll_heat=2.34971, q=1000
[2018-07-04 17:35:44,453] [train] [INFO] epoch=15.00 step=116900, 10.6404 examples/sec lr=0.000004, loss=16.1568, loss_ll=2.93447, loss_ll_paf=4.20214, loss_ll_heat=1.6668, q=1000
[2018-07-04 17:38:29,568] [train] [INFO] epoch=15.00 step=117000, 10.6395 examples/sec lr=0.000004, loss=27.201, loss_ll=4.84567, loss_ll_paf=6.9013, loss_ll_heat=2.79003, q=1000
[2018-07-04 17:41:30,799] [train] [INFO] epoch=15.00 step=117100, 10.6377 examples/sec lr=0.000004, loss=24.2882, loss_ll=4.04903, loss_ll_paf=5.79011, loss_ll_heat=2.30796, q=1000
[2018-07-04 17:44:18,243] [train] [INFO] epoch=15.00 step=117200, 10.6367 examples/sec lr=0.000004, loss=33.5096, loss_ll=6.13443, loss_ll_paf=9.98924, loss_ll_heat=2.27961, q=1000
[2018-07-04 17:47:03,936] [train] [INFO] epoch=15.00 step=117300, 10.6357 examples/sec lr=0.000004, loss=31.4243, loss_ll=5.83171, loss_ll_paf=8.86353, loss_ll_heat=2.79989, q=1000
[2018-07-04 17:49:43,326] [train] [INFO] epoch=15.00 step=117400, 10.6352 examples/sec lr=0.000004, loss=17.8939, loss_ll=3.18731, loss_ll_paf=4.53828, loss_ll_heat=1.83633, q=1000
[2018-07-04 17:52:26,060] [train] [INFO] epoch=15.00 step=117500, 10.6345 examples/sec lr=0.000004, loss=23.2691, loss_ll=3.96108, loss_ll_paf=5.87238, loss_ll_heat=2.04977, q=1000
[2018-07-04 17:55:08,896] [train] [INFO] epoch=15.00 step=117600, 10.6337 examples/sec lr=0.000004, loss=27.6736, loss_ll=5.22173, loss_ll_paf=7.46184, loss_ll_heat=2.98162, q=1000
[2018-07-04 17:57:50,808] [train] [INFO] epoch=15.00 step=117700, 10.6330 examples/sec lr=0.000004, loss=17.6911, loss_ll=3.03234, loss_ll_paf=4.20406, loss_ll_heat=1.86061, q=1000
[2018-07-04 18:00:33,761] [train] [INFO] epoch=15.00 step=117800, 10.6323 examples/sec lr=0.000004, loss=21.5, loss_ll=3.85365, loss_ll_paf=5.61449, loss_ll_heat=2.09281, q=1000
[2018-07-04 18:03:16,766] [train] [INFO] epoch=15.00 step=117900, 10.6315 examples/sec lr=0.000004, loss=25.8712, loss_ll=4.70181, loss_ll_paf=7.2166, loss_ll_heat=2.18703, q=1000
[2018-07-04 18:06:00,275] [train] [INFO] epoch=15.00 step=118000, 10.6307 examples/sec lr=0.000004, loss=16.5017, loss_ll=3.07388, loss_ll_paf=4.4222, loss_ll_heat=1.72557, q=1000
[2018-07-04 18:08:59,842] [train] [INFO] epoch=15.00 step=118100, 10.6290 examples/sec lr=0.000004, loss=24.2602, loss_ll=4.63247, loss_ll_paf=7.11669, loss_ll_heat=2.14825, q=1000
[2018-07-04 18:11:40,995] [train] [INFO] epoch=15.00 step=118200, 10.6284 examples/sec lr=0.000004, loss=24.5279, loss_ll=4.51122, loss_ll_paf=6.78119, loss_ll_heat=2.24125, q=1000
[2018-07-04 18:14:21,700] [train] [INFO] epoch=15.00 step=118300, 10.6278 examples/sec lr=0.000004, loss=23.1842, loss_ll=3.70766, loss_ll_paf=4.98001, loss_ll_heat=2.43532, q=1000
[2018-07-04 18:17:03,328] [train] [INFO] epoch=15.00 step=118400, 10.6271 examples/sec lr=0.000004, loss=16.2333, loss_ll=2.79827, loss_ll_paf=4.10053, loss_ll_heat=1.496, q=1000
[2018-07-04 18:19:43,042] [train] [INFO] epoch=15.00 step=118500, 10.6266 examples/sec lr=0.000004, loss=29.8413, loss_ll=5.39055, loss_ll_paf=8.66057, loss_ll_heat=2.12053, q=1000
[2018-07-04 18:22:24,195] [train] [INFO] epoch=15.00 step=118600, 10.6259 examples/sec lr=0.000004, loss=16.7363, loss_ll=2.63607, loss_ll_paf=3.83803, loss_ll_heat=1.4341, q=1000
[2018-07-04 18:25:05,362] [train] [INFO] epoch=15.00 step=118700, 10.6253 examples/sec lr=0.000004, loss=20.1742, loss_ll=3.68132, loss_ll_paf=5.41804, loss_ll_heat=1.94459, q=1000
[2018-07-04 18:27:46,880] [train] [INFO] epoch=15.00 step=118800, 10.6246 examples/sec lr=0.000004, loss=19.9383, loss_ll=3.51163, loss_ll_paf=5.12569, loss_ll_heat=1.89757, q=1000
[2018-07-04 18:30:29,482] [train] [INFO] epoch=15.00 step=118900, 10.6239 examples/sec lr=0.000004, loss=17.5727, loss_ll=3.05959, loss_ll_paf=3.68188, loss_ll_heat=2.4373, q=1000
[2018-07-04 18:33:10,200] [train] [INFO] epoch=15.00 step=119000, 10.6233 examples/sec lr=0.000004, loss=26.0103, loss_ll=4.8833, loss_ll_paf=7.34194, loss_ll_heat=2.42467, q=1000
[2018-07-04 18:36:07,569] [train] [INFO] epoch=15.00 step=119100, 10.6218 examples/sec lr=0.000004, loss=17.8485, loss_ll=2.87605, loss_ll_paf=3.92461, loss_ll_heat=1.82749, q=1000
[2018-07-04 18:38:48,525] [train] [INFO] epoch=15.00 step=119200, 10.6211 examples/sec lr=0.000004, loss=21.9125, loss_ll=3.95522, loss_ll_paf=5.72413, loss_ll_heat=2.18631, q=1000
[2018-07-04 18:41:30,881] [train] [INFO] epoch=15.00 step=119300, 10.6205 examples/sec lr=0.000004, loss=20.6065, loss_ll=3.82215, loss_ll_paf=5.39404, loss_ll_heat=2.25027, q=1000
[2018-07-04 18:44:13,505] [train] [INFO] epoch=15.00 step=119400, 10.6197 examples/sec lr=0.000004, loss=21.823, loss_ll=3.84945, loss_ll_paf=5.85262, loss_ll_heat=1.84629, q=1000
[2018-07-04 18:46:53,183] [train] [INFO] epoch=15.00 step=119500, 10.6192 examples/sec lr=0.000004, loss=26.3921, loss_ll=4.69043, loss_ll_paf=7.06054, loss_ll_heat=2.32032, q=1000
[2018-07-04 18:49:35,765] [train] [INFO] epoch=15.00 step=119600, 10.6185 examples/sec lr=0.000004, loss=26.1444, loss_ll=4.67216, loss_ll_paf=7.30356, loss_ll_heat=2.04076, q=1000
[2018-07-04 18:52:18,233] [train] [INFO] epoch=15.00 step=119700, 10.6178 examples/sec lr=0.000004, loss=31.19, loss_ll=5.53691, loss_ll_paf=8.7275, loss_ll_heat=2.34631, q=1000
[2018-07-04 18:54:58,929] [train] [INFO] epoch=15.00 step=119800, 10.6172 examples/sec lr=0.000004, loss=23.6405, loss_ll=4.47204, loss_ll_paf=7.04159, loss_ll_heat=1.90249, q=1000
[2018-07-04 18:57:41,542] [train] [INFO] epoch=15.00 step=119900, 10.6165 examples/sec lr=0.000004, loss=23.009, loss_ll=4.14646, loss_ll_paf=5.80608, loss_ll_heat=2.48685, q=1000
[2018-07-04 19:00:21,725] [train] [INFO] epoch=15.00 step=120000, 10.6160 examples/sec lr=0.000001, loss=20.3791, loss_ll=3.65872, loss_ll_paf=4.76202, loss_ll_heat=2.55542, q=1000
[2018-07-04 19:03:15,779] [train] [INFO] epoch=15.00 step=120100, 10.6146 examples/sec lr=0.000001, loss=24.4542, loss_ll=4.23576, loss_ll_paf=6.4745, loss_ll_heat=1.99701, q=1000
[2018-07-04 19:05:57,484] [train] [INFO] epoch=15.00 step=120200, 10.6140 examples/sec lr=0.000001, loss=15.7132, loss_ll=2.64465, loss_ll_paf=3.61826, loss_ll_heat=1.67105, q=1000
[2018-07-04 19:08:37,572] [train] [INFO] epoch=15.00 step=120300, 10.6134 examples/sec lr=0.000001, loss=36.3252, loss_ll=7.0386, loss_ll_paf=11.2483, loss_ll_heat=2.82892, q=1000
[2018-07-04 19:11:18,150] [train] [INFO] epoch=15.00 step=120400, 10.6128 examples/sec lr=0.000001, loss=21.4969, loss_ll=3.86717, loss_ll_paf=6.11961, loss_ll_heat=1.61472, q=1000
[2018-07-04 19:14:03,131] [train] [INFO] epoch=15.00 step=120500, 10.6120 examples/sec lr=0.000001, loss=23.932, loss_ll=4.30694, loss_ll_paf=6.95832, loss_ll_heat=1.65555, q=1000
[2018-07-04 19:16:44,479] [train] [INFO] epoch=15.00 step=120600, 10.6114 examples/sec lr=0.000001, loss=28.3472, loss_ll=4.66042, loss_ll_paf=7.18917, loss_ll_heat=2.13167, q=1000
[2018-07-04 19:19:26,563] [train] [INFO] epoch=15.00 step=120700, 10.6107 examples/sec lr=0.000001, loss=19.9543, loss_ll=3.50267, loss_ll_paf=5.49408, loss_ll_heat=1.51127, q=1000
[2018-07-04 19:22:07,003] [train] [INFO] epoch=15.00 step=120800, 10.6102 examples/sec lr=0.000001, loss=24.1127, loss_ll=4.38348, loss_ll_paf=6.59258, loss_ll_heat=2.17438, q=1000
[2018-07-04 19:24:48,521] [train] [INFO] epoch=15.00 step=120900, 10.6095 examples/sec lr=0.000001, loss=22.8056, loss_ll=3.9422, loss_ll_paf=5.84131, loss_ll_heat=2.04309, q=1000
[2018-07-04 19:27:31,140] [train] [INFO] epoch=15.00 step=121000, 10.6089 examples/sec lr=0.000001, loss=22.365, loss_ll=3.92316, loss_ll_paf=5.83606, loss_ll_heat=2.01025, q=1000
[2018-07-04 19:30:26,757] [train] [INFO] epoch=15.00 step=121100, 10.6074 examples/sec lr=0.000001, loss=15.1307, loss_ll=2.6174, loss_ll_paf=3.50994, loss_ll_heat=1.72485, q=1000
[2018-07-04 19:33:08,392] [train] [INFO] epoch=15.00 step=121200, 10.6068 examples/sec lr=0.000001, loss=14.723, loss_ll=2.75552, loss_ll_paf=3.60564, loss_ll_heat=1.9054, q=1000
[2018-07-04 19:35:50,743] [train] [INFO] epoch=15.00 step=121300, 10.6061 examples/sec lr=0.000001, loss=25.5889, loss_ll=4.50274, loss_ll_paf=6.67489, loss_ll_heat=2.3306, q=1000
[2018-07-04 19:38:31,210] [train] [INFO] epoch=15.00 step=121400, 10.6056 examples/sec lr=0.000001, loss=18.3707, loss_ll=3.32485, loss_ll_paf=4.67733, loss_ll_heat=1.97237, q=1000
[2018-07-04 19:41:12,747] [train] [INFO] epoch=15.00 step=121500, 10.6050 examples/sec lr=0.000001, loss=19.9473, loss_ll=3.63282, loss_ll_paf=5.37108, loss_ll_heat=1.89457, q=1000
[2018-07-04 19:43:51,377] [train] [INFO] epoch=15.00 step=121600, 10.6045 examples/sec lr=0.000001, loss=22.3628, loss_ll=3.99472, loss_ll_paf=6.09301, loss_ll_heat=1.89643, q=1000
[2018-07-04 19:46:35,138] [train] [INFO] epoch=15.00 step=121700, 10.6038 examples/sec lr=0.000001, loss=19.3828, loss_ll=3.30784, loss_ll_paf=5.0711, loss_ll_heat=1.54457, q=1000
[2018-07-04 19:49:16,649] [train] [INFO] epoch=16.00 step=121800, 10.6031 examples/sec lr=0.000001, loss=22.8248, loss_ll=3.90471, loss_ll_paf=5.85159, loss_ll_heat=1.95782, q=1000
[2018-07-04 19:51:58,867] [train] [INFO] epoch=16.00 step=121900, 10.6025 examples/sec lr=0.000001, loss=17.6457, loss_ll=2.96265, loss_ll_paf=4.09038, loss_ll_heat=1.83492, q=1000
[2018-07-04 19:54:40,043] [train] [INFO] epoch=16.00 step=122000, 10.6019 examples/sec lr=0.000001, loss=17.3403, loss_ll=2.94993, loss_ll_paf=4.30862, loss_ll_heat=1.59125, q=1000
[2018-07-04 19:57:38,203] [train] [INFO] epoch=16.00 step=122100, 10.6003 examples/sec lr=0.000001, loss=19.7422, loss_ll=3.66323, loss_ll_paf=4.90453, loss_ll_heat=2.42193, q=1000
[2018-07-04 20:00:20,052] [train] [INFO] epoch=16.00 step=122200, 10.5997 examples/sec lr=0.000001, loss=21.1955, loss_ll=3.77166, loss_ll_paf=5.60589, loss_ll_heat=1.93743, q=1000
[2018-07-04 20:03:00,705] [train] [INFO] epoch=16.00 step=122300, 10.5992 examples/sec lr=0.000001, loss=20.4484, loss_ll=3.56533, loss_ll_paf=5.00392, loss_ll_heat=2.12674, q=1000
[2018-07-04 20:05:43,372] [train] [INFO] epoch=16.00 step=122400, 10.5985 examples/sec lr=0.000001, loss=20.5457, loss_ll=3.22879, loss_ll_paf=4.42912, loss_ll_heat=2.02846, q=1000
[2018-07-04 20:08:25,158] [train] [INFO] epoch=16.00 step=122500, 10.5979 examples/sec lr=0.000001, loss=20.2621, loss_ll=3.73375, loss_ll_paf=4.86558, loss_ll_heat=2.60192, q=1000
[2018-07-04 20:11:07,050] [train] [INFO] epoch=16.00 step=122600, 10.5972 examples/sec lr=0.000001, loss=20.1032, loss_ll=3.76554, loss_ll_paf=6.02269, loss_ll_heat=1.50838, q=1000
[2018-07-04 20:13:46,595] [train] [INFO] epoch=16.00 step=122700, 10.5967 examples/sec lr=0.000001, loss=14.0915, loss_ll=2.34239, loss_ll_paf=3.01851, loss_ll_heat=1.66627, q=1000
[2018-07-04 20:16:30,301] [train] [INFO] epoch=16.00 step=122800, 10.5960 examples/sec lr=0.000001, loss=21.2411, loss_ll=3.63699, loss_ll_paf=5.2854, loss_ll_heat=1.98858, q=1000
[2018-07-04 20:19:13,419] [train] [INFO] epoch=16.00 step=122900, 10.5953 examples/sec lr=0.000001, loss=16.4525, loss_ll=2.80747, loss_ll_paf=3.74509, loss_ll_heat=1.86986, q=1000
[2018-07-04 20:21:56,172] [train] [INFO] epoch=16.00 step=123000, 10.5947 examples/sec lr=0.000001, loss=20.8189, loss_ll=3.73336, loss_ll_paf=5.13431, loss_ll_heat=2.33241, q=1000
[2018-07-04 20:24:54,952] [train] [INFO] epoch=16.00 step=123100, 10.5931 examples/sec lr=0.000001, loss=23.899, loss_ll=4.30371, loss_ll_paf=6.55798, loss_ll_heat=2.04945, q=1000
[2018-07-04 20:27:37,006] [train] [INFO] epoch=16.00 step=123200, 10.5924 examples/sec lr=0.000001, loss=30.9993, loss_ll=5.08758, loss_ll_paf=7.96333, loss_ll_heat=2.21184, q=1000
[2018-07-04 20:30:18,475] [train] [INFO] epoch=16.00 step=123300, 10.5919 examples/sec lr=0.000001, loss=21.1496, loss_ll=4.03127, loss_ll_paf=6.07826, loss_ll_heat=1.98428, q=1000
[2018-07-04 20:33:00,702] [train] [INFO] epoch=16.00 step=123400, 10.5912 examples/sec lr=0.000001, loss=19.2901, loss_ll=3.4793, loss_ll_paf=4.94181, loss_ll_heat=2.01679, q=1000
[2018-07-04 20:35:41,467] [train] [INFO] epoch=16.00 step=123500, 10.5907 examples/sec lr=0.000001, loss=18.4217, loss_ll=3.40166, loss_ll_paf=4.82388, loss_ll_heat=1.97944, q=1000
[2018-07-04 20:38:24,341] [train] [INFO] epoch=16.00 step=123600, 10.5900 examples/sec lr=0.000001, loss=19.6765, loss_ll=3.54877, loss_ll_paf=4.64805, loss_ll_heat=2.44949, q=1000
[2018-07-04 20:41:05,008] [train] [INFO] epoch=16.00 step=123700, 10.5895 examples/sec lr=0.000001, loss=30.3106, loss_ll=5.39754, loss_ll_paf=8.35655, loss_ll_heat=2.43852, q=1000
[2018-07-04 20:43:47,846] [train] [INFO] epoch=16.00 step=123800, 10.5888 examples/sec lr=0.000001, loss=18.7968, loss_ll=3.18534, loss_ll_paf=4.955, loss_ll_heat=1.41569, q=1000
[2018-07-04 20:46:30,117] [train] [INFO] epoch=16.00 step=123900, 10.5882 examples/sec lr=0.000001, loss=21.3223, loss_ll=3.60557, loss_ll_paf=5.37481, loss_ll_heat=1.83633, q=1000
[2018-07-04 20:49:11,364] [train] [INFO] epoch=16.00 step=124000, 10.5876 examples/sec lr=0.000001, loss=28.5112, loss_ll=4.40601, loss_ll_paf=6.49444, loss_ll_heat=2.31757, q=1000
[2018-07-04 20:52:08,136] [train] [INFO] epoch=16.00 step=124100, 10.5861 examples/sec lr=0.000001, loss=20.1327, loss_ll=3.37993, loss_ll_paf=5.02602, loss_ll_heat=1.73385, q=1000
[2018-07-04 20:54:51,228] [train] [INFO] epoch=16.00 step=124200, 10.5855 examples/sec lr=0.000001, loss=24.0805, loss_ll=4.46181, loss_ll_paf=6.88705, loss_ll_heat=2.03657, q=1000
[2018-07-04 20:57:33,938] [train] [INFO] epoch=16.00 step=124300, 10.5848 examples/sec lr=0.000001, loss=21.2374, loss_ll=3.77785, loss_ll_paf=5.44618, loss_ll_heat=2.10951, q=1000
[2018-07-04 21:00:16,896] [train] [INFO] epoch=16.00 step=124400, 10.5842 examples/sec lr=0.000001, loss=16.1786, loss_ll=2.59068, loss_ll_paf=3.63906, loss_ll_heat=1.5423, q=1000
[2018-07-04 21:02:58,200] [train] [INFO] epoch=16.00 step=124500, 10.5836 examples/sec lr=0.000001, loss=21.3817, loss_ll=3.68337, loss_ll_paf=5.32418, loss_ll_heat=2.04256, q=1000
[2018-07-04 21:05:40,690] [train] [INFO] epoch=16.00 step=124600, 10.5829 examples/sec lr=0.000001, loss=14.3545, loss_ll=2.58195, loss_ll_paf=3.67026, loss_ll_heat=1.49364, q=1000
[2018-07-04 21:08:23,138] [train] [INFO] epoch=16.00 step=124700, 10.5823 examples/sec lr=0.000001, loss=17.0611, loss_ll=2.71525, loss_ll_paf=4.08458, loss_ll_heat=1.34591, q=1000
[2018-07-04 21:11:05,619] [train] [INFO] epoch=16.00 step=124800, 10.5817 examples/sec lr=0.000001, loss=20.6808, loss_ll=3.8573, loss_ll_paf=5.69098, loss_ll_heat=2.02361, q=1000
[2018-07-04 21:13:48,667] [train] [INFO] epoch=16.00 step=124900, 10.5810 examples/sec lr=0.000001, loss=15.9055, loss_ll=2.87318, loss_ll_paf=4.10045, loss_ll_heat=1.64592, q=1000
[2018-07-04 21:16:31,073] [train] [INFO] epoch=16.00 step=125000, 10.5804 examples/sec lr=0.000001, loss=20.9747, loss_ll=3.62526, loss_ll_paf=5.86877, loss_ll_heat=1.38174, q=1000
[2018-07-04 21:19:31,991] [train] [INFO] epoch=16.00 step=125100, 10.5787 examples/sec lr=0.000001, loss=16.6544, loss_ll=3.04413, loss_ll_paf=3.95371, loss_ll_heat=2.13455, q=1000
[2018-07-04 21:22:15,751] [train] [INFO] epoch=16.00 step=125200, 10.5780 examples/sec lr=0.000001, loss=13.2457, loss_ll=2.26295, loss_ll_paf=3.03969, loss_ll_heat=1.48622, q=1000
[2018-07-04 21:25:01,286] [train] [INFO] epoch=16.00 step=125300, 10.5772 examples/sec lr=0.000001, loss=16.2193, loss_ll=2.94859, loss_ll_paf=4.06083, loss_ll_heat=1.83635, q=1000
[2018-07-04 21:27:44,598] [train] [INFO] epoch=16.00 step=125400, 10.5766 examples/sec lr=0.000001, loss=23.3093, loss_ll=3.88456, loss_ll_paf=5.18935, loss_ll_heat=2.57976, q=1000
[2018-07-04 21:30:28,768] [train] [INFO] epoch=16.00 step=125500, 10.5758 examples/sec lr=0.000001, loss=15.0005, loss_ll=2.72425, loss_ll_paf=3.69934, loss_ll_heat=1.74916, q=1000
[2018-07-04 21:33:11,729] [train] [INFO] epoch=16.00 step=125600, 10.5752 examples/sec lr=0.000001, loss=20.658, loss_ll=3.45828, loss_ll_paf=4.6611, loss_ll_heat=2.25547, q=1000
[2018-07-04 21:35:57,865] [train] [INFO] epoch=16.00 step=125700, 10.5744 examples/sec lr=0.000001, loss=19.5635, loss_ll=3.40988, loss_ll_paf=4.99852, loss_ll_heat=1.82124, q=1000
[2018-07-04 21:38:36,988] [train] [INFO] epoch=16.00 step=125800, 10.5739 examples/sec lr=0.000001, loss=17.4416, loss_ll=3.17079, loss_ll_paf=4.31081, loss_ll_heat=2.03078, q=1000
[2018-07-04 21:41:21,957] [train] [INFO] epoch=16.00 step=125900, 10.5732 examples/sec lr=0.000001, loss=34.759, loss_ll=6.21154, loss_ll_paf=9.63884, loss_ll_heat=2.78423, q=1000
[2018-07-04 21:44:05,839] [train] [INFO] epoch=16.00 step=126000, 10.5725 examples/sec lr=0.000001, loss=27.2053, loss_ll=5.0932, loss_ll_paf=7.71303, loss_ll_heat=2.47337, q=1000
[2018-07-04 21:47:06,658] [train] [INFO] epoch=16.00 step=126100, 10.5708 examples/sec lr=0.000001, loss=14.899, loss_ll=2.68233, loss_ll_paf=3.71993, loss_ll_heat=1.64474, q=1000
[2018-07-04 21:49:50,463] [train] [INFO] epoch=16.00 step=126200, 10.5702 examples/sec lr=0.000001, loss=33.9803, loss_ll=6.38744, loss_ll_paf=10.8651, loss_ll_heat=1.90975, q=1000
[2018-07-04 21:52:35,818] [train] [INFO] epoch=16.00 step=126300, 10.5694 examples/sec lr=0.000001, loss=16.6534, loss_ll=2.95747, loss_ll_paf=4.30222, loss_ll_heat=1.61273, q=1000
[2018-07-04 21:55:23,413] [train] [INFO] epoch=16.00 step=126400, 10.5685 examples/sec lr=0.000001, loss=17.9508, loss_ll=3.30781, loss_ll_paf=4.39053, loss_ll_heat=2.2251, q=1000
[2018-07-04 21:58:06,622] [train] [INFO] epoch=16.00 step=126500, 10.5678 examples/sec lr=0.000001, loss=19.9643, loss_ll=3.54573, loss_ll_paf=4.99891, loss_ll_heat=2.09255, q=1000
[2018-07-04 22:00:48,177] [train] [INFO] epoch=16.00 step=126600, 10.5673 examples/sec lr=0.000001, loss=16.2121, loss_ll=2.82084, loss_ll_paf=4.07007, loss_ll_heat=1.5716, q=1000
[2018-07-04 22:03:31,498] [train] [INFO] epoch=16.00 step=126700, 10.5666 examples/sec lr=0.000001, loss=28.3436, loss_ll=5.05625, loss_ll_paf=7.84068, loss_ll_heat=2.27182, q=1000
[2018-07-04 22:06:10,838] [train] [INFO] epoch=16.00 step=126800, 10.5662 examples/sec lr=0.000001, loss=16.9942, loss_ll=2.85536, loss_ll_paf=3.9801, loss_ll_heat=1.73063, q=1000
[2018-07-04 22:08:44,696] [train] [INFO] epoch=16.00 step=126900, 10.5661 examples/sec lr=0.000001, loss=17.6591, loss_ll=3.00717, loss_ll_paf=3.99503, loss_ll_heat=2.01931, q=1000
[2018-07-04 22:11:18,583] [train] [INFO] epoch=16.00 step=127000, 10.5659 examples/sec lr=0.000001, loss=15.1549, loss_ll=2.65298, loss_ll_paf=3.63436, loss_ll_heat=1.6716, q=1000
[2018-07-04 22:14:07,651] [train] [INFO] epoch=16.00 step=127100, 10.5650 examples/sec lr=0.000001, loss=22.8234, loss_ll=4.08315, loss_ll_paf=5.94541, loss_ll_heat=2.22089, q=1000
[2018-07-04 22:16:39,339] [train] [INFO] epoch=16.00 step=127200, 10.5649 examples/sec lr=0.000001, loss=19.3969, loss_ll=3.34273, loss_ll_paf=5.19771, loss_ll_heat=1.48776, q=1000
[2018-07-04 22:19:13,491] [train] [INFO] epoch=16.00 step=127300, 10.5648 examples/sec lr=0.000001, loss=33.7914, loss_ll=6.47871, loss_ll_paf=10.6179, loss_ll_heat=2.33953, q=1000
[2018-07-04 22:21:46,227] [train] [INFO] epoch=16.00 step=127400, 10.5647 examples/sec lr=0.000001, loss=21.3922, loss_ll=3.8607, loss_ll_paf=5.52626, loss_ll_heat=2.19514, q=1000
[2018-07-04 22:24:20,332] [train] [INFO] epoch=16.00 step=127500, 10.5646 examples/sec lr=0.000001, loss=25.8585, loss_ll=4.71756, loss_ll_paf=7.71399, loss_ll_heat=1.72114, q=1000
[2018-07-04 22:26:56,440] [train] [INFO] epoch=16.00 step=127600, 10.5643 examples/sec lr=0.000001, loss=14.9496, loss_ll=2.6056, loss_ll_paf=3.35419, loss_ll_heat=1.857, q=1000
[2018-07-04 22:29:29,967] [train] [INFO] epoch=16.00 step=127700, 10.5642 examples/sec lr=0.000001, loss=21.602, loss_ll=3.79251, loss_ll_paf=5.72337, loss_ll_heat=1.86165, q=1000
[2018-07-04 22:32:03,106] [train] [INFO] epoch=16.00 step=127800, 10.5641 examples/sec lr=0.000001, loss=21.3847, loss_ll=3.87989, loss_ll_paf=5.87679, loss_ll_heat=1.88299, q=1000
[2018-07-04 22:34:38,270] [train] [INFO] epoch=16.00 step=127900, 10.5639 examples/sec lr=0.000001, loss=24.9336, loss_ll=4.57016, loss_ll_paf=7.44437, loss_ll_heat=1.69595, q=1000
[2018-07-04 22:37:12,393] [train] [INFO] epoch=16.00 step=128000, 10.5638 examples/sec lr=0.000001, loss=19.2068, loss_ll=3.32829, loss_ll_paf=4.96347, loss_ll_heat=1.6931, q=1000
[2018-07-04 22:39:59,263] [train] [INFO] epoch=16.00 step=128100, 10.5629 examples/sec lr=0.000001, loss=17.505, loss_ll=3.05529, loss_ll_paf=4.7475, loss_ll_heat=1.36309, q=1000
[2018-07-04 22:42:34,575] [train] [INFO] epoch=16.00 step=128200, 10.5627 examples/sec lr=0.000001, loss=17.3823, loss_ll=2.78788, loss_ll_paf=4.02176, loss_ll_heat=1.55399, q=1000
[2018-07-04 22:45:09,016] [train] [INFO] epoch=16.00 step=128300, 10.5626 examples/sec lr=0.000001, loss=17.5048, loss_ll=3.0295, loss_ll_paf=3.9828, loss_ll_heat=2.07619, q=1000
[2018-07-04 22:47:42,862] [train] [INFO] epoch=16.00 step=128400, 10.5624 examples/sec lr=0.000001, loss=22.0633, loss_ll=4.07881, loss_ll_paf=5.26561, loss_ll_heat=2.892, q=1000
[2018-07-04 22:50:17,621] [train] [INFO] epoch=16.00 step=128500, 10.5623 examples/sec lr=0.000001, loss=14.7784, loss_ll=2.76542, loss_ll_paf=3.67896, loss_ll_heat=1.85188, q=1000
[2018-07-04 22:52:51,656] [train] [INFO] epoch=16.00 step=128600, 10.5621 examples/sec lr=0.000001, loss=19.9758, loss_ll=3.70041, loss_ll_paf=5.29951, loss_ll_heat=2.10132, q=1000
[2018-07-04 22:55:28,081] [train] [INFO] epoch=16.00 step=128700, 10.5618 examples/sec lr=0.000001, loss=15.9375, loss_ll=2.86322, loss_ll_paf=3.81676, loss_ll_heat=1.90967, q=1000
[2018-07-04 22:58:01,659] [train] [INFO] epoch=16.00 step=128800, 10.5617 examples/sec lr=0.000001, loss=19.5432, loss_ll=3.31208, loss_ll_paf=4.76454, loss_ll_heat=1.85962, q=1000
[2018-07-04 23:00:33,670] [train] [INFO] epoch=16.00 step=128900, 10.5617 examples/sec lr=0.000001, loss=17.3773, loss_ll=2.8296, loss_ll_paf=4.12795, loss_ll_heat=1.53124, q=1000
[2018-07-04 23:03:07,759] [train] [INFO] epoch=16.00 step=129000, 10.5616 examples/sec lr=0.000001, loss=14.1753, loss_ll=2.55323, loss_ll_paf=3.44866, loss_ll_heat=1.65781, q=1000
[2018-07-04 23:05:57,498] [train] [INFO] epoch=16.00 step=129100, 10.5606 examples/sec lr=0.000001, loss=23.6176, loss_ll=4.12126, loss_ll_paf=6.51853, loss_ll_heat=1.72399, q=1000
[2018-07-04 23:08:31,398] [train] [INFO] epoch=16.00 step=129200, 10.5604 examples/sec lr=0.000001, loss=24.9945, loss_ll=4.62324, loss_ll_paf=7.2022, loss_ll_heat=2.04429, q=1000
[2018-07-04 23:11:03,953] [train] [INFO] epoch=16.00 step=129300, 10.5604 examples/sec lr=0.000001, loss=30.0375, loss_ll=4.85632, loss_ll_paf=7.87821, loss_ll_heat=1.83444, q=1000
[2018-07-04 23:13:36,900] [train] [INFO] epoch=17.00 step=129400, 10.5603 examples/sec lr=0.000001, loss=21.2695, loss_ll=3.70365, loss_ll_paf=5.55592, loss_ll_heat=1.85137, q=1000
[2018-07-04 23:16:09,076] [train] [INFO] epoch=17.00 step=129500, 10.5603 examples/sec lr=0.000001, loss=22.5, loss_ll=3.92064, loss_ll_paf=6.0583, loss_ll_heat=1.78298, q=1000
[2018-07-04 23:18:43,021] [train] [INFO] epoch=17.00 step=129600, 10.5601 examples/sec lr=0.000001, loss=19.4159, loss_ll=2.95179, loss_ll_paf=4.33587, loss_ll_heat=1.56771, q=1000
[2018-07-04 23:21:15,512] [train] [INFO] epoch=17.00 step=129700, 10.5601 examples/sec lr=0.000001, loss=23.926, loss_ll=3.8515, loss_ll_paf=5.9851, loss_ll_heat=1.71791, q=1000
[2018-07-04 23:23:48,230] [train] [INFO] epoch=17.00 step=129800, 10.5600 examples/sec lr=0.000001, loss=17.7458, loss_ll=3.10133, loss_ll_paf=4.32074, loss_ll_heat=1.88192, q=1000
[2018-07-04 23:26:31,978] [train] [INFO] epoch=17.00 step=129900, 10.5594 examples/sec lr=0.000001, loss=19.8317, loss_ll=3.44189, loss_ll_paf=4.58861, loss_ll_heat=2.29516, q=1000
[2018-07-04 23:29:05,490] [train] [INFO] epoch=17.00 step=130000, 10.5593 examples/sec lr=0.000001, loss=16.3516, loss_ll=2.80021, loss_ll_paf=3.58458, loss_ll_heat=2.01584, q=1000
[2018-07-04 23:31:58,438] [train] [INFO] epoch=17.00 step=130100, 10.5581 examples/sec lr=0.000001, loss=16.6587, loss_ll=2.77663, loss_ll_paf=3.62614, loss_ll_heat=1.92712, q=1000
[2018-07-04 23:34:30,255] [train] [INFO] epoch=17.00 step=130200, 10.5581 examples/sec lr=0.000001, loss=14.0788, loss_ll=2.27448, loss_ll_paf=3.161, loss_ll_heat=1.38797, q=1000
[2018-07-04 23:37:06,217] [train] [INFO] epoch=17.00 step=130300, 10.5579 examples/sec lr=0.000001, loss=25.1727, loss_ll=4.7106, loss_ll_paf=7.12217, loss_ll_heat=2.29903, q=1000
[2018-07-04 23:39:41,244] [train] [INFO] epoch=17.00 step=130400, 10.5577 examples/sec lr=0.000001, loss=20.3661, loss_ll=3.89238, loss_ll_paf=5.69494, loss_ll_heat=2.08982, q=1000
[2018-07-04 23:42:16,007] [train] [INFO] epoch=17.00 step=130500, 10.5575 examples/sec lr=0.000001, loss=20.8593, loss_ll=3.86779, loss_ll_paf=5.37334, loss_ll_heat=2.36223, q=1000
[2018-07-04 23:44:55,762] [train] [INFO] epoch=17.00 step=130600, 10.5571 examples/sec lr=0.000001, loss=20.3791, loss_ll=3.74445, loss_ll_paf=5.81619, loss_ll_heat=1.67271, q=1000
[2018-07-04 23:47:29,732] [train] [INFO] epoch=17.00 step=130700, 10.5569 examples/sec lr=0.000001, loss=18.5884, loss_ll=3.29716, loss_ll_paf=4.85369, loss_ll_heat=1.74062, q=1000
[2018-07-04 23:50:07,079] [train] [INFO] epoch=17.00 step=130800, 10.5566 examples/sec lr=0.000001, loss=21.427, loss_ll=3.80422, loss_ll_paf=5.59221, loss_ll_heat=2.01622, q=1000
[2018-07-04 23:52:40,960] [train] [INFO] epoch=17.00 step=130900, 10.5565 examples/sec lr=0.000001, loss=27.3971, loss_ll=4.82082, loss_ll_paf=7.17891, loss_ll_heat=2.46274, q=1000
[2018-07-04 23:55:16,009] [train] [INFO] epoch=17.00 step=131000, 10.5563 examples/sec lr=0.000001, loss=18.388, loss_ll=2.91447, loss_ll_paf=3.89135, loss_ll_heat=1.93759, q=1000
[2018-07-04 23:58:07,524] [train] [INFO] epoch=17.00 step=131100, 10.5553 examples/sec lr=0.000001, loss=18.6118, loss_ll=3.16361, loss_ll_paf=4.46107, loss_ll_heat=1.86616, q=1000
[2018-07-05 00:00:42,483] [train] [INFO] epoch=17.00 step=131200, 10.5551 examples/sec lr=0.000001, loss=19.7285, loss_ll=3.64473, loss_ll_paf=5.1819, loss_ll_heat=2.10756, q=1000
[2018-07-05 00:03:17,208] [train] [INFO] epoch=17.00 step=131300, 10.5549 examples/sec lr=0.000001, loss=31.4403, loss_ll=6.04236, loss_ll_paf=8.89053, loss_ll_heat=3.19419, q=1000
[2018-07-05 00:05:49,984] [train] [INFO] epoch=17.00 step=131400, 10.5549 examples/sec lr=0.000001, loss=18.7473, loss_ll=3.26889, loss_ll_paf=4.56183, loss_ll_heat=1.97596, q=1000
[2018-07-05 00:08:22,062] [train] [INFO] epoch=17.00 step=131500, 10.5548 examples/sec lr=0.000001, loss=20.7111, loss_ll=3.94107, loss_ll_paf=6.15802, loss_ll_heat=1.72413, q=1000
[2018-07-05 00:11:10,672] [train] [INFO] epoch=17.00 step=131600, 10.5539 examples/sec lr=0.000001, loss=27.535, loss_ll=4.09037, loss_ll_paf=6.11899, loss_ll_heat=2.06176, q=1000
[2018-07-05 00:13:45,081] [train] [INFO] epoch=17.00 step=131700, 10.5538 examples/sec lr=0.000001, loss=19.9561, loss_ll=3.43711, loss_ll_paf=4.95297, loss_ll_heat=1.92124, q=1000
[2018-07-05 00:16:17,650] [train] [INFO] epoch=17.00 step=131800, 10.5537 examples/sec lr=0.000001, loss=17.7129, loss_ll=2.79762, loss_ll_paf=3.99034, loss_ll_heat=1.6049, q=1000
[2018-07-05 00:18:51,415] [train] [INFO] epoch=17.00 step=131900, 10.5536 examples/sec lr=0.000001, loss=20.9647, loss_ll=3.7953, loss_ll_paf=6.05678, loss_ll_heat=1.53383, q=1000
[2018-07-05 00:21:26,050] [train] [INFO] epoch=17.00 step=132000, 10.5535 examples/sec lr=0.000001, loss=27.7493, loss_ll=5.01787, loss_ll_paf=8.13085, loss_ll_heat=1.90488, q=1000
[2018-07-05 00:24:15,966] [train] [INFO] epoch=17.00 step=132100, 10.5525 examples/sec lr=0.000001, loss=16.0631, loss_ll=2.70067, loss_ll_paf=3.47914, loss_ll_heat=1.9222, q=1000
[2018-07-05 00:26:48,587] [train] [INFO] epoch=17.00 step=132200, 10.5524 examples/sec lr=0.000001, loss=17.058, loss_ll=2.83309, loss_ll_paf=4.12891, loss_ll_heat=1.53727, q=1000
[2018-07-05 00:29:23,853] [train] [INFO] epoch=17.00 step=132300, 10.5523 examples/sec lr=0.000001, loss=20.0374, loss_ll=3.47509, loss_ll_paf=4.7863, loss_ll_heat=2.16389, q=1000
[2018-07-05 00:31:56,047] [train] [INFO] epoch=17.00 step=132400, 10.5522 examples/sec lr=0.000001, loss=26.856, loss_ll=4.67934, loss_ll_paf=6.18468, loss_ll_heat=3.17401, q=1000
[2018-07-05 00:34:28,053] [train] [INFO] epoch=17.00 step=132500, 10.5522 examples/sec lr=0.000001, loss=19.4313, loss_ll=3.60898, loss_ll_paf=4.99427, loss_ll_heat=2.22369, q=1000
[2018-07-05 00:36:59,862] [train] [INFO] epoch=17.00 step=132600, 10.5522 examples/sec lr=0.000001, loss=16.465, loss_ll=2.58844, loss_ll_paf=3.25483, loss_ll_heat=1.92206, q=1000
[2018-07-05 00:39:35,270] [train] [INFO] epoch=17.00 step=132700, 10.5520 examples/sec lr=0.000001, loss=17.4604, loss_ll=3.03048, loss_ll_paf=4.28986, loss_ll_heat=1.77111, q=1000
[2018-07-05 00:42:08,883] [train] [INFO] epoch=17.00 step=132800, 10.5519 examples/sec lr=0.000001, loss=20.9575, loss_ll=3.9041, loss_ll_paf=5.61436, loss_ll_heat=2.19385, q=1000
[2018-07-05 00:44:42,100] [train] [INFO] epoch=17.00 step=132900, 10.5518 examples/sec lr=0.000001, loss=15.5456, loss_ll=2.52779, loss_ll_paf=3.19529, loss_ll_heat=1.86028, q=1000
[2018-07-05 00:47:16,518] [train] [INFO] epoch=17.00 step=133000, 10.5517 examples/sec lr=0.000001, loss=21.1234, loss_ll=3.85944, loss_ll_paf=5.70845, loss_ll_heat=2.01044, q=1000
[2018-07-05 00:50:04,214] [train] [INFO] epoch=17.00 step=133100, 10.5508 examples/sec lr=0.000001, loss=20.3732, loss_ll=3.53756, loss_ll_paf=4.98316, loss_ll_heat=2.09196, q=1000
[2018-07-05 00:52:39,896] [train] [INFO] epoch=17.00 step=133200, 10.5506 examples/sec lr=0.000001, loss=28.1801, loss_ll=4.97772, loss_ll_paf=7.22713, loss_ll_heat=2.7283, q=1000
[2018-07-05 00:55:12,808] [train] [INFO] epoch=17.00 step=133300, 10.5505 examples/sec lr=0.000001, loss=20.2376, loss_ll=3.77457, loss_ll_paf=5.79419, loss_ll_heat=1.75494, q=1000
[2018-07-05 00:57:46,247] [train] [INFO] epoch=17.00 step=133400, 10.5505 examples/sec lr=0.000001, loss=21.0658, loss_ll=4.05396, loss_ll_paf=6.33649, loss_ll_heat=1.77143, q=1000
[2018-07-05 01:00:18,874] [train] [INFO] epoch=17.00 step=133500, 10.5504 examples/sec lr=0.000001, loss=27.5362, loss_ll=4.94322, loss_ll_paf=7.54613, loss_ll_heat=2.34031, q=1000
[2018-07-05 01:02:50,999] [train] [INFO] epoch=17.00 step=133600, 10.5504 examples/sec lr=0.000001, loss=15.0314, loss_ll=2.66478, loss_ll_paf=3.59701, loss_ll_heat=1.73254, q=1000
[2018-07-05 01:05:24,577] [train] [INFO] epoch=17.00 step=133700, 10.5503 examples/sec lr=0.000001, loss=16.7929, loss_ll=3.00963, loss_ll_paf=4.40982, loss_ll_heat=1.60945, q=1000
[2018-07-05 01:07:59,227] [train] [INFO] epoch=17.00 step=133800, 10.5501 examples/sec lr=0.000001, loss=18.2935, loss_ll=3.1703, loss_ll_paf=3.88356, loss_ll_heat=2.45704, q=1000
[2018-07-05 01:10:33,824] [train] [INFO] epoch=17.00 step=133900, 10.5500 examples/sec lr=0.000001, loss=23.5043, loss_ll=4.18954, loss_ll_paf=6.28526, loss_ll_heat=2.09381, q=1000
[2018-07-05 01:13:08,363] [train] [INFO] epoch=17.00 step=134000, 10.5498 examples/sec lr=0.000001, loss=26.4783, loss_ll=4.82617, loss_ll_paf=6.97604, loss_ll_heat=2.6763, q=1000
[2018-07-05 01:15:57,165] [train] [INFO] epoch=17.00 step=134100, 10.5489 examples/sec lr=0.000001, loss=23.2813, loss_ll=4.04218, loss_ll_paf=5.74163, loss_ll_heat=2.34273, q=1000
[2018-07-05 01:18:31,078] [train] [INFO] epoch=17.00 step=134200, 10.5488 examples/sec lr=0.000001, loss=11.7936, loss_ll=2.07902, loss_ll_paf=2.77854, loss_ll_heat=1.3795, q=1000
[2018-07-05 01:21:05,644] [train] [INFO] epoch=17.00 step=134300, 10.5487 examples/sec lr=0.000001, loss=15.84, loss_ll=2.96706, loss_ll_paf=3.87364, loss_ll_heat=2.06048, q=1000
[2018-07-05 01:23:41,213] [train] [INFO] epoch=17.00 step=134400, 10.5485 examples/sec lr=0.000001, loss=19.6577, loss_ll=3.55422, loss_ll_paf=5.11714, loss_ll_heat=1.9913, q=1000
[2018-07-05 01:26:14,880] [train] [INFO] epoch=17.00 step=134500, 10.5484 examples/sec lr=0.000001, loss=17.6875, loss_ll=3.17139, loss_ll_paf=4.64316, loss_ll_heat=1.69963, q=1000
[2018-07-05 01:28:47,894] [train] [INFO] epoch=17.00 step=134600, 10.5483 examples/sec lr=0.000001, loss=21.6448, loss_ll=3.57759, loss_ll_paf=5.13378, loss_ll_heat=2.0214, q=1000
[2018-07-05 01:31:22,167] [train] [INFO] epoch=17.00 step=134700, 10.5482 examples/sec lr=0.000001, loss=17.8756, loss_ll=3.23304, loss_ll_paf=4.53212, loss_ll_heat=1.93396, q=1000
[2018-07-05 01:33:55,861] [train] [INFO] epoch=17.00 step=134800, 10.5481 examples/sec lr=0.000001, loss=27.692, loss_ll=5.22954, loss_ll_paf=8.18068, loss_ll_heat=2.2784, q=1000
[2018-07-05 01:36:29,463] [train] [INFO] epoch=17.00 step=134900, 10.5480 examples/sec lr=0.000001, loss=24.5321, loss_ll=4.49377, loss_ll_paf=6.82221, loss_ll_heat=2.16533, q=1000
[2018-07-05 01:39:01,202] [train] [INFO] epoch=17.00 step=135000, 10.5480 examples/sec lr=0.000001, loss=18.0021, loss_ll=3.1995, loss_ll_paf=4.09174, loss_ll_heat=2.30726, q=1000
[2018-07-05 01:41:50,223] [train] [INFO] epoch=17.00 step=135100, 10.5471 examples/sec lr=0.000001, loss=18.6096, loss_ll=3.3578, loss_ll_paf=4.92249, loss_ll_heat=1.7931, q=1000
[2018-07-05 01:44:21,209] [train] [INFO] epoch=17.00 step=135200, 10.5471 examples/sec lr=0.000001, loss=27.5661, loss_ll=4.63968, loss_ll_paf=7.15524, loss_ll_heat=2.12412, q=1000
[2018-07-05 01:46:53,246] [train] [INFO] epoch=17.00 step=135300, 10.5471 examples/sec lr=0.000001, loss=17.0828, loss_ll=3.08273, loss_ll_paf=4.72372, loss_ll_heat=1.44174, q=1000
[2018-07-05 01:49:27,057] [train] [INFO] epoch=17.00 step=135400, 10.5470 examples/sec lr=0.000001, loss=21.6714, loss_ll=3.98055, loss_ll_paf=5.71937, loss_ll_heat=2.24173, q=1000
[2018-07-05 01:52:03,612] [train] [INFO] epoch=17.00 step=135500, 10.5467 examples/sec lr=0.000001, loss=25.8004, loss_ll=5.01367, loss_ll_paf=7.54951, loss_ll_heat=2.47784, q=1000
[2018-07-05 01:54:37,551] [train] [INFO] epoch=17.00 step=135600, 10.5466 examples/sec lr=0.000001, loss=25.4959, loss_ll=4.4961, loss_ll_paf=6.6765, loss_ll_heat=2.3157, q=1000
[2018-07-05 01:57:09,630] [train] [INFO] epoch=17.00 step=135700, 10.5466 examples/sec lr=0.000001, loss=17.0686, loss_ll=2.96889, loss_ll_paf=3.83783, loss_ll_heat=2.09994, q=1000
[2018-07-05 01:59:43,279] [train] [INFO] epoch=17.00 step=135800, 10.5465 examples/sec lr=0.000001, loss=20.0698, loss_ll=3.60623, loss_ll_paf=5.35447, loss_ll_heat=1.85799, q=1000
[2018-07-05 02:02:17,809] [train] [INFO] epoch=17.00 step=135900, 10.5463 examples/sec lr=0.000001, loss=28.2268, loss_ll=5.11056, loss_ll_paf=7.73126, loss_ll_heat=2.48985, q=1000
[2018-07-05 02:04:51,288] [train] [INFO] epoch=17.00 step=136000, 10.5463 examples/sec lr=0.000001, loss=40.8391, loss_ll=7.36092, loss_ll_paf=11.3652, loss_ll_heat=3.35664, q=1000
[2018-07-05 02:07:38,260] [train] [INFO] epoch=17.00 step=136100, 10.5455 examples/sec lr=0.000001, loss=18.3749, loss_ll=3.34603, loss_ll_paf=5.14835, loss_ll_heat=1.54372, q=1000
[2018-07-05 02:10:14,280] [train] [INFO] epoch=17.00 step=136200, 10.5453 examples/sec lr=0.000001, loss=16.4044, loss_ll=2.91975, loss_ll_paf=4.12571, loss_ll_heat=1.71378, q=1000
[2018-07-05 02:12:49,535] [train] [INFO] epoch=17.00 step=136300, 10.5451 examples/sec lr=0.000001, loss=16.7036, loss_ll=2.85185, loss_ll_paf=4.02334, loss_ll_heat=1.68036, q=1000
[2018-07-05 02:15:23,232] [train] [INFO] epoch=17.00 step=136400, 10.5450 examples/sec lr=0.000001, loss=16.1757, loss_ll=2.97289, loss_ll_paf=4.68721, loss_ll_heat=1.25857, q=1000
[2018-07-05 02:17:57,480] [train] [INFO] epoch=17.00 step=136500, 10.5448 examples/sec lr=0.000001, loss=23.1225, loss_ll=3.87098, loss_ll_paf=6.11244, loss_ll_heat=1.62952, q=1000
[2018-07-05 02:20:28,875] [train] [INFO] epoch=17.00 step=136600, 10.5449 examples/sec lr=0.000001, loss=16.4648, loss_ll=2.80331, loss_ll_paf=3.98923, loss_ll_heat=1.61738, q=1000
[2018-07-05 02:23:01,235] [train] [INFO] epoch=17.00 step=136700, 10.5448 examples/sec lr=0.000001, loss=14.996, loss_ll=2.72518, loss_ll_paf=3.55196, loss_ll_heat=1.8984, q=1000
[2018-07-05 02:25:36,566] [train] [INFO] epoch=17.00 step=136800, 10.5447 examples/sec lr=0.000001, loss=12.7696, loss_ll=2.13842, loss_ll_paf=2.78733, loss_ll_heat=1.48951, q=1000
[2018-07-05 02:28:10,779] [train] [INFO] epoch=17.00 step=136900, 10.5445 examples/sec lr=0.000001, loss=22.986, loss_ll=4.07388, loss_ll_paf=5.08312, loss_ll_heat=3.06465, q=1000
[2018-07-05 02:30:44,543] [train] [INFO] epoch=18.00 step=137000, 10.5444 examples/sec lr=0.000001, loss=16.7912, loss_ll=2.77957, loss_ll_paf=3.67222, loss_ll_heat=1.88692, q=1000
[2018-07-05 02:33:33,231] [train] [INFO] epoch=18.00 step=137100, 10.5436 examples/sec lr=0.000001, loss=29.0506, loss_ll=5.30974, loss_ll_paf=8.06192, loss_ll_heat=2.55756, q=1000
[2018-07-05 02:36:07,653] [train] [INFO] epoch=18.00 step=137200, 10.5434 examples/sec lr=0.000001, loss=12.1498, loss_ll=2.04118, loss_ll_paf=2.77919, loss_ll_heat=1.30316, q=1000
[2018-07-05 02:38:41,306] [train] [INFO] epoch=18.00 step=137300, 10.5433 examples/sec lr=0.000001, loss=16.3112, loss_ll=2.85228, loss_ll_paf=4.03257, loss_ll_heat=1.67199, q=1000
[2018-07-05 02:41:13,614] [train] [INFO] epoch=18.00 step=137400, 10.5433 examples/sec lr=0.000001, loss=23.1396, loss_ll=3.86369, loss_ll_paf=5.44712, loss_ll_heat=2.28025, q=1000
[2018-07-05 02:43:46,822] [train] [INFO] epoch=18.00 step=137500, 10.5432 examples/sec lr=0.000001, loss=17.648, loss_ll=3.12844, loss_ll_paf=4.23824, loss_ll_heat=2.01863, q=1000
[2018-07-05 02:46:20,027] [train] [INFO] epoch=18.00 step=137600, 10.5432 examples/sec lr=0.000001, loss=19.0817, loss_ll=3.20466, loss_ll_paf=4.74586, loss_ll_heat=1.66346, q=1000
[2018-07-05 02:48:55,184] [train] [INFO] epoch=18.00 step=137700, 10.5430 examples/sec lr=0.000001, loss=16.865, loss_ll=2.87556, loss_ll_paf=4.07812, loss_ll_heat=1.67299, q=1000
[2018-07-05 02:51:28,984] [train] [INFO] epoch=18.00 step=137800, 10.5429 examples/sec lr=0.000001, loss=24.9963, loss_ll=4.62244, loss_ll_paf=6.94084, loss_ll_heat=2.30404, q=1000
[2018-07-05 02:54:02,166] [train] [INFO] epoch=18.00 step=137900, 10.5428 examples/sec lr=0.000001, loss=20.577, loss_ll=3.71369, loss_ll_paf=5.46246, loss_ll_heat=1.96492, q=1000
[2018-07-05 02:56:34,723] [train] [INFO] epoch=18.00 step=138000, 10.5428 examples/sec lr=0.000001, loss=22.1055, loss_ll=3.76842, loss_ll_paf=6.05672, loss_ll_heat=1.48011, q=1000
[2018-07-05 02:59:24,481] [train] [INFO] epoch=18.00 step=138100, 10.5419 examples/sec lr=0.000001, loss=18.318, loss_ll=3.22321, loss_ll_paf=4.80648, loss_ll_heat=1.63995, q=1000
[2018-07-05 03:01:55,358] [train] [INFO] epoch=18.00 step=138200, 10.5419 examples/sec lr=0.000001, loss=16.9412, loss_ll=2.98382, loss_ll_paf=4.04496, loss_ll_heat=1.92268, q=1000
[2018-07-05 03:04:27,512] [train] [INFO] epoch=18.00 step=138300, 10.5419 examples/sec lr=0.000001, loss=18.5624, loss_ll=2.91826, loss_ll_paf=4.03818, loss_ll_heat=1.79833, q=1000
[2018-07-05 03:07:00,090] [train] [INFO] epoch=18.00 step=138400, 10.5419 examples/sec lr=0.000001, loss=18.2451, loss_ll=3.30999, loss_ll_paf=4.61266, loss_ll_heat=2.00732, q=1000
[2018-07-05 03:09:34,756] [train] [INFO] epoch=18.00 step=138500, 10.5417 examples/sec lr=0.000001, loss=23.8561, loss_ll=4.41583, loss_ll_paf=6.65767, loss_ll_heat=2.17399, q=1000
[2018-07-05 03:12:10,084] [train] [INFO] epoch=18.00 step=138600, 10.5415 examples/sec lr=0.000001, loss=16.475, loss_ll=2.92038, loss_ll_paf=4.324, loss_ll_heat=1.51677, q=1000
[2018-07-05 03:14:45,931] [train] [INFO] epoch=18.00 step=138700, 10.5413 examples/sec lr=0.000001, loss=14.3819, loss_ll=2.52639, loss_ll_paf=3.52797, loss_ll_heat=1.52481, q=1000
[2018-07-05 03:17:21,001] [train] [INFO] epoch=18.00 step=138800, 10.5412 examples/sec lr=0.000001, loss=18.1914, loss_ll=3.09104, loss_ll_paf=4.47427, loss_ll_heat=1.70781, q=1000
[2018-07-05 03:19:55,801] [train] [INFO] epoch=18.00 step=138900, 10.5410 examples/sec lr=0.000001, loss=20.7806, loss_ll=3.70933, loss_ll_paf=5.29366, loss_ll_heat=2.12499, q=1000
[2018-07-05 03:22:30,386] [train] [INFO] epoch=18.00 step=139000, 10.5409 examples/sec lr=0.000001, loss=29.241, loss_ll=5.25151, loss_ll_paf=8.23914, loss_ll_heat=2.26388, q=1000
[2018-07-05 03:25:21,719] [train] [INFO] epoch=18.00 step=139100, 10.5399 examples/sec lr=0.000001, loss=20.4841, loss_ll=3.43089, loss_ll_paf=5.01591, loss_ll_heat=1.84587, q=1000
[2018-07-05 03:27:55,569] [train] [INFO] epoch=18.00 step=139200, 10.5398 examples/sec lr=0.000001, loss=24.6328, loss_ll=4.3445, loss_ll_paf=6.02932, loss_ll_heat=2.65967, q=1000
[2018-07-05 03:30:28,225] [train] [INFO] epoch=18.00 step=139300, 10.5398 examples/sec lr=0.000001, loss=21.1369, loss_ll=3.88548, loss_ll_paf=5.85923, loss_ll_heat=1.91173, q=1000
[2018-07-05 03:33:01,291] [train] [INFO] epoch=18.00 step=139400, 10.5397 examples/sec lr=0.000001, loss=20.3493, loss_ll=3.22088, loss_ll_paf=4.92826, loss_ll_heat=1.51349, q=1000
[2018-07-05 03:35:33,527] [train] [INFO] epoch=18.00 step=139500, 10.5397 examples/sec lr=0.000001, loss=19.4666, loss_ll=3.44705, loss_ll_paf=5.02097, loss_ll_heat=1.87312, q=1000
[2018-07-05 03:38:06,790] [train] [INFO] epoch=18.00 step=139600, 10.5396 examples/sec lr=0.000001, loss=20.7572, loss_ll=3.52127, loss_ll_paf=4.71177, loss_ll_heat=2.33077, q=1000
[2018-07-05 03:40:40,231] [train] [INFO] epoch=18.00 step=139700, 10.5395 examples/sec lr=0.000001, loss=24.2874, loss_ll=4.04161, loss_ll_paf=6.22289, loss_ll_heat=1.86033, q=1000
[2018-07-05 03:43:11,991] [train] [INFO] epoch=18.00 step=139800, 10.5395 examples/sec lr=0.000001, loss=17.2198, loss_ll=3.02048, loss_ll_paf=3.76061, loss_ll_heat=2.28035, q=1000
[2018-07-05 03:45:46,869] [train] [INFO] epoch=18.00 step=139900, 10.5394 examples/sec lr=0.000001, loss=19.7748, loss_ll=3.40261, loss_ll_paf=4.85098, loss_ll_heat=1.95425, q=1000
[2018-07-05 03:48:18,994] [train] [INFO] epoch=18.00 step=140000, 10.5393 examples/sec lr=0.000001, loss=19.6386, loss_ll=3.45683, loss_ll_paf=5.3483, loss_ll_heat=1.56536, q=1000
[2018-07-05 03:51:07,251] [train] [INFO] epoch=18.00 step=140100, 10.5385 examples/sec lr=0.000001, loss=28.8394, loss_ll=4.88891, loss_ll_paf=7.13303, loss_ll_heat=2.64479, q=1000
[2018-07-05 03:53:42,791] [train] [INFO] epoch=18.00 step=140200, 10.5384 examples/sec lr=0.000001, loss=15.4016, loss_ll=2.78634, loss_ll_paf=3.95333, loss_ll_heat=1.61936, q=1000
[2018-07-05 03:56:15,460] [train] [INFO] epoch=18.00 step=140300, 10.5383 examples/sec lr=0.000001, loss=24.5855, loss_ll=4.36121, loss_ll_paf=6.10759, loss_ll_heat=2.61483, q=1000
[2018-07-05 03:58:48,697] [train] [INFO] epoch=18.00 step=140400, 10.5382 examples/sec lr=0.000001, loss=21.0554, loss_ll=3.96934, loss_ll_paf=6.30293, loss_ll_heat=1.63575, q=1000
[2018-07-05 04:01:22,320] [train] [INFO] epoch=18.00 step=140500, 10.5382 examples/sec lr=0.000001, loss=21.4058, loss_ll=3.65392, loss_ll_paf=5.25941, loss_ll_heat=2.04843, q=1000
[2018-07-05 04:03:57,190] [train] [INFO] epoch=18.00 step=140600, 10.5380 examples/sec lr=0.000001, loss=23.6185, loss_ll=4.31795, loss_ll_paf=6.82859, loss_ll_heat=1.80732, q=1000
[2018-07-05 04:06:30,847] [train] [INFO] epoch=18.00 step=140700, 10.5379 examples/sec lr=0.000001, loss=16.1803, loss_ll=2.87538, loss_ll_paf=3.41884, loss_ll_heat=2.33192, q=1000
[2018-07-05 04:09:03,568] [train] [INFO] epoch=18.00 step=140800, 10.5379 examples/sec lr=0.000001, loss=14.3862, loss_ll=2.36329, loss_ll_paf=3.1457, loss_ll_heat=1.58089, q=1000
[2018-07-05 04:11:37,590] [train] [INFO] epoch=18.00 step=140900, 10.5378 examples/sec lr=0.000001, loss=23.9981, loss_ll=4.37458, loss_ll_paf=6.55227, loss_ll_heat=2.1969, q=1000
[2018-07-05 04:14:12,336] [train] [INFO] epoch=18.00 step=141000, 10.5376 examples/sec lr=0.000001, loss=21.0053, loss_ll=4.01636, loss_ll_paf=5.44927, loss_ll_heat=2.58346, q=1000
[2018-07-05 04:17:00,052] [train] [INFO] epoch=18.00 step=141100, 10.5368 examples/sec lr=0.000001, loss=15.9068, loss_ll=2.81958, loss_ll_paf=4.39609, loss_ll_heat=1.24307, q=1000
[2018-07-05 04:19:32,413] [train] [INFO] epoch=18.00 step=141200, 10.5368 examples/sec lr=0.000001, loss=21.5053, loss_ll=3.78403, loss_ll_paf=5.36765, loss_ll_heat=2.20041, q=1000
[2018-07-05 04:22:05,694] [train] [INFO] epoch=18.00 step=141300, 10.5367 examples/sec lr=0.000001, loss=18.0775, loss_ll=3.19674, loss_ll_paf=4.64967, loss_ll_heat=1.74382, q=1000
[2018-07-05 04:24:39,183] [train] [INFO] epoch=18.00 step=141400, 10.5367 examples/sec lr=0.000001, loss=12.1781, loss_ll=2.18763, loss_ll_paf=3.02386, loss_ll_heat=1.35139, q=1000
[2018-07-05 04:27:10,378] [train] [INFO] epoch=18.00 step=141500, 10.5367 examples/sec lr=0.000001, loss=23.1824, loss_ll=4.07827, loss_ll_paf=5.53949, loss_ll_heat=2.61706, q=1000
[2018-07-05 04:29:42,039] [train] [INFO] epoch=18.00 step=141600, 10.5367 examples/sec lr=0.000001, loss=21.7782, loss_ll=3.73465, loss_ll_paf=5.36687, loss_ll_heat=2.10243, q=1000
[2018-07-05 04:32:14,536] [train] [INFO] epoch=18.00 step=141700, 10.5367 examples/sec lr=0.000001, loss=20.0431, loss_ll=3.65877, loss_ll_paf=5.45893, loss_ll_heat=1.85861, q=1000
[2018-07-05 04:34:48,836] [train] [INFO] epoch=18.00 step=141800, 10.5365 examples/sec lr=0.000001, loss=21.7971, loss_ll=3.76687, loss_ll_paf=5.19347, loss_ll_heat=2.34027, q=1000
[2018-07-05 04:37:21,378] [train] [INFO] epoch=18.00 step=141900, 10.5365 examples/sec lr=0.000001, loss=21.9349, loss_ll=3.60996, loss_ll_paf=4.96699, loss_ll_heat=2.25293, q=1000
[2018-07-05 04:39:55,497] [train] [INFO] epoch=18.00 step=142000, 10.5364 examples/sec lr=0.000001, loss=33.374, loss_ll=5.61708, loss_ll_paf=8.62094, loss_ll_heat=2.61322, q=1000
[2018-07-05 04:42:42,758] [train] [INFO] epoch=18.00 step=142100, 10.5357 examples/sec lr=0.000001, loss=18.0237, loss_ll=3.06803, loss_ll_paf=4.02214, loss_ll_heat=2.11392, q=1000
[2018-07-05 04:45:17,608] [train] [INFO] epoch=18.00 step=142200, 10.5355 examples/sec lr=0.000001, loss=14.0452, loss_ll=2.5129, loss_ll_paf=3.1618, loss_ll_heat=1.864, q=1000
[2018-07-05 04:47:49,605] [train] [INFO] epoch=18.00 step=142300, 10.5355 examples/sec lr=0.000001, loss=17.2788, loss_ll=2.95826, loss_ll_paf=4.21859, loss_ll_heat=1.69793, q=1000
[2018-07-05 04:50:23,852] [train] [INFO] epoch=18.00 step=142400, 10.5354 examples/sec lr=0.000001, loss=19.3036, loss_ll=3.35647, loss_ll_paf=4.55768, loss_ll_heat=2.15526, q=1000
[2018-07-05 04:52:58,604] [train] [INFO] epoch=18.00 step=142500, 10.5352 examples/sec lr=0.000001, loss=15.5428, loss_ll=2.63777, loss_ll_paf=3.89465, loss_ll_heat=1.38089, q=1000
[2018-07-05 04:55:32,777] [train] [INFO] epoch=18.00 step=142600, 10.5351 examples/sec lr=0.000001, loss=19.1246, loss_ll=3.15581, loss_ll_paf=4.71921, loss_ll_heat=1.59241, q=1000
[2018-07-05 04:58:06,475] [train] [INFO] epoch=18.00 step=142700, 10.5350 examples/sec lr=0.000001, loss=21.689, loss_ll=3.87582, loss_ll_paf=5.56089, loss_ll_heat=2.19075, q=1000
[2018-07-05 05:00:38,637] [train] [INFO] epoch=18.00 step=142800, 10.5350 examples/sec lr=0.000001, loss=16.8317, loss_ll=2.99903, loss_ll_paf=3.9187, loss_ll_heat=2.07935, q=1000
[2018-07-05 05:03:13,001] [train] [INFO] epoch=18.00 step=142900, 10.5349 examples/sec lr=0.000001, loss=15.581, loss_ll=2.84297, loss_ll_paf=4.2684, loss_ll_heat=1.41754, q=1000
[2018-07-05 05:05:46,690] [train] [INFO] epoch=18.00 step=143000, 10.5348 examples/sec lr=0.000001, loss=13.9227, loss_ll=2.56079, loss_ll_paf=3.51698, loss_ll_heat=1.60461, q=1000
[2018-07-05 05:08:35,766] [train] [INFO] epoch=18.00 step=143100, 10.5340 examples/sec lr=0.000001, loss=14.4075, loss_ll=2.47737, loss_ll_paf=3.58576, loss_ll_heat=1.36898, q=1000
[2018-07-05 05:11:08,257] [train] [INFO] epoch=18.00 step=143200, 10.5340 examples/sec lr=0.000001, loss=20.2458, loss_ll=3.34102, loss_ll_paf=4.5632, loss_ll_heat=2.11885, q=1000
[2018-07-05 05:13:41,455] [train] [INFO] epoch=18.00 step=143300, 10.5339 examples/sec lr=0.000001, loss=19.7956, loss_ll=3.53219, loss_ll_paf=5.34327, loss_ll_heat=1.7211, q=1000
[2018-07-05 05:16:15,536] [train] [INFO] epoch=18.00 step=143400, 10.5338 examples/sec lr=0.000001, loss=21.0372, loss_ll=3.71344, loss_ll_paf=5.43656, loss_ll_heat=1.99031, q=1000
[2018-07-05 05:18:49,338] [train] [INFO] epoch=18.00 step=143500, 10.5337 examples/sec lr=0.000001, loss=17.381, loss_ll=3.04745, loss_ll_paf=4.18575, loss_ll_heat=1.90915, q=1000
[2018-07-05 05:21:21,403] [train] [INFO] epoch=18.00 step=143600, 10.5337 examples/sec lr=0.000001, loss=13.6415, loss_ll=2.3971, loss_ll_paf=3.32961, loss_ll_heat=1.4646, q=1000
[2018-07-05 05:23:55,275] [train] [INFO] epoch=18.00 step=143700, 10.5336 examples/sec lr=0.000001, loss=21.2745, loss_ll=3.74998, loss_ll_paf=5.42206, loss_ll_heat=2.07789, q=1000
[2018-07-05 05:26:29,836] [train] [INFO] epoch=18.00 step=143800, 10.5335 examples/sec lr=0.000001, loss=21.0423, loss_ll=3.7028, loss_ll_paf=5.15783, loss_ll_heat=2.24777, q=1000
[2018-07-05 05:29:03,784] [train] [INFO] epoch=18.00 step=143900, 10.5334 examples/sec lr=0.000001, loss=41.1893, loss_ll=7.86255, loss_ll_paf=13.4449, loss_ll_heat=2.28023, q=1000
[2018-07-05 05:31:35,957] [train] [INFO] epoch=18.00 step=144000, 10.5334 examples/sec lr=0.000001, loss=24.7007, loss_ll=4.46372, loss_ll_paf=7.28684, loss_ll_heat=1.64059, q=1000
[2018-07-05 05:34:23,229] [train] [INFO] epoch=18.00 step=144100, 10.5326 examples/sec lr=0.000001, loss=22.1629, loss_ll=3.82537, loss_ll_paf=5.60599, loss_ll_heat=2.04475, q=1000
[2018-07-05 05:36:55,033] [train] [INFO] epoch=18.00 step=144200, 10.5326 examples/sec lr=0.000001, loss=20.4929, loss_ll=3.61315, loss_ll_paf=4.95946, loss_ll_heat=2.26685, q=1000
[2018-07-05 05:39:27,052] [train] [INFO] epoch=18.00 step=144300, 10.5326 examples/sec lr=0.000001, loss=19.4988, loss_ll=3.35447, loss_ll_paf=4.61427, loss_ll_heat=2.09467, q=1000
[2018-07-05 05:42:00,617] [train] [INFO] epoch=18.00 step=144400, 10.5325 examples/sec lr=0.000001, loss=12.9999, loss_ll=2.31667, loss_ll_paf=2.95019, loss_ll_heat=1.68315, q=1000
[2018-07-05 05:44:34,659] [train] [INFO] epoch=18.00 step=144500, 10.5324 examples/sec lr=0.000001, loss=31.5566, loss_ll=5.69726, loss_ll_paf=9.10441, loss_ll_heat=2.29011, q=1000
[2018-07-05 05:47:06,683] [train] [INFO] epoch=19.00 step=144600, 10.5324 examples/sec lr=0.000001, loss=40.7737, loss_ll=7.43234, loss_ll_paf=11.9987, loss_ll_heat=2.86602, q=1000
[2018-07-05 05:49:38,694] [train] [INFO] epoch=19.00 step=144700, 10.5324 examples/sec lr=0.000001, loss=19.8149, loss_ll=3.44138, loss_ll_paf=4.78356, loss_ll_heat=2.0992, q=1000
[2018-07-05 05:52:10,808] [train] [INFO] epoch=19.00 step=144800, 10.5324 examples/sec lr=0.000001, loss=16.4419, loss_ll=2.84572, loss_ll_paf=4.27071, loss_ll_heat=1.42073, q=1000
[2018-07-05 05:54:44,764] [train] [INFO] epoch=19.00 step=144900, 10.5323 examples/sec lr=0.000001, loss=14.1719, loss_ll=2.43596, loss_ll_paf=3.31692, loss_ll_heat=1.555, q=1000
[2018-07-05 05:57:18,037] [train] [INFO] epoch=19.00 step=145000, 10.5322 examples/sec lr=0.000001, loss=18.6787, loss_ll=3.27797, loss_ll_paf=4.626, loss_ll_heat=1.92995, q=1000
[2018-07-05 06:00:06,585] [train] [INFO] epoch=19.00 step=145100, 10.5315 examples/sec lr=0.000001, loss=17.5984, loss_ll=3.19035, loss_ll_paf=4.13527, loss_ll_heat=2.24543, q=1000
[2018-07-05 06:02:38,256] [train] [INFO] epoch=19.00 step=145200, 10.5315 examples/sec lr=0.000001, loss=15.0523, loss_ll=2.63619, loss_ll_paf=2.95203, loss_ll_heat=2.32035, q=1000
[2018-07-05 06:05:11,683] [train] [INFO] epoch=19.00 step=145300, 10.5314 examples/sec lr=0.000001, loss=20.5258, loss_ll=3.04479, loss_ll_paf=4.63962, loss_ll_heat=1.44995, q=1000
[2018-07-05 06:07:43,817] [train] [INFO] epoch=19.00 step=145400, 10.5314 examples/sec lr=0.000001, loss=26.4617, loss_ll=4.84052, loss_ll_paf=7.50532, loss_ll_heat=2.17571, q=1000
[2018-07-05 06:10:17,792] [train] [INFO] epoch=19.00 step=145500, 10.5313 examples/sec lr=0.000001, loss=24.7003, loss_ll=4.15501, loss_ll_paf=5.90272, loss_ll_heat=2.40729, q=1000
[2018-07-05 06:12:52,220] [train] [INFO] epoch=19.00 step=145600, 10.5312 examples/sec lr=0.000001, loss=18.0294, loss_ll=3.10805, loss_ll_paf=4.02347, loss_ll_heat=2.19263, q=1000
[2018-07-05 06:15:26,436] [train] [INFO] epoch=19.00 step=145700, 10.5311 examples/sec lr=0.000001, loss=14.5288, loss_ll=2.38873, loss_ll_paf=3.40156, loss_ll_heat=1.3759, q=1000
[2018-07-05 06:18:00,295] [train] [INFO] epoch=19.00 step=145800, 10.5310 examples/sec lr=0.000001, loss=18.1451, loss_ll=3.1021, loss_ll_paf=4.39118, loss_ll_heat=1.81302, q=1000
[2018-07-05 06:20:32,680] [train] [INFO] epoch=19.00 step=145900, 10.5309 examples/sec lr=0.000001, loss=18.351, loss_ll=3.11757, loss_ll_paf=4.74082, loss_ll_heat=1.49432, q=1000
[2018-07-05 06:23:06,194] [train] [INFO] epoch=19.00 step=146000, 10.5309 examples/sec lr=0.000001, loss=17.4115, loss_ll=3.15963, loss_ll_paf=4.72732, loss_ll_heat=1.59194, q=1000
[2018-07-05 06:25:52,771] [train] [INFO] epoch=19.00 step=146100, 10.5302 examples/sec lr=0.000001, loss=22.391, loss_ll=3.96215, loss_ll_paf=5.12828, loss_ll_heat=2.79602, q=1000
[2018-07-05 06:28:27,559] [train] [INFO] epoch=19.00 step=146200, 10.5300 examples/sec lr=0.000001, loss=14.2766, loss_ll=2.52828, loss_ll_paf=3.14056, loss_ll_heat=1.91601, q=1000
[2018-07-05 06:31:01,884] [train] [INFO] epoch=19.00 step=146300, 10.5299 examples/sec lr=0.000001, loss=15.7731, loss_ll=2.61413, loss_ll_paf=3.43924, loss_ll_heat=1.78902, q=1000
[2018-07-05 06:33:35,999] [train] [INFO] epoch=19.00 step=146400, 10.5298 examples/sec lr=0.000001, loss=24.1035, loss_ll=4.42734, loss_ll_paf=6.49604, loss_ll_heat=2.35864, q=1000
[2018-07-05 06:36:11,234] [train] [INFO] epoch=19.00 step=146500, 10.5297 examples/sec lr=0.000001, loss=21.8819, loss_ll=3.72225, loss_ll_paf=5.0716, loss_ll_heat=2.37291, q=1000
[2018-07-05 06:38:45,613] [train] [INFO] epoch=19.00 step=146600, 10.5296 examples/sec lr=0.000001, loss=21.8985, loss_ll=4.02429, loss_ll_paf=6.03724, loss_ll_heat=2.01135, q=1000
[2018-07-05 06:41:19,402] [train] [INFO] epoch=19.00 step=146700, 10.5295 examples/sec lr=0.000001, loss=25.3179, loss_ll=4.61197, loss_ll_paf=7.33489, loss_ll_heat=1.88905, q=1000
[2018-07-05 06:43:52,581] [train] [INFO] epoch=19.00 step=146800, 10.5294 examples/sec lr=0.000001, loss=25.8444, loss_ll=4.82916, loss_ll_paf=6.90316, loss_ll_heat=2.75516, q=1000
[2018-07-05 06:46:25,857] [train] [INFO] epoch=19.00 step=146900, 10.5293 examples/sec lr=0.000001, loss=17.0929, loss_ll=3.05497, loss_ll_paf=4.20274, loss_ll_heat=1.9072, q=1000
[2018-07-05 06:48:58,457] [train] [INFO] epoch=19.00 step=147000, 10.5293 examples/sec lr=0.000001, loss=15.9935, loss_ll=2.87885, loss_ll_paf=4.07761, loss_ll_heat=1.68009, q=1000
[2018-07-05 06:51:46,433] [train] [INFO] epoch=19.00 step=147100, 10.5286 examples/sec lr=0.000001, loss=16.5653, loss_ll=2.75382, loss_ll_paf=3.81322, loss_ll_heat=1.69442, q=1000
[2018-07-05 06:54:20,432] [train] [INFO] epoch=19.00 step=147200, 10.5285 examples/sec lr=0.000001, loss=22.56, loss_ll=3.97809, loss_ll_paf=5.80017, loss_ll_heat=2.15601, q=1000
[2018-07-05 06:56:53,277] [train] [INFO] epoch=19.00 step=147300, 10.5284 examples/sec lr=0.000001, loss=17.0882, loss_ll=3.03106, loss_ll_paf=4.29241, loss_ll_heat=1.76971, q=1000
[2018-07-05 06:59:26,271] [train] [INFO] epoch=19.00 step=147400, 10.5284 examples/sec lr=0.000001, loss=13.4666, loss_ll=2.30007, loss_ll_paf=3.05718, loss_ll_heat=1.54296, q=1000
[2018-07-05 07:01:59,371] [train] [INFO] epoch=19.00 step=147500, 10.5283 examples/sec lr=0.000001, loss=20.5499, loss_ll=3.73073, loss_ll_paf=5.0605, loss_ll_heat=2.40096, q=1000
[2018-07-05 07:04:32,751] [train] [INFO] epoch=19.00 step=147600, 10.5283 examples/sec lr=0.000001, loss=28.5869, loss_ll=5.19731, loss_ll_paf=8.18834, loss_ll_heat=2.20628, q=1000
[2018-07-05 07:07:06,025] [train] [INFO] epoch=19.00 step=147700, 10.5282 examples/sec lr=0.000001, loss=19.3419, loss_ll=3.31665, loss_ll_paf=5.14359, loss_ll_heat=1.48972, q=1000
[2018-07-05 07:09:38,814] [train] [INFO] epoch=19.00 step=147800, 10.5282 examples/sec lr=0.000001, loss=22.516, loss_ll=4.01545, loss_ll_paf=6.17423, loss_ll_heat=1.85667, q=1000
[2018-07-05 07:12:12,353] [train] [INFO] epoch=19.00 step=147900, 10.5281 examples/sec lr=0.000001, loss=19.7776, loss_ll=3.61146, loss_ll_paf=5.26389, loss_ll_heat=1.95903, q=1000
[2018-07-05 07:14:46,842] [train] [INFO] epoch=19.00 step=148000, 10.5280 examples/sec lr=0.000001, loss=22.9843, loss_ll=3.81456, loss_ll_paf=5.12015, loss_ll_heat=2.50898, q=1000
[2018-07-05 07:17:36,826] [train] [INFO] epoch=19.00 step=148100, 10.5271 examples/sec lr=0.000001, loss=19.9476, loss_ll=3.43289, loss_ll_paf=4.83704, loss_ll_heat=2.02874, q=1000
[2018-07-05 07:20:11,027] [train] [INFO] epoch=19.00 step=148200, 10.5270 examples/sec lr=0.000001, loss=22.1044, loss_ll=3.87136, loss_ll_paf=6.32802, loss_ll_heat=1.41471, q=1000
[2018-07-05 07:22:44,732] [train] [INFO] epoch=19.00 step=148300, 10.5269 examples/sec lr=0.000001, loss=17.1841, loss_ll=3.01815, loss_ll_paf=4.10538, loss_ll_heat=1.93092, q=1000
[2018-07-05 07:25:18,459] [train] [INFO] epoch=19.00 step=148400, 10.5269 examples/sec lr=0.000001, loss=18.5973, loss_ll=3.34705, loss_ll_paf=4.92284, loss_ll_heat=1.77126, q=1000
[2018-07-05 07:27:52,475] [train] [INFO] epoch=19.00 step=148500, 10.5268 examples/sec lr=0.000001, loss=16.6506, loss_ll=2.68747, loss_ll_paf=3.81555, loss_ll_heat=1.55939, q=1000
[2018-07-05 07:30:24,458] [train] [INFO] epoch=19.00 step=148600, 10.5268 examples/sec lr=0.000001, loss=19.6692, loss_ll=3.76042, loss_ll_paf=5.87759, loss_ll_heat=1.64326, q=1000
[2018-07-05 07:32:59,402] [train] [INFO] epoch=19.00 step=148700, 10.5266 examples/sec lr=0.000001, loss=25.9777, loss_ll=4.80073, loss_ll_paf=7.84045, loss_ll_heat=1.76101, q=1000
[2018-07-05 07:35:35,672] [train] [INFO] epoch=19.00 step=148800, 10.5264 examples/sec lr=0.000001, loss=21.74, loss_ll=2.87215, loss_ll_paf=3.74192, loss_ll_heat=2.00237, q=1000
[2018-07-05 07:38:10,699] [train] [INFO] epoch=19.00 step=148900, 10.5263 examples/sec lr=0.000001, loss=21.9203, loss_ll=4.08935, loss_ll_paf=6.22774, loss_ll_heat=1.95096, q=1000
[2018-07-05 07:40:43,779] [train] [INFO] epoch=19.00 step=149000, 10.5262 examples/sec lr=0.000001, loss=12.9947, loss_ll=2.38543, loss_ll_paf=3.37694, loss_ll_heat=1.39392, q=1000
[2018-07-05 07:43:32,079] [train] [INFO] epoch=19.00 step=149100, 10.5255 examples/sec lr=0.000001, loss=25.4122, loss_ll=4.08009, loss_ll_paf=6.2252, loss_ll_heat=1.93498, q=1000
[2018-07-05 07:46:04,673] [train] [INFO] epoch=19.00 step=149200, 10.5255 examples/sec lr=0.000001, loss=17.0666, loss_ll=2.86868, loss_ll_paf=3.7727, loss_ll_heat=1.96467, q=1000
[2018-07-05 07:48:37,621] [train] [INFO] epoch=19.00 step=149300, 10.5254 examples/sec lr=0.000001, loss=19.3984, loss_ll=3.22448, loss_ll_paf=4.83098, loss_ll_heat=1.61798, q=1000
[2018-07-05 07:51:10,896] [train] [INFO] epoch=19.00 step=149400, 10.5254 examples/sec lr=0.000001, loss=23.7327, loss_ll=4.44078, loss_ll_paf=7.14153, loss_ll_heat=1.74004, q=1000
[2018-07-05 07:53:46,225] [train] [INFO] epoch=19.00 step=149500, 10.5252 examples/sec lr=0.000001, loss=18.0738, loss_ll=3.24092, loss_ll_paf=4.75473, loss_ll_heat=1.72711, q=1000
[2018-07-05 07:56:22,208] [train] [INFO] epoch=19.00 step=149600, 10.5250 examples/sec lr=0.000001, loss=15.5203, loss_ll=2.69387, loss_ll_paf=3.8108, loss_ll_heat=1.57694, q=1000
[2018-07-05 07:58:57,733] [train] [INFO] epoch=19.00 step=149700, 10.5249 examples/sec lr=0.000001, loss=19.3137, loss_ll=3.45146, loss_ll_paf=5.27812, loss_ll_heat=1.62481, q=1000
[2018-07-05 08:01:33,861] [train] [INFO] epoch=19.00 step=149800, 10.5247 examples/sec lr=0.000001, loss=19.9888, loss_ll=3.55426, loss_ll_paf=5.15012, loss_ll_heat=1.9584, q=1000
[2018-07-05 08:04:08,336] [train] [INFO] epoch=19.00 step=149900, 10.5246 examples/sec lr=0.000001, loss=20.0534, loss_ll=3.36709, loss_ll_paf=4.81265, loss_ll_heat=1.92153, q=1000
[2018-07-05 08:06:40,617] [train] [INFO] epoch=19.00 step=150000, 10.5245 examples/sec lr=0.000000, loss=22.4224, loss_ll=3.83271, loss_ll_paf=5.6855, loss_ll_heat=1.97992, q=1000
[2018-07-05 08:09:30,032] [train] [INFO] epoch=19.00 step=150100, 10.5237 examples/sec lr=0.000000, loss=28.7232, loss_ll=5.37652, loss_ll_paf=8.77656, loss_ll_heat=1.97648, q=1000
[2018-07-05 08:12:01,467] [train] [INFO] epoch=19.00 step=150200, 10.5238 examples/sec lr=0.000000, loss=19.4546, loss_ll=3.40902, loss_ll_paf=4.46145, loss_ll_heat=2.35658, q=1000
[2018-07-05 08:14:34,717] [train] [INFO] epoch=19.00 step=150300, 10.5237 examples/sec lr=0.000000, loss=19.6443, loss_ll=3.51351, loss_ll_paf=4.91175, loss_ll_heat=2.11526, q=1000
[2018-07-05 08:17:08,329] [train] [INFO] epoch=19.00 step=150400, 10.5236 examples/sec lr=0.000000, loss=29.4834, loss_ll=5.1528, loss_ll_paf=8.21043, loss_ll_heat=2.09517, q=1000
[2018-07-05 08:19:42,987] [train] [INFO] epoch=19.00 step=150500, 10.5235 examples/sec lr=0.000000, loss=24.7055, loss_ll=4.55953, loss_ll_paf=6.52236, loss_ll_heat=2.59669, q=1000
[2018-07-05 08:22:16,728] [train] [INFO] epoch=19.00 step=150600, 10.5234 examples/sec lr=0.000000, loss=18.235, loss_ll=3.09819, loss_ll_paf=4.22436, loss_ll_heat=1.97203, q=1000
[2018-07-05 08:24:49,444] [train] [INFO] epoch=19.00 step=150700, 10.5234 examples/sec lr=0.000000, loss=14.6708, loss_ll=2.48642, loss_ll_paf=3.56366, loss_ll_heat=1.40918, q=1000
[2018-07-05 08:27:22,278] [train] [INFO] epoch=19.00 step=150800, 10.5234 examples/sec lr=0.000000, loss=16.2953, loss_ll=2.5372, loss_ll_paf=3.22925, loss_ll_heat=1.84516, q=1000
[2018-07-05 08:29:56,739] [train] [INFO] epoch=19.00 step=150900, 10.5233 examples/sec lr=0.000000, loss=25.4725, loss_ll=4.88824, loss_ll_paf=7.82538, loss_ll_heat=1.9511, q=1000
[2018-07-05 08:32:30,392] [train] [INFO] epoch=19.00 step=151000, 10.5232 examples/sec lr=0.000000, loss=22.3695, loss_ll=3.76928, loss_ll_paf=5.57507, loss_ll_heat=1.96349, q=1000
[2018-07-05 08:35:22,120] [train] [INFO] epoch=19.00 step=151100, 10.5223 examples/sec lr=0.000000, loss=25.4132, loss_ll=4.61787, loss_ll_paf=6.87737, loss_ll_heat=2.35838, q=1000
[2018-07-05 08:37:56,748] [train] [INFO] epoch=19.00 step=151200, 10.5222 examples/sec lr=0.000000, loss=21.8734, loss_ll=3.76505, loss_ll_paf=5.12857, loss_ll_heat=2.40153, q=1000
[2018-07-05 08:40:30,881] [train] [INFO] epoch=19.00 step=151300, 10.5221 examples/sec lr=0.000000, loss=23.7904, loss_ll=4.37559, loss_ll_paf=6.19858, loss_ll_heat=2.5526, q=1000
[2018-07-05 08:43:06,422] [train] [INFO] epoch=19.00 step=151400, 10.5219 examples/sec lr=0.000000, loss=18.3118, loss_ll=3.34762, loss_ll_paf=4.10887, loss_ll_heat=2.58637, q=1000
[2018-07-05 08:45:38,527] [train] [INFO] epoch=19.00 step=151500, 10.5219 examples/sec lr=0.000000, loss=17.6884, loss_ll=3.19896, loss_ll_paf=3.66236, loss_ll_heat=2.73556, q=1000
[2018-07-05 08:48:11,788] [train] [INFO] epoch=19.00 step=151600, 10.5219 examples/sec lr=0.000000, loss=19.8751, loss_ll=3.56847, loss_ll_paf=5.08345, loss_ll_heat=2.05349, q=1000
[2018-07-05 08:50:43,998] [train] [INFO] epoch=19.00 step=151700, 10.5219 examples/sec lr=0.000000, loss=24.7878, loss_ll=4.53702, loss_ll_paf=7.04311, loss_ll_heat=2.03094, q=1000
[2018-07-05 08:53:18,196] [train] [INFO] epoch=19.00 step=151800, 10.5218 examples/sec lr=0.000000, loss=17.5798, loss_ll=3.11643, loss_ll_paf=4.56549, loss_ll_heat=1.66737, q=1000
[2018-07-05 08:55:51,212] [train] [INFO] epoch=19.00 step=151900, 10.5217 examples/sec lr=0.000000, loss=20.4097, loss_ll=3.68881, loss_ll_paf=5.0117, loss_ll_heat=2.36591, q=1000
[2018-07-05 08:58:23,979] [train] [INFO] epoch=19.00 step=152000, 10.5217 examples/sec lr=0.000000, loss=30.0773, loss_ll=5.42333, loss_ll_paf=7.39409, loss_ll_heat=3.45258, q=1000
[2018-07-05 09:01:13,470] [train] [INFO] epoch=19.00 step=152100, 10.5209 examples/sec lr=0.000000, loss=16.4534, loss_ll=3.05826, loss_ll_paf=4.51406, loss_ll_heat=1.60245, q=1000
[2018-07-05 09:03:47,471] [train] [INFO] epoch=20.00 step=152200, 10.5208 examples/sec lr=0.000000, loss=18.5686, loss_ll=3.32183, loss_ll_paf=5.02214, loss_ll_heat=1.62153, q=1000
[2018-07-05 09:06:21,378] [train] [INFO] epoch=20.00 step=152300, 10.5207 examples/sec lr=0.000000, loss=16.9285, loss_ll=3.08928, loss_ll_paf=4.36439, loss_ll_heat=1.81418, q=1000
[2018-07-05 09:08:54,315] [train] [INFO] epoch=20.00 step=152400, 10.5207 examples/sec lr=0.000000, loss=22.8412, loss_ll=4.15959, loss_ll_paf=6.37229, loss_ll_heat=1.94688, q=1000
[2018-07-05 09:11:26,888] [train] [INFO] epoch=20.00 step=152500, 10.5207 examples/sec lr=0.000000, loss=19.8242, loss_ll=3.37089, loss_ll_paf=4.31415, loss_ll_heat=2.42763, q=1000
[2018-07-05 09:13:59,623] [train] [INFO] epoch=20.00 step=152600, 10.5206 examples/sec lr=0.000000, loss=13.0378, loss_ll=2.12474, loss_ll_paf=2.93388, loss_ll_heat=1.31559, q=1000
[2018-07-05 09:16:31,517] [train] [INFO] epoch=20.00 step=152700, 10.5206 examples/sec lr=0.000000, loss=17.9499, loss_ll=3.03053, loss_ll_paf=4.20967, loss_ll_heat=1.8514, q=1000
[2018-07-05 09:19:02,594] [train] [INFO] epoch=20.00 step=152800, 10.5207 examples/sec lr=0.000000, loss=17.8076, loss_ll=3.11524, loss_ll_paf=4.14586, loss_ll_heat=2.08462, q=1000
[2018-07-05 09:21:36,851] [train] [INFO] epoch=20.00 step=152900, 10.5206 examples/sec lr=0.000000, loss=20.8835, loss_ll=3.59559, loss_ll_paf=5.58634, loss_ll_heat=1.60484, q=1000
[2018-07-05 09:24:09,240] [train] [INFO] epoch=20.00 step=153000, 10.5206 examples/sec lr=0.000000, loss=16.8813, loss_ll=2.83447, loss_ll_paf=3.63651, loss_ll_heat=2.03243, q=1000
[2018-07-05 09:26:57,539] [train] [INFO] epoch=20.00 step=153100, 10.5198 examples/sec lr=0.000000, loss=26.1251, loss_ll=4.54134, loss_ll_paf=6.7881, loss_ll_heat=2.29459, q=1000
[2018-07-05 09:29:30,674] [train] [INFO] epoch=20.00 step=153200, 10.5198 examples/sec lr=0.000000, loss=15.0273, loss_ll=2.60737, loss_ll_paf=3.52464, loss_ll_heat=1.69011, q=1000
[2018-07-05 09:32:02,099] [train] [INFO] epoch=20.00 step=153300, 10.5198 examples/sec lr=0.000000, loss=25.6894, loss_ll=4.11308, loss_ll_paf=6.38355, loss_ll_heat=1.84261, q=1000
[2018-07-05 09:34:33,607] [train] [INFO] epoch=20.00 step=153400, 10.5198 examples/sec lr=0.000000, loss=26.0266, loss_ll=4.68367, loss_ll_paf=6.89386, loss_ll_heat=2.47348, q=1000
[2018-07-05 09:37:06,777] [train] [INFO] epoch=20.00 step=153500, 10.5198 examples/sec lr=0.000000, loss=19.4279, loss_ll=3.46593, loss_ll_paf=4.84416, loss_ll_heat=2.08769, q=1000
[2018-07-05 09:39:40,553] [train] [INFO] epoch=20.00 step=153600, 10.5197 examples/sec lr=0.000000, loss=22.6715, loss_ll=3.70008, loss_ll_paf=5.14252, loss_ll_heat=2.25765, q=1000
[2018-07-05 09:42:13,618] [train] [INFO] epoch=20.00 step=153700, 10.5197 examples/sec lr=0.000000, loss=15.6267, loss_ll=2.47491, loss_ll_paf=3.45394, loss_ll_heat=1.49588, q=1000
[2018-07-05 09:44:45,089] [train] [INFO] epoch=20.00 step=153800, 10.5197 examples/sec lr=0.000000, loss=20.0677, loss_ll=3.52477, loss_ll_paf=5.1303, loss_ll_heat=1.91924, q=1000
[2018-07-05 09:47:17,598] [train] [INFO] epoch=20.00 step=153900, 10.5197 examples/sec lr=0.000000, loss=21.3261, loss_ll=3.6839, loss_ll_paf=5.28736, loss_ll_heat=2.08044, q=1000
[2018-07-05 09:49:49,409] [train] [INFO] epoch=20.00 step=154000, 10.5197 examples/sec lr=0.000000, loss=32.3389, loss_ll=5.50789, loss_ll_paf=7.9419, loss_ll_heat=3.07387, q=1000
[2018-07-05 09:52:37,377] [train] [INFO] epoch=20.00 step=154100, 10.5190 examples/sec lr=0.000000, loss=21.1696, loss_ll=3.60141, loss_ll_paf=5.02627, loss_ll_heat=2.17655, q=1000
[2018-07-05 09:55:11,188] [train] [INFO] epoch=20.00 step=154200, 10.5189 examples/sec lr=0.000000, loss=24.0152, loss_ll=4.15538, loss_ll_paf=6.23923, loss_ll_heat=2.07152, q=1000
[2018-07-05 09:57:45,116] [train] [INFO] epoch=20.00 step=154300, 10.5188 examples/sec lr=0.000000, loss=20.0232, loss_ll=3.73313, loss_ll_paf=5.03969, loss_ll_heat=2.42657, q=1000
[2018-07-05 10:00:16,591] [train] [INFO] epoch=20.00 step=154400, 10.5189 examples/sec lr=0.000000, loss=23.2275, loss_ll=4.06967, loss_ll_paf=6.33319, loss_ll_heat=1.80616, q=1000
[2018-07-05 10:02:47,284] [train] [INFO] epoch=20.00 step=154500, 10.5189 examples/sec lr=0.000000, loss=30.344, loss_ll=5.26756, loss_ll_paf=8.12, loss_ll_heat=2.41512, q=1000
[2018-07-05 10:05:19,029] [train] [INFO] epoch=20.00 step=154600, 10.5189 examples/sec lr=0.000000, loss=12.7018, loss_ll=2.17121, loss_ll_paf=2.64826, loss_ll_heat=1.69416, q=1000
[2018-07-05 10:07:51,054] [train] [INFO] epoch=20.00 step=154700, 10.5189 examples/sec lr=0.000000, loss=17.6335, loss_ll=3.24268, loss_ll_paf=4.75351, loss_ll_heat=1.73185, q=1000
[2018-07-05 10:10:21,721] [train] [INFO] epoch=20.00 step=154800, 10.5190 examples/sec lr=0.000000, loss=17.0506, loss_ll=3.09948, loss_ll_paf=4.12835, loss_ll_heat=2.07061, q=1000
[2018-07-05 10:12:55,721] [train] [INFO] epoch=20.00 step=154900, 10.5189 examples/sec lr=0.000000, loss=17.504, loss_ll=3.17675, loss_ll_paf=4.52294, loss_ll_heat=1.83056, q=1000
[2018-07-05 10:15:28,666] [train] [INFO] epoch=20.00 step=155000, 10.5189 examples/sec lr=0.000000, loss=30.2734, loss_ll=5.70293, loss_ll_paf=8.69556, loss_ll_heat=2.7103, q=1000
[2018-07-05 10:18:16,446] [train] [INFO] epoch=20.00 step=155100, 10.5182 examples/sec lr=0.000000, loss=25.169, loss_ll=4.56535, loss_ll_paf=6.09909, loss_ll_heat=3.03161, q=1000
[2018-07-05 10:20:50,713] [train] [INFO] epoch=20.00 step=155200, 10.5181 examples/sec lr=0.000000, loss=44.1623, loss_ll=8.576, loss_ll_paf=14.4385, loss_ll_heat=2.71352, q=1000
[2018-07-05 10:23:23,447] [train] [INFO] epoch=20.00 step=155300, 10.5181 examples/sec lr=0.000000, loss=12.7315, loss_ll=2.22958, loss_ll_paf=2.90864, loss_ll_heat=1.55053, q=1000
[2018-07-05 10:25:58,042] [train] [INFO] epoch=20.00 step=155400, 10.5179 examples/sec lr=0.000000, loss=21.7579, loss_ll=4.05137, loss_ll_paf=5.9236, loss_ll_heat=2.17913, q=1000
[2018-07-05 10:28:29,996] [train] [INFO] epoch=20.00 step=155500, 10.5180 examples/sec lr=0.000000, loss=23.746, loss_ll=4.30168, loss_ll_paf=6.04878, loss_ll_heat=2.55458, q=1000
[2018-07-05 10:31:03,631] [train] [INFO] epoch=20.00 step=155600, 10.5179 examples/sec lr=0.000000, loss=22.561, loss_ll=3.97257, loss_ll_paf=6.35457, loss_ll_heat=1.59057, q=1000
[2018-07-05 10:33:37,973] [train] [INFO] epoch=20.00 step=155700, 10.5178 examples/sec lr=0.000000, loss=16.7729, loss_ll=2.87038, loss_ll_paf=3.51545, loss_ll_heat=2.22531, q=1000
[2018-07-05 10:36:10,126] [train] [INFO] epoch=20.00 step=155800, 10.5178 examples/sec lr=0.000000, loss=11.851, loss_ll=2.07205, loss_ll_paf=2.8832, loss_ll_heat=1.26089, q=1000
[2018-07-05 10:38:44,835] [train] [INFO] epoch=20.00 step=155900, 10.5177 examples/sec lr=0.000000, loss=17.9328, loss_ll=3.03643, loss_ll_paf=4.61076, loss_ll_heat=1.4621, q=1000
[2018-07-05 10:41:16,894] [train] [INFO] epoch=20.00 step=156000, 10.5177 examples/sec lr=0.000000, loss=20.5696, loss_ll=3.60486, loss_ll_paf=4.68426, loss_ll_heat=2.52546, q=1000
[2018-07-05 10:44:05,340] [train] [INFO] epoch=20.00 step=156100, 10.5170 examples/sec lr=0.000000, loss=20.0004, loss_ll=3.57096, loss_ll_paf=5.1808, loss_ll_heat=1.96113, q=1000
[2018-07-05 10:46:38,563] [train] [INFO] epoch=20.00 step=156200, 10.5169 examples/sec lr=0.000000, loss=25.2882, loss_ll=4.79305, loss_ll_paf=7.40623, loss_ll_heat=2.17987, q=1000
[2018-07-05 10:49:10,536] [train] [INFO] epoch=20.00 step=156300, 10.5169 examples/sec lr=0.000000, loss=22.7688, loss_ll=3.84905, loss_ll_paf=6.16821, loss_ll_heat=1.52989, q=1000
[2018-07-05 10:51:42,996] [train] [INFO] epoch=20.00 step=156400, 10.5169 examples/sec lr=0.000000, loss=14.1545, loss_ll=2.47322, loss_ll_paf=3.09357, loss_ll_heat=1.85286, q=1000
[2018-07-05 10:54:16,448] [train] [INFO] epoch=20.00 step=156500, 10.5168 examples/sec lr=0.000000, loss=15.0384, loss_ll=2.82629, loss_ll_paf=4.22604, loss_ll_heat=1.42654, q=1000
[2018-07-05 10:56:50,887] [train] [INFO] epoch=20.00 step=156600, 10.5167 examples/sec lr=0.000000, loss=18.0438, loss_ll=3.30174, loss_ll_paf=4.77976, loss_ll_heat=1.82372, q=1000
[2018-07-05 10:59:23,602] [train] [INFO] epoch=20.00 step=156700, 10.5167 examples/sec lr=0.000000, loss=13.4777, loss_ll=2.39086, loss_ll_paf=3.50707, loss_ll_heat=1.27466, q=1000
[2018-07-05 11:01:57,938] [train] [INFO] epoch=20.00 step=156800, 10.5166 examples/sec lr=0.000000, loss=20.6174, loss_ll=3.56757, loss_ll_paf=5.41427, loss_ll_heat=1.72086, q=1000
[2018-07-05 11:04:30,133] [train] [INFO] epoch=20.00 step=156900, 10.5166 examples/sec lr=0.000000, loss=16.861, loss_ll=2.98254, loss_ll_paf=4.00091, loss_ll_heat=1.96416, q=1000
[2018-07-05 11:07:00,467] [train] [INFO] epoch=20.00 step=157000, 10.5167 examples/sec lr=0.000000, loss=16.2249, loss_ll=2.77289, loss_ll_paf=4.02378, loss_ll_heat=1.52201, q=1000
[2018-07-05 11:09:48,253] [train] [INFO] epoch=20.00 step=157100, 10.5160 examples/sec lr=0.000000, loss=21.7368, loss_ll=3.45715, loss_ll_paf=5.29017, loss_ll_heat=1.62413, q=1000
[2018-07-05 11:12:20,115] [train] [INFO] epoch=20.00 step=157200, 10.5160 examples/sec lr=0.000000, loss=12.1713, loss_ll=2.14066, loss_ll_paf=2.77872, loss_ll_heat=1.5026, q=1000
[2018-07-05 11:14:51,766] [train] [INFO] epoch=20.00 step=157300, 10.5160 examples/sec lr=0.000000, loss=19.4166, loss_ll=3.22569, loss_ll_paf=4.42922, loss_ll_heat=2.02215, q=1000
[2018-07-05 11:17:23,875] [train] [INFO] epoch=20.00 step=157400, 10.5160 examples/sec lr=0.000000, loss=12.9916, loss_ll=2.20072, loss_ll_paf=3.04583, loss_ll_heat=1.35561, q=1000
[2018-07-05 11:20:44,581] [train] [INFO] epoch=20.00 step=157500, 10.5139 examples/sec lr=0.000000, loss=20.4675, loss_ll=3.82496, loss_ll_paf=5.58566, loss_ll_heat=2.06426, q=1000
[2018-07-05 11:23:19,587] [train] [INFO] epoch=20.00 step=157600, 10.5138 examples/sec lr=0.000000, loss=20.8323, loss_ll=3.61797, loss_ll_paf=5.08938, loss_ll_heat=2.14656, q=1000
[2018-07-05 11:25:52,018] [train] [INFO] epoch=20.00 step=157700, 10.5138 examples/sec lr=0.000000, loss=19.9208, loss_ll=3.54885, loss_ll_paf=4.63126, loss_ll_heat=2.46644, q=1000
[2018-07-05 11:28:25,684] [train] [INFO] epoch=20.00 step=157800, 10.5137 examples/sec lr=0.000000, loss=14.3506, loss_ll=2.64977, loss_ll_paf=3.72374, loss_ll_heat=1.57579, q=1000
[2018-07-05 11:31:01,457] [train] [INFO] epoch=20.00 step=157900, 10.5136 examples/sec lr=0.000000, loss=19.4353, loss_ll=3.59097, loss_ll_paf=5.32526, loss_ll_heat=1.85667, q=1000
[2018-07-05 11:33:30,569] [train] [INFO] epoch=20.00 step=158000, 10.5137 examples/sec lr=0.000000, loss=25.9167, loss_ll=4.83934, loss_ll_paf=6.87465, loss_ll_heat=2.80403, q=1000
[2018-07-05 11:36:17,848] [train] [INFO] epoch=20.00 step=158100, 10.5130 examples/sec lr=0.000000, loss=19.0106, loss_ll=3.4895, loss_ll_paf=4.81578, loss_ll_heat=2.16322, q=1000
[2018-07-05 11:38:50,527] [train] [INFO] epoch=20.00 step=158200, 10.5130 examples/sec lr=0.000000, loss=21.4247, loss_ll=3.48396, loss_ll_paf=5.23624, loss_ll_heat=1.73168, q=1000
[2018-07-05 11:41:24,118] [train] [INFO] epoch=20.00 step=158300, 10.5129 examples/sec lr=0.000000, loss=25.0386, loss_ll=4.11704, loss_ll_paf=5.73488, loss_ll_heat=2.4992, q=1000
[2018-07-05 11:44:01,210] [train] [INFO] epoch=20.00 step=158400, 10.5127 examples/sec lr=0.000000, loss=18.2425, loss_ll=3.36924, loss_ll_paf=4.79022, loss_ll_heat=1.94825, q=1000
[2018-07-05 11:46:35,433] [train] [INFO] epoch=20.00 step=158500, 10.5126 examples/sec lr=0.000000, loss=14.8804, loss_ll=2.71697, loss_ll_paf=3.83943, loss_ll_heat=1.59452, q=1000
[2018-07-05 11:49:09,042] [train] [INFO] epoch=20.00 step=158600, 10.5126 examples/sec lr=0.000000, loss=19.9967, loss_ll=3.35856, loss_ll_paf=4.60771, loss_ll_heat=2.1094, q=1000
[2018-07-05 11:51:42,257] [train] [INFO] epoch=20.00 step=158700, 10.5125 examples/sec lr=0.000000, loss=21.8567, loss_ll=4.12598, loss_ll_paf=6.24806, loss_ll_heat=2.00389, q=1000
[2018-07-05 11:54:15,263] [train] [INFO] epoch=20.00 step=158800, 10.5125 examples/sec lr=0.000000, loss=27.1233, loss_ll=4.5695, loss_ll_paf=6.30744, loss_ll_heat=2.83157, q=1000
[2018-07-05 11:56:46,493] [train] [INFO] epoch=20.00 step=158900, 10.5125 examples/sec lr=0.000000, loss=15.0305, loss_ll=2.65781, loss_ll_paf=3.3988, loss_ll_heat=1.91682, q=1000
[2018-07-05 11:59:18,133] [train] [INFO] epoch=20.00 step=159000, 10.5126 examples/sec lr=0.000000, loss=20.0992, loss_ll=3.59002, loss_ll_paf=5.46608, loss_ll_heat=1.71396, q=1000
[2018-07-05 12:02:13,916] [train] [INFO] epoch=20.00 step=159100, 10.5115 examples/sec lr=0.000000, loss=25.4206, loss_ll=4.55678, loss_ll_paf=7.46199, loss_ll_heat=1.65157, q=1000
[2018-07-05 12:04:47,260] [train] [INFO] epoch=20.00 step=159200, 10.5115 examples/sec lr=0.000000, loss=24.0766, loss_ll=4.093, loss_ll_paf=6.27409, loss_ll_heat=1.91191, q=1000
[2018-07-05 12:07:20,715] [train] [INFO] epoch=20.00 step=159300, 10.5114 examples/sec lr=0.000000, loss=30.2464, loss_ll=5.39679, loss_ll_paf=8.50717, loss_ll_heat=2.28642, q=1000
[2018-07-05 12:09:54,228] [train] [INFO] epoch=20.00 step=159400, 10.5114 examples/sec lr=0.000000, loss=27.9754, loss_ll=4.71695, loss_ll_paf=6.67522, loss_ll_heat=2.75868, q=1000
[2018-07-05 12:12:27,653] [train] [INFO] epoch=20.00 step=159500, 10.5113 examples/sec lr=0.000000, loss=20.5692, loss_ll=3.78698, loss_ll_paf=5.80033, loss_ll_heat=1.77363, q=1000
[2018-07-05 12:15:00,770] [train] [INFO] epoch=20.00 step=159600, 10.5113 examples/sec lr=0.000000, loss=21.8091, loss_ll=4.01323, loss_ll_paf=5.50585, loss_ll_heat=2.52061, q=1000
[2018-07-05 12:17:34,277] [train] [INFO] epoch=20.00 step=159700, 10.5112 examples/sec lr=0.000000, loss=21.6152, loss_ll=4.01745, loss_ll_paf=5.81467, loss_ll_heat=2.22024, q=1000
[2018-07-05 12:20:05,933] [train] [INFO] epoch=21.00 step=159800, 10.5113 examples/sec lr=0.000000, loss=17.7335, loss_ll=2.98633, loss_ll_paf=4.67065, loss_ll_heat=1.30202, q=1000
[2018-07-05 12:22:39,852] [train] [INFO] epoch=21.00 step=159900, 10.5112 examples/sec lr=0.000000, loss=22.6273, loss_ll=4.06731, loss_ll_paf=5.65289, loss_ll_heat=2.48173, q=1000
[2018-07-05 12:25:13,380] [train] [INFO] epoch=21.00 step=160000, 10.5111 examples/sec lr=0.000000, loss=14.5948, loss_ll=2.11316, loss_ll_paf=2.8698, loss_ll_heat=1.35653, q=1000
[2018-07-05 12:28:03,357] [train] [INFO] epoch=21.00 step=160100, 10.5104 examples/sec lr=0.000000, loss=31.1053, loss_ll=5.18205, loss_ll_paf=8.01394, loss_ll_heat=2.35016, q=1000
[2018-07-05 12:30:37,047] [train] [INFO] epoch=21.00 step=160200, 10.5103 examples/sec lr=0.000000, loss=27.7974, loss_ll=5.13194, loss_ll_paf=7.57897, loss_ll_heat=2.68492, q=1000
[2018-07-05 12:33:13,309] [train] [INFO] epoch=21.00 step=160300, 10.5101 examples/sec lr=0.000000, loss=20.4859, loss_ll=3.61928, loss_ll_paf=5.00043, loss_ll_heat=2.23814, q=1000
[2018-07-05 12:35:43,801] [train] [INFO] epoch=21.00 step=160400, 10.5102 examples/sec lr=0.000000, loss=16.0257, loss_ll=2.9702, loss_ll_paf=4.37865, loss_ll_heat=1.56175, q=1000
[2018-07-05 12:38:17,118] [train] [INFO] epoch=21.00 step=160500, 10.5102 examples/sec lr=0.000000, loss=17.5166, loss_ll=3.15202, loss_ll_paf=4.40714, loss_ll_heat=1.8969, q=1000
[2018-07-05 12:40:49,247] [train] [INFO] epoch=21.00 step=160600, 10.5102 examples/sec lr=0.000000, loss=16.8003, loss_ll=2.99947, loss_ll_paf=4.04465, loss_ll_heat=1.95429, q=1000
[2018-07-05 12:43:21,721] [train] [INFO] epoch=21.00 step=160700, 10.5102 examples/sec lr=0.000000, loss=21.1316, loss_ll=3.69956, loss_ll_paf=5.28956, loss_ll_heat=2.10955, q=1000
[2018-07-05 12:45:55,816] [train] [INFO] epoch=21.00 step=160800, 10.5101 examples/sec lr=0.000000, loss=23.0654, loss_ll=4.41169, loss_ll_paf=6.64342, loss_ll_heat=2.17996, q=1000
[2018-07-05 12:48:28,311] [train] [INFO] epoch=21.00 step=160900, 10.5101 examples/sec lr=0.000000, loss=27.3221, loss_ll=5.05191, loss_ll_paf=8.06359, loss_ll_heat=2.04023, q=1000
[2018-07-05 12:51:02,542] [train] [INFO] epoch=21.00 step=161000, 10.5100 examples/sec lr=0.000000, loss=22.4695, loss_ll=4.27202, loss_ll_paf=6.57802, loss_ll_heat=1.96602, q=1000
[2018-07-05 12:53:51,564] [train] [INFO] epoch=21.00 step=161100, 10.5093 examples/sec lr=0.000000, loss=22.2888, loss_ll=4.13024, loss_ll_paf=5.9344, loss_ll_heat=2.32608, q=1000
[2018-07-05 12:56:23,196] [train] [INFO] epoch=21.00 step=161200, 10.5093 examples/sec lr=0.000000, loss=18.7841, loss_ll=3.36066, loss_ll_paf=4.94486, loss_ll_heat=1.77645, q=1000
[2018-07-05 12:58:58,756] [train] [INFO] epoch=21.00 step=161300, 10.5091 examples/sec lr=0.000000, loss=15.2543, loss_ll=2.55495, loss_ll_paf=3.1654, loss_ll_heat=1.9445, q=1000
[2018-07-05 13:01:32,438] [train] [INFO] epoch=21.00 step=161400, 10.5091 examples/sec lr=0.000000, loss=18.7254, loss_ll=3.1705, loss_ll_paf=4.30538, loss_ll_heat=2.03562, q=1000
[2018-07-05 13:04:06,903] [train] [INFO] epoch=21.00 step=161500, 10.5090 examples/sec lr=0.000000, loss=20.0623, loss_ll=3.51094, loss_ll_paf=5.25757, loss_ll_heat=1.76431, q=1000
[2018-07-05 13:06:37,677] [train] [INFO] epoch=21.00 step=161600, 10.5091 examples/sec lr=0.000000, loss=22.9658, loss_ll=4.09861, loss_ll_paf=5.94361, loss_ll_heat=2.2536, q=1000
[2018-07-05 13:09:07,270] [train] [INFO] epoch=21.00 step=161700, 10.5092 examples/sec lr=0.000000, loss=21.7551, loss_ll=3.9912, loss_ll_paf=5.99316, loss_ll_heat=1.98924, q=1000
[2018-07-05 13:11:40,021] [train] [INFO] epoch=21.00 step=161800, 10.5091 examples/sec lr=0.000000, loss=23.2522, loss_ll=4.02857, loss_ll_paf=5.55651, loss_ll_heat=2.50064, q=1000
[2018-07-05 13:14:14,680] [train] [INFO] epoch=21.00 step=161900, 10.5090 examples/sec lr=0.000000, loss=19.865, loss_ll=3.71606, loss_ll_paf=5.45502, loss_ll_heat=1.9771, q=1000
[2018-07-05 13:16:46,353] [train] [INFO] epoch=21.00 step=162000, 10.5091 examples/sec lr=0.000000, loss=17.7678, loss_ll=2.99422, loss_ll_paf=4.15118, loss_ll_heat=1.83725, q=1000
[2018-07-05 13:19:35,981] [train] [INFO] epoch=21.00 step=162100, 10.5083 examples/sec lr=0.000000, loss=25.1807, loss_ll=4.65038, loss_ll_paf=7.13275, loss_ll_heat=2.16801, q=1000
[2018-07-05 13:22:10,863] [train] [INFO] epoch=21.00 step=162200, 10.5082 examples/sec lr=0.000000, loss=14.9187, loss_ll=2.73375, loss_ll_paf=3.77619, loss_ll_heat=1.69131, q=1000
[2018-07-05 13:24:43,576] [train] [INFO] epoch=21.00 step=162300, 10.5082 examples/sec lr=0.000000, loss=15.9583, loss_ll=2.63602, loss_ll_paf=3.74348, loss_ll_heat=1.52857, q=1000
[2018-07-05 13:27:14,565] [train] [INFO] epoch=21.00 step=162400, 10.5082 examples/sec lr=0.000000, loss=23.7474, loss_ll=4.48334, loss_ll_paf=6.96522, loss_ll_heat=2.00146, q=1000
[2018-07-05 13:29:47,965] [train] [INFO] epoch=21.00 step=162500, 10.5082 examples/sec lr=0.000000, loss=21.0822, loss_ll=3.77278, loss_ll_paf=5.59095, loss_ll_heat=1.95461, q=1000
[2018-07-05 13:32:21,439] [train] [INFO] epoch=21.00 step=162600, 10.5081 examples/sec lr=0.000000, loss=18.3483, loss_ll=3.27465, loss_ll_paf=4.91534, loss_ll_heat=1.63397, q=1000
[2018-07-05 13:34:55,818] [train] [INFO] epoch=21.00 step=162700, 10.5081 examples/sec lr=0.000000, loss=15.8053, loss_ll=2.77838, loss_ll_paf=3.62416, loss_ll_heat=1.93261, q=1000
[2018-07-05 13:37:30,701] [train] [INFO] epoch=21.00 step=162800, 10.5079 examples/sec lr=0.000000, loss=22.1873, loss_ll=3.71389, loss_ll_paf=5.49799, loss_ll_heat=1.92978, q=1000
[2018-07-05 13:40:04,080] [train] [INFO] epoch=21.00 step=162900, 10.5079 examples/sec lr=0.000000, loss=22.2848, loss_ll=3.78302, loss_ll_paf=5.78664, loss_ll_heat=1.77939, q=1000
[2018-07-05 13:42:37,411] [train] [INFO] epoch=21.00 step=163000, 10.5079 examples/sec lr=0.000000, loss=17.4086, loss_ll=3.11814, loss_ll_paf=4.83621, loss_ll_heat=1.40006, q=1000
[2018-07-05 13:45:25,843] [train] [INFO] epoch=21.00 step=163100, 10.5072 examples/sec lr=0.000000, loss=12.4748, loss_ll=2.28825, loss_ll_paf=3.19529, loss_ll_heat=1.38121, q=1000
[2018-07-05 13:47:57,487] [train] [INFO] epoch=21.00 step=163200, 10.5072 examples/sec lr=0.000000, loss=20.7621, loss_ll=3.53102, loss_ll_paf=4.97958, loss_ll_heat=2.08245, q=1000
[2018-07-05 13:50:29,249] [train] [INFO] epoch=21.00 step=163300, 10.5072 examples/sec lr=0.000000, loss=17.8498, loss_ll=2.68209, loss_ll_paf=3.72469, loss_ll_heat=1.63948, q=1000
[2018-07-05 13:53:02,999] [train] [INFO] epoch=21.00 step=163400, 10.5072 examples/sec lr=0.000000, loss=22.7147, loss_ll=4.2647, loss_ll_paf=5.49016, loss_ll_heat=3.03923, q=1000
[2018-07-05 13:55:36,484] [train] [INFO] epoch=21.00 step=163500, 10.5071 examples/sec lr=0.000000, loss=17.7233, loss_ll=3.26313, loss_ll_paf=4.50729, loss_ll_heat=2.01897, q=1000
[2018-07-05 13:58:08,766] [train] [INFO] epoch=21.00 step=163600, 10.5071 examples/sec lr=0.000000, loss=18.5771, loss_ll=3.19448, loss_ll_paf=4.63787, loss_ll_heat=1.75109, q=1000
[2018-07-05 14:00:43,435] [train] [INFO] epoch=21.00 step=163700, 10.5070 examples/sec lr=0.000000, loss=16.4418, loss_ll=2.81254, loss_ll_paf=3.62378, loss_ll_heat=2.0013, q=1000
[2018-07-05 14:03:17,536] [train] [INFO] epoch=21.00 step=163800, 10.5069 examples/sec lr=0.000000, loss=16.7515, loss_ll=3.02345, loss_ll_paf=4.28978, loss_ll_heat=1.75712, q=1000
[2018-07-05 14:05:50,317] [train] [INFO] epoch=21.00 step=163900, 10.5069 examples/sec lr=0.000000, loss=27.7392, loss_ll=4.73217, loss_ll_paf=6.71486, loss_ll_heat=2.74948, q=1000
[2018-07-05 14:08:22,589] [train] [INFO] epoch=21.00 step=164000, 10.5069 examples/sec lr=0.000000, loss=15.5375, loss_ll=2.66411, loss_ll_paf=3.54181, loss_ll_heat=1.78642, q=1000
[2018-07-05 14:11:11,334] [train] [INFO] epoch=21.00 step=164100, 10.5062 examples/sec lr=0.000000, loss=19.6258, loss_ll=3.6203, loss_ll_paf=5.05679, loss_ll_heat=2.1838, q=1000
[2018-07-05 14:13:45,544] [train] [INFO] epoch=21.00 step=164200, 10.5061 examples/sec lr=0.000000, loss=21.5763, loss_ll=3.6795, loss_ll_paf=5.43388, loss_ll_heat=1.92512, q=1000
[2018-07-05 14:16:19,511] [train] [INFO] epoch=21.00 step=164300, 10.5061 examples/sec lr=0.000000, loss=15.237, loss_ll=2.61181, loss_ll_paf=3.65059, loss_ll_heat=1.57304, q=1000
[2018-07-05 14:18:50,819] [train] [INFO] epoch=21.00 step=164400, 10.5061 examples/sec lr=0.000000, loss=15.0289, loss_ll=2.63271, loss_ll_paf=3.68041, loss_ll_heat=1.58502, q=1000
[2018-07-05 14:21:24,802] [train] [INFO] epoch=21.00 step=164500, 10.5060 examples/sec lr=0.000000, loss=16.2852, loss_ll=2.90389, loss_ll_paf=4.14561, loss_ll_heat=1.66217, q=1000
[2018-07-05 14:23:55,903] [train] [INFO] epoch=21.00 step=164600, 10.5061 examples/sec lr=0.000000, loss=23.24, loss_ll=4.19827, loss_ll_paf=5.58967, loss_ll_heat=2.80686, q=1000
[2018-07-05 14:26:28,683] [train] [INFO] epoch=21.00 step=164700, 10.5061 examples/sec lr=0.000000, loss=21.298, loss_ll=3.85173, loss_ll_paf=5.8803, loss_ll_heat=1.82316, q=1000
[2018-07-05 14:29:00,685] [train] [INFO] epoch=21.00 step=164800, 10.5061 examples/sec lr=0.000000, loss=17.7161, loss_ll=2.95611, loss_ll_paf=3.99921, loss_ll_heat=1.91301, q=1000
[2018-07-05 14:31:33,275] [train] [INFO] epoch=21.00 step=164900, 10.5061 examples/sec lr=0.000000, loss=21.0079, loss_ll=3.79735, loss_ll_paf=5.48745, loss_ll_heat=2.10725, q=1000
[2018-07-05 14:34:04,979] [train] [INFO] epoch=21.00 step=165000, 10.5061 examples/sec lr=0.000000, loss=19.4741, loss_ll=3.37035, loss_ll_paf=4.85821, loss_ll_heat=1.8825, q=1000
[2018-07-05 14:36:53,475] [train] [INFO] epoch=21.00 step=165100, 10.5054 examples/sec lr=0.000000, loss=17.0493, loss_ll=3.05661, loss_ll_paf=4.55791, loss_ll_heat=1.55531, q=1000
[2018-07-05 14:39:27,306] [train] [INFO] epoch=21.00 step=165200, 10.5053 examples/sec lr=0.000000, loss=20.236, loss_ll=3.4434, loss_ll_paf=5.08076, loss_ll_heat=1.80604, q=1000
[2018-07-05 14:42:00,863] [train] [INFO] epoch=21.00 step=165300, 10.5053 examples/sec lr=0.000000, loss=17.7401, loss_ll=3.09762, loss_ll_paf=4.85604, loss_ll_heat=1.3392, q=1000
[2018-07-05 14:44:32,397] [train] [INFO] epoch=21.00 step=165400, 10.5053 examples/sec lr=0.000000, loss=19.2075, loss_ll=3.22015, loss_ll_paf=4.43346, loss_ll_heat=2.00684, q=1000
[2018-07-05 14:47:05,241] [train] [INFO] epoch=21.00 step=165500, 10.5053 examples/sec lr=0.000000, loss=20.226, loss_ll=3.76795, loss_ll_paf=5.32496, loss_ll_heat=2.21093, q=1000
[2018-07-05 14:49:37,175] [train] [INFO] epoch=21.00 step=165600, 10.5053 examples/sec lr=0.000000, loss=20.2455, loss_ll=3.75954, loss_ll_paf=5.07837, loss_ll_heat=2.44071, q=1000
[2018-07-05 14:52:10,420] [train] [INFO] epoch=21.00 step=165700, 10.5053 examples/sec lr=0.000000, loss=21.9569, loss_ll=3.84602, loss_ll_paf=5.74715, loss_ll_heat=1.94488, q=1000
[2018-07-05 14:54:42,947] [train] [INFO] epoch=21.00 step=165800, 10.5053 examples/sec lr=0.000000, loss=20.3502, loss_ll=3.67617, loss_ll_paf=5.15407, loss_ll_heat=2.19828, q=1000
[2018-07-05 14:57:18,317] [train] [INFO] epoch=21.00 step=165900, 10.5051 examples/sec lr=0.000000, loss=22.3503, loss_ll=3.76122, loss_ll_paf=5.42385, loss_ll_heat=2.0986, q=1000
[2018-07-05 14:59:50,397] [train] [INFO] epoch=21.00 step=166000, 10.5052 examples/sec lr=0.000000, loss=25.8356, loss_ll=4.47632, loss_ll_paf=6.51375, loss_ll_heat=2.43888, q=1000
[2018-07-05 15:02:39,935] [train] [INFO] epoch=21.00 step=166100, 10.5044 examples/sec lr=0.000000, loss=24.7349, loss_ll=4.36362, loss_ll_paf=6.63523, loss_ll_heat=2.09201, q=1000
[2018-07-05 15:05:11,714] [train] [INFO] epoch=21.00 step=166200, 10.5045 examples/sec lr=0.000000, loss=22.1477, loss_ll=4.05087, loss_ll_paf=5.87047, loss_ll_heat=2.23127, q=1000
[2018-07-05 15:07:46,389] [train] [INFO] epoch=21.00 step=166300, 10.5044 examples/sec lr=0.000000, loss=20.3905, loss_ll=3.62194, loss_ll_paf=5.12168, loss_ll_heat=2.1222, q=1000
[2018-07-05 15:10:16,703] [train] [INFO] epoch=21.00 step=166400, 10.5044 examples/sec lr=0.000000, loss=27.0882, loss_ll=5.02054, loss_ll_paf=7.51234, loss_ll_heat=2.52874, q=1000
[2018-07-05 15:12:49,505] [train] [INFO] epoch=21.00 step=166500, 10.5044 examples/sec lr=0.000000, loss=26.1457, loss_ll=4.69033, loss_ll_paf=6.40586, loss_ll_heat=2.9748, q=1000
[2018-07-05 15:15:23,284] [train] [INFO] epoch=21.00 step=166600, 10.5044 examples/sec lr=0.000000, loss=24.1643, loss_ll=4.50582, loss_ll_paf=6.26845, loss_ll_heat=2.74319, q=1000
[2018-07-05 15:17:57,810] [train] [INFO] epoch=21.00 step=166700, 10.5043 examples/sec lr=0.000000, loss=22.5693, loss_ll=4.07225, loss_ll_paf=5.57639, loss_ll_heat=2.56812, q=1000
[2018-07-05 15:20:31,489] [train] [INFO] epoch=21.00 step=166800, 10.5042 examples/sec lr=0.000000, loss=19.6027, loss_ll=3.16465, loss_ll_paf=4.75084, loss_ll_heat=1.57845, q=1000
[2018-07-05 15:23:04,405] [train] [INFO] epoch=21.00 step=166900, 10.5042 examples/sec lr=0.000000, loss=18.7284, loss_ll=3.21022, loss_ll_paf=4.83306, loss_ll_heat=1.58737, q=1000
[2018-07-05 15:25:35,835] [train] [INFO] epoch=21.00 step=167000, 10.5042 examples/sec lr=0.000000, loss=21.8594, loss_ll=3.83686, loss_ll_paf=5.63821, loss_ll_heat=2.03551, q=1000
[2018-07-05 15:28:25,252] [train] [INFO] epoch=21.00 step=167100, 10.5035 examples/sec lr=0.000000, loss=14.8374, loss_ll=2.49897, loss_ll_paf=3.42591, loss_ll_heat=1.57204, q=1000
[2018-07-05 15:30:59,861] [train] [INFO] epoch=21.00 step=167200, 10.5034 examples/sec lr=0.000000, loss=17.5437, loss_ll=3.03909, loss_ll_paf=4.40861, loss_ll_heat=1.66956, q=1000
[2018-07-05 15:33:31,628] [train] [INFO] epoch=21.00 step=167300, 10.5035 examples/sec lr=0.000000, loss=15.2313, loss_ll=2.64686, loss_ll_paf=3.80441, loss_ll_heat=1.48931, q=1000
[2018-07-05 15:36:05,190] [train] [INFO] epoch=22.00 step=167400, 10.5034 examples/sec lr=0.000000, loss=30.3545, loss_ll=5.58267, loss_ll_paf=8.19041, loss_ll_heat=2.97493, q=1000
[2018-07-05 15:38:39,253] [train] [INFO] epoch=22.00 step=167500, 10.5033 examples/sec lr=0.000000, loss=18.388, loss_ll=2.73636, loss_ll_paf=3.95864, loss_ll_heat=1.51408, q=1000
[2018-07-05 15:41:13,800] [train] [INFO] epoch=22.00 step=167600, 10.5032 examples/sec lr=0.000000, loss=21.6676, loss_ll=3.99224, loss_ll_paf=5.95865, loss_ll_heat=2.02583, q=1000
[2018-07-05 15:43:46,503] [train] [INFO] epoch=22.00 step=167700, 10.5032 examples/sec lr=0.000000, loss=22.7034, loss_ll=3.91157, loss_ll_paf=5.91272, loss_ll_heat=1.91042, q=1000
[2018-07-05 15:46:18,931] [train] [INFO] epoch=22.00 step=167800, 10.5032 examples/sec lr=0.000000, loss=13.285, loss_ll=2.26714, loss_ll_paf=2.88039, loss_ll_heat=1.65389, q=1000
[2018-07-05 15:48:52,890] [train] [INFO] epoch=22.00 step=167900, 10.5032 examples/sec lr=0.000000, loss=26.6797, loss_ll=4.87969, loss_ll_paf=7.99226, loss_ll_heat=1.76711, q=1000
[2018-07-05 15:51:26,221] [train] [INFO] epoch=22.00 step=168000, 10.5031 examples/sec lr=0.000000, loss=16.7313, loss_ll=2.90253, loss_ll_paf=4.28072, loss_ll_heat=1.52435, q=1000
[2018-07-05 15:54:17,464] [train] [INFO] epoch=22.00 step=168100, 10.5023 examples/sec lr=0.000000, loss=19.8369, loss_ll=3.6746, loss_ll_paf=5.51315, loss_ll_heat=1.83606, q=1000
[2018-07-05 15:56:49,677] [train] [INFO] epoch=22.00 step=168200, 10.5023 examples/sec lr=0.000000, loss=19.7568, loss_ll=3.58404, loss_ll_paf=4.65339, loss_ll_heat=2.5147, q=1000
[2018-07-05 15:59:24,376] [train] [INFO] epoch=22.00 step=168300, 10.5022 examples/sec lr=0.000000, loss=15.7899, loss_ll=2.55637, loss_ll_paf=3.39834, loss_ll_heat=1.71439, q=1000
[2018-07-05 16:01:56,802] [train] [INFO] epoch=22.00 step=168400, 10.5022 examples/sec lr=0.000000, loss=23.0405, loss_ll=4.19281, loss_ll_paf=6.45295, loss_ll_heat=1.93267, q=1000
[2018-07-05 16:04:31,538] [train] [INFO] epoch=22.00 step=168500, 10.5021 examples/sec lr=0.000000, loss=20.9087, loss_ll=4.03404, loss_ll_paf=5.84412, loss_ll_heat=2.22396, q=1000
[2018-07-05 16:07:06,159] [train] [INFO] epoch=22.00 step=168600, 10.5021 examples/sec lr=0.000000, loss=25.5691, loss_ll=4.34376, loss_ll_paf=6.67998, loss_ll_heat=2.00754, q=1000
[2018-07-05 16:09:38,794] [train] [INFO] epoch=22.00 step=168700, 10.5020 examples/sec lr=0.000000, loss=16.1973, loss_ll=2.68181, loss_ll_paf=3.92992, loss_ll_heat=1.43369, q=1000
[2018-07-05 16:12:11,855] [train] [INFO] epoch=22.00 step=168800, 10.5020 examples/sec lr=0.000000, loss=19.1472, loss_ll=3.34401, loss_ll_paf=4.37526, loss_ll_heat=2.31275, q=1000
[2018-07-05 16:14:43,644] [train] [INFO] epoch=22.00 step=168900, 10.5020 examples/sec lr=0.000000, loss=17.6033, loss_ll=3.22586, loss_ll_paf=4.79246, loss_ll_heat=1.65926, q=1000
[2018-07-05 16:17:15,576] [train] [INFO] epoch=22.00 step=169000, 10.5021 examples/sec lr=0.000000, loss=24.7834, loss_ll=4.61736, loss_ll_paf=7.29811, loss_ll_heat=1.93661, q=1000
[2018-07-05 16:20:03,326] [train] [INFO] epoch=22.00 step=169100, 10.5014 examples/sec lr=0.000000, loss=22.7153, loss_ll=4.08718, loss_ll_paf=6.21683, loss_ll_heat=1.95753, q=1000
[2018-07-05 16:22:36,662] [train] [INFO] epoch=22.00 step=169200, 10.5014 examples/sec lr=0.000000, loss=24.2744, loss_ll=4.042, loss_ll_paf=5.89202, loss_ll_heat=2.19198, q=1000
[2018-07-05 16:25:09,737] [train] [INFO] epoch=22.00 step=169300, 10.5014 examples/sec lr=0.000000, loss=16.6119, loss_ll=2.65393, loss_ll_paf=3.33776, loss_ll_heat=1.9701, q=1000
[2018-07-05 16:27:44,561] [train] [INFO] epoch=22.00 step=169400, 10.5013 examples/sec lr=0.000000, loss=22.8924, loss_ll=4.21329, loss_ll_paf=6.50283, loss_ll_heat=1.92375, q=1000
[2018-07-05 16:30:18,382] [train] [INFO] epoch=22.00 step=169500, 10.5012 examples/sec lr=0.000000, loss=19.4248, loss_ll=3.37955, loss_ll_paf=4.77706, loss_ll_heat=1.98203, q=1000
[2018-07-05 16:32:53,389] [train] [INFO] epoch=22.00 step=169600, 10.5011 examples/sec lr=0.000000, loss=16.6565, loss_ll=3.04638, loss_ll_paf=4.54777, loss_ll_heat=1.54498, q=1000
[2018-07-05 16:35:30,712] [train] [INFO] epoch=22.00 step=169700, 10.5009 examples/sec lr=0.000000, loss=17.0891, loss_ll=2.9598, loss_ll_paf=4.3442, loss_ll_heat=1.5754, q=1000
[2018-07-05 16:38:03,431] [train] [INFO] epoch=22.00 step=169800, 10.5009 examples/sec lr=0.000000, loss=21.7544, loss_ll=3.77147, loss_ll_paf=5.65041, loss_ll_heat=1.89253, q=1000
[2018-07-05 16:40:35,011] [train] [INFO] epoch=22.00 step=169900, 10.5009 examples/sec lr=0.000000, loss=15.5265, loss_ll=2.77815, loss_ll_paf=3.63479, loss_ll_heat=1.9215, q=1000
[2018-07-05 16:43:09,555] [train] [INFO] epoch=22.00 step=170000, 10.5008 examples/sec lr=0.000000, loss=13.2449, loss_ll=2.25483, loss_ll_paf=3.13244, loss_ll_heat=1.37721, q=1000
[2018-07-05 16:45:59,258] [train] [INFO] epoch=22.00 step=170100, 10.5001 examples/sec lr=0.000000, loss=21.2529, loss_ll=3.57478, loss_ll_paf=4.77439, loss_ll_heat=2.37516, q=1000
[2018-07-05 16:48:32,708] [train] [INFO] epoch=22.00 step=170200, 10.5001 examples/sec lr=0.000000, loss=13.5214, loss_ll=2.35351, loss_ll_paf=3.2416, loss_ll_heat=1.46542, q=1000
[2018-07-05 16:51:04,951] [train] [INFO] epoch=22.00 step=170300, 10.5001 examples/sec lr=0.000000, loss=21.09, loss_ll=3.82043, loss_ll_paf=5.82656, loss_ll_heat=1.8143, q=1000
[2018-07-05 16:53:37,498] [train] [INFO] epoch=22.00 step=170400, 10.5001 examples/sec lr=0.000000, loss=16.7379, loss_ll=2.99963, loss_ll_paf=4.27083, loss_ll_heat=1.72842, q=1000
[2018-07-05 16:56:08,637] [train] [INFO] epoch=22.00 step=170500, 10.5001 examples/sec lr=0.000000, loss=17.8378, loss_ll=3.13943, loss_ll_paf=4.31719, loss_ll_heat=1.96168, q=1000
[2018-07-05 16:58:41,533] [train] [INFO] epoch=22.00 step=170600, 10.5001 examples/sec lr=0.000000, loss=12.4359, loss_ll=2.23915, loss_ll_paf=2.97092, loss_ll_heat=1.50737, q=1000
[2018-07-05 17:01:13,432] [train] [INFO] epoch=22.00 step=170700, 10.5001 examples/sec lr=0.000000, loss=14.0425, loss_ll=2.36914, loss_ll_paf=3.1683, loss_ll_heat=1.56998, q=1000
[2018-07-05 17:03:46,571] [train] [INFO] epoch=22.00 step=170800, 10.5001 examples/sec lr=0.000000, loss=25.4587, loss_ll=4.61651, loss_ll_paf=6.85144, loss_ll_heat=2.38157, q=1000
[2018-07-05 17:06:18,326] [train] [INFO] epoch=22.00 step=170900, 10.5001 examples/sec lr=0.000000, loss=18.7021, loss_ll=3.26251, loss_ll_paf=4.643, loss_ll_heat=1.88203, q=1000
[2018-07-05 17:08:49,798] [train] [INFO] epoch=22.00 step=171000, 10.5002 examples/sec lr=0.000000, loss=15.1596, loss_ll=2.54259, loss_ll_paf=3.82467, loss_ll_heat=1.26052, q=1000
[2018-07-05 17:11:37,420] [train] [INFO] epoch=22.00 step=171100, 10.4995 examples/sec lr=0.000000, loss=15.4158, loss_ll=2.66497, loss_ll_paf=3.83938, loss_ll_heat=1.49056, q=1000
[2018-07-05 17:14:10,310] [train] [INFO] epoch=22.00 step=171200, 10.4995 examples/sec lr=0.000000, loss=18.7067, loss_ll=3.35448, loss_ll_paf=4.74775, loss_ll_heat=1.9612, q=1000
[2018-07-05 17:16:43,232] [train] [INFO] epoch=22.00 step=171300, 10.4995 examples/sec lr=0.000000, loss=16.4749, loss_ll=2.98058, loss_ll_paf=3.90512, loss_ll_heat=2.05603, q=1000
[2018-07-05 17:19:17,475] [train] [INFO] epoch=22.00 step=171400, 10.4994 examples/sec lr=0.000000, loss=24.4275, loss_ll=4.36047, loss_ll_paf=6.4931, loss_ll_heat=2.22784, q=1000
[2018-07-05 17:21:51,286] [train] [INFO] epoch=22.00 step=171500, 10.4994 examples/sec lr=0.000000, loss=26.5238, loss_ll=4.52018, loss_ll_paf=7.42208, loss_ll_heat=1.61828, q=1000
[2018-07-05 17:24:25,691] [train] [INFO] epoch=22.00 step=171600, 10.4993 examples/sec lr=0.000000, loss=13.2225, loss_ll=2.36703, loss_ll_paf=2.98223, loss_ll_heat=1.75184, q=1000
[2018-07-05 17:26:59,152] [train] [INFO] epoch=22.00 step=171700, 10.4992 examples/sec lr=0.000000, loss=29.7741, loss_ll=5.32478, loss_ll_paf=7.96144, loss_ll_heat=2.68812, q=1000
[2018-07-05 17:29:35,760] [train] [INFO] epoch=22.00 step=171800, 10.4991 examples/sec lr=0.000000, loss=13.2617, loss_ll=2.44661, loss_ll_paf=3.25597, loss_ll_heat=1.63726, q=1000
[2018-07-05 17:32:10,771] [train] [INFO] epoch=22.00 step=171900, 10.4990 examples/sec lr=0.000000, loss=18.8571, loss_ll=3.10837, loss_ll_paf=4.54089, loss_ll_heat=1.67586, q=1000
[2018-07-05 17:34:42,851] [train] [INFO] epoch=22.00 step=172000, 10.4990 examples/sec lr=0.000000, loss=17.768, loss_ll=3.10221, loss_ll_paf=4.41251, loss_ll_heat=1.79191, q=1000
[2018-07-05 17:37:30,149] [train] [INFO] epoch=22.00 step=172100, 10.4984 examples/sec lr=0.000000, loss=28.9058, loss_ll=5.2531, loss_ll_paf=8.14614, loss_ll_heat=2.36005, q=1000
[2018-07-05 17:40:03,268] [train] [INFO] epoch=22.00 step=172200, 10.4984 examples/sec lr=0.000000, loss=23.6454, loss_ll=4.42389, loss_ll_paf=6.66635, loss_ll_heat=2.18142, q=1000
[2018-07-05 17:42:39,718] [train] [INFO] epoch=22.00 step=172300, 10.4982 examples/sec lr=0.000000, loss=16.421, loss_ll=2.86426, loss_ll_paf=4.02106, loss_ll_heat=1.70746, q=1000
[2018-07-05 17:45:13,339] [train] [INFO] epoch=22.00 step=172400, 10.4981 examples/sec lr=0.000000, loss=14.613, loss_ll=2.44517, loss_ll_paf=3.01178, loss_ll_heat=1.87856, q=1000
[2018-07-05 17:47:46,019] [train] [INFO] epoch=22.00 step=172500, 10.4981 examples/sec lr=0.000000, loss=26.8032, loss_ll=4.6444, loss_ll_paf=6.8285, loss_ll_heat=2.4603, q=1000
[2018-07-05 17:50:19,449] [train] [INFO] epoch=22.00 step=172600, 10.4981 examples/sec lr=0.000000, loss=21.6493, loss_ll=3.72338, loss_ll_paf=4.87157, loss_ll_heat=2.57518, q=1000
[2018-07-05 17:52:51,723] [train] [INFO] epoch=22.00 step=172700, 10.4981 examples/sec lr=0.000000, loss=37.6905, loss_ll=7.10783, loss_ll_paf=10.566, loss_ll_heat=3.64968, q=1000
[2018-07-05 17:55:24,595] [train] [INFO] epoch=22.00 step=172800, 10.4981 examples/sec lr=0.000000, loss=22.883, loss_ll=4.14408, loss_ll_paf=5.81285, loss_ll_heat=2.47531, q=1000
[2018-07-05 17:57:57,315] [train] [INFO] epoch=22.00 step=172900, 10.4981 examples/sec lr=0.000000, loss=21.8665, loss_ll=3.99214, loss_ll_paf=5.87097, loss_ll_heat=2.11331, q=1000
[2018-07-05 18:00:31,835] [train] [INFO] epoch=22.00 step=173000, 10.4980 examples/sec lr=0.000000, loss=20.7707, loss_ll=3.46605, loss_ll_paf=4.89143, loss_ll_heat=2.04066, q=1000
[2018-07-05 18:03:21,701] [train] [INFO] epoch=22.00 step=173100, 10.4973 examples/sec lr=0.000000, loss=28.5746, loss_ll=5.10947, loss_ll_paf=7.21798, loss_ll_heat=3.00096, q=1000
[2018-07-05 18:05:55,896] [train] [INFO] epoch=22.00 step=173200, 10.4972 examples/sec lr=0.000000, loss=25.4963, loss_ll=4.85725, loss_ll_paf=7.18107, loss_ll_heat=2.53344, q=1000
[2018-07-05 18:08:28,028] [train] [INFO] epoch=22.00 step=173300, 10.4972 examples/sec lr=0.000000, loss=19.8229, loss_ll=3.21077, loss_ll_paf=4.8393, loss_ll_heat=1.58224, q=1000
[2018-07-05 18:11:02,506] [train] [INFO] epoch=22.00 step=173400, 10.4971 examples/sec lr=0.000000, loss=19.4909, loss_ll=3.4644, loss_ll_paf=4.97823, loss_ll_heat=1.95057, q=1000
[2018-07-05 18:13:37,241] [train] [INFO] epoch=22.00 step=173500, 10.4971 examples/sec lr=0.000000, loss=13.008, loss_ll=2.1998, loss_ll_paf=2.79513, loss_ll_heat=1.60446, q=1000
[2018-07-05 18:16:11,577] [train] [INFO] epoch=22.00 step=173600, 10.4970 examples/sec lr=0.000000, loss=23.0009, loss_ll=4.07563, loss_ll_paf=6.3271, loss_ll_heat=1.82416, q=1000
[2018-07-05 18:18:44,334] [train] [INFO] epoch=22.00 step=173700, 10.4970 examples/sec lr=0.000000, loss=14.2144, loss_ll=2.58893, loss_ll_paf=3.46479, loss_ll_heat=1.71308, q=1000
[2018-07-05 18:21:18,634] [train] [INFO] epoch=22.00 step=173800, 10.4969 examples/sec lr=0.000000, loss=18.3366, loss_ll=3.20305, loss_ll_paf=4.13129, loss_ll_heat=2.27482, q=1000
[2018-07-05 18:23:52,155] [train] [INFO] epoch=22.00 step=173900, 10.4968 examples/sec lr=0.000000, loss=16.5224, loss_ll=2.77762, loss_ll_paf=3.60756, loss_ll_heat=1.94769, q=1000
[2018-07-05 18:26:25,177] [train] [INFO] epoch=22.00 step=174000, 10.4968 examples/sec lr=0.000000, loss=30.6001, loss_ll=5.57242, loss_ll_paf=8.52478, loss_ll_heat=2.62007, q=1000
[2018-07-05 18:29:24,500] [train] [INFO] epoch=22.00 step=174100, 10.4958 examples/sec lr=0.000000, loss=20.1829, loss_ll=3.71265, loss_ll_paf=5.33996, loss_ll_heat=2.08533, q=1000
[2018-07-05 18:31:58,642] [train] [INFO] epoch=22.00 step=174200, 10.4957 examples/sec lr=0.000000, loss=39.2702, loss_ll=7.40946, loss_ll_paf=11.849, loss_ll_heat=2.9699, q=1000
[2018-07-05 18:34:32,516] [train] [INFO] epoch=22.00 step=174300, 10.4956 examples/sec lr=0.000000, loss=23.8946, loss_ll=4.43233, loss_ll_paf=6.45318, loss_ll_heat=2.41148, q=1000
[2018-07-05 18:37:06,854] [train] [INFO] epoch=22.00 step=174400, 10.4956 examples/sec lr=0.000000, loss=16.5152, loss_ll=2.91674, loss_ll_paf=4.13202, loss_ll_heat=1.70147, q=1000
[2018-07-05 18:39:43,226] [train] [INFO] epoch=22.00 step=174500, 10.4954 examples/sec lr=0.000000, loss=20.5886, loss_ll=3.8081, loss_ll_paf=5.30361, loss_ll_heat=2.31259, q=1000
[2018-07-05 18:42:16,053] [train] [INFO] epoch=22.00 step=174600, 10.4954 examples/sec lr=0.000000, loss=20.6887, loss_ll=3.70374, loss_ll_paf=5.43177, loss_ll_heat=1.97571, q=1000
[2018-07-05 18:44:48,534] [train] [INFO] epoch=22.00 step=174700, 10.4954 examples/sec lr=0.000000, loss=23.9905, loss_ll=4.28633, loss_ll_paf=6.92651, loss_ll_heat=1.64615, q=1000
[2018-07-05 18:47:22,129] [train] [INFO] epoch=22.00 step=174800, 10.4953 examples/sec lr=0.000000, loss=20.8497, loss_ll=3.91978, loss_ll_paf=5.96933, loss_ll_heat=1.87022, q=1000
[2018-07-05 18:49:57,058] [train] [INFO] epoch=22.00 step=174900, 10.4953 examples/sec lr=0.000000, loss=13.3925, loss_ll=2.41364, loss_ll_paf=3.39557, loss_ll_heat=1.43171, q=1000
[2018-07-05 18:52:30,713] [train] [INFO] epoch=22.00 step=175000, 10.4952 examples/sec lr=0.000000, loss=18.1388, loss_ll=3.32704, loss_ll_paf=4.44953, loss_ll_heat=2.20454, q=1000
2018-07-05 18:53:09.375042: W tensorflow/core/kernels/queue_base.cc:295] _0_fifo_queue: Skipping cancelled enqueue attempt with queue not closed
[2018-07-05 18:53:09,375] [train] [INFO] optimization finished. 266827.219435
[2018-07-05 18:53:09,381] [pose_dataset] [ERROR] err type1, placeholders=[<tf.Tensor 'image:0' shape=(16, 368, 368, 3) dtype=float32>, <tf.Tensor 'heatmap:0' shape=(16, 46, 46, 16) dtype=float32>, <tf.Tensor 'vectmap:0' shape=(16, 46, 46, 32) dtype=float32>, <tf.Tensor 'offsetmap:0' shape=(16, 46, 46, 32) dtype=float32>, <tf.Tensor 'heatmap_valid:0' shape=(16, 46, 46, 16) dtype=float32>, <tf.Tensor 'vectmap_valid:0' shape=(16, 46, 46, 32) dtype=float32>]
[2018-07-05 18:53:09,392] [pose_dataset] [INFO] Thread-1 Exited.
6 / 225
conv4_5_1x1_reduce/bn/beta   117 / 225
conv3_4_1x1_up/bias   118 / 225
conv3_1_1x1_up/bias   119 / 225
conv3_3_3x3/bn/beta   120 / 225
conv2_3_1x1_increase/bn/beta   121 / 225
conv4_4_3x3/kernel   122 / 225
conv5_3_1x1_up/bias   123 / 225
conv4_5_1x1_increase/kernel   124 / 225
conv4_4_1x1_down/kernel   125 / 225
conv3_1_3x3/kernel   126 / 225
conv4_1_1x1_increase/kernel   127 / 225
conv2_3_1x1_increase/kernel   128 / 225
conv4_6_1x1_increase/bn/gamma   129 / 225
conv3_2_1x1_increase/bn/beta   130 / 225
conv4_1_1x1_reduce/bn/gamma   131 / 225
conv4_3_1x1_increase/kernel   132 / 225
conv2_2_1x1_reduce/kernel   133 / 225
conv4_5_1x1_up/kernel   134 / 225
conv4_2_3x3/bn/beta   135 / 225
conv4_3_1x1_up/kernel   136 / 225
conv2_1_1x1_proj/kernel   137 / 225
conv5_1_1x1_reduce/kernel   138 / 225
conv2_3_1x1_up/bias   139 / 225
conv2_3_1x1_down/kernel   140 / 225
conv3_3_1x1_increase/bn/beta   141 / 225
conv4_5_3x3/bn/beta   142 / 225
conv5_3_1x1_down/bias   143 / 225
conv4_6_1x1_down/bias   144 / 225
conv4_4_1x1_increase/bn/beta   145 / 225
conv5_1_1x1_proj/bn/gamma   146 / 225
conv3_2_1x1_reduce/bn/gamma   147 / 225
conv4_6_1x1_increase/kernel   148 / 225
conv2_3_1x1_down/bias   149 / 225
conv5_3_1x1_reduce/kernel   150 / 225
conv4_3_1x1_down/bias   151 / 225
conv3_2_1x1_increase/kernel   152 / 225
conv2_3_1x1_reduce/kernel   153 / 225
conv2_2_1x1_increase/bn/gamma   154 / 225
conv5_2_1x1_up/kernel   155 / 225
conv5_1_3x3/kernel   156 / 225
conv5_2_3x3/kernel   157 / 225
conv4_5_1x1_reduce/kernel   158 / 225
conv3_2_1x1_increase/bn/gamma   159 / 225
conv5_1_1x1_increase/bn/gamma   160 / 225
conv3_4_3x3/bn/beta   161 / 225
conv5_1_1x1_increase/bn/beta   162 / 225
conv5_2_3x3/bn/beta   163 / 225
conv3_1_3x3/bn/beta   164 / 225
conv5_1_1x1_proj/bn/beta   165 / 225
conv3_2_1x1_reduce/bn/beta   166 / 225
conv5_2_1x1_reduce/bn/beta   167 / 225
conv4_1_1x1_increase/bn/beta   168 / 225
conv3_1_1x1_up/kernel   169 / 225
conv4_2_1x1_increase/bn/gamma   170 / 225
conv3_2_3x3/bn/beta   171 / 225
conv2_1_1x1_proj/bn/beta   172 / 225
conv3_3_1x1_up/bias   173 / 225
conv4_1_1x1_down/bias   174 / 225
conv2_2_1x1_down/kernel   175 / 225
conv4_5_1x1_down/kernel   176 / 225
conv2_2_1x1_up/kernel   177 / 225
conv3_1_1x1_increase/kernel   178 / 225
conv4_5_1x1_reduce/bn/gamma   179 / 225
conv2_1_1x1_reduce/bn/beta   180 / 225
conv4_1_1x1_proj/bn/gamma   181 / 225
conv4_3_1x1_reduce/bn/gamma   182 / 225
conv2_1_1x1_reduce/bn/gamma   183 / 225
conv5_2_1x1_increase/bn/beta   184 / 225
conv2_1_3x3/bn/beta   185 / 225
conv4_6_1x1_down/kernel   186 / 225
conv4_6_3x3/bn/beta   187 / 225
conv4_3_1x1_increase/bn/beta   188 / 225
conv4_3_1x1_up/bias   189 / 225
conv4_5_3x3/kernel   190 / 225
conv5_1_1x1_up/bias   191 / 225
conv3_1_1x1_down/bias   192 / 225
conv2_3_1x1_increase/bn/gamma   193 / 225
conv3_4_1x1_reduce/kernel   194 / 225
conv2_2_1x1_increase/kernel   195 / 225
conv4_5_1x1_down/bias   196 / 225
conv2_2_1x1_up/bias   197 / 225
conv4_5_1x1_increase/bn/gamma   198 / 225
conv5_3_3x3/bn/gamma   199 / 225
conv3_4_1x1_up/kernel   200 / 225
conv3_1_1x1_proj/kernel   201 / 225
conv4_4_1x1_up/kernel   202 / 225
conv3_1_1x1_reduce/bn/gamma   203 / 225
conv3_1_1x1_reduce/kernel   204 / 225
conv5_2_1x1_up/bias   205 / 225
conv2_2_1x1_increase/bn/beta   206 / 225
conv3_3_3x3/kernel   207 / 225
conv2_1_1x1_down/kernel   208 / 225
dense/kernel   209 / 225
conv3_1_1x1_increase/bn/gamma   210 / 225
conv2_1_1x1_up/bias   211 / 225
conv5_2_1x1_increase/kernel   212 / 225
conv5_3_1x1_up/kernel   213 / 225
conv3_3_3x3/bn/gamma   214 / 225
conv5_2_1x1_down/bias   215 / 225
conv4_1_1x1_down/kernel   216 / 225
conv2_2_1x1_reduce/bn/gamma   217 / 225
conv4_4_1x1_down/bias   218 / 225
conv4_6_3x3/bn/gamma   219 / 225
conv4_2_1x1_down/kernel   220 / 225
conv3_3_1x1_increase/kernel   221 / 225
conv1/7x7_s2/bn/beta   222 / 225
conv5_2_1x1_increase/bn/gamma   223 / 225
conv4_5_1x1_increase/bn/beta   224 / 225
<tf.Variable 'Variable:0' shape=() dtype=int32_ref>
