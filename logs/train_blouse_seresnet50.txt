/usr/lib64/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
[2018-07-09 14:59:13,954] [train] [INFO] define model+
[2018-07-09 14:59:13,961] [pose_dataset] [INFO] dataflow img_path=/home/shy/projects/tf-openpose/data/
[2018-07-09 14:59:16,743] [pose_dataset] [INFO] /home/shy/projects/tf-openpose/data/train/train_bak.csv dataset 12694
[32m[0709 14:59:16 @parallel.py:178][0m [MultiProcessPrefetchData] Will fork a dataflow more than one times. This assumes the datapoints are i.i.d.
[32m[0709 14:59:18 @parallel.py:178][0m [MultiProcessPrefetchData] Will fork a dataflow more than one times. This assumes the datapoints are i.i.d.
[2018-07-09 14:59:19,103] [pose_dataset] [INFO] dataflow img_path=/home/shy/projects/tf-openpose/data/
[2018-07-09 14:59:20,604] [pose_dataset] [INFO] /home/shy/projects/tf-openpose/data/train/val_bak.csv dataset 1411
[32m[0709 14:59:20 @parallel.py:178][0m [MultiProcessPrefetchData] Will fork a dataflow more than one times. This assumes the datapoints are i.i.d.
[32m[0709 14:59:20 @parallel.py:178][0m [MultiProcessPrefetchData] Will fork a dataflow more than one times. This assumes the datapoints are i.i.d.
[2018-07-09 14:59:24,409] [train] [INFO] tensorboard val image: 12
[2018-07-09 14:59:24,410] [train] [INFO] Tensor("fifo_queue_Dequeue:0", shape=(16, 368, 368, 3), dtype=float32, device=/device:GPU:0)
[2018-07-09 14:59:24,411] [train] [INFO] Tensor("fifo_queue_Dequeue:1", shape=(16, 46, 46, 14), dtype=float32, device=/device:GPU:0)
[2018-07-09 14:59:24,411] [train] [INFO] Tensor("fifo_queue_Dequeue:2", shape=(16, 46, 46, 28), dtype=float32, device=/device:GPU:0)
[2018-07-09 15:01:01,868] [train] [INFO] define model-
2018-07-09 15:01:04.608227: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-07-09 15:01:04.608284: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-07-09 15:01:04.608293: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-07-09 15:01:04.608300: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-07-09 15:01:04.608306: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2018-07-09 15:01:13.190329: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: 
name: GeForce GTX 1080 Ti
major: 6 minor: 1 memoryClockRate (GHz) 1.582
pciBusID 0000:04:00.0
Total memory: 10.92GiB
Free memory: 10.76GiB
2018-07-09 15:01:13.389647: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x576783b0 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.
2018-07-09 15:01:13.391019: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 1 with properties: 
name: GeForce GTX 1080 Ti
major: 6 minor: 1 memoryClockRate (GHz) 1.582
pciBusID 0000:05:00.0
Total memory: 10.92GiB
Free memory: 10.76GiB
2018-07-09 15:01:13.588237: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x575aab40 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.
2018-07-09 15:01:13.590210: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 2 with properties: 
name: GeForce GTX 1080 Ti
major: 6 minor: 1 memoryClockRate (GHz) 1.582
pciBusID 0000:06:00.0
Total memory: 10.92GiB
Free memory: 10.76GiB
2018-07-09 15:01:13.813444: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x576a2680 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.
2018-07-09 15:01:13.814884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 3 with properties: 
name: GeForce GTX 1080 Ti
major: 6 minor: 1 memoryClockRate (GHz) 1.582
pciBusID 0000:07:00.0
Total memory: 10.92GiB
Free memory: 10.76GiB
2018-07-09 15:01:14.033534: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x76aa9020 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.
2018-07-09 15:01:14.035430: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 4 with properties: 
name: GeForce GTX 1080 Ti
major: 6 minor: 1 memoryClockRate (GHz) 1.582
pciBusID 0000:08:00.0
Total memory: 10.92GiB
Free memory: 10.76GiB
2018-07-09 15:01:14.261421: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x575ad7a0 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.
2018-07-09 15:01:14.263482: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 5 with properties: 
name: GeForce GTX 1080 Ti
major: 6 minor: 1 memoryClockRate (GHz) 1.582
pciBusID 0000:0b:00.0
Total memory: 10.92GiB
Free memory: 10.76GiB
2018-07-09 15:01:14.501392: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x575b0970 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.
2018-07-09 15:01:14.503279: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 6 with properties: 
name: GeForce GTX 1080 Ti
major: 6 minor: 1 memoryClockRate (GHz) 1.582
pciBusID 0000:0c:00.0
Total memory: 10.92GiB
Free memory: 10.76GiB
2018-07-09 15:01:14.763369: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x575dda60 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.
2018-07-09 15:01:14.764882: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 7 with properties: 
name: GeForce GTX 1080 Ti
major: 6 minor: 1 memoryClockRate (GHz) 1.582
pciBusID 0000:0d:00.0
Total memory: 10.92GiB
Free memory: 10.76GiB
2018-07-09 15:01:14.811942: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 1 2 3 4 5 6 7 
2018-07-09 15:01:14.811997: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y Y Y Y Y Y Y Y 
2018-07-09 15:01:14.812008: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 1:   Y Y Y Y Y Y Y Y 
2018-07-09 15:01:14.812017: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 2:   Y Y Y Y Y Y Y Y 
2018-07-09 15:01:14.812032: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 3:   Y Y Y Y Y Y Y Y 
2018-07-09 15:01:14.812040: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 4:   Y Y Y Y Y Y Y Y 
2018-07-09 15:01:14.812048: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 5:   Y Y Y Y Y Y Y Y 
2018-07-09 15:01:14.812057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 6:   Y Y Y Y Y Y Y Y 
2018-07-09 15:01:14.812065: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 7:   Y Y Y Y Y Y Y Y 
2018-07-09 15:01:14.812094: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:04:00.0)
2018-07-09 15:01:14.812106: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:05:00.0)
2018-07-09 15:01:14.812115: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:2) -> (device: 2, name: GeForce GTX 1080 Ti, pci bus id: 0000:06:00.0)
2018-07-09 15:01:14.812123: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:3) -> (device: 3, name: GeForce GTX 1080 Ti, pci bus id: 0000:07:00.0)
2018-07-09 15:01:14.812144: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:4) -> (device: 4, name: GeForce GTX 1080 Ti, pci bus id: 0000:08:00.0)
2018-07-09 15:01:14.812152: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:5) -> (device: 5, name: GeForce GTX 1080 Ti, pci bus id: 0000:0b:00.0)
2018-07-09 15:01:14.812160: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:6) -> (device: 6, name: GeForce GTX 1080 Ti, pci bus id: 0000:0c:00.0)
2018-07-09 15:01:14.812168: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:7) -> (device: 7, name: GeForce GTX 1080 Ti, pci bus id: 0000:0d:00.0)
[2018-07-09 15:01:15,853] [train] [INFO] model weights initialization
[2018-07-09 15:01:55,583] [train] [INFO] Restore pretrained weights from /home/shy/projects/tf-openpose/models/numpy/se_resnet50.npy ...
conv4_1_3x3/bn/gamma   0 / 225
conv4_2_1x1_increase/bn/beta   1 / 225
conv2_1_1x1_increase/bn/gamma   2 / 225
conv4_1_1x1_proj/kernel   3 / 225
conv5_2_1x1_reduce/kernel   4 / 225
conv3_2_3x3/bn/gamma   5 / 225
conv2_3_3x3/bn/gamma   6 / 225
conv3_4_1x1_reduce/bn/beta   7 / 225
conv5_2_1x1_down/kernel   8 / 225
conv3_2_1x1_down/kernel   9 / 225
conv3_4_1x1_increase/bn/beta   10 / 225
conv2_3_3x3/bn/beta   11 / 225
conv5_1_1x1_down/bias   12 / 225
conv4_4_3x3/bn/beta   13 / 225
conv2_1_1x1_proj/bn/gamma   14 / 225
conv3_2_1x1_down/bias   15 / 225
conv4_6_1x1_up/kernel   16 / 225
conv4_4_1x1_reduce/bn/gamma   17 / 225
conv5_2_1x1_reduce/bn/gamma   18 / 225
conv3_4_3x3/kernel   19 / 225
conv1/7x7_s2/bn/gamma   20 / 225
conv4_6_1x1_up/bias   21 / 225
conv3_3_1x1_down/kernel   22 / 225
conv4_1_1x1_reduce/kernel   23 / 225
conv2_2_3x3/kernel   24 / 225
conv2_1_3x3/kernel   25 / 225
conv5_3_1x1_down/kernel   26 / 225
conv2_1_1x1_reduce/kernel   27 / 225
conv3_2_1x1_reduce/kernel   28 / 225
conv4_3_3x3/bn/gamma   29 / 225
conv4_2_1x1_up/bias   30 / 225
conv5_1_1x1_increase/kernel   31 / 225
dense/bias   32 / 225
conv4_4_1x1_reduce/bn/beta   33 / 225
conv3_1_1x1_proj/bn/gamma   34 / 225
conv5_1_1x1_proj/kernel   35 / 225
conv5_1_3x3/bn/beta   36 / 225
conv5_1_1x1_up/kernel   37 / 225
conv4_3_1x1_reduce/kernel   38 / 225
conv4_6_1x1_reduce/bn/beta   39 / 225
conv5_3_1x1_increase/bn/beta   40 / 225
conv4_4_1x1_increase/kernel   41 / 225
conv3_4_1x1_increase/kernel   42 / 225
conv3_3_1x1_down/bias   43 / 225
conv3_2_3x3/kernel   44 / 225
conv5_3_1x1_increase/kernel   45 / 225
conv5_3_3x3/bn/beta   46 / 225
conv5_3_1x1_reduce/bn/beta   47 / 225
conv4_6_1x1_increase/bn/beta   48 / 225
conv2_2_1x1_reduce/bn/beta   49 / 225
conv5_2_3x3/bn/gamma   50 / 225
conv2_1_1x1_increase/bn/beta   51 / 225
conv4_1_1x1_up/kernel   52 / 225
conv3_2_1x1_up/kernel   53 / 225
conv5_1_1x1_reduce/bn/beta   54 / 225
conv4_4_1x1_increase/bn/gamma   55 / 225
conv4_2_3x3/bn/gamma   56 / 225
conv4_3_1x1_down/kernel   57 / 225
conv4_1_3x3/bn/beta   58 / 225
conv3_4_1x1_increase/bn/gamma   59 / 225
conv3_3_1x1_reduce/bn/beta   60 / 225
conv4_1_3x3/kernel   61 / 225
conv4_3_3x3/bn/beta   62 / 225
conv4_1_1x1_proj/bn/beta   63 / 225
conv2_3_1x1_up/kernel   64 / 225
conv4_5_1x1_up/bias   65 / 225
conv4_2_1x1_up/kernel   66 / 225
conv4_1_1x1_increase/bn/gamma   67 / 225
conv3_4_3x3/bn/gamma   68 / 225
conv5_1_1x1_reduce/bn/gamma   69 / 225
conv3_1_3x3/bn/gamma   70 / 225
conv3_2_1x1_up/bias   71 / 225
conv2_1_1x1_down/bias   72 / 225
conv2_2_3x3/bn/gamma   73 / 225
conv2_1_1x1_up/kernel   74 / 225
conv4_2_1x1_reduce/bn/gamma   75 / 225
conv4_1_1x1_reduce/bn/beta   76 / 225
conv3_3_1x1_up/kernel   77 / 225
conv3_1_1x1_proj/bn/beta   78 / 225
conv5_1_3x3/bn/gamma   79 / 225
conv3_1_1x1_reduce/bn/beta   80 / 225
conv5_3_3x3/kernel   81 / 225
conv3_4_1x1_down/kernel   82 / 225
conv4_2_1x1_reduce/kernel   83 / 225
conv4_3_3x3/kernel   84 / 225
conv3_4_1x1_down/bias   85 / 225
conv2_3_1x1_reduce/bn/gamma   86 / 225
conv4_4_3x3/bn/gamma   87 / 225
conv1/7x7_s2/kernel   88 / 225
conv3_3_1x1_reduce/bn/gamma   89 / 225
conv4_1_1x1_up/bias   90 / 225
conv2_1_1x1_increase/kernel   91 / 225
conv5_1_1x1_down/kernel   92 / 225
conv4_5_3x3/bn/gamma   93 / 225
conv4_6_3x3/kernel   94 / 225
conv4_2_1x1_down/bias   95 / 225
conv4_3_1x1_increase/bn/gamma   96 / 225
conv4_2_3x3/kernel   97 / 225
conv4_2_1x1_reduce/bn/beta   98 / 225
conv3_3_1x1_reduce/kernel   99 / 225
conv4_4_1x1_reduce/kernel   100 / 225
conv5_3_1x1_reduce/bn/gamma   101 / 225
conv2_2_1x1_down/bias   102 / 225
conv4_6_1x1_reduce/kernel   103 / 225
conv2_3_1x1_reduce/bn/beta   104 / 225
conv3_1_1x1_increase/bn/beta   105 / 225
conv4_3_1x1_reduce/bn/beta   106 / 225
conv4_6_1x1_reduce/bn/gamma   107 / 225
conv3_1_1x1_down/kernel   108 / 225
conv3_3_1x1_increase/bn/gamma   109 / 225
conv5_3_1x1_increase/bn/gamma   110 / 225
conv2_1_3x3/bn/gamma   111 / 225
conv2_3_3x3/kernel   112 / 225
conv4_4_1x1_up/bias   113 / 225
conv4_2_1x1_increase/kernel   114 / 225
conv3_4_1x1_reduce/bn/gamma   115 / 225
conv2_2_3x3/bn/beta   11[2018-07-09 16:55:40,685] [train] [INFO] Restore pretrained weights...Done
[2018-07-09 16:55:40,687] [train] [INFO] prepare file writer
[2018-07-09 16:56:03,660] [train] [INFO] prepare coordinator
[2018-07-09 16:56:04,411] [train] [INFO] Training Started.
[2018-07-09 17:01:41,329] [train] [INFO] epoch=0.00 step=100, 4.7490 examples/sec lr=0.000100, loss=324.174, loss_ll=64.7424, loss_ll_paf=104.029, loss_ll_heat=25.4555, q=375
[2018-07-09 17:04:47,154] [train] [INFO] epoch=0.00 step=200, 6.1216 examples/sec lr=0.000100, loss=295.14, loss_ll=58.9387, loss_ll_paf=91.7719, loss_ll_heat=26.1054, q=578
[2018-07-09 17:07:54,282] [train] [INFO] epoch=0.00 step=300, 6.7618 examples/sec lr=0.000100, loss=193.565, loss_ll=38.5636, loss_ll_paf=59.3759, loss_ll_heat=17.7514, q=784
[2018-07-09 17:10:58,606] [train] [INFO] epoch=0.00 step=400, 7.1573 examples/sec lr=0.000100, loss=229.667, loss_ll=45.6873, loss_ll_paf=70.1623, loss_ll_heat=21.2122, q=986
[2018-07-09 17:13:28,507] [train] [INFO] epoch=0.00 step=500, 7.6621 examples/sec lr=0.000100, loss=259.798, loss_ll=51.2916, loss_ll_paf=81.3994, loss_ll_heat=21.1837, q=1000
[2018-07-09 17:15:59,775] [train] [INFO] epoch=0.00 step=600, 8.0310 examples/sec lr=0.000100, loss=226.98, loss_ll=43.976, loss_ll_paf=70.1533, loss_ll_heat=17.7988, q=1000
[2018-07-09 17:18:28,275] [train] [INFO] epoch=0.00 step=700, 8.3342 examples/sec lr=0.000100, loss=188.379, loss_ll=36.1272, loss_ll_paf=56.0038, loss_ll_heat=16.2506, q=1000
[2018-07-09 17:20:56,988] [train] [INFO] epoch=0.00 step=800, 8.5758 examples/sec lr=0.000100, loss=201.676, loss_ll=39.5784, loss_ll_paf=62.8204, loss_ll_heat=16.3365, q=1000
[2018-07-09 17:23:27,630] [train] [INFO] epoch=0.00 step=900, 8.7633 examples/sec lr=0.000100, loss=133.193, loss_ll=25.4259, loss_ll_paf=36.6151, loss_ll_heat=14.2366, q=1000
[2018-07-09 17:25:56,107] [train] [INFO] epoch=0.00 step=1000, 8.9301 examples/sec lr=0.000100, loss=166.039, loss_ll=32.3319, loss_ll_paf=50.5669, loss_ll_heat=14.0968, q=1000
[2018-07-09 17:28:51,714] [train] [INFO] epoch=0.00 step=1100, 8.9463 examples/sec lr=0.000100, loss=160.934, loss_ll=31.277, loss_ll_paf=48.2585, loss_ll_heat=14.2956, q=1000
[2018-07-09 17:31:21,054] [train] [INFO] epoch=0.00 step=1200, 9.0710 examples/sec lr=0.000100, loss=182.565, loss_ll=35.6142, loss_ll_paf=57.186, loss_ll_heat=14.0424, q=1000
[2018-07-09 17:33:49,504] [train] [INFO] epoch=0.00 step=1300, 9.1829 examples/sec lr=0.000100, loss=172.855, loss_ll=33.849, loss_ll_paf=53.9215, loss_ll_heat=13.7765, q=1000
[2018-07-09 17:36:16,447] [train] [INFO] epoch=0.00 step=1400, 9.2868 examples/sec lr=0.000100, loss=128.055, loss_ll=24.5459, loss_ll_paf=36.0412, loss_ll_heat=13.0505, q=1000
[2018-07-09 17:38:41,086] [train] [INFO] epoch=0.00 step=1500, 9.3872 examples/sec lr=0.000100, loss=160.595, loss_ll=30.6503, loss_ll_paf=49.7027, loss_ll_heat=11.5979, q=1000
[2018-07-09 17:41:09,743] [train] [INFO] epoch=0.00 step=1600, 9.4628 examples/sec lr=0.000100, loss=141.825, loss_ll=27.3249, loss_ll_paf=43.5967, loss_ll_heat=11.053, q=1000
[2018-07-09 17:43:36,509] [train] [INFO] epoch=0.00 step=1700, 9.5368 examples/sec lr=0.000100, loss=168.689, loss_ll=33.0421, loss_ll_paf=54.5555, loss_ll_heat=11.5286, q=1000
[2018-07-09 17:46:01,793] [train] [INFO] epoch=0.00 step=1800, 9.6084 examples/sec lr=0.000100, loss=120.336, loss_ll=23.0804, loss_ll_paf=35.2135, loss_ll_heat=10.9474, q=1000
[2018-07-09 17:48:26,714] [train] [INFO] epoch=0.00 step=1900, 9.6744 examples/sec lr=0.000100, loss=114.76, loss_ll=21.7604, loss_ll_paf=34.3776, loss_ll_heat=9.14308, q=1000
[2018-07-09 17:50:55,892] [train] [INFO] epoch=0.00 step=2000, 9.7221 examples/sec lr=0.000100, loss=131.217, loss_ll=25.3599, loss_ll_paf=41.2305, loss_ll_heat=9.48932, q=1000
[2018-07-09 17:53:39,348] [train] [INFO] epoch=0.00 step=2100, 9.7252 examples/sec lr=0.000100, loss=116.331, loss_ll=21.9743, loss_ll_paf=33.7146, loss_ll_heat=10.234, q=1000
[2018-07-09 17:56:09,140] [train] [INFO] epoch=0.00 step=2200, 9.7650 examples/sec lr=0.000100, loss=134.454, loss_ll=25.5127, loss_ll_paf=41.5278, loss_ll_heat=9.49764, q=1000
[2018-07-09 17:58:33,573] [train] [INFO] epoch=0.00 step=2300, 9.8155 examples/sec lr=0.000100, loss=131.53, loss_ll=25.1909, loss_ll_paf=40.8542, loss_ll_heat=9.52754, q=1000
[2018-07-09 18:01:00,507] [train] [INFO] epoch=0.00 step=2400, 9.8560 examples/sec lr=0.000100, loss=118.788, loss_ll=22.5588, loss_ll_paf=37.3964, loss_ll_heat=7.72108, q=1000
[2018-07-09 18:03:26,854] [train] [INFO] epoch=0.00 step=2500, 9.8950 examples/sec lr=0.000100, loss=117.026, loss_ll=22.0577, loss_ll_paf=35.4817, loss_ll_heat=8.63366, q=1000
[2018-07-09 18:05:53,509] [train] [INFO] epoch=0.00 step=2600, 9.9305 examples/sec lr=0.000100, loss=94.9316, loss_ll=18.5998, loss_ll_paf=28.3615, loss_ll_heat=8.83801, q=1000
[2018-07-09 18:08:20,141] [train] [INFO] epoch=0.00 step=2700, 9.9637 examples/sec lr=0.000100, loss=141.759, loss_ll=27.1192, loss_ll_paf=44.9286, loss_ll_heat=9.30984, q=1000
[2018-07-09 18:10:47,621] [train] [INFO] epoch=0.00 step=2800, 9.9928 examples/sec lr=0.000100, loss=129.625, loss_ll=24.8679, loss_ll_paf=40.1965, loss_ll_heat=9.53916, q=1000
[2018-07-09 18:13:13,566] [train] [INFO] epoch=0.00 step=2900, 10.0234 examples/sec lr=0.000100, loss=135.713, loss_ll=26.2132, loss_ll_paf=42.3779, loss_ll_heat=10.0485, q=1000
[2018-07-09 18:15:40,942] [train] [INFO] epoch=0.00 step=3000, 10.0491 examples/sec lr=0.000100, loss=108.202, loss_ll=20.4109, loss_ll_paf=32.9953, loss_ll_heat=7.82655, q=1000
[2018-07-09 18:18:20,836] [train] [INFO] epoch=0.00 step=3100, 10.0478 examples/sec lr=0.000100, loss=126.103, loss_ll=24.2946, loss_ll_paf=38.9466, loss_ll_heat=9.64274, q=1000
[2018-07-09 18:20:46,153] [train] [INFO] epoch=0.00 step=3200, 10.0753 examples/sec lr=0.000100, loss=86.9474, loss_ll=16.1122, loss_ll_paf=24.8472, loss_ll_heat=7.3773, q=1000
[2018-07-09 18:23:12,434] [train] [INFO] epoch=0.00 step=3300, 10.0994 examples/sec lr=0.000100, loss=132.013, loss_ll=25.4275, loss_ll_paf=40.0158, loss_ll_heat=10.8392, q=1000
[2018-07-09 18:25:39,898] [train] [INFO] epoch=0.00 step=3400, 10.1200 examples/sec lr=0.000100, loss=96.2201, loss_ll=17.5986, loss_ll_paf=28.0875, loss_ll_heat=7.10963, q=1000
[2018-07-09 18:28:06,788] [train] [INFO] epoch=0.00 step=3500, 10.1406 examples/sec lr=0.000100, loss=113.905, loss_ll=20.9747, loss_ll_paf=34.0966, loss_ll_heat=7.85278, q=1000
[2018-07-09 18:30:32,727] [train] [INFO] epoch=0.00 step=3600, 10.1618 examples/sec lr=0.000100, loss=129.746, loss_ll=24.9198, loss_ll_paf=39.5908, loss_ll_heat=10.2488, q=1000
[2018-07-09 18:32:58,010] [train] [INFO] epoch=0.00 step=3700, 10.1830 examples/sec lr=0.000100, loss=111.195, loss_ll=21.0836, loss_ll_paf=33.6463, loss_ll_heat=8.52091, q=1000
[2018-07-09 18:35:25,862] [train] [INFO] epoch=0.00 step=3800, 10.1989 examples/sec lr=0.000100, loss=77.4021, loss_ll=14.4301, loss_ll_paf=22.1447, loss_ll_heat=6.71543, q=1000
[2018-07-09 18:37:50,435] [train] [INFO] epoch=0.00 step=3900, 10.2194 examples/sec lr=0.000100, loss=115.585, loss_ll=21.6893, loss_ll_paf=35.2141, loss_ll_heat=8.16462, q=1000
[2018-07-09 18:40:17,537] [train] [INFO] epoch=0.00 step=4000, 10.2349 examples/sec lr=0.000100, loss=103.757, loss_ll=18.9717, loss_ll_paf=28.7345, loss_ll_heat=9.20879, q=1000
[2018-07-09 18:42:53,040] [train] [INFO] epoch=0.00 step=4100, 10.2362 examples/sec lr=0.000100, loss=134.013, loss_ll=25.606, loss_ll_paf=41.3356, loss_ll_heat=9.87637, q=1000
[2018-07-09 18:45:18,118] [train] [INFO] epoch=0.00 step=4200, 10.2537 examples/sec lr=0.000100, loss=85.2211, loss_ll=15.5472, loss_ll_paf=23.9907, loss_ll_heat=7.1038, q=1000
[2018-07-09 18:47:44,810] [train] [INFO] epoch=0.00 step=4300, 10.2680 examples/sec lr=0.000100, loss=123.192, loss_ll=23.1227, loss_ll_paf=37.0889, loss_ll_heat=9.15654, q=1000
[2018-07-09 18:50:11,839] [train] [INFO] epoch=0.00 step=4400, 10.2812 examples/sec lr=0.000100, loss=98.8317, loss_ll=18.1323, loss_ll_paf=27.893, loss_ll_heat=8.3715, q=1000
[2018-07-09 18:52:38,580] [train] [INFO] epoch=0.00 step=4500, 10.2943 examples/sec lr=0.000100, loss=104.283, loss_ll=19.9595, loss_ll_paf=33.0811, loss_ll_heat=6.83786, q=1000
[2018-07-09 18:55:04,303] [train] [INFO] epoch=0.00 step=4600, 10.3083 examples/sec lr=0.000100, loss=109.381, loss_ll=19.804, loss_ll_paf=30.3309, loss_ll_heat=9.27718, q=1000
[2018-07-09 18:57:30,697] [train] [INFO] epoch=0.00 step=4700, 10.3208 examples/sec lr=0.000100, loss=108.635, loss_ll=19.7966, loss_ll_paf=31.6299, loss_ll_heat=7.96325, q=1000
[2018-07-09 18:59:56,642] [train] [INFO] epoch=0.00 step=4800, 10.3334 examples/sec lr=0.000100, loss=102.316, loss_ll=18.8685, loss_ll_paf=29.6187, loss_ll_heat=8.11823, q=1000
[2018-07-09 19:02:21,383] [train] [INFO] epoch=0.00 step=4900, 10.3471 examples/sec lr=0.000100, loss=94.5682, loss_ll=17.8854, loss_ll_paf=28.235, loss_ll_heat=7.53581, q=1000
[2018-07-09 19:04:46,918] [train] [INFO] epoch=0.00 step=5000, 10.3593 examples/sec lr=0.000100, loss=87.7153, loss_ll=16.2379, loss_ll_paf=25.2925, loss_ll_heat=7.18331, q=1000
[2018-07-09 19:07:27,414] [train] [INFO] epoch=0.00 step=5100, 10.3514 examples/sec lr=0.000100, loss=100.106, loss_ll=18.6757, loss_ll_paf=29.4799, loss_ll_heat=7.87158, q=1000
[2018-07-09 19:09:54,556] [train] [INFO] epoch=0.00 step=5200, 10.3610 examples/sec lr=0.000100, loss=97.9474, loss_ll=17.7002, loss_ll_paf=28.8872, loss_ll_heat=6.51327, q=1000
[2018-07-09 19:12:19,861] [train] [INFO] epoch=0.00 step=5300, 10.3725 examples/sec lr=0.000100, loss=93.3914, loss_ll=16.9543, loss_ll_paf=27.9744, loss_ll_heat=5.93424, q=1000
[2018-07-09 19:14:44,258] [train] [INFO] epoch=0.00 step=5400, 10.3848 examples/sec lr=0.000100, loss=102.202, loss_ll=18.7606, loss_ll_paf=29.1964, loss_ll_heat=8.32477, q=1000
[2018-07-09 19:17:09,882] [train] [INFO] epoch=0.00 step=5500, 10.3952 examples/sec lr=0.000100, loss=112.303, loss_ll=21.3214, loss_ll_paf=34.5016, loss_ll_heat=8.14124, q=1000
[2018-07-09 19:19:34,821] [train] [INFO] epoch=0.00 step=5600, 10.4060 examples/sec lr=0.000100, loss=97.716, loss_ll=18.0919, loss_ll_paf=28.0226, loss_ll_heat=8.16121, q=1000
[2018-07-09 19:22:02,256] [train] [INFO] epoch=0.00 step=5700, 10.4135 examples/sec lr=0.000100, loss=97.6086, loss_ll=18.1556, loss_ll_paf=28.602, loss_ll_heat=7.70917, q=1000
[2018-07-09 19:24:27,786] [train] [INFO] epoch=0.00 step=5800, 10.4230 examples/sec lr=0.000100, loss=101.814, loss_ll=18.9177, loss_ll_paf=29.5393, loss_ll_heat=8.29608, q=1000
[2018-07-09 19:26:53,410] [train] [INFO] epoch=0.00 step=5900, 10.4321 examples/sec lr=0.000100, loss=87.5973, loss_ll=16.273, loss_ll_paf=25.2335, loss_ll_heat=7.31255, q=1000
[2018-07-09 19:29:20,555] [train] [INFO] epoch=0.00 step=6000, 10.4392 examples/sec lr=0.000100, loss=122.066, loss_ll=22.6276, loss_ll_paf=36.9354, loss_ll_heat=8.31974, q=1000
[2018-07-09 19:31:57,792] [train] [INFO] epoch=0.00 step=6100, 10.4347 examples/sec lr=0.000100, loss=102.759, loss_ll=19.5176, loss_ll_paf=32.2805, loss_ll_heat=6.75473, q=1000
[2018-07-09 19:34:19,748] [train] [INFO] epoch=0.00 step=6200, 10.4472 examples/sec lr=0.000100, loss=87.6484, loss_ll=16.0992, loss_ll_paf=25.6514, loss_ll_heat=6.54696, q=1000
[2018-07-09 19:36:45,217] [train] [INFO] epoch=0.00 step=6300, 10.4556 examples/sec lr=0.000100, loss=93.4971, loss_ll=17.3201, loss_ll_paf=26.8367, loss_ll_heat=7.80345, q=1000
[2018-07-09 19:39:12,547] [train] [INFO] epoch=0.00 step=6400, 10.4616 examples/sec lr=0.000100, loss=88.6331, loss_ll=16.5516, loss_ll_paf=26.6657, loss_ll_heat=6.4375, q=1000
[2018-07-09 19:41:36,622] [train] [INFO] epoch=0.00 step=6500, 10.4710 examples/sec lr=0.000100, loss=76.0762, loss_ll=13.9955, loss_ll_paf=22.6396, loss_ll_heat=5.35146, q=1000
[2018-07-09 19:44:04,476] [train] [INFO] epoch=0.00 step=6600, 10.4761 examples/sec lr=0.000100, loss=110.966, loss_ll=20.2028, loss_ll_paf=32.3232, loss_ll_heat=8.08233, q=1000
[2018-07-09 19:46:28,282] [train] [INFO] epoch=0.00 step=6700, 10.4853 examples/sec lr=0.000100, loss=79.4035, loss_ll=14.2331, loss_ll_paf=21.0307, loss_ll_heat=7.43551, q=1000
[2018-07-09 19:48:49,625] [train] [INFO] epoch=0.00 step=6800, 10.4967 examples/sec lr=0.000100, loss=115.859, loss_ll=22.1006, loss_ll_paf=35.5968, loss_ll_heat=8.60432, q=1000
[2018-07-09 19:51:12,367] [train] [INFO] epoch=0.00 step=6900, 10.5063 examples/sec lr=0.000100, loss=101.9, loss_ll=19.6877, loss_ll_paf=31.5887, loss_ll_heat=7.78663, q=1000
[2018-07-09 19:53:38,402] [train] [INFO] epoch=0.00 step=7000, 10.5125 examples/sec lr=0.000100, loss=126.222, loss_ll=23.3249, loss_ll_paf=38.1135, loss_ll_heat=8.53637, q=1000
[2018-07-09 19:56:17,395] [train] [INFO] epoch=0.00 step=7100, 10.5059 examples/sec lr=0.000100, loss=97.2768, loss_ll=17.1137, loss_ll_paf=28.2173, loss_ll_heat=6.01004, q=1000
[2018-07-09 19:58:42,566] [train] [INFO] epoch=0.00 step=7200, 10.5127 examples/sec lr=0.000100, loss=110.819, loss_ll=21.0128, loss_ll_paf=32.5651, loss_ll_heat=9.46055, q=1000
[2018-07-09 20:01:07,210] [train] [INFO] epoch=0.00 step=7300, 10.5199 examples/sec lr=0.000100, loss=83.3755, loss_ll=15.136, loss_ll_paf=24.2653, loss_ll_heat=6.00674, q=1000
[2018-07-09 20:03:33,947] [train] [INFO] epoch=0.00 step=7400, 10.5249 examples/sec lr=0.000100, loss=65.3297, loss_ll=11.6589, loss_ll_paf=18.0075, loss_ll_heat=5.31031, q=1000
[2018-07-09 20:06:00,922] [train] [INFO] epoch=0.00 step=7500, 10.5295 examples/sec lr=0.000100, loss=84.5152, loss_ll=15.6171, loss_ll_paf=24.1578, loss_ll_heat=7.07639, q=1000
[2018-07-09 20:08:26,900] [train] [INFO] epoch=0.00 step=7600, 10.5350 examples/sec lr=0.000100, loss=80.8291, loss_ll=14.6631, loss_ll_paf=22.136, loss_ll_heat=7.19017, q=1000
[2018-07-09 20:10:52,281] [train] [INFO] epoch=1.00 step=7700, 10.5408 examples/sec lr=0.000100, loss=80.4245, loss_ll=15.5658, loss_ll_paf=24.1873, loss_ll_heat=6.94428, q=1000
[2018-07-09 20:13:17,222] [train] [INFO] epoch=1.00 step=7800, 10.5469 examples/sec lr=0.000100, loss=77.1942, loss_ll=14.0395, loss_ll_paf=21.2433, loss_ll_heat=6.83566, q=1000
[2018-07-09 20:15:41,838] [train] [INFO] epoch=1.00 step=7900, 10.5532 examples/sec lr=0.000100, loss=101.678, loss_ll=19.2652, loss_ll_paf=31.345, loss_ll_heat=7.18546, q=1000
[2018-07-09 20:18:05,362] [train] [INFO] epoch=1.00 step=8000, 10.5602 examples/sec lr=0.000100, loss=96.7509, loss_ll=17.7665, loss_ll_paf=27.8722, loss_ll_heat=7.66078, q=1000
[2018-07-09 20:20:43,439] [train] [INFO] epoch=1.00 step=8100, 10.5546 examples/sec lr=0.000100, loss=53.4592, loss_ll=9.24278, loss_ll_paf=13.1648, loss_ll_heat=5.32078, q=1000
[2018-07-09 20:23:07,209] [train] [INFO] epoch=1.00 step=8200, 10.5612 examples/sec lr=0.000100, loss=83.0441, loss_ll=15.2343, loss_ll_paf=23.9611, loss_ll_heat=6.50748, q=1000
[2018-07-09 20:25:29,803] [train] [INFO] epoch=1.00 step=8300, 10.5687 examples/sec lr=0.000100, loss=88.9009, loss_ll=16.7167, loss_ll_paf=26.9259, loss_ll_heat=6.50738, q=1000
[2018-07-09 20:27:52,895] [train] [INFO] epoch=1.00 step=8400, 10.5756 examples/sec lr=0.000100, loss=74.964, loss_ll=13.2209, loss_ll_paf=20.1587, loss_ll_heat=6.283, q=1000
[2018-07-09 20:30:15,485] [train] [INFO] epoch=1.00 step=8500, 10.5828 examples/sec lr=0.000100, loss=92.5817, loss_ll=17.776, loss_ll_paf=29.1659, loss_ll_heat=6.38612, q=1000
[2018-07-09 20:32:39,190] [train] [INFO] epoch=1.00 step=8600, 10.5889 examples/sec lr=0.000100, loss=92.2541, loss_ll=17.3505, loss_ll_paf=27.1045, loss_ll_heat=7.59661, q=1000
[2018-07-09 20:35:04,040] [train] [INFO] epoch=1.00 step=8700, 10.5939 examples/sec lr=0.000100, loss=57.1995, loss_ll=10.3806, loss_ll_paf=15.2184, loss_ll_heat=5.54275, q=1000
[2018-07-09 20:37:30,410] [train] [INFO] epoch=1.00 step=8800, 10.5976 examples/sec lr=0.000100, loss=83.8942, loss_ll=15.8606, loss_ll_paf=24.3754, loss_ll_heat=7.34577, q=1000
[2018-07-09 20:39:56,901] [train] [INFO] epoch=1.00 step=8900, 10.6012 examples/sec lr=0.000100, loss=70.2742, loss_ll=12.5619, loss_ll_paf=18.9563, loss_ll_heat=6.16754, q=1000
[2018-07-09 20:42:22,713] [train] [INFO] epoch=1.00 step=9000, 10.6052 examples/sec lr=0.000100, loss=79.6061, loss_ll=14.2908, loss_ll_paf=23.4945, loss_ll_heat=5.08712, q=1000
[2018-07-09 20:44:58,418] [train] [INFO] epoch=1.00 step=9100, 10.6014 examples/sec lr=0.000100, loss=82.7093, loss_ll=14.6337, loss_ll_paf=22.7556, loss_ll_heat=6.51171, q=1000
[2018-07-09 20:47:22,087] [train] [INFO] epoch=1.00 step=9200, 10.6070 examples/sec lr=0.000100, loss=99.5311, loss_ll=19.1114, loss_ll_paf=31.2724, loss_ll_heat=6.95038, q=1000
[2018-07-09 20:49:47,995] [train] [INFO] epoch=1.00 step=9300, 10.6107 examples/sec lr=0.000100, loss=78.3034, loss_ll=14.2441, loss_ll_paf=23.3711, loss_ll_heat=5.11709, q=1000
[2018-07-09 20:52:15,844] [train] [INFO] epoch=1.00 step=9400, 10.6129 examples/sec lr=0.000100, loss=69.028, loss_ll=12.7468, loss_ll_paf=19.2892, loss_ll_heat=6.20429, q=1000
[2018-07-09 20:54:42,961] [train] [INFO] epoch=1.00 step=9500, 10.6156 examples/sec lr=0.000100, loss=81.864, loss_ll=15.0689, loss_ll_paf=21.7264, loss_ll_heat=8.41148, q=1000
[2018-07-09 20:57:09,061] [train] [INFO] epoch=1.00 step=9600, 10.6190 examples/sec lr=0.000100, loss=99.9803, loss_ll=18.8322, loss_ll_paf=30.9619, loss_ll_heat=6.70253, q=1000
[2018-07-09 20:59:31,144] [train] [INFO] epoch=1.00 step=9700, 10.6252 examples/sec lr=0.000100, loss=88.5641, loss_ll=16.7036, loss_ll_paf=26.5234, loss_ll_heat=6.88383, q=1000
[2018-07-09 21:01:58,926] [train] [INFO] epoch=1.00 step=9800, 10.6273 examples/sec lr=0.000100, loss=79.7219, loss_ll=13.9401, loss_ll_paf=22.9397, loss_ll_heat=4.94052, q=1000
[2018-07-09 21:04:25,100] [train] [INFO] epoch=1.00 step=9900, 10.6304 examples/sec lr=0.000100, loss=87.4632, loss_ll=17.061, loss_ll_paf=27.4458, loss_ll_heat=6.67622, q=1000
[2018-07-09 21:06:48,267] [train] [INFO] epoch=1.00 step=10000, 10.6356 examples/sec lr=0.000100, loss=69.6343, loss_ll=12.575, loss_ll_paf=20.1374, loss_ll_heat=5.01255, q=1000
[2018-07-09 21:09:23,483] [train] [INFO] epoch=1.00 step=10100, 10.6322 examples/sec lr=0.000100, loss=84.2931, loss_ll=15.6821, loss_ll_paf=24.0051, loss_ll_heat=7.35906, q=1000
[2018-07-09 21:11:47,886] [train] [INFO] epoch=1.00 step=10200, 10.6364 examples/sec lr=0.000100, loss=67.0198, loss_ll=12.2483, loss_ll_paf=19.7519, loss_ll_heat=4.74469, q=1000
[2018-07-09 21:14:11,710] [train] [INFO] epoch=1.00 step=10300, 10.6410 examples/sec lr=0.000100, loss=77.6748, loss_ll=13.9251, loss_ll_paf=21.7408, loss_ll_heat=6.10944, q=1000
[2018-07-09 21:16:38,392] [train] [INFO] epoch=1.00 step=10400, 10.6435 examples/sec lr=0.000100, loss=97.8825, loss_ll=18.9446, loss_ll_paf=30.9299, loss_ll_heat=6.95927, q=1000
[2018-07-09 21:19:03,749] [train] [INFO] epoch=1.00 step=10500, 10.6468 examples/sec lr=0.000100, loss=55.3029, loss_ll=10.2723, loss_ll_paf=16.4684, loss_ll_heat=4.07627, q=1000
[2018-07-09 21:21:26,869] [train] [INFO] epoch=1.00 step=10600, 10.6516 examples/sec lr=0.000100, loss=76.4088, loss_ll=13.9526, loss_ll_paf=22.1089, loss_ll_heat=5.79639, q=1000
[2018-07-09 21:23:50,418] [train] [INFO] epoch=1.00 step=10700, 10.6560 examples/sec lr=0.000100, loss=64.5865, loss_ll=11.3926, loss_ll_paf=17.6681, loss_ll_heat=5.1172, q=1000
[2018-07-09 21:26:15,719] [train] [INFO] epoch=1.00 step=10800, 10.6592 examples/sec lr=0.000100, loss=92.6847, loss_ll=17.1772, loss_ll_paf=27.0929, loss_ll_heat=7.26149, q=1000
[2018-07-09 21:28:41,890] [train] [INFO] epoch=1.00 step=10900, 10.6618 examples/sec lr=0.000100, loss=83.3837, loss_ll=15.7933, loss_ll_paf=24.5199, loss_ll_heat=7.06667, q=1000
[2018-07-09 21:31:05,459] [train] [INFO] epoch=1.00 step=11000, 10.6660 examples/sec lr=0.000100, loss=57.9599, loss_ll=11.035, loss_ll_paf=15.831, loss_ll_heat=6.2389, q=1000
[2018-07-09 21:33:42,689] [train] [INFO] epoch=1.00 step=11100, 10.6614 examples/sec lr=0.000100, loss=100.041, loss_ll=18.6713, loss_ll_paf=29.4947, loss_ll_heat=7.84782, q=1000
[2018-07-09 21:36:05,703] [train] [INFO] epoch=1.00 step=11200, 10.6658 examples/sec lr=0.000100, loss=75.1999, loss_ll=13.9273, loss_ll_paf=23.1417, loss_ll_heat=4.71289, q=1000
[2018-07-09 21:38:29,722] [train] [INFO] epoch=1.00 step=11300, 10.6696 examples/sec lr=0.000100, loss=117.437, loss_ll=22.2136, loss_ll_paf=35.8018, loss_ll_heat=8.62533, q=1000
[2018-07-09 21:40:53,053] [train] [INFO] epoch=1.00 step=11400, 10.6738 examples/sec lr=0.000100, loss=67.9383, loss_ll=12.3806, loss_ll_paf=18.6819, loss_ll_heat=6.07935, q=1000
[2018-07-09 21:43:16,248] [train] [INFO] epoch=1.00 step=11500, 10.6779 examples/sec lr=0.000100, loss=86.9068, loss_ll=16.5795, loss_ll_paf=25.0873, loss_ll_heat=8.07167, q=1000
[2018-07-09 21:45:39,385] [train] [INFO] epoch=1.00 step=11600, 10.6820 examples/sec lr=0.000100, loss=72.8131, loss_ll=13.6142, loss_ll_paf=21.9765, loss_ll_heat=5.25191, q=1000
[2018-07-09 21:48:02,326] [train] [INFO] epoch=1.00 step=11700, 10.6862 examples/sec lr=0.000100, loss=60.4448, loss_ll=11.2334, loss_ll_paf=16.8643, loss_ll_heat=5.60242, q=1000
[2018-07-09 21:50:27,264] [train] [INFO] epoch=1.00 step=11800, 10.6891 examples/sec lr=0.000100, loss=70.2798, loss_ll=12.7168, loss_ll_paf=19.9577, loss_ll_heat=5.47588, q=1000
[2018-07-09 21:52:52,552] [train] [INFO] epoch=1.00 step=11900, 10.6917 examples/sec lr=0.000100, loss=72.9848, loss_ll=13.9273, loss_ll_paf=21.1235, loss_ll_heat=6.73118, q=1000
[2018-07-09 21:55:16,537] [train] [INFO] epoch=1.00 step=12000, 10.6951 examples/sec lr=0.000100, loss=106.131, loss_ll=19.6992, loss_ll_paf=33.7353, loss_ll_heat=5.66315, q=1000
[2018-07-09 21:57:51,167] [train] [INFO] epoch=1.00 step=12100, 10.6921 examples/sec lr=0.000100, loss=104.723, loss_ll=18.2371, loss_ll_paf=29.8253, loss_ll_heat=6.6489, q=1000
[2018-07-09 22:00:14,155] [train] [INFO] epoch=1.00 step=12200, 10.6960 examples/sec lr=0.000100, loss=76.5847, loss_ll=13.8645, loss_ll_paf=22.9238, loss_ll_heat=4.80518, q=1000
[2018-07-09 22:02:35,986] [train] [INFO] epoch=1.00 step=12300, 10.7006 examples/sec lr=0.000100, loss=65.5289, loss_ll=12.4415, loss_ll_paf=19.0473, loss_ll_heat=5.83576, q=1000
[2018-07-09 22:05:02,604] [train] [INFO] epoch=1.00 step=12400, 10.7022 examples/sec lr=0.000100, loss=76.6847, loss_ll=13.7127, loss_ll_paf=20.7345, loss_ll_heat=6.69092, q=1000
[2018-07-09 22:07:29,663] [train] [INFO] epoch=1.00 step=12500, 10.7036 examples/sec lr=0.000100, loss=74.5417, loss_ll=13.7496, loss_ll_paf=21.7324, loss_ll_heat=5.7667, q=1000
[2018-07-09 22:09:54,114] [train] [INFO] epoch=1.00 step=12600, 10.7065 examples/sec lr=0.000100, loss=83.6444, loss_ll=15.493, loss_ll_paf=23.8139, loss_ll_heat=7.17206, q=1000
[2018-07-09 22:12:16,651] [train] [INFO] epoch=1.00 step=12700, 10.7104 examples/sec lr=0.000100, loss=118.353, loss_ll=22.0402, loss_ll_paf=36.5331, loss_ll_heat=7.54729, q=1000
[2018-07-09 22:14:39,298] [train] [INFO] epoch=1.00 step=12800, 10.7142 examples/sec lr=0.000100, loss=85.0328, loss_ll=15.9563, loss_ll_paf=24.6525, loss_ll_heat=7.26015, q=1000
[2018-07-09 22:17:03,796] [train] [INFO] epoch=1.00 step=12900, 10.7169 examples/sec lr=0.000100, loss=84.38, loss_ll=15.6921, loss_ll_paf=24.1707, loss_ll_heat=7.21354, q=1000
[2018-07-09 22:19:28,989] [train] [INFO] epoch=1.00 step=13000, 10.7191 examples/sec lr=0.000100, loss=47.0758, loss_ll=8.09418, loss_ll_paf=10.9822, loss_ll_heat=5.20618, q=1000
[2018-07-09 22:22:06,318] [train] [INFO] epoch=1.00 step=13100, 10.7147 examples/sec lr=0.000100, loss=78.4241, loss_ll=14.3778, loss_ll_paf=21.2559, loss_ll_heat=7.49961, q=1000
[2018-07-09 22:24:32,861] [train] [INFO] epoch=1.00 step=13200, 10.7162 examples/sec lr=0.000100, loss=69.5495, loss_ll=12.6209, loss_ll_paf=20.052, loss_ll_heat=5.18973, q=1000
[2018-07-09 22:26:57,124] [train] [INFO] epoch=1.00 step=13300, 10.7189 examples/sec lr=0.000100, loss=69.4526, loss_ll=12.5233, loss_ll_paf=18.5566, loss_ll_heat=6.49, q=1000
[2018-07-09 22:29:22,865] [train] [INFO] epoch=1.00 step=13400, 10.7208 examples/sec lr=0.000100, loss=77.8359, loss_ll=14.4683, loss_ll_paf=23.1032, loss_ll_heat=5.8334, q=1000
[2018-07-09 22:31:47,321] [train] [INFO] epoch=1.00 step=13500, 10.7234 examples/sec lr=0.000100, loss=79.3087, loss_ll=14.8158, loss_ll_paf=22.5371, loss_ll_heat=7.09459, q=1000
[2018-07-09 22:34:12,732] [train] [INFO] epoch=1.00 step=13600, 10.7254 examples/sec lr=0.000100, loss=65.1638, loss_ll=11.903, loss_ll_paf=17.4446, loss_ll_heat=6.36129, q=1000
[2018-07-09 22:36:37,439] [train] [INFO] epoch=1.00 step=13700, 10.7277 examples/sec lr=0.000100, loss=82.4154, loss_ll=16.1239, loss_ll_paf=25.2525, loss_ll_heat=6.99521, q=1000
[2018-07-09 22:39:02,969] [train] [INFO] epoch=1.00 step=13800, 10.7296 examples/sec lr=0.000100, loss=84.8136, loss_ll=15.0179, loss_ll_paf=23.6487, loss_ll_heat=6.38713, q=1000
[2018-07-09 22:41:28,293] [train] [INFO] epoch=1.00 step=13900, 10.7316 examples/sec lr=0.000100, loss=64.607, loss_ll=11.543, loss_ll_paf=17.502, loss_ll_heat=5.58406, q=1000
[2018-07-09 22:43:53,169] [train] [INFO] epoch=1.00 step=14000, 10.7337 examples/sec lr=0.000100, loss=64.8018, loss_ll=11.2867, loss_ll_paf=17.6688, loss_ll_heat=4.9045, q=1000
[2018-07-09 22:46:31,897] [train] [INFO] epoch=1.00 step=14100, 10.7288 examples/sec lr=0.000100, loss=73.1489, loss_ll=13.5212, loss_ll_paf=20.1415, loss_ll_heat=6.90088, q=1000
[2018-07-09 22:48:55,813] [train] [INFO] epoch=1.00 step=14200, 10.7315 examples/sec lr=0.000100, loss=79.9441, loss_ll=15.1267, loss_ll_paf=22.6645, loss_ll_heat=7.58883, q=1000
[2018-07-09 22:51:21,950] [train] [INFO] epoch=1.00 step=14300, 10.7329 examples/sec lr=0.000100, loss=62.9042, loss_ll=11.7186, loss_ll_paf=18.6112, loss_ll_heat=4.82611, q=1000
[2018-07-09 22:53:48,783] [train] [INFO] epoch=1.00 step=14400, 10.7341 examples/sec lr=0.000100, loss=80.7907, loss_ll=15.6192, loss_ll_paf=25.0319, loss_ll_heat=6.20653, q=1000
[2018-07-09 22:56:15,318] [train] [INFO] epoch=1.00 step=14500, 10.7353 examples/sec lr=0.000100, loss=65.8833, loss_ll=11.4827, loss_ll_paf=16.4718, loss_ll_heat=6.4936, q=1000
[2018-07-09 22:58:39,395] [train] [INFO] epoch=1.00 step=14600, 10.7378 examples/sec lr=0.000100, loss=55.8747, loss_ll=9.78913, loss_ll_paf=14.9634, loss_ll_heat=4.61484, q=1000
[2018-07-09 23:01:02,113] [train] [INFO] epoch=1.00 step=14700, 10.7409 examples/sec lr=0.000100, loss=84.6687, loss_ll=15.6613, loss_ll_paf=24.6927, loss_ll_heat=6.63002, q=1000
[2018-07-09 23:03:26,864] [train] [INFO] epoch=1.00 step=14800, 10.7429 examples/sec lr=0.000100, loss=71.6541, loss_ll=13.5608, loss_ll_paf=20.1118, loss_ll_heat=7.00991, q=1000
[2018-07-09 23:05:49,543] [train] [INFO] epoch=1.00 step=14900, 10.7459 examples/sec lr=0.000100, loss=64.5557, loss_ll=12.0287, loss_ll_paf=18.3962, loss_ll_heat=5.66114, q=1000
[2018-07-09 23:08:13,281] [train] [INFO] epoch=1.00 step=15000, 10.7484 examples/sec lr=0.000100, loss=78.5846, loss_ll=14.6906, loss_ll_paf=24.3883, loss_ll_heat=4.99294, q=1000
[2018-07-09 23:10:51,084] [train] [INFO] epoch=1.00 step=15100, 10.7441 examples/sec lr=0.000100, loss=76.2154, loss_ll=14.416, loss_ll_paf=22.1884, loss_ll_heat=6.64367, q=1000
[2018-07-09 23:13:16,616] [train] [INFO] epoch=1.00 step=15200, 10.7458 examples/sec lr=0.000100, loss=54.8537, loss_ll=10.1685, loss_ll_paf=16.0958, loss_ll_heat=4.24122, q=1000
[2018-07-09 23:15:39,882] [train] [INFO] epoch=2.00 step=15300, 10.7484 examples/sec lr=0.000100, loss=70.3261, loss_ll=13.4029, loss_ll_paf=22.3514, loss_ll_heat=4.45447, q=1000
[2018-07-09 23:18:03,145] [train] [INFO] epoch=2.00 step=15400, 10.7510 examples/sec lr=0.000100, loss=60.2769, loss_ll=11.0055, loss_ll_paf=15.5939, loss_ll_heat=6.41716, q=1000
[2018-07-09 23:20:25,613] [train] [INFO] epoch=2.00 step=15500, 10.7540 examples/sec lr=0.000100, loss=91.4152, loss_ll=17.7337, loss_ll_paf=29.7896, loss_ll_heat=5.67782, q=1000
[2018-07-09 23:22:50,674] [train] [INFO] epoch=2.00 step=15600, 10.7557 examples/sec lr=0.000100, loss=58.1483, loss_ll=10.546, loss_ll_paf=16.2723, loss_ll_heat=4.81971, q=1000
[2018-07-09 23:25:13,381] [train] [INFO] epoch=2.00 step=15700, 10.7585 examples/sec lr=0.000100, loss=74.5241, loss_ll=13.467, loss_ll_paf=22.0904, loss_ll_heat=4.84352, q=1000
[2018-07-09 23:27:38,267] [train] [INFO] epoch=2.00 step=15800, 10.7603 examples/sec lr=0.000100, loss=67.2463, loss_ll=12.6957, loss_ll_paf=18.7403, loss_ll_heat=6.6511, q=1000
[2018-07-09 23:30:01,516] [train] [INFO] epoch=2.00 step=15900, 10.7627 examples/sec lr=0.000100, loss=96.5706, loss_ll=18.2107, loss_ll_paf=29.0371, loss_ll_heat=7.38435, q=1000
[2018-07-09 23:32:24,581] [train] [INFO] epoch=2.00 step=16000, 10.7653 examples/sec lr=0.000100, loss=49.9615, loss_ll=9.20607, loss_ll_paf=13.28, loss_ll_heat=5.13215, q=1000
[2018-07-09 23:35:01,702] [train] [INFO] epoch=2.00 step=16100, 10.7615 examples/sec lr=0.000100, loss=53.7842, loss_ll=9.831, loss_ll_paf=15.1228, loss_ll_heat=4.53924, q=1000
[2018-07-09 23:37:28,853] [train] [INFO] epoch=2.00 step=16200, 10.7621 examples/sec lr=0.000100, loss=76.1585, loss_ll=13.9415, loss_ll_paf=21.494, loss_ll_heat=6.38896, q=1000
[2018-07-09 23:39:53,676] [train] [INFO] epoch=2.00 step=16300, 10.7638 examples/sec lr=0.000100, loss=64.5042, loss_ll=12.1491, loss_ll_paf=19.2461, loss_ll_heat=5.05216, q=1000
[2018-07-09 23:42:16,253] [train] [INFO] epoch=2.00 step=16400, 10.7665 examples/sec lr=0.000100, loss=56.3721, loss_ll=10.3916, loss_ll_paf=15.411, loss_ll_heat=5.37222, q=1000
[2018-07-09 23:44:41,448] [train] [INFO] epoch=2.00 step=16500, 10.7680 examples/sec lr=0.000100, loss=58.2902, loss_ll=10.8231, loss_ll_paf=16.308, loss_ll_heat=5.33828, q=1000
[2018-07-09 23:47:10,040] [train] [INFO] epoch=2.00 step=16600, 10.7680 examples/sec lr=0.000100, loss=61.1842, loss_ll=11.3291, loss_ll_paf=17.0288, loss_ll_heat=5.6293, q=1000
[2018-07-09 23:49:34,428] [train] [INFO] epoch=2.00 step=16700, 10.7698 examples/sec lr=0.000100, loss=59.688, loss_ll=10.6257, loss_ll_paf=15.6651, loss_ll_heat=5.58627, q=1000
[2018-07-09 23:51:58,977] [train] [INFO] epoch=2.00 step=16800, 10.7716 examples/sec lr=0.000100, loss=66.8401, loss_ll=11.8408, loss_ll_paf=17.2822, loss_ll_heat=6.39942, q=1000
[2018-07-09 23:54:25,014] [train] [INFO] epoch=2.00 step=16900, 10.7727 examples/sec lr=0.000100, loss=68.7706, loss_ll=12.5328, loss_ll_paf=19.6999, loss_ll_heat=5.36571, q=1000
[2018-07-09 23:56:47,939] [train] [INFO] epoch=2.00 step=17000, 10.7750 examples/sec lr=0.000100, loss=80.922, loss_ll=14.4872, loss_ll_paf=23.0809, loss_ll_heat=5.89347, q=1000
[2018-07-09 23:59:22,529] [train] [INFO] epoch=2.00 step=17100, 10.7725 examples/sec lr=0.000100, loss=60.0156, loss_ll=10.9567, loss_ll_paf=15.7732, loss_ll_heat=6.14013, q=1000
[2018-07-10 00:01:47,324] [train] [INFO] epoch=2.00 step=17200, 10.7740 examples/sec lr=0.000100, loss=69.288, loss_ll=13.0677, loss_ll_paf=20.207, loss_ll_heat=5.92846, q=1000
[2018-07-10 00:04:10,905] [train] [INFO] epoch=2.00 step=17300, 10.7761 examples/sec lr=0.000100, loss=82.304, loss_ll=15.8282, loss_ll_paf=24.2866, loss_ll_heat=7.36966, q=1000
[2018-07-10 00:06:33,254] [train] [INFO] epoch=2.00 step=17400, 10.7786 examples/sec lr=0.000100, loss=60.3669, loss_ll=11.1251, loss_ll_paf=17.2421, loss_ll_heat=5.00807, q=1000
[2018-07-10 00:08:56,150] [train] [INFO] epoch=2.00 step=17500, 10.7810 examples/sec lr=0.000100, loss=63.5003, loss_ll=11.5896, loss_ll_paf=18.5291, loss_ll_heat=4.65001, q=1000
[2018-07-10 00:11:20,609] [train] [INFO] epoch=2.00 step=17600, 10.7826 examples/sec lr=0.000100, loss=69.5167, loss_ll=13.3791, loss_ll_paf=20.6159, loss_ll_heat=6.14235, q=1000
[2018-07-10 00:13:44,052] [train] [INFO] epoch=2.00 step=17700, 10.7846 examples/sec lr=0.000100, loss=77.4049, loss_ll=14.2304, loss_ll_paf=21.9071, loss_ll_heat=6.55357, q=1000
[2018-07-10 00:16:07,067] [train] [INFO] epoch=2.00 step=17800, 10.7868 examples/sec lr=0.000100, loss=88.6499, loss_ll=15.6152, loss_ll_paf=24.2583, loss_ll_heat=6.97212, q=1000
[2018-07-10 00:18:30,823] [train] [INFO] epoch=2.00 step=17900, 10.7887 examples/sec lr=0.000100, loss=69.1814, loss_ll=12.4656, loss_ll_paf=18.6646, loss_ll_heat=6.26651, q=1000
[2018-07-10 00:20:55,671] [train] [INFO] epoch=2.00 step=18000, 10.7901 examples/sec lr=0.000100, loss=52.7478, loss_ll=9.38695, loss_ll_paf=14.4676, loss_ll_heat=4.30627, q=1000
[2018-07-10 00:23:33,946] [train] [INFO] epoch=2.00 step=18100, 10.7860 examples/sec lr=0.000100, loss=69.1325, loss_ll=13.1448, loss_ll_paf=19.8683, loss_ll_heat=6.42134, q=1000
[2018-07-10 00:25:58,431] [train] [INFO] epoch=2.00 step=18200, 10.7876 examples/sec lr=0.000100, loss=59.5841, loss_ll=11.093, loss_ll_paf=16.8312, loss_ll_heat=5.35466, q=1000
[2018-07-10 00:28:21,009] [train] [INFO] epoch=2.00 step=18300, 10.7899 examples/sec lr=0.000100, loss=60.0616, loss_ll=11.1322, loss_ll_paf=16.7378, loss_ll_heat=5.52669, q=1000
[2018-07-10 00:30:45,294] [train] [INFO] epoch=2.00 step=18400, 10.7914 examples/sec lr=0.000100, loss=96.4711, loss_ll=17.8513, loss_ll_paf=28.6961, loss_ll_heat=7.00651, q=1000
[2018-07-10 00:33:08,975] [train] [INFO] epoch=2.00 step=18500, 10.7932 examples/sec lr=0.000100, loss=92.5581, loss_ll=17.0039, loss_ll_paf=27.0676, loss_ll_heat=6.94021, q=1000
[2018-07-10 00:35:32,025] [train] [INFO] epoch=2.00 step=18600, 10.7953 examples/sec lr=0.000100, loss=71.9804, loss_ll=13.3329, loss_ll_paf=20.7429, loss_ll_heat=5.92302, q=1000
[2018-07-10 00:37:56,584] [train] [INFO] epoch=2.00 step=18700, 10.7967 examples/sec lr=0.000100, loss=81.8892, loss_ll=15.0599, loss_ll_paf=24.4341, loss_ll_heat=5.68566, q=1000
[2018-07-10 00:40:21,873] [train] [INFO] epoch=2.00 step=18800, 10.7978 examples/sec lr=0.000100, loss=71.4986, loss_ll=13.7195, loss_ll_paf=21.9423, loss_ll_heat=5.49678, q=1000
[2018-07-10 00:42:44,383] [train] [INFO] epoch=2.00 step=18900, 10.8000 examples/sec lr=0.000100, loss=61.4567, loss_ll=11.1321, loss_ll_paf=17.6362, loss_ll_heat=4.62793, q=1000
[2018-07-10 00:45:06,859] [train] [INFO] epoch=2.00 step=19000, 10.8022 examples/sec lr=0.000100, loss=70.3158, loss_ll=13.2546, loss_ll_paf=20.8218, loss_ll_heat=5.6873, q=1000
[2018-07-10 00:47:43,270] [train] [INFO] epoch=2.00 step=19100, 10.7990 examples/sec lr=0.000100, loss=75.1596, loss_ll=14.1479, loss_ll_paf=21.8367, loss_ll_heat=6.45912, q=1000
[2018-07-10 00:50:10,296] [train] [INFO] epoch=2.00 step=19200, 10.7995 examples/sec lr=0.000100, loss=70.5504, loss_ll=13.2407, loss_ll_paf=20.0758, loss_ll_heat=6.4057, q=1000
[2018-07-10 00:52:33,790] [train] [INFO] epoch=2.00 step=19300, 10.8012 examples/sec lr=0.000100, loss=87.1072, loss_ll=15.9544, loss_ll_paf=24.739, loss_ll_heat=7.16984, q=1000
[2018-07-10 00:54:56,392] [train] [INFO] epoch=2.00 step=19400, 10.8033 examples/sec lr=0.000100, loss=82.818, loss_ll=14.83, loss_ll_paf=22.6966, loss_ll_heat=6.9635, q=1000
[2018-07-10 00:57:21,649] [train] [INFO] epoch=2.00 step=19500, 10.8044 examples/sec lr=0.000100, loss=93.7013, loss_ll=18.3292, loss_ll_paf=29.9543, loss_ll_heat=6.70404, q=1000
[2018-07-10 00:59:42,659] [train] [INFO] epoch=2.00 step=19600, 10.8070 examples/sec lr=0.000100, loss=75.1774, loss_ll=14.5328, loss_ll_paf=22.4078, loss_ll_heat=6.65782, q=1000
[2018-07-10 01:02:05,697] [train] [INFO] epoch=2.00 step=19700, 10.8089 examples/sec lr=0.000100, loss=55.8856, loss_ll=10.3237, loss_ll_paf=14.6674, loss_ll_heat=5.98007, q=1000
[2018-07-10 01:04:28,951] [train] [INFO] epoch=2.00 step=19800, 10.8106 examples/sec lr=0.000100, loss=63.3169, loss_ll=11.9733, loss_ll_paf=18.557, loss_ll_heat=5.38962, q=1000
[2018-07-10 01:06:52,282] [train] [INFO] epoch=2.00 step=19900, 10.8123 examples/sec lr=0.000100, loss=63.1469, loss_ll=11.1521, loss_ll_paf=16.6806, loss_ll_heat=5.62363, q=1000
[2018-07-10 01:09:14,653] [train] [INFO] epoch=2.00 step=20000, 10.8144 examples/sec lr=0.000100, loss=51.5014, loss_ll=9.11445, loss_ll_paf=13.3376, loss_ll_heat=4.89133, q=1000
[2018-07-10 01:11:50,030] [train] [INFO] epoch=2.00 step=20100, 10.8117 examples/sec lr=0.000100, loss=69.7092, loss_ll=12.73, loss_ll_paf=19.2996, loss_ll_heat=6.16039, q=1000
[2018-07-10 01:14:12,732] [train] [INFO] epoch=2.00 step=20200, 10.8136 examples/sec lr=0.000100, loss=69.5488, loss_ll=13.1235, loss_ll_paf=20.2636, loss_ll_heat=5.98337, q=1000
[2018-07-10 01:16:36,782] [train] [INFO] epoch=2.00 step=20300, 10.8150 examples/sec lr=0.000100, loss=69.5695, loss_ll=12.6866, loss_ll_paf=19.661, loss_ll_heat=5.7122, q=1000
[2018-07-10 01:19:01,719] [train] [INFO] epoch=2.00 step=20400, 10.8161 examples/sec lr=0.000100, loss=59.4828, loss_ll=10.7899, loss_ll_paf=14.9447, loss_ll_heat=6.63507, q=1000
[2018-07-10 01:21:27,636] [train] [INFO] epoch=2.00 step=20500, 10.8168 examples/sec lr=0.000100, loss=51.3121, loss_ll=9.38671, loss_ll_paf=14.6632, loss_ll_heat=4.11028, q=1000
[2018-07-10 01:23:51,060] [train] [INFO] epoch=2.00 step=20600, 10.8184 examples/sec lr=0.000100, loss=77.9718, loss_ll=14.2225, loss_ll_paf=23.0235, loss_ll_heat=5.42149, q=1000
[2018-07-10 01:26:13,768] [train] [INFO] epoch=2.00 step=20700, 10.8202 examples/sec lr=0.000100, loss=65.8447, loss_ll=11.7482, loss_ll_paf=18.0293, loss_ll_heat=5.46698, q=1000
[2018-07-10 01:28:40,609] [train] [INFO] epoch=2.00 step=20800, 10.8206 examples/sec lr=0.000100, loss=71.7723, loss_ll=13.3608, loss_ll_paf=19.8798, loss_ll_heat=6.84178, q=1000
[2018-07-10 01:31:07,000] [train] [INFO] epoch=2.00 step=20900, 10.8211 examples/sec lr=0.000100, loss=62.7841, loss_ll=11.222, loss_ll_paf=17.0025, loss_ll_heat=5.44158, q=1000
[2018-07-10 01:33:30,145] [train] [INFO] epoch=2.00 step=21000, 10.8227 examples/sec lr=0.000100, loss=60.9586, loss_ll=11.6136, loss_ll_paf=18.0751, loss_ll_heat=5.15215, q=1000
[2018-07-10 01:36:05,498] [train] [INFO] epoch=2.00 step=21100, 10.8201 examples/sec lr=0.000100, loss=55.9432, loss_ll=9.5903, loss_ll_paf=13.8006, loss_ll_heat=5.38, q=1000
[2018-07-10 01:38:29,979] [train] [INFO] epoch=2.00 step=21200, 10.8213 examples/sec lr=0.000100, loss=71.1917, loss_ll=13.2467, loss_ll_paf=21.1236, loss_ll_heat=5.36979, q=1000
[2018-07-10 01:40:54,878] [train] [INFO] epoch=2.00 step=21300, 10.8223 examples/sec lr=0.000100, loss=64.632, loss_ll=11.5826, loss_ll_paf=17.0644, loss_ll_heat=6.10087, q=1000
[2018-07-10 01:43:21,352] [train] [INFO] epoch=2.00 step=21400, 10.8228 examples/sec lr=0.000100, loss=85.794, loss_ll=16.1273, loss_ll_paf=25.1383, loss_ll_heat=7.11624, q=1000
[2018-07-10 01:45:45,516] [train] [INFO] epoch=2.00 step=21500, 10.8240 examples/sec lr=0.000100, loss=59.8828, loss_ll=11.19, loss_ll_paf=17.7117, loss_ll_heat=4.66825, q=1000
[2018-07-10 01:48:09,388] [train] [INFO] epoch=2.00 step=21600, 10.8254 examples/sec lr=0.000100, loss=92.0596, loss_ll=17.4242, loss_ll_paf=27.7082, loss_ll_heat=7.14016, q=1000
[2018-07-10 01:50:32,364] [train] [INFO] epoch=2.00 step=21700, 10.8270 examples/sec lr=0.000100, loss=59.4316, loss_ll=11.3445, loss_ll_paf=17.5444, loss_ll_heat=5.14463, q=1000
[2018-07-10 01:52:56,949] [train] [INFO] epoch=2.00 step=21800, 10.8281 examples/sec lr=0.000100, loss=49.0822, loss_ll=8.78507, loss_ll_paf=12.2428, loss_ll_heat=5.32732, q=1000
[2018-07-10 01:55:19,728] [train] [INFO] epoch=2.00 step=21900, 10.8298 examples/sec lr=0.000100, loss=100.759, loss_ll=18.6203, loss_ll_paf=30.2861, loss_ll_heat=6.95459, q=1000
[2018-07-10 01:57:43,913] [train] [INFO] epoch=2.00 step=22000, 10.8309 examples/sec lr=0.000100, loss=53.9451, loss_ll=10.0342, loss_ll_paf=14.3075, loss_ll_heat=5.76085, q=1000
[2018-07-10 02:00:23,361] [train] [INFO] epoch=2.00 step=22100, 10.8270 examples/sec lr=0.000100, loss=83.1509, loss_ll=15.2954, loss_ll_paf=24.6319, loss_ll_heat=5.95894, q=1000
[2018-07-10 02:02:48,982] [train] [INFO] epoch=2.00 step=22200, 10.8278 examples/sec lr=0.000100, loss=58.0205, loss_ll=11.2306, loss_ll_paf=16.835, loss_ll_heat=5.62611, q=1000
[2018-07-10 02:05:13,036] [train] [INFO] epoch=2.00 step=22300, 10.8290 examples/sec lr=0.000100, loss=75.7001, loss_ll=14.3481, loss_ll_paf=22.5393, loss_ll_heat=6.15688, q=1000
[2018-07-10 02:07:35,813] [train] [INFO] epoch=2.00 step=22400, 10.8306 examples/sec lr=0.000100, loss=51.9513, loss_ll=9.75237, loss_ll_paf=15.0588, loss_ll_heat=4.44594, q=1000
[2018-07-10 02:10:04,299] [train] [INFO] epoch=2.00 step=22500, 10.8304 examples/sec lr=0.000100, loss=72.8926, loss_ll=13.9753, loss_ll_paf=21.8685, loss_ll_heat=6.08207, q=1000
[2018-07-10 02:12:28,873] [train] [INFO] epoch=2.00 step=22600, 10.8314 examples/sec lr=0.000100, loss=55.464, loss_ll=10.3055, loss_ll_paf=15.6629, loss_ll_heat=4.94806, q=1000
[2018-07-10 02:14:52,508] [train] [INFO] epoch=2.00 step=22700, 10.8327 examples/sec lr=0.000100, loss=51.1452, loss_ll=9.20295, loss_ll_paf=13.6331, loss_ll_heat=4.77278, q=1000
[2018-07-10 02:17:16,252] [train] [INFO] epoch=2.00 step=22800, 10.8340 examples/sec lr=0.000100, loss=52.4455, loss_ll=9.15803, loss_ll_paf=14.2097, loss_ll_heat=4.10631, q=1000
[2018-07-10 02:19:41,009] [train] [INFO] epoch=3.00 step=22900, 10.8349 examples/sec lr=0.000100, loss=49.1968, loss_ll=8.99836, loss_ll_paf=13.8205, loss_ll_heat=4.17627, q=1000
[2018-07-10 02:22:03,725] [train] [INFO] epoch=3.00 step=23000, 10.8365 examples/sec lr=0.000100, loss=52.4324, loss_ll=9.27492, loss_ll_paf=13.6784, loss_ll_heat=4.8714, q=1000
[2018-07-10 02:24:42,038] [train] [INFO] epoch=3.00 step=23100, 10.8331 examples/sec lr=0.000100, loss=74.8113, loss_ll=13.352, loss_ll_paf=20.7488, loss_ll_heat=5.95513, q=1000
[2018-07-10 02:27:07,942] [train] [INFO] epoch=3.00 step=23200, 10.8337 examples/sec lr=0.000100, loss=42.8224, loss_ll=7.24215, loss_ll_paf=10.1475, loss_ll_heat=4.33677, q=1000
[2018-07-10 02:29:31,502] [train] [INFO] epoch=3.00 step=23300, 10.8350 examples/sec lr=0.000100, loss=68.6711, loss_ll=12.832, loss_ll_paf=20.1517, loss_ll_heat=5.51233, q=1000
[2018-07-10 02:31:54,853] [train] [INFO] epoch=3.00 step=23400, 10.8363 examples/sec lr=0.000100, loss=71.0345, loss_ll=13.1547, loss_ll_paf=21.2413, loss_ll_heat=5.06814, q=1000
[2018-07-10 02:34:21,428] [train] [INFO] epoch=3.00 step=23500, 10.8367 examples/sec lr=0.000100, loss=42.0464, loss_ll=7.22219, loss_ll_paf=10.227, loss_ll_heat=4.2174, q=1000
[2018-07-10 02:36:46,574] [train] [INFO] epoch=3.00 step=23600, 10.8374 examples/sec lr=0.000100, loss=57.6427, loss_ll=10.3723, loss_ll_paf=15.6177, loss_ll_heat=5.12683, q=1000
[2018-07-10 02:39:10,002] [train] [INFO] epoch=3.00 step=23700, 10.8387 examples/sec lr=0.000100, loss=47.3989, loss_ll=8.50752, loss_ll_paf=11.8177, loss_ll_heat=5.19737, q=1000
[2018-07-10 02:41:37,030] [train] [INFO] epoch=3.00 step=23800, 10.8389 examples/sec lr=0.000100, loss=81.2961, loss_ll=14.9381, loss_ll_paf=22.5184, loss_ll_heat=7.3579, q=1000
[2018-07-10 02:44:04,186] [train] [INFO] epoch=3.00 step=23900, 10.8391 examples/sec lr=0.000100, loss=70.3385, loss_ll=13.284, loss_ll_paf=20.3248, loss_ll_heat=6.24314, q=1000
[2018-07-10 02:46:25,830] [train] [INFO] epoch=3.00 step=24000, 10.8409 examples/sec lr=0.000100, loss=39.3472, loss_ll=6.99751, loss_ll_paf=9.83127, loss_ll_heat=4.16374, q=1000
[2018-07-10 02:49:03,720] [train] [INFO] epoch=3.00 step=24100, 10.8378 examples/sec lr=0.000100, loss=63.5932, loss_ll=11.6071, loss_ll_paf=17.8651, loss_ll_heat=5.34918, q=1000
[2018-07-10 02:51:28,142] [train] [INFO] epoch=3.00 step=24200, 10.8387 examples/sec lr=0.000100, loss=81.3488, loss_ll=14.4987, loss_ll_paf=22.4188, loss_ll_heat=6.57858, q=1000
[2018-07-10 02:53:51,313] [train] [INFO] epoch=3.00 step=24300, 10.8401 examples/sec lr=0.000100, loss=58.8044, loss_ll=10.9036, loss_ll_paf=16.5231, loss_ll_heat=5.28413, q=1000
[2018-07-10 02:56:18,774] [train] [INFO] epoch=3.00 step=24400, 10.8401 examples/sec lr=0.000100, loss=64.5841, loss_ll=12.2476, loss_ll_paf=19.9539, loss_ll_heat=4.54127, q=1000
[2018-07-10 02:58:45,821] [train] [INFO] epoch=3.00 step=24500, 10.8403 examples/sec lr=0.000100, loss=59.3641, loss_ll=10.6565, loss_ll_paf=16.642, loss_ll_heat=4.67105, q=1000
[2018-07-10 03:01:10,623] [train] [INFO] epoch=3.00 step=24600, 10.8411 examples/sec lr=0.000100, loss=78.3015, loss_ll=14.5467, loss_ll_paf=22.461, loss_ll_heat=6.63232, q=1000
[2018-07-10 03:03:33,121] [train] [INFO] epoch=3.00 step=24700, 10.8426 examples/sec lr=0.000100, loss=58.0547, loss_ll=10.5826, loss_ll_paf=15.4388, loss_ll_heat=5.72646, q=1000
[2018-07-10 03:05:56,925] [train] [INFO] epoch=3.00 step=24800, 10.8437 examples/sec lr=0.000100, loss=52.69, loss_ll=10.098, loss_ll_paf=14.9791, loss_ll_heat=5.21698, q=1000
[2018-07-10 03:08:22,268] [train] [INFO] epoch=3.00 step=24900, 10.8444 examples/sec lr=0.000100, loss=76.1925, loss_ll=13.901, loss_ll_paf=20.8532, loss_ll_heat=6.94885, q=1000
[2018-07-10 03:10:46,473] [train] [INFO] epoch=3.00 step=25000, 10.8454 examples/sec lr=0.000100, loss=71.2106, loss_ll=13.0677, loss_ll_paf=20.6712, loss_ll_heat=5.46407, q=1000
[2018-07-10 03:13:24,088] [train] [INFO] epoch=3.00 step=25100, 10.8424 examples/sec lr=0.000100, loss=61.795, loss_ll=11.34, loss_ll_paf=17.5991, loss_ll_heat=5.08084, q=1000
[2018-07-10 03:15:47,453] [train] [INFO] epoch=3.00 step=25200, 10.8437 examples/sec lr=0.000100, loss=64.9799, loss_ll=11.5926, loss_ll_paf=17.2467, loss_ll_heat=5.93859, q=1000
[2018-07-10 03:18:13,697] [train] [INFO] epoch=3.00 step=25300, 10.8440 examples/sec lr=0.000100, loss=62.3474, loss_ll=11.3798, loss_ll_paf=17.4933, loss_ll_heat=5.26628, q=1000
[2018-07-10 03:20:38,938] [train] [INFO] epoch=3.00 step=25400, 10.8447 examples/sec lr=0.000100, loss=47.1074, loss_ll=8.57992, loss_ll_paf=12.5393, loss_ll_heat=4.62056, q=1000
[2018-07-10 03:23:00,994] [train] [INFO] epoch=3.00 step=25500, 10.8463 examples/sec lr=0.000100, loss=51.3628, loss_ll=9.47915, loss_ll_paf=14.3265, loss_ll_heat=4.63185, q=1000
[2018-07-10 03:25:22,136] [train] [INFO] epoch=3.00 step=25600, 10.8481 examples/sec lr=0.000100, loss=56.7522, loss_ll=10.5027, loss_ll_paf=15.8909, loss_ll_heat=5.11459, q=1000
[2018-07-10 03:27:46,413] [train] [INFO] epoch=3.00 step=25700, 10.8490 examples/sec lr=0.000100, loss=78.6023, loss_ll=14.5238, loss_ll_paf=23.7645, loss_ll_heat=5.28298, q=1000
[2018-07-10 03:30:13,125] [train] [INFO] epoch=3.00 step=25800, 10.8493 examples/sec lr=0.000100, loss=40.3467, loss_ll=7.4282, loss_ll_paf=10.7896, loss_ll_heat=4.06675, q=1000
[2018-07-10 03:32:38,105] [train] [INFO] epoch=3.00 step=25900, 10.8500 examples/sec lr=0.000100, loss=36.8456, loss_ll=6.19462, loss_ll_paf=8.57696, loss_ll_heat=3.81229, q=1000
[2018-07-10 03:35:03,103] [train] [INFO] epoch=3.00 step=26000, 10.8507 examples/sec lr=0.000100, loss=64.6246, loss_ll=12.4002, loss_ll_paf=18.2183, loss_ll_heat=6.58207, q=1000
[2018-07-10 03:37:39,878] [train] [INFO] epoch=3.00 step=26100, 10.8480 examples/sec lr=0.000100, loss=49.4715, loss_ll=9.01084, loss_ll_paf=13.7743, loss_ll_heat=4.24741, q=1000
[2018-07-10 03:40:05,982] [train] [INFO] epoch=3.00 step=26200, 10.8484 examples/sec lr=0.000100, loss=62.027, loss_ll=11.5448, loss_ll_paf=18.4844, loss_ll_heat=4.60522, q=1000
[2018-07-10 03:42:30,778] [train] [INFO] epoch=3.00 step=26300, 10.8492 examples/sec lr=0.000100, loss=74.7758, loss_ll=13.9403, loss_ll_paf=22.3984, loss_ll_heat=5.4822, q=1000
[2018-07-10 03:44:53,844] [train] [INFO] epoch=3.00 step=26400, 10.8504 examples/sec lr=0.000100, loss=45.7625, loss_ll=8.48573, loss_ll_paf=13.0314, loss_ll_heat=3.94002, q=1000
[2018-07-10 03:47:18,717] [train] [INFO] epoch=3.00 step=26500, 10.8511 examples/sec lr=0.000100, loss=60.7394, loss_ll=11.3404, loss_ll_paf=17.971, loss_ll_heat=4.7098, q=1000
[2018-07-10 03:49:42,157] [train] [INFO] epoch=3.00 step=26600, 10.8522 examples/sec lr=0.000100, loss=53.1777, loss_ll=9.81007, loss_ll_paf=15.6399, loss_ll_heat=3.98028, q=1000
[2018-07-10 03:52:05,628] [train] [INFO] epoch=3.00 step=26700, 10.8533 examples/sec lr=0.000100, loss=64.1836, loss_ll=11.678, loss_ll_paf=17.6022, loss_ll_heat=5.75386, q=1000
[2018-07-10 03:54:29,595] [train] [INFO] epoch=3.00 step=26800, 10.8543 examples/sec lr=0.000100, loss=57.9197, loss_ll=10.9014, loss_ll_paf=16.7994, loss_ll_heat=5.00344, q=1000
[2018-07-10 03:56:54,122] [train] [INFO] epoch=3.00 step=26900, 10.8551 examples/sec lr=0.000100, loss=69.163, loss_ll=12.6277, loss_ll_paf=20.0071, loss_ll_heat=5.24827, q=1000
[2018-07-10 03:59:18,510] [train] [INFO] epoch=3.00 step=27000, 10.8559 examples/sec lr=0.000100, loss=81.6245, loss_ll=14.9213, loss_ll_paf=24.272, loss_ll_heat=5.57057, q=1000
[2018-07-10 04:01:54,322] [train] [INFO] epoch=3.00 step=27100, 10.8536 examples/sec lr=0.000100, loss=57.9231, loss_ll=9.97626, loss_ll_paf=15.3134, loss_ll_heat=4.63907, q=1000
[2018-07-10 04:04:17,430] [train] [INFO] epoch=3.00 step=27200, 10.8548 examples/sec lr=0.000100, loss=51.0235, loss_ll=8.99875, loss_ll_paf=13.4996, loss_ll_heat=4.49786, q=1000
[2018-07-10 04:06:42,070] [train] [INFO] epoch=3.00 step=27300, 10.8555 examples/sec lr=0.000100, loss=80.4939, loss_ll=14.6952, loss_ll_paf=23.1935, loss_ll_heat=6.19699, q=1000
[2018-07-10 04:09:05,206] [train] [INFO] epoch=3.00 step=27400, 10.8566 examples/sec lr=0.000100, loss=67.1437, loss_ll=12.068, loss_ll_paf=19.2599, loss_ll_heat=4.87612, q=1000
[2018-07-10 04:11:28,786] [train] [INFO] epoch=3.00 step=27500, 10.8577 examples/sec lr=0.000100, loss=54.768, loss_ll=9.81693, loss_ll_paf=15.2178, loss_ll_heat=4.41606, q=1000
[2018-07-10 04:13:53,271] [train] [INFO] epoch=3.00 step=27600, 10.8584 examples/sec lr=0.000100, loss=80.0067, loss_ll=14.963, loss_ll_paf=22.9982, loss_ll_heat=6.92785, q=1000
[2018-07-10 04:16:15,365] [train] [INFO] epoch=3.00 step=27700, 10.8598 examples/sec lr=0.000100, loss=73.1094, loss_ll=13.0856, loss_ll_paf=19.7811, loss_ll_heat=6.39007, q=1000
[2018-07-10 04:18:41,905] [train] [INFO] epoch=3.00 step=27800, 10.8600 examples/sec lr=0.000100, loss=43.2818, loss_ll=7.6544, loss_ll_paf=11.167, loss_ll_heat=4.14185, q=1000
[2018-07-10 04:21:07,268] [train] [INFO] epoch=3.00 step=27900, 10.8606 examples/sec lr=0.000100, loss=81.203, loss_ll=15.2371, loss_ll_paf=23.3097, loss_ll_heat=7.1644, q=1000
[2018-07-10 04:23:30,263] [train] [INFO] epoch=3.00 step=28000, 10.8617 examples/sec lr=0.000100, loss=51.8061, loss_ll=9.31212, loss_ll_paf=13.1516, loss_ll_heat=5.4726, q=1000
[2018-07-10 04:26:04,386] [train] [INFO] epoch=3.00 step=28100, 10.8599 examples/sec lr=0.000100, loss=68.8219, loss_ll=12.2787, loss_ll_paf=18.2842, loss_ll_heat=6.27321, q=1000
[2018-07-10 04:28:28,442] [train] [INFO] epoch=3.00 step=28200, 10.8608 examples/sec lr=0.000100, loss=95.0399, loss_ll=16.9842, loss_ll_paf=27.5333, loss_ll_heat=6.43506, q=1000
[2018-07-10 04:30:53,118] [train] [INFO] epoch=3.00 step=28300, 10.8615 examples/sec lr=0.000100, loss=61.8354, loss_ll=11.4635, loss_ll_paf=18.2105, loss_ll_heat=4.71649, q=1000
[2018-07-10 04:33:19,435] [train] [INFO] epoch=3.00 step=28400, 10.8617 examples/sec lr=0.000100, loss=57.9519, loss_ll=10.4645, loss_ll_paf=16.2259, loss_ll_heat=4.70302, q=1000
[2018-07-10 04:35:43,157] [train] [INFO] epoch=3.00 step=28500, 10.8626 examples/sec lr=0.000100, loss=47.3517, loss_ll=8.59951, loss_ll_paf=12.8717, loss_ll_heat=4.32732, q=1000
[2018-07-10 04:38:06,958] [train] [INFO] epoch=3.00 step=28600, 10.8635 examples/sec lr=0.000100, loss=88.1019, loss_ll=16.5442, loss_ll_paf=25.8184, loss_ll_heat=7.26993, q=1000
[2018-07-10 04:40:29,724] [train] [INFO] epoch=3.00 step=28700, 10.8647 examples/sec lr=0.000100, loss=53.6063, loss_ll=9.98851, loss_ll_paf=14.9212, loss_ll_heat=5.05582, q=1000
[2018-07-10 04:42:56,539] [train] [INFO] epoch=3.00 step=28800, 10.8648 examples/sec lr=0.000100, loss=51.2965, loss_ll=8.9029, loss_ll_paf=13.7855, loss_ll_heat=4.02026, q=1000
[2018-07-10 04:45:20,195] [train] [INFO] epoch=3.00 step=28900, 10.8657 examples/sec lr=0.000100, loss=41.3932, loss_ll=7.11972, loss_ll_paf=9.93911, loss_ll_heat=4.30033, q=1000
[2018-07-10 04:47:43,989] [train] [INFO] epoch=3.00 step=29000, 10.8666 examples/sec lr=0.000100, loss=49.2546, loss_ll=8.71453, loss_ll_paf=13.5816, loss_ll_heat=3.84749, q=1000
[2018-07-10 04:50:23,108] [train] [INFO] epoch=3.00 step=29100, 10.8636 examples/sec lr=0.000100, loss=60.7424, loss_ll=11.2176, loss_ll_paf=17.7413, loss_ll_heat=4.69387, q=1000
[2018-07-10 04:52:47,710] [train] [INFO] epoch=3.00 step=29200, 10.8643 examples/sec lr=0.000100, loss=60.9119, loss_ll=10.6497, loss_ll_paf=16.8852, loss_ll_heat=4.41418, q=1000
[2018-07-10 04:55:14,360] [train] [INFO] epoch=3.00 step=29300, 10.8644 examples/sec lr=0.000100, loss=80.291, loss_ll=14.1543, loss_ll_paf=22.9519, loss_ll_heat=5.35681, q=1000
[2018-07-10 04:57:42,524] [train] [INFO] epoch=3.00 step=29400, 10.8642 examples/sec lr=0.000100, loss=68.6404, loss_ll=12.7294, loss_ll_paf=18.8082, loss_ll_heat=6.65074, q=1000
[2018-07-10 05:00:09,307] [train] [INFO] epoch=3.00 step=29500, 10.8643 examples/sec lr=0.000100, loss=47.2617, loss_ll=8.34947, loss_ll_paf=12.7126, loss_ll_heat=3.98636, q=1000
[2018-07-10 05:02:36,613] [train] [INFO] epoch=3.00 step=29600, 10.8643 examples/sec lr=0.000100, loss=49.5507, loss_ll=8.31141, loss_ll_paf=12.4528, loss_ll_heat=4.17003, q=1000
[2018-07-10 05:05:01,767] [train] [INFO] epoch=3.00 step=29700, 10.8649 examples/sec lr=0.000100, loss=46.9274, loss_ll=8.62798, loss_ll_paf=12.7969, loss_ll_heat=4.45903, q=1000
[2018-07-10 05:07:29,542] [train] [INFO] epoch=3.00 step=29800, 10.8647 examples/sec lr=0.000100, loss=57.6674, loss_ll=10.3362, loss_ll_paf=16.0422, loss_ll_heat=4.63023, q=1000
[2018-07-10 05:09:55,270] [train] [INFO] epoch=3.00 step=29900, 10.8651 examples/sec lr=0.000100, loss=57.4259, loss_ll=10.2995, loss_ll_paf=16.4245, loss_ll_heat=4.17454, q=1000
[2018-07-10 05:12:20,191] [train] [INFO] epoch=3.00 step=30000, 10.8657 examples/sec lr=0.000033, loss=42.2611, loss_ll=7.22049, loss_ll_paf=10.2186, loss_ll_heat=4.22239, q=1000
[2018-07-10 05:14:56,982] [train] [INFO] epoch=3.00 step=30100, 10.8633 examples/sec lr=0.000033, loss=57.1689, loss_ll=10.2881, loss_ll_paf=15.356, loss_ll_heat=5.22018, q=1000
[2018-07-10 05:17:25,647] [train] [INFO] epoch=3.00 step=30200, 10.8630 examples/sec lr=0.000033, loss=57.4025, loss_ll=10.3287, loss_ll_paf=15.5343, loss_ll_heat=5.12318, q=1000
[2018-07-10 05:19:50,638] [train] [INFO] epoch=3.00 step=30300, 10.8636 examples/sec lr=0.000033, loss=64.9455, loss_ll=11.6805, loss_ll_paf=17.9482, loss_ll_heat=5.41283, q=1000
[2018-07-10 05:22:14,815] [train] [INFO] epoch=3.00 step=30400, 10.8643 examples/sec lr=0.000033, loss=51.9365, loss_ll=9.51703, loss_ll_paf=15.0348, loss_ll_heat=3.99929, q=1000
[2018-07-10 05:24:38,043] [train] [INFO] epoch=4.00 step=30500, 10.8653 examples/sec lr=0.000033, loss=42.5715, loss_ll=7.6005, loss_ll_paf=11.3225, loss_ll_heat=3.87854, q=1000
[2018-07-10 05:27:05,404] [train] [INFO] epoch=4.00 step=30600, 10.8653 examples/sec lr=0.000033, loss=59.8779, loss_ll=10.9825, loss_ll_paf=17.5023, loss_ll_heat=4.46273, q=1000
[2018-07-10 05:29:32,627] [train] [INFO] epoch=4.00 step=30700, 10.8653 examples/sec lr=0.000033, loss=55.3167, loss_ll=10.0648, loss_ll_paf=16.3099, loss_ll_heat=3.81972, q=1000
[2018-07-10 05:31:57,421] [train] [INFO] epoch=4.00 step=30800, 10.8659 examples/sec lr=0.000033, loss=49.7949, loss_ll=8.95552, loss_ll_paf=13.3428, loss_ll_heat=4.56821, q=1000
[2018-07-10 05:34:21,847] [train] [INFO] epoch=4.00 step=30900, 10.8665 examples/sec lr=0.000033, loss=53.2986, loss_ll=9.30823, loss_ll_paf=13.9848, loss_ll_heat=4.6317, q=1000
[2018-07-10 05:36:47,423] [train] [INFO] epoch=4.00 step=31000, 10.8669 examples/sec lr=0.000033, loss=43.9891, loss_ll=7.38868, loss_ll_paf=10.3161, loss_ll_heat=4.46129, q=1000
[2018-07-10 05:39:27,996] [train] [INFO] epoch=4.00 step=31100, 10.8638 examples/sec lr=0.000033, loss=66.1544, loss_ll=12.4672, loss_ll_paf=19.9205, loss_ll_heat=5.01377, q=1000
[2018-07-10 05:41:53,392] [train] [INFO] epoch=4.00 step=31200, 10.8642 examples/sec lr=0.000033, loss=57.9115, loss_ll=10.2997, loss_ll_paf=15.4007, loss_ll_heat=5.19871, q=1000
[2018-07-10 05:44:18,158] [train] [INFO] epoch=4.00 step=31300, 10.8648 examples/sec lr=0.000033, loss=63.3685, loss_ll=11.9889, loss_ll_paf=18.8042, loss_ll_heat=5.17368, q=1000
[2018-07-10 05:46:46,340] [train] [INFO] epoch=4.00 step=31400, 10.8646 examples/sec lr=0.000033, loss=50.6103, loss_ll=9.48106, loss_ll_paf=14.3931, loss_ll_heat=4.56903, q=1000
[2018-07-10 05:49:15,867] [train] [INFO] epoch=4.00 step=31500, 10.8641 examples/sec lr=0.000033, loss=32.328, loss_ll=5.66107, loss_ll_paf=8.1758, loss_ll_heat=3.14634, q=1000
[2018-07-10 05:51:41,700] [train] [INFO] epoch=4.00 step=31600, 10.8644 examples/sec lr=0.000033, loss=64.145, loss_ll=11.5753, loss_ll_paf=17.6078, loss_ll_heat=5.54272, q=1000
[2018-07-10 05:54:05,431] [train] [INFO] epoch=4.00 step=31700, 10.8652 examples/sec lr=0.000033, loss=80.4704, loss_ll=14.7557, loss_ll_paf=23.8214, loss_ll_heat=5.69013, q=1000
[2018-07-10 05:56:31,402] [train] [INFO] epoch=4.00 step=31800, 10.8655 examples/sec lr=0.000033, loss=46.8257, loss_ll=8.12432, loss_ll_paf=11.798, loss_ll_heat=4.45063, q=1000
[2018-07-10 05:58:58,298] [train] [INFO] epoch=4.00 step=31900, 10.8656 examples/sec lr=0.000033, loss=72.1691, loss_ll=12.9949, loss_ll_paf=20.9604, loss_ll_heat=5.02949, q=1000
[2018-07-10 06:01:24,245] [train] [INFO] epoch=4.00 step=32000, 10.8659 examples/sec lr=0.000033, loss=64.861, loss_ll=12.7414, loss_ll_paf=20.8431, loss_ll_heat=4.63978, q=1000
[2018-07-10 06:04:01,344] [train] [INFO] epoch=4.00 step=32100, 10.8636 examples/sec lr=0.000033, loss=51.9889, loss_ll=9.71326, loss_ll_paf=14.74, loss_ll_heat=4.68647, q=1000
[2018-07-10 06:06:27,655] [train] [INFO] epoch=4.00 step=32200, 10.8639 examples/sec lr=0.000033, loss=49.8469, loss_ll=9.64961, loss_ll_paf=14.3204, loss_ll_heat=4.9788, q=1000
[2018-07-10 06:08:55,202] [train] [INFO] epoch=4.00 step=32300, 10.8638 examples/sec lr=0.000033, loss=54.2955, loss_ll=10.3287, loss_ll_paf=15.8837, loss_ll_heat=4.77376, q=1000
[2018-07-10 06:11:21,108] [train] [INFO] epoch=4.00 step=32400, 10.8641 examples/sec lr=0.000033, loss=47.5432, loss_ll=8.94274, loss_ll_paf=13.27, loss_ll_heat=4.61551, q=1000
[2018-07-10 06:13:46,375] [train] [INFO] epoch=4.00 step=32500, 10.8646 examples/sec lr=0.000033, loss=61.7082, loss_ll=11.0228, loss_ll_paf=17.1174, loss_ll_heat=4.92825, q=1000
[2018-07-10 06:16:12,099] [train] [INFO] epoch=4.00 step=32600, 10.8649 examples/sec lr=0.000033, loss=49.6313, loss_ll=9.19839, loss_ll_paf=14.0086, loss_ll_heat=4.3882, q=1000
[2018-07-10 06:18:37,937] [train] [INFO] epoch=4.00 step=32700, 10.8652 examples/sec lr=0.000033, loss=72.6524, loss_ll=13.6039, loss_ll_paf=20.8762, loss_ll_heat=6.33159, q=1000
[2018-07-10 06:21:06,132] [train] [INFO] epoch=4.00 step=32800, 10.8650 examples/sec lr=0.000033, loss=70.7006, loss_ll=13.001, loss_ll_paf=20.4617, loss_ll_heat=5.54017, q=1000
[2018-07-10 06:23:34,753] [train] [INFO] epoch=4.00 step=32900, 10.8647 examples/sec lr=0.000033, loss=58.5756, loss_ll=10.0215, loss_ll_paf=14.7454, loss_ll_heat=5.29766, q=1000
[2018-07-10 06:26:00,247] [train] [INFO] epoch=4.00 step=33000, 10.8651 examples/sec lr=0.000033, loss=64.7931, loss_ll=11.8258, loss_ll_paf=18.1993, loss_ll_heat=5.45231, q=1000
[2018-07-10 06:28:39,374] [train] [INFO] epoch=4.00 step=33100, 10.8625 examples/sec lr=0.000033, loss=61.6801, loss_ll=11.4083, loss_ll_paf=17.6243, loss_ll_heat=5.19231, q=1000
[2018-07-10 06:31:10,090] [train] [INFO] epoch=4.00 step=33200, 10.8617 examples/sec lr=0.000033, loss=71.5116, loss_ll=13.8305, loss_ll_paf=21.4361, loss_ll_heat=6.22492, q=1000
[2018-07-10 06:33:35,756] [train] [INFO] epoch=4.00 step=33300, 10.8621 examples/sec lr=0.000033, loss=46.575, loss_ll=8.29776, loss_ll_paf=12.7153, loss_ll_heat=3.88019, q=1000
[2018-07-10 06:36:02,032] [train] [INFO] epoch=4.00 step=33400, 10.8623 examples/sec lr=0.000033, loss=53.1685, loss_ll=9.44151, loss_ll_paf=14.8404, loss_ll_heat=4.04261, q=1000
[2018-07-10 06:38:28,639] [train] [INFO] epoch=4.00 step=33500, 10.8625 examples/sec lr=0.000033, loss=64.5091, loss_ll=11.95, loss_ll_paf=19.3676, loss_ll_heat=4.53233, q=1000
[2018-07-10 06:40:57,140] [train] [INFO] epoch=4.00 step=33600, 10.8622 examples/sec lr=0.000033, loss=47.6056, loss_ll=8.70417, loss_ll_paf=12.2436, loss_ll_heat=5.1647, q=1000
[2018-07-10 06:43:23,303] [train] [INFO] epoch=4.00 step=33700, 10.8625 examples/sec lr=0.000033, loss=88.4814, loss_ll=16.8878, loss_ll_paf=28.063, loss_ll_heat=5.71262, q=1000
[2018-07-10 06:45:51,968] [train] [INFO] epoch=4.00 step=33800, 10.8622 examples/sec lr=0.000033, loss=32.2338, loss_ll=5.73725, loss_ll_paf=8.29882, loss_ll_heat=3.17568, q=1000
[2018-07-10 06:48:19,605] [train] [INFO] epoch=4.00 step=33900, 10.8621 examples/sec lr=0.000033, loss=30.6762, loss_ll=5.40583, loss_ll_paf=7.39319, loss_ll_heat=3.41847, q=1000
[2018-07-10 06:50:47,900] [train] [INFO] epoch=4.00 step=34000, 10.8619 examples/sec lr=0.000033, loss=27.7867, loss_ll=5.06781, loss_ll_paf=6.51774, loss_ll_heat=3.61789, q=1000
[2018-07-10 06:53:24,548] [train] [INFO] epoch=4.00 step=34100, 10.8598 examples/sec lr=0.000033, loss=52.5181, loss_ll=9.60265, loss_ll_paf=14.7449, loss_ll_heat=4.4604, q=1000
[2018-07-10 06:55:53,661] [train] [INFO] epoch=4.00 step=34200, 10.8595 examples/sec lr=0.000033, loss=52.1529, loss_ll=9.29408, loss_ll_paf=14.2272, loss_ll_heat=4.36098, q=1000
[2018-07-10 06:58:17,874] [train] [INFO] epoch=4.00 step=34300, 10.8601 examples/sec lr=0.000033, loss=71.3103, loss_ll=13.0995, loss_ll_paf=20.6062, loss_ll_heat=5.59287, q=1000
[2018-07-10 07:00:41,195] [train] [INFO] epoch=4.00 step=34400, 10.8610 examples/sec lr=0.000033, loss=48.7444, loss_ll=8.64391, loss_ll_paf=13.3071, loss_ll_heat=3.98071, q=1000
[2018-07-10 07:03:09,576] [train] [INFO] epoch=4.00 step=34500, 10.8608 examples/sec lr=0.000033, loss=60.246, loss_ll=10.9848, loss_ll_paf=17.4339, loss_ll_heat=4.53573, q=1000
[2018-07-10 07:05:36,472] [train] [INFO] epoch=4.00 step=34600, 10.8609 examples/sec lr=0.000033, loss=42.791, loss_ll=7.69533, loss_ll_paf=11.2292, loss_ll_heat=4.16148, q=1000
[2018-07-10 07:08:05,331] [train] [INFO] epoch=4.00 step=34700, 10.8605 examples/sec lr=0.000033, loss=55.324, loss_ll=10.665, loss_ll_paf=16.184, loss_ll_heat=5.14608, q=1000
[2018-07-10 07:10:35,321] [train] [INFO] epoch=4.00 step=34800, 10.8600 examples/sec lr=0.000033, loss=38.4059, loss_ll=6.38763, loss_ll_paf=9.52966, loss_ll_heat=3.24559, q=1000
[2018-07-10 07:13:01,058] [train] [INFO] epoch=4.00 step=34900, 10.8603 examples/sec lr=0.000033, loss=49.0003, loss_ll=8.84528, loss_ll_paf=12.35, loss_ll_heat=5.34056, q=1000
[2018-07-10 07:15:26,072] [train] [INFO] epoch=4.00 step=35000, 10.8608 examples/sec lr=0.000033, loss=45.8321, loss_ll=8.27286, loss_ll_paf=12.4061, loss_ll_heat=4.13962, q=1000
[2018-07-10 07:18:10,750] [train] [INFO] epoch=4.00 step=35100, 10.8571 examples/sec lr=0.000033, loss=52.3821, loss_ll=9.96583, loss_ll_paf=14.8281, loss_ll_heat=5.1036, q=1000
[2018-07-10 07:20:39,590] [train] [INFO] epoch=4.00 step=35200, 10.8568 examples/sec lr=0.000033, loss=62.3085, loss_ll=11.5567, loss_ll_paf=19.0444, loss_ll_heat=4.06901, q=1000
[2018-07-10 07:23:07,642] [train] [INFO] epoch=4.00 step=35300, 10.8567 examples/sec lr=0.000033, loss=67.0576, loss_ll=12.0311, loss_ll_paf=18.8115, loss_ll_heat=5.25069, q=1000
[2018-07-10 07:25:34,847] [train] [INFO] epoch=4.00 step=35400, 10.8567 examples/sec lr=0.000033, loss=59.4909, loss_ll=11.1988, loss_ll_paf=17.5768, loss_ll_heat=4.82089, q=1000
[2018-07-10 07:27:59,983] [train] [INFO] epoch=4.00 step=35500, 10.8572 examples/sec lr=0.000033, loss=34.4554, loss_ll=5.87427, loss_ll_paf=8.71658, loss_ll_heat=3.03195, q=1000
[2018-07-10 07:30:25,366] [train] [INFO] epoch=4.00 step=35600, 10.8576 examples/sec lr=0.000033, loss=43.8321, loss_ll=7.45533, loss_ll_paf=11.1531, loss_ll_heat=3.75754, q=1000
[2018-07-10 07:32:55,175] [train] [INFO] epoch=4.00 step=35700, 10.8571 examples/sec lr=0.000033, loss=42.9967, loss_ll=7.64291, loss_ll_paf=11.9584, loss_ll_heat=3.32742, q=1000
[2018-07-10 07:35:20,083] [train] [INFO] epoch=4.00 step=35800, 10.8576 examples/sec lr=0.000033, loss=44.873, loss_ll=8.15022, loss_ll_paf=12.4022, loss_ll_heat=3.89827, q=1000
[2018-07-10 07:37:48,946] [train] [INFO] epoch=4.00 step=35900, 10.8573 examples/sec lr=0.000033, loss=41.0973, loss_ll=7.56393, loss_ll_paf=11.6094, loss_ll_heat=3.51847, q=1000
[2018-07-10 07:40:14,338] [train] [INFO] epoch=4.00 step=36000, 10.8577 examples/sec lr=0.000033, loss=72.1244, loss_ll=12.9601, loss_ll_paf=19.338, loss_ll_heat=6.58225, q=1000
[2018-07-10 07:42:55,010] [train] [INFO] epoch=4.00 step=36100, 10.8550 examples/sec lr=0.000033, loss=43.6176, loss_ll=7.89953, loss_ll_paf=11.8874, loss_ll_heat=3.91163, q=1000
[2018-07-10 07:45:21,478] [train] [INFO] epoch=4.00 step=36200, 10.8552 examples/sec lr=0.000033, loss=52.7628, loss_ll=9.02075, loss_ll_paf=13.9533, loss_ll_heat=4.08817, q=1000
[2018-07-10 07:47:47,663] [train] [INFO] epoch=4.00 step=36300, 10.8554 examples/sec lr=0.000033, loss=45.9596, loss_ll=8.73272, loss_ll_paf=12.8011, loss_ll_heat=4.66437, q=1000
[2018-07-10 07:50:16,341] [train] [INFO] epoch=4.00 step=36400, 10.8552 examples/sec lr=0.000033, loss=35.8668, loss_ll=6.22712, loss_ll_paf=8.70792, loss_ll_heat=3.74631, q=1000
[2018-07-10 07:52:41,948] [train] [INFO] epoch=4.00 step=36500, 10.8555 examples/sec lr=0.000033, loss=38.4281, loss_ll=6.9682, loss_ll_paf=10.053, loss_ll_heat=3.8834, q=1000
[2018-07-10 07:55:09,464] [train] [INFO] epoch=4.00 step=36600, 10.8555 examples/sec lr=0.000033, loss=68.8502, loss_ll=12.768, loss_ll_paf=19.6156, loss_ll_heat=5.92048, q=1000
[2018-07-10 07:57:36,744] [train] [INFO] epoch=4.00 step=36700, 10.8555 examples/sec lr=0.000033, loss=29.8984, loss_ll=5.11587, loss_ll_paf=7.32334, loss_ll_heat=2.9084, q=1000
[2018-07-10 08:00:06,029] [train] [INFO] epoch=4.00 step=36800, 10.8551 examples/sec lr=0.000033, loss=49.9848, loss_ll=8.72614, loss_ll_paf=13.6421, loss_ll_heat=3.81023, q=1000
[2018-07-10 08:02:33,459] [train] [INFO] epoch=4.00 step=36900, 10.8551 examples/sec lr=0.000033, loss=47.3521, loss_ll=8.17865, loss_ll_paf=11.8043, loss_ll_heat=4.553, q=1000
[2018-07-10 08:05:01,319] [train] [INFO] epoch=4.00 step=37000, 10.8550 examples/sec lr=0.000033, loss=41.8257, loss_ll=7.76985, loss_ll_paf=11.1708, loss_ll_heat=4.36889, q=1000
[2018-07-10 08:07:40,541] [train] [INFO] epoch=4.00 step=37100, 10.8527 examples/sec lr=0.000033, loss=54.0349, loss_ll=9.60923, loss_ll_paf=15.2843, loss_ll_heat=3.93415, q=1000
[2018-07-10 08:10:07,158] [train] [INFO] epoch=4.00 step=37200, 10.8528 examples/sec lr=0.000033, loss=69.9311, loss_ll=12.7199, loss_ll_paf=20.4049, loss_ll_heat=5.03485, q=1000
[2018-07-10 08:12:35,672] [train] [INFO] epoch=4.00 step=37300, 10.8526 examples/sec lr=0.000033, loss=70.4369, loss_ll=12.5353, loss_ll_paf=20.0205, loss_ll_heat=5.05002, q=1000
[2018-07-10 08:15:02,184] [train] [INFO] epoch=4.00 step=37400, 10.8528 examples/sec lr=0.000033, loss=58.1709, loss_ll=10.9498, loss_ll_paf=16.9768, loss_ll_heat=4.9229, q=1000
[2018-07-10 08:17:28,154] [train] [INFO] epoch=4.00 step=37500, 10.8531 examples/sec lr=0.000033, loss=62.8196, loss_ll=11.9491, loss_ll_paf=18.7067, loss_ll_heat=5.19145, q=1000
[2018-07-10 08:19:55,327] [train] [INFO] epoch=4.00 step=37600, 10.8531 examples/sec lr=0.000033, loss=49.4011, loss_ll=8.63267, loss_ll_paf=12.7341, loss_ll_heat=4.53128, q=1000
[2018-07-10 08:22:22,280] [train] [INFO] epoch=4.00 step=37700, 10.8532 examples/sec lr=0.000033, loss=42.2926, loss_ll=7.78585, loss_ll_paf=11.5614, loss_ll_heat=4.01024, q=1000
[2018-07-10 08:24:52,631] [train] [INFO] epoch=4.00 step=37800, 10.8527 examples/sec lr=0.000033, loss=50.7866, loss_ll=9.03445, loss_ll_paf=13.2739, loss_ll_heat=4.79502, q=1000
[2018-07-10 08:27:20,228] [train] [INFO] epoch=4.00 step=37900, 10.8526 examples/sec lr=0.000033, loss=41.4601, loss_ll=7.74856, loss_ll_paf=11.818, loss_ll_heat=3.67912, q=1000
[2018-07-10 08:29:47,983] [train] [INFO] epoch=4.00 step=38000, 10.8526 examples/sec lr=0.000033, loss=53.7418, loss_ll=9.36559, loss_ll_paf=14.1624, loss_ll_heat=4.5688, q=1000
[2018-07-10 08:32:27,283] [train] [INFO] epoch=5.00 step=38100, 10.8503 examples/sec lr=0.000033, loss=50.9141, loss_ll=9.36762, loss_ll_paf=13.5973, loss_ll_heat=5.13795, q=1000
[2018-07-10 08:34:55,904] [train] [INFO] epoch=5.00 step=38200, 10.8501 examples/sec lr=0.000033, loss=42.4429, loss_ll=7.34558, loss_ll_paf=10.6092, loss_ll_heat=4.08192, q=1000
[2018-07-10 08:37:23,752] [train] [INFO] epoch=5.00 step=38300, 10.8500 examples/sec lr=0.000033, loss=37.6606, loss_ll=6.52759, loss_ll_paf=9.79528, loss_ll_heat=3.2599, q=1000
[2018-07-10 08:39:52,896] [train] [INFO] epoch=5.00 step=38400, 10.8497 examples/sec lr=0.000033, loss=33.5843, loss_ll=5.73954, loss_ll_paf=8.07196, loss_ll_heat=3.40712, q=1000
[2018-07-10 08:42:22,758] [train] [INFO] epoch=5.00 step=38500, 10.8492 examples/sec lr=0.000033, loss=43.1244, loss_ll=8.0982, loss_ll_paf=11.7602, loss_ll_heat=4.4362, q=1000
[2018-07-10 08:44:49,401] [train] [INFO] epoch=5.00 step=38600, 10.8494 examples/sec lr=0.000033, loss=47.1181, loss_ll=8.67712, loss_ll_paf=13.1212, loss_ll_heat=4.23307, q=1000
[2018-07-10 08:47:17,033] [train] [INFO] epoch=5.00 step=38700, 10.8493 examples/sec lr=0.000033, loss=50.1181, loss_ll=8.7703, loss_ll_paf=14.0521, loss_ll_heat=3.48854, q=1000
[2018-07-10 08:49:43,591] [train] [INFO] epoch=5.00 step=38800, 10.8495 examples/sec lr=0.000033, loss=36.6019, loss_ll=6.67443, loss_ll_paf=9.85538, loss_ll_heat=3.49347, q=1000
[2018-07-10 08:52:13,104] [train] [INFO] epoch=5.00 step=38900, 10.8491 examples/sec lr=0.000033, loss=37.5319, loss_ll=6.66221, loss_ll_paf=9.69332, loss_ll_heat=3.6311, q=1000
[2018-07-10 08:54:42,682] [train] [INFO] epoch=5.00 step=39000, 10.8487 examples/sec lr=0.000033, loss=61.5491, loss_ll=11.048, loss_ll_paf=18.2201, loss_ll_heat=3.87582, q=1000
[2018-07-10 08:57:25,530] [train] [INFO] epoch=5.00 step=39100, 10.8458 examples/sec lr=0.000033, loss=39.2033, loss_ll=7.04036, loss_ll_paf=10.2758, loss_ll_heat=3.80496, q=1000
[2018-07-10 08:59:57,276] [train] [INFO] epoch=5.00 step=39200, 10.8450 examples/sec lr=0.000033, loss=36.685, loss_ll=6.77779, loss_ll_paf=9.12026, loss_ll_heat=4.43532, q=1000
[2018-07-10 09:02:24,908] [train] [INFO] epoch=5.00 step=39300, 10.8450 examples/sec lr=0.000033, loss=47.6282, loss_ll=8.60316, loss_ll_paf=12.7402, loss_ll_heat=4.46616, q=1000
[2018-07-10 09:04:53,221] [train] [INFO] epoch=5.00 step=39400, 10.8449 examples/sec lr=0.000033, loss=60.3394, loss_ll=10.4881, loss_ll_paf=16.3258, loss_ll_heat=4.65034, q=1000
[2018-07-10 09:07:23,529] [train] [INFO] epoch=5.00 step=39500, 10.8444 examples/sec lr=0.000033, loss=49.543, loss_ll=8.95732, loss_ll_paf=12.7532, loss_ll_heat=5.16142, q=1000
[2018-07-10 09:09:50,831] [train] [INFO] epoch=5.00 step=39600, 10.8444 examples/sec lr=0.000033, loss=44.9953, loss_ll=8.19839, loss_ll_paf=11.5511, loss_ll_heat=4.84572, q=1000
[2018-07-10 09:12:19,697] [train] [INFO] epoch=5.00 step=39700, 10.8442 examples/sec lr=0.000033, loss=39.0318, loss_ll=7.08143, loss_ll_paf=11.0252, loss_ll_heat=3.13763, q=1000
[2018-07-10 09:14:48,395] [train] [INFO] epoch=5.00 step=39800, 10.8440 examples/sec lr=0.000033, loss=39.9419, loss_ll=7.01583, loss_ll_paf=10.559, loss_ll_heat=3.47265, q=1000
[2018-07-10 09:17:16,603] [train] [INFO] epoch=5.00 step=39900, 10.8438 examples/sec lr=0.000033, loss=40.4267, loss_ll=6.89586, loss_ll_paf=10.5487, loss_ll_heat=3.24298, q=1000
[2018-07-10 09:19:44,739] [train] [INFO] epoch=5.00 step=40000, 10.8437 examples/sec lr=0.000033, loss=51.3016, loss_ll=9.38381, loss_ll_paf=13.6126, loss_ll_heat=5.15504, q=1000
[2018-07-10 09:22:26,921] [train] [INFO] epoch=5.00 step=40100, 10.8410 examples/sec lr=0.000033, loss=42.5546, loss_ll=7.51735, loss_ll_paf=10.8626, loss_ll_heat=4.17209, q=1000
[2018-07-10 09:24:53,438] [train] [INFO] epoch=5.00 step=40200, 10.8412 examples/sec lr=0.000033, loss=55.3744, loss_ll=10.4564, loss_ll_paf=16.0623, loss_ll_heat=4.85037, q=1000
[2018-07-10 09:27:20,081] [train] [INFO] epoch=5.00 step=40300, 10.8414 examples/sec lr=0.000033, loss=35.5997, loss_ll=6.40601, loss_ll_paf=9.2255, loss_ll_heat=3.58653, q=1000
[2018-07-10 09:29:48,858] [train] [INFO] epoch=5.00 step=40400, 10.8412 examples/sec lr=0.000033, loss=37.8447, loss_ll=7.04473, loss_ll_paf=10.0223, loss_ll_heat=4.06714, q=1000
[2018-07-10 09:32:15,826] [train] [INFO] epoch=5.00 step=40500, 10.8413 examples/sec lr=0.000033, loss=39.8968, loss_ll=6.68062, loss_ll_paf=9.56182, loss_ll_heat=3.79943, q=1000
[2018-07-10 09:34:42,896] [train] [INFO] epoch=5.00 step=40600, 10.8414 examples/sec lr=0.000033, loss=45.6823, loss_ll=8.3315, loss_ll_paf=12.746, loss_ll_heat=3.91699, q=1000
[2018-07-10 09:37:10,647] [train] [INFO] epoch=5.00 step=40700, 10.8414 examples/sec lr=0.000033, loss=62.4534, loss_ll=11.0724, loss_ll_paf=18.1258, loss_ll_heat=4.01905, q=1000
[2018-07-10 09:39:40,343] [train] [INFO] epoch=5.00 step=40800, 10.8410 examples/sec lr=0.000033, loss=37.7163, loss_ll=7.01764, loss_ll_paf=11.1983, loss_ll_heat=2.83697, q=1000
[2018-07-10 09:42:09,146] [train] [INFO] epoch=5.00 step=40900, 10.8408 examples/sec lr=0.000033, loss=46.2703, loss_ll=8.28845, loss_ll_paf=12.048, loss_ll_heat=4.5289, q=1000
[2018-07-10 09:44:40,813] [train] [INFO] epoch=5.00 step=41000, 10.8400 examples/sec lr=0.000033, loss=48.0993, loss_ll=8.82841, loss_ll_paf=12.5001, loss_ll_heat=5.15674, q=1000
[2018-07-10 09:47:22,320] [train] [INFO] epoch=5.00 step=41100, 10.8376 examples/sec lr=0.000033, loss=39.6923, loss_ll=7.34974, loss_ll_paf=10.6284, loss_ll_heat=4.07105, q=1000
[2018-07-10 09:49:51,753] [train] [INFO] epoch=5.00 step=41200, 10.8372 examples/sec lr=0.000033, loss=38.1727, loss_ll=6.55898, loss_ll_paf=9.8431, loss_ll_heat=3.27486, q=1000
[2018-07-10 09:52:21,833] [train] [INFO] epoch=5.00 step=41300, 10.8368 examples/sec lr=0.000033, loss=41.1368, loss_ll=7.5696, loss_ll_paf=11.4523, loss_ll_heat=3.68695, q=1000
[2018-07-10 09:54:50,160] [train] [INFO] epoch=5.00 step=41400, 10.8367 examples/sec lr=0.000033, loss=47.8394, loss_ll=8.86939, loss_ll_paf=13.59, loss_ll_heat=4.14879, q=1000
[2018-07-10 09:57:18,987] [train] [INFO] epoch=5.00 step=41500, 10.8365 examples/sec lr=0.000033, loss=48.9545, loss_ll=8.45388, loss_ll_paf=12.6294, loss_ll_heat=4.27832, q=1000
[2018-07-10 09:59:47,973] [train] [INFO] epoch=5.00 step=41600, 10.8362 examples/sec lr=0.000033, loss=55.9963, loss_ll=10.1286, loss_ll_paf=15.6566, loss_ll_heat=4.60064, q=1000
[2018-07-10 10:02:17,322] [train] [INFO] epoch=5.00 step=41700, 10.8359 examples/sec lr=0.000033, loss=40.7305, loss_ll=7.59608, loss_ll_paf=10.8521, loss_ll_heat=4.34007, q=1000
[2018-07-10 10:04:45,205] [train] [INFO] epoch=5.00 step=41800, 10.8359 examples/sec lr=0.000033, loss=27.9278, loss_ll=4.71855, loss_ll_paf=6.50628, loss_ll_heat=2.93082, q=1000
[2018-07-10 10:07:14,150] [train] [INFO] epoch=5.00 step=41900, 10.8357 examples/sec lr=0.000033, loss=40.1685, loss_ll=7.25128, loss_ll_paf=10.4926, loss_ll_heat=4.00999, q=1000
[2018-07-10 10:09:41,348] [train] [INFO] epoch=5.00 step=42000, 10.8357 examples/sec lr=0.000033, loss=47.1522, loss_ll=8.69015, loss_ll_paf=12.9413, loss_ll_heat=4.43903, q=1000
[2018-07-10 10:12:20,266] [train] [INFO] epoch=5.00 step=42100, 10.8338 examples/sec lr=0.000033, loss=61.4497, loss_ll=10.9667, loss_ll_paf=17.8455, loss_ll_heat=4.08795, q=1000
[2018-07-10 10:14:45,656] [train] [INFO] epoch=5.00 step=42200, 10.8342 examples/sec lr=0.000033, loss=52.3986, loss_ll=9.30834, loss_ll_paf=13.7891, loss_ll_heat=4.82762, q=1000
[2018-07-10 10:17:13,506] [train] [INFO] epoch=5.00 step=42300, 10.8342 examples/sec lr=0.000033, loss=34.4821, loss_ll=6.02528, loss_ll_paf=9.12316, loss_ll_heat=2.92739, q=1000
[2018-07-10 10:19:40,389] [train] [INFO] epoch=5.00 step=42400, 10.8343 examples/sec lr=0.000033, loss=52.4688, loss_ll=10.0956, loss_ll_paf=16.0213, loss_ll_heat=4.16988, q=1000
[2018-07-10 10:22:08,756] [train] [INFO] epoch=5.00 step=42500, 10.8342 examples/sec lr=0.000033, loss=37.2361, loss_ll=6.65763, loss_ll_paf=9.44304, loss_ll_heat=3.87222, q=1000
[2018-07-10 10:24:36,341] [train] [INFO] epoch=5.00 step=42600, 10.8342 examples/sec lr=0.000033, loss=39.7561, loss_ll=6.83804, loss_ll_paf=9.95531, loss_ll_heat=3.72077, q=1000
[2018-07-10 10:27:04,107] [train] [INFO] epoch=5.00 step=42700, 10.8342 examples/sec lr=0.000033, loss=49.6232, loss_ll=8.74283, loss_ll_paf=13.559, loss_ll_heat=3.92664, q=1000
[2018-07-10 10:29:30,685] [train] [INFO] epoch=5.00 step=42800, 10.8344 examples/sec lr=0.000033, loss=35.7145, loss_ll=5.75698, loss_ll_paf=8.45279, loss_ll_heat=3.06116, q=1000
[2018-07-10 10:31:59,489] [train] [INFO] epoch=5.00 step=42900, 10.8342 examples/sec lr=0.000033, loss=39.8377, loss_ll=7.36048, loss_ll_paf=10.3755, loss_ll_heat=4.34544, q=1000
[2018-07-10 10:34:28,676] [train] [INFO] epoch=5.00 step=43000, 10.8339 examples/sec lr=0.000033, loss=43.5773, loss_ll=7.48278, loss_ll_paf=10.8646, loss_ll_heat=4.10099, q=1000
[2018-07-10 10:37:10,918] [train] [INFO] epoch=5.00 step=43100, 10.8314 examples/sec lr=0.000033, loss=48.5583, loss_ll=9.07745, loss_ll_paf=14.3747, loss_ll_heat=3.78018, q=1000
[2018-07-10 10:39:38,346] [train] [INFO] epoch=5.00 step=43200, 10.8315 examples/sec lr=0.000033, loss=43.1558, loss_ll=7.51685, loss_ll_paf=11.7682, loss_ll_heat=3.26553, q=1000
[2018-07-10 10:42:04,879] [train] [INFO] epoch=5.00 step=43300, 10.8317 examples/sec lr=0.000033, loss=38.7016, loss_ll=6.67807, loss_ll_paf=9.87683, loss_ll_heat=3.47931, q=1000
[2018-07-10 10:44:35,351] [train] [INFO] epoch=5.00 step=43400, 10.8312 examples/sec lr=0.000033, loss=48.6787, loss_ll=8.81813, loss_ll_paf=13.8555, loss_ll_heat=3.78071, q=1000
[2018-07-10 10:47:06,854] [train] [INFO] epoch=5.00 step=43500, 10.8306 examples/sec lr=0.000033, loss=36.4513, loss_ll=6.49672, loss_ll_paf=8.69644, loss_ll_heat=4.297, q=1000
[2018-07-10 10:49:37,185] [train] [INFO] epoch=5.00 step=43600, 10.8302 examples/sec lr=0.000033, loss=60.5557, loss_ll=10.7416, loss_ll_paf=17.2136, loss_ll_heat=4.26965, q=1000
[2018-07-10 10:52:04,534] [train] [INFO] epoch=5.00 step=43700, 10.8302 examples/sec lr=0.000033, loss=27.0726, loss_ll=4.35055, loss_ll_paf=6.29313, loss_ll_heat=2.40796, q=1000
[2018-07-10 10:54:33,551] [train] [INFO] epoch=5.00 step=43800, 10.8300 examples/sec lr=0.000033, loss=51.3521, loss_ll=9.13509, loss_ll_paf=14.2888, loss_ll_heat=3.98137, q=1000
[2018-07-10 10:57:03,266] [train] [INFO] epoch=5.00 step=43900, 10.8297 examples/sec lr=0.000033, loss=31.5996, loss_ll=5.37162, loss_ll_paf=7.13393, loss_ll_heat=3.60931, q=1000
[2018-07-10 10:59:29,283] [train] [INFO] epoch=5.00 step=44000, 10.8300 examples/sec lr=0.000033, loss=41.4511, loss_ll=7.33353, loss_ll_paf=10.561, loss_ll_heat=4.10609, q=1000
[2018-07-10 11:02:08,630] [train] [INFO] epoch=5.00 step=44100, 10.8280 examples/sec lr=0.000033, loss=34.2977, loss_ll=5.69797, loss_ll_paf=7.69554, loss_ll_heat=3.70039, q=1000
[2018-07-10 11:04:40,455] [train] [INFO] epoch=5.00 step=44200, 10.8274 examples/sec lr=0.000033, loss=41.7211, loss_ll=7.61688, loss_ll_paf=11.6847, loss_ll_heat=3.54908, q=1000
[2018-07-10 11:07:08,079] [train] [INFO] epoch=5.00 step=44300, 10.8274 examples/sec lr=0.000033, loss=41.9393, loss_ll=7.41101, loss_ll_paf=11.4292, loss_ll_heat=3.39281, q=1000
[2018-07-10 11:09:39,071] [train] [INFO] epoch=5.00 step=44400, 10.8268 examples/sec lr=0.000033, loss=35.8956, loss_ll=6.16295, loss_ll_paf=8.92371, loss_ll_heat=3.40218, q=1000
[2018-07-10 11:12:07,428] [train] [INFO] epoch=5.00 step=44500, 10.8268 examples/sec lr=0.000033, loss=32.0843, loss_ll=5.64187, loss_ll_paf=7.91472, loss_ll_heat=3.36902, q=1000
[2018-07-10 11:14:35,560] [train] [INFO] epoch=5.00 step=44600, 10.8267 examples/sec lr=0.000033, loss=47.75, loss_ll=8.84681, loss_ll_paf=13.3273, loss_ll_heat=4.36629, q=1000
[2018-07-10 11:17:07,657] [train] [INFO] epoch=5.00 step=44700, 10.8260 examples/sec lr=0.000033, loss=53.9484, loss_ll=10.5135, loss_ll_paf=16.3907, loss_ll_heat=4.63627, q=1000
[2018-07-10 11:19:33,989] [train] [INFO] epoch=5.00 step=44800, 10.8262 examples/sec lr=0.000033, loss=57.8015, loss_ll=10.3943, loss_ll_paf=16.5108, loss_ll_heat=4.27778, q=1000
[2018-07-10 11:22:01,090] [train] [INFO] epoch=5.00 step=44900, 10.8263 examples/sec lr=0.000033, loss=55.7949, loss_ll=10.2233, loss_ll_paf=15.4274, loss_ll_heat=5.01927, q=1000
[2018-07-10 11:24:30,301] [train] [INFO] epoch=5.00 step=45000, 10.8261 examples/sec lr=0.000033, loss=38.6999, loss_ll=6.78805, loss_ll_paf=10.0716, loss_ll_heat=3.50446, q=1000
[2018-07-10 11:27:10,317] [train] [INFO] epoch=5.00 step=45100, 10.8241 examples/sec lr=0.000033, loss=52.2413, loss_ll=10.1958, loss_ll_paf=15.0982, loss_ll_heat=5.29342, q=1000
[2018-07-10 11:29:38,578] [train] [INFO] epoch=5.00 step=45200, 10.8241 examples/sec lr=0.000033, loss=41.491, loss_ll=7.90064, loss_ll_paf=11.4018, loss_ll_heat=4.39943, q=1000
[2018-07-10 11:32:08,350] [train] [INFO] epoch=5.00 step=45300, 10.8237 examples/sec lr=0.000033, loss=53.0386, loss_ll=9.80928, loss_ll_paf=15.9545, loss_ll_heat=3.66406, q=1000
[2018-07-10 11:34:36,947] [train] [INFO] epoch=5.00 step=45400, 10.8236 examples/sec lr=0.000033, loss=37.6854, loss_ll=6.3408, loss_ll_paf=8.86677, loss_ll_heat=3.81483, q=1000
[2018-07-10 11:37:06,974] [train] [INFO] epoch=5.00 step=45500, 10.8233 examples/sec lr=0.000033, loss=36.9626, loss_ll=6.57116, loss_ll_paf=10.5726, loss_ll_heat=2.56968, q=1000
[2018-07-10 11:39:37,392] [train] [INFO] epoch=5.00 step=45600, 10.8228 examples/sec lr=0.000033, loss=49.0943, loss_ll=9.69787, loss_ll_paf=15.4975, loss_ll_heat=3.8982, q=1000
[2018-07-10 11:42:09,702] [train] [INFO] epoch=6.00 step=45700, 10.8221 examples/sec lr=0.000033, loss=54.3038, loss_ll=9.91963, loss_ll_paf=16.0829, loss_ll_heat=3.75632, q=1000
[2018-07-10 11:44:38,816] [train] [INFO] epoch=6.00 step=45800, 10.8219 examples/sec lr=0.000033, loss=39.4593, loss_ll=7.10545, loss_ll_paf=10.8473, loss_ll_heat=3.3636, q=1000
[2018-07-10 11:47:07,595] [train] [INFO] epoch=6.00 step=45900, 10.8218 examples/sec lr=0.000033, loss=45.5131, loss_ll=8.01515, loss_ll_paf=12.6157, loss_ll_heat=3.41461, q=1000
[2018-07-10 11:49:35,777] [train] [INFO] epoch=6.00 step=46000, 10.8217 examples/sec lr=0.000033, loss=55.7043, loss_ll=9.93592, loss_ll_paf=15.5629, loss_ll_heat=4.30899, q=1000
[2018-07-10 11:52:20,049] [train] [INFO] epoch=6.00 step=46100, 10.8191 examples/sec lr=0.000033, loss=60.4996, loss_ll=10.8801, loss_ll_paf=17.754, loss_ll_heat=4.00631, q=1000
[2018-07-10 11:54:48,564] [train] [INFO] epoch=6.00 step=46200, 10.8190 examples/sec lr=0.000033, loss=36.8789, loss_ll=6.9506, loss_ll_paf=10.616, loss_ll_heat=3.28523, q=1000
[2018-07-10 11:57:16,826] [train] [INFO] epoch=6.00 step=46300, 10.8190 examples/sec lr=0.000033, loss=51.9058, loss_ll=9.61957, loss_ll_paf=15.0273, loss_ll_heat=4.2118, q=1000
[2018-07-10 11:59:45,912] [train] [INFO] epoch=6.00 step=46400, 10.8188 examples/sec lr=0.000033, loss=39.5925, loss_ll=7.24234, loss_ll_paf=10.5006, loss_ll_heat=3.98405, q=1000
[2018-07-10 12:02:14,345] [train] [INFO] epoch=6.00 step=46500, 10.8187 examples/sec lr=0.000033, loss=50.5682, loss_ll=9.18654, loss_ll_paf=15.0493, loss_ll_heat=3.32382, q=1000
[2018-07-10 12:04:41,167] [train] [INFO] epoch=6.00 step=46600, 10.8188 examples/sec lr=0.000033, loss=33.8115, loss_ll=6.02823, loss_ll_paf=8.87816, loss_ll_heat=3.17829, q=1000
[2018-07-10 12:07:13,951] [train] [INFO] epoch=6.00 step=46700, 10.8181 examples/sec lr=0.000033, loss=54.3664, loss_ll=9.74469, loss_ll_paf=14.8979, loss_ll_heat=4.59152, q=1000
[2018-07-10 12:09:44,969] [train] [INFO] epoch=6.00 step=46800, 10.8176 examples/sec lr=0.000033, loss=37.0813, loss_ll=6.37115, loss_ll_paf=9.61344, loss_ll_heat=3.12887, q=1000
[2018-07-10 12:12:11,816] [train] [INFO] epoch=6.00 step=46900, 10.8178 examples/sec lr=0.000033, loss=38.9295, loss_ll=6.83089, loss_ll_paf=10.302, loss_ll_heat=3.35982, q=1000
[2018-07-10 12:14:40,596] [train] [INFO] epoch=6.00 step=47000, 10.8176 examples/sec lr=0.000033, loss=48.5558, loss_ll=8.73372, loss_ll_paf=13.587, loss_ll_heat=3.8805, q=1000
[2018-07-10 12:17:21,825] [train] [INFO] epoch=6.00 step=47100, 10.8156 examples/sec lr=0.000033, loss=29.0355, loss_ll=5.37447, loss_ll_paf=7.948, loss_ll_heat=2.80095, q=1000
[2018-07-10 12:19:51,908] [train] [INFO] epoch=6.00 step=47200, 10.8152 examples/sec lr=0.000033, loss=38.1605, loss_ll=6.85766, loss_ll_paf=9.80707, loss_ll_heat=3.90825, q=1000
[2018-07-10 12:22:20,042] [train] [INFO] epoch=6.00 step=47300, 10.8152 examples/sec lr=0.000033, loss=38.3393, loss_ll=6.57234, loss_ll_paf=9.64674, loss_ll_heat=3.49793, q=1000
[2018-07-10 12:24:49,390] [train] [INFO] epoch=6.00 step=47400, 10.8150 examples/sec lr=0.000033, loss=44.7338, loss_ll=7.86693, loss_ll_paf=11.8354, loss_ll_heat=3.89846, q=1000
[2018-07-10 12:27:19,024] [train] [INFO] epoch=6.00 step=47500, 10.8147 examples/sec lr=0.000033, loss=41.8907, loss_ll=6.93232, loss_ll_paf=10.9002, loss_ll_heat=2.96445, q=1000
[2018-07-10 12:29:47,518] [train] [INFO] epoch=6.00 step=47600, 10.8146 examples/sec lr=0.000033, loss=34.5773, loss_ll=6.07942, loss_ll_paf=7.74696, loss_ll_heat=4.41188, q=1000
[2018-07-10 12:32:14,976] [train] [INFO] epoch=6.00 step=47700, 10.8147 examples/sec lr=0.000033, loss=38.226, loss_ll=6.79718, loss_ll_paf=10.0021, loss_ll_heat=3.59222, q=1000
[2018-07-10 12:34:42,701] [train] [INFO] epoch=6.00 step=47800, 10.8147 examples/sec lr=0.000033, loss=47.2658, loss_ll=8.74525, loss_ll_paf=14.1428, loss_ll_heat=3.34767, q=1000
[2018-07-10 12:37:11,026] [train] [INFO] epoch=6.00 step=47900, 10.8147 examples/sec lr=0.000033, loss=34.4821, loss_ll=6.36964, loss_ll_paf=9.36186, loss_ll_heat=3.37742, q=1000
[2018-07-10 12:39:37,470] [train] [INFO] epoch=6.00 step=48000, 10.8149 examples/sec lr=0.000033, loss=36.6784, loss_ll=6.14938, loss_ll_paf=8.82026, loss_ll_heat=3.4785, q=1000
[2018-07-10 12:42:17,135] [train] [INFO] epoch=6.00 step=48100, 10.8131 examples/sec lr=0.000033, loss=47.2083, loss_ll=8.45477, loss_ll_paf=13.5434, loss_ll_heat=3.36611, q=1000
[2018-07-10 12:44:44,996] [train] [INFO] epoch=6.00 step=48200, 10.8131 examples/sec lr=0.000033, loss=56.4046, loss_ll=10.8886, loss_ll_paf=17.7622, loss_ll_heat=4.0149, q=1000
[2018-07-10 12:47:11,978] [train] [INFO] epoch=6.00 step=48300, 10.8133 examples/sec lr=0.000033, loss=40.6957, loss_ll=7.47977, loss_ll_paf=11.7106, loss_ll_heat=3.24896, q=1000
[2018-07-10 12:49:39,410] [train] [INFO] epoch=6.00 step=48400, 10.8134 examples/sec lr=0.000033, loss=35.0112, loss_ll=6.21304, loss_ll_paf=8.94974, loss_ll_heat=3.47635, q=1000
[2018-07-10 12:52:10,985] [train] [INFO] epoch=6.00 step=48500, 10.8128 examples/sec lr=0.000033, loss=27.9487, loss_ll=4.65795, loss_ll_paf=6.37418, loss_ll_heat=2.94173, q=1000
[2018-07-10 12:54:38,042] [train] [INFO] epoch=6.00 step=48600, 10.8130 examples/sec lr=0.000033, loss=54.2576, loss_ll=9.44211, loss_ll_paf=15.3338, loss_ll_heat=3.55044, q=1000
[2018-07-10 12:57:05,651] [train] [INFO] epoch=6.00 step=48700, 10.8130 examples/sec lr=0.000033, loss=47.8091, loss_ll=9.01151, loss_ll_paf=13.2477, loss_ll_heat=4.77529, q=1000
[2018-07-10 12:59:35,390] [train] [INFO] epoch=6.00 step=48800, 10.8128 examples/sec lr=0.000033, loss=39.4762, loss_ll=6.50651, loss_ll_paf=9.73887, loss_ll_heat=3.27415, q=1000
[2018-07-10 13:02:05,919] [train] [INFO] epoch=6.00 step=48900, 10.8124 examples/sec lr=0.000033, loss=31.9798, loss_ll=5.70228, loss_ll_paf=8.40485, loss_ll_heat=2.99971, q=1000
[2018-07-10 13:04:34,355] [train] [INFO] epoch=6.00 step=49000, 10.8123 examples/sec lr=0.000033, loss=54.9574, loss_ll=10.1724, loss_ll_paf=15.4711, loss_ll_heat=4.87373, q=1000
[2018-07-10 13:07:14,642] [train] [INFO] epoch=6.00 step=49100, 10.8105 examples/sec lr=0.000033, loss=39.7742, loss_ll=7.16737, loss_ll_paf=10.248, loss_ll_heat=4.08679, q=1000
[2018-07-10 13:09:41,406] [train] [INFO] epoch=6.00 step=49200, 10.8107 examples/sec lr=0.000033, loss=40.6041, loss_ll=7.5388, loss_ll_paf=10.8032, loss_ll_heat=4.27444, q=1000
[2018-07-10 13:12:11,406] [train] [INFO] epoch=6.00 step=49300, 10.8104 examples/sec lr=0.000033, loss=53.3187, loss_ll=9.46929, loss_ll_paf=14.9919, loss_ll_heat=3.94671, q=1000
[2018-07-10 13:14:41,176] [train] [INFO] epoch=6.00 step=49400, 10.8101 examples/sec lr=0.000033, loss=40.1645, loss_ll=7.033, loss_ll_paf=10.6333, loss_ll_heat=3.43266, q=1000
[2018-07-10 13:17:08,680] [train] [INFO] epoch=6.00 step=49500, 10.8102 examples/sec lr=0.000033, loss=51.1432, loss_ll=9.24897, loss_ll_paf=13.6412, loss_ll_heat=4.85674, q=1000
[2018-07-10 13:19:37,339] [train] [INFO] epoch=6.00 step=49600, 10.8101 examples/sec lr=0.000033, loss=33.2845, loss_ll=5.82565, loss_ll_paf=8.28144, loss_ll_heat=3.36986, q=1000
[2018-07-10 13:22:03,744] [train] [INFO] epoch=6.00 step=49700, 10.8103 examples/sec lr=0.000033, loss=41.5581, loss_ll=6.98319, loss_ll_paf=9.86012, loss_ll_heat=4.10627, q=1000
[2018-07-10 13:24:32,405] [train] [INFO] epoch=6.00 step=49800, 10.8102 examples/sec lr=0.000033, loss=43.8901, loss_ll=7.70613, loss_ll_paf=11.4166, loss_ll_heat=3.99561, q=1000
[2018-07-10 13:27:01,217] [train] [INFO] epoch=6.00 step=49900, 10.8101 examples/sec lr=0.000033, loss=44.5241, loss_ll=7.89195, loss_ll_paf=11.7856, loss_ll_heat=3.99832, q=1000
[2018-07-10 13:29:32,571] [train] [INFO] epoch=6.00 step=50000, 10.8096 examples/sec lr=0.000033, loss=51.5539, loss_ll=9.33304, loss_ll_paf=14.3068, loss_ll_heat=4.35926, q=1000
[2018-07-10 13:32:11,125] [train] [INFO] epoch=6.00 step=50100, 10.8081 examples/sec lr=0.000033, loss=40.0679, loss_ll=6.67447, loss_ll_paf=10.2848, loss_ll_heat=3.06413, q=1000
[2018-07-10 13:34:38,244] [train] [INFO] epoch=6.00 step=50200, 10.8082 examples/sec lr=0.000033, loss=36.3579, loss_ll=6.79451, loss_ll_paf=9.63897, loss_ll_heat=3.95006, q=1000
[2018-07-10 13:37:07,948] [train] [INFO] epoch=6.00 step=50300, 10.8080 examples/sec lr=0.000033, loss=39.7481, loss_ll=6.90522, loss_ll_paf=10.8377, loss_ll_heat=2.97275, q=1000
[2018-07-10 13:39:36,023] [train] [INFO] epoch=6.00 step=50400, 10.8080 examples/sec lr=0.000033, loss=41.4293, loss_ll=6.89754, loss_ll_paf=9.96279, loss_ll_heat=3.83229, q=1000
[2018-07-10 13:42:05,088] [train] [INFO] epoch=6.00 step=50500, 10.8078 examples/sec lr=0.000033, loss=43.3707, loss_ll=7.95495, loss_ll_paf=12.4065, loss_ll_heat=3.50341, q=1000
[2018-07-10 13:44:35,820] [train] [INFO] epoch=6.00 step=50600, 10.8074 examples/sec lr=0.000033, loss=35.2879, loss_ll=6.36818, loss_ll_paf=9.71458, loss_ll_heat=3.02178, q=1000
[2018-07-10 13:47:04,082] [train] [INFO] epoch=6.00 step=50700, 10.8074 examples/sec lr=0.000033, loss=65.8947, loss_ll=12.6945, loss_ll_paf=19.8278, loss_ll_heat=5.56127, q=1000
[2018-07-10 13:49:33,007] [train] [INFO] epoch=6.00 step=50800, 10.8073 examples/sec lr=0.000033, loss=31.3745, loss_ll=5.6007, loss_ll_paf=7.76643, loss_ll_heat=3.43496, q=1000
[2018-07-10 13:52:05,941] [train] [INFO] epoch=6.00 step=50900, 10.8066 examples/sec lr=0.000033, loss=54.7824, loss_ll=10.2939, loss_ll_paf=16.7991, loss_ll_heat=3.78864, q=1000
[2018-07-10 13:54:36,473] [train] [INFO] epoch=6.00 step=51000, 10.8062 examples/sec lr=0.000033, loss=35.8483, loss_ll=6.4929, loss_ll_paf=10.4858, loss_ll_heat=2.49997, q=1000
[2018-07-10 13:57:18,492] [train] [INFO] epoch=6.00 step=51100, 10.8042 examples/sec lr=0.000033, loss=55.3516, loss_ll=10.4615, loss_ll_paf=16.8369, loss_ll_heat=4.08601, q=1000
[2018-07-10 13:59:50,643] [train] [INFO] epoch=6.00 step=51200, 10.8036 examples/sec lr=0.000033, loss=43.5811, loss_ll=8.33156, loss_ll_paf=13.0379, loss_ll_heat=3.62526, q=1000
[2018-07-10 14:02:22,435] [train] [INFO] epoch=6.00 step=51300, 10.8031 examples/sec lr=0.000033, loss=39.2714, loss_ll=6.79272, loss_ll_paf=9.66355, loss_ll_heat=3.92189, q=1000
[2018-07-10 14:04:51,194] [train] [INFO] epoch=6.00 step=51400, 10.8030 examples/sec lr=0.000033, loss=33.0327, loss_ll=5.4265, loss_ll_paf=7.66379, loss_ll_heat=3.1892, q=1000
[2018-07-10 14:07:19,006] [train] [INFO] epoch=6.00 step=51500, 10.8031 examples/sec lr=0.000033, loss=33.2469, loss_ll=5.99552, loss_ll_paf=8.7274, loss_ll_heat=3.26365, q=1000
[2018-07-10 14:09:45,142] [train] [INFO] epoch=6.00 step=51600, 10.8034 examples/sec lr=0.000033, loss=41.0406, loss_ll=6.998, loss_ll_paf=10.8799, loss_ll_heat=3.11609, q=1000
[2018-07-10 14:12:13,546] [train] [INFO] epoch=6.00 step=51700, 10.8033 examples/sec lr=0.000033, loss=47.3064, loss_ll=8.74058, loss_ll_paf=13.1108, loss_ll_heat=4.37032, q=1000
[2018-07-10 14:14:40,247] [train] [INFO] epoch=6.00 step=51800, 10.8035 examples/sec lr=0.000033, loss=58.9402, loss_ll=10.8523, loss_ll_paf=16.6297, loss_ll_heat=5.075, q=1000
[2018-07-10 14:17:12,506] [train] [INFO] epoch=6.00 step=51900, 10.8029 examples/sec lr=0.000033, loss=53.2013, loss_ll=9.71237, loss_ll_paf=14.6727, loss_ll_heat=4.75199, q=1000
[2018-07-10 14:19:41,578] [train] [INFO] epoch=6.00 step=52000, 10.8028 examples/sec lr=0.000033, loss=54.2598, loss_ll=10.1828, loss_ll_paf=15.8641, loss_ll_heat=4.50148, q=1000
[2018-07-10 14:22:24,034] [train] [INFO] epoch=6.00 step=52100, 10.8008 examples/sec lr=0.000033, loss=36.8765, loss_ll=6.3304, loss_ll_paf=8.99729, loss_ll_heat=3.66351, q=1000
[2018-07-10 14:24:53,045] [train] [INFO] epoch=6.00 step=52200, 10.8007 examples/sec lr=0.000033, loss=34.3329, loss_ll=6.11142, loss_ll_paf=8.42524, loss_ll_heat=3.79759, q=1000
[2018-07-10 14:27:21,282] [train] [INFO] epoch=6.00 step=52300, 10.8006 examples/sec lr=0.000033, loss=26.0428, loss_ll=4.43515, loss_ll_paf=5.95037, loss_ll_heat=2.91994, q=1000
[2018-07-10 14:29:49,084] [train] [INFO] epoch=6.00 step=52400, 10.8007 examples/sec lr=0.000033, loss=44.945, loss_ll=7.9037, loss_ll_paf=11.4237, loss_ll_heat=4.38366, q=1000
[2018-07-10 14:32:20,350] [train] [INFO] epoch=6.00 step=52500, 10.8003 examples/sec lr=0.000033, loss=41.3764, loss_ll=7.91515, loss_ll_paf=12.2957, loss_ll_heat=3.53458, q=1000
[2018-07-10 14:34:52,395] [train] [INFO] epoch=6.00 step=52600, 10.7997 examples/sec lr=0.000033, loss=39.6576, loss_ll=7.44574, loss_ll_paf=11.7741, loss_ll_heat=3.11735, q=1000
[2018-07-10 14:37:22,850] [train] [INFO] epoch=6.00 step=52700, 10.7994 examples/sec lr=0.000033, loss=43.7466, loss_ll=7.81142, loss_ll_paf=12.1957, loss_ll_heat=3.42715, q=1000
[2018-07-10 14:39:57,000] [train] [INFO] epoch=6.00 step=52800, 10.7986 examples/sec lr=0.000033, loss=46.5592, loss_ll=8.51724, loss_ll_paf=12.7874, loss_ll_heat=4.24703, q=1000
[2018-07-10 14:42:26,097] [train] [INFO] epoch=6.00 step=52900, 10.7984 examples/sec lr=0.000033, loss=40.0062, loss_ll=6.85198, loss_ll_paf=10.8345, loss_ll_heat=2.86947, q=1000
[2018-07-10 14:44:58,404] [train] [INFO] epoch=6.00 step=53000, 10.7979 examples/sec lr=0.000033, loss=37.1702, loss_ll=6.90233, loss_ll_paf=10.6662, loss_ll_heat=3.13849, q=1000
[2018-07-10 14:47:41,074] [train] [INFO] epoch=6.00 step=53100, 10.7959 examples/sec lr=0.000033, loss=32.7093, loss_ll=5.52485, loss_ll_paf=7.61374, loss_ll_heat=3.43596, q=1000
[2018-07-10 14:50:12,786] [train] [INFO] epoch=6.00 step=53200, 10.7954 examples/sec lr=0.000033, loss=53.3062, loss_ll=10.1898, loss_ll_paf=16.1424, loss_ll_heat=4.23727, q=1000
[2018-07-10 14:52:44,655] [train] [INFO] epoch=7.00 step=53300, 10.7949 examples/sec lr=0.000033, loss=34.7873, loss_ll=6.499, loss_ll_paf=9.5059, loss_ll_heat=3.4921, q=1000
[2018-07-10 14:55:12,604] [train] [INFO] epoch=7.00 step=53400, 10.7949 examples/sec lr=0.000033, loss=36.3332, loss_ll=6.3151, loss_ll_paf=8.46377, loss_ll_heat=4.16643, q=1000
[2018-07-10 14:57:40,705] [train] [INFO] epoch=7.00 step=53500, 10.7950 examples/sec lr=0.000033, loss=36.3413, loss_ll=5.97807, loss_ll_paf=8.80762, loss_ll_heat=3.14851, q=1000
[2018-07-10 15:00:10,660] [train] [INFO] epoch=7.00 step=53600, 10.7947 examples/sec lr=0.000033, loss=35.6756, loss_ll=6.44646, loss_ll_paf=9.47113, loss_ll_heat=3.42179, q=1000
[2018-07-10 15:02:41,900] [train] [INFO] epoch=7.00 step=53700, 10.7943 examples/sec lr=0.000033, loss=44.0938, loss_ll=7.87006, loss_ll_paf=11.879, loss_ll_heat=3.86116, q=1000
[2018-07-10 15:05:08,910] [train] [INFO] epoch=7.00 step=53800, 10.7945 examples/sec lr=0.000033, loss=58.2834, loss_ll=10.8909, loss_ll_paf=17.094, loss_ll_heat=4.68784, q=1000
[2018-07-10 15:07:38,630] [train] [INFO] epoch=7.00 step=53900, 10.7943 examples/sec lr=0.000033, loss=44.0883, loss_ll=7.55398, loss_ll_paf=11.934, loss_ll_heat=3.17395, q=1000
[2018-07-10 15:10:08,745] [train] [INFO] epoch=7.00 step=54000, 10.7940 examples/sec lr=0.000033, loss=35.6007, loss_ll=6.52912, loss_ll_paf=10.3726, loss_ll_heat=2.68563, q=1000
[2018-07-10 15:12:47,512] [train] [INFO] epoch=7.00 step=54100, 10.7926 examples/sec lr=0.000033, loss=43.8924, loss_ll=7.76108, loss_ll_paf=12.3832, loss_ll_heat=3.139, q=1000
[2018-07-10 15:15:17,531] [train] [INFO] epoch=7.00 step=54200, 10.7924 examples/sec lr=0.000033, loss=40.328, loss_ll=7.2409, loss_ll_paf=10.3085, loss_ll_heat=4.17328, q=1000
[2018-07-10 15:17:49,434] [train] [INFO] epoch=7.00 step=54300, 10.7919 examples/sec lr=0.000033, loss=41.3959, loss_ll=7.51774, loss_ll_paf=11.9878, loss_ll_heat=3.04763, q=1000
[2018-07-10 15:20:17,885] [train] [INFO] epoch=7.00 step=54400, 10.7918 examples/sec lr=0.000033, loss=56.0053, loss_ll=10.482, loss_ll_paf=16.5161, loss_ll_heat=4.4479, q=1000
[2018-07-10 15:22:47,351] [train] [INFO] epoch=7.00 step=54500, 10.7917 examples/sec lr=0.000033, loss=38.3331, loss_ll=6.71482, loss_ll_paf=9.78099, loss_ll_heat=3.64865, q=1000
[2018-07-10 15:25:15,953] [train] [INFO] epoch=7.00 step=54600, 10.7916 examples/sec lr=0.000033, loss=49.1947, loss_ll=8.49273, loss_ll_paf=13.4975, loss_ll_heat=3.48791, q=1000
[2018-07-10 15:27:45,305] [train] [INFO] epoch=7.00 step=54700, 10.7915 examples/sec lr=0.000033, loss=41.8736, loss_ll=7.87807, loss_ll_paf=11.9683, loss_ll_heat=3.78779, q=1000
[2018-07-10 15:30:14,804] [train] [INFO] epoch=7.00 step=54800, 10.7913 examples/sec lr=0.000033, loss=46.5322, loss_ll=8.24498, loss_ll_paf=12.2915, loss_ll_heat=4.19849, q=1000
[2018-07-10 15:32:45,224] [train] [INFO] epoch=7.00 step=54900, 10.7910 examples/sec lr=0.000033, loss=41.887, loss_ll=7.18833, loss_ll_paf=11.1947, loss_ll_heat=3.18194, q=1000
[2018-07-10 15:35:15,183] [train] [INFO] epoch=7.00 step=55000, 10.7908 examples/sec lr=0.000033, loss=38.467, loss_ll=6.30171, loss_ll_paf=9.57012, loss_ll_heat=3.0333, q=1000
[2018-07-10 15:37:58,911] [train] [INFO] epoch=7.00 step=55100, 10.7888 examples/sec lr=0.000033, loss=49.3602, loss_ll=9.08435, loss_ll_paf=13.7992, loss_ll_heat=4.36951, q=1000
[2018-07-10 15:40:24,713] [train] [INFO] epoch=7.00 step=55200, 10.7891 examples/sec lr=0.000033, loss=29.5111, loss_ll=5.08328, loss_ll_paf=7.14932, loss_ll_heat=3.01723, q=1000
[2018-07-10 15:42:57,189] [train] [INFO] epoch=7.00 step=55300, 10.7886 examples/sec lr=0.000033, loss=26.6816, loss_ll=4.55855, loss_ll_paf=6.4244, loss_ll_heat=2.6927, q=1000
[2018-07-10 15:45:27,498] [train] [INFO] epoch=7.00 step=55400, 10.7883 examples/sec lr=0.000033, loss=45.0131, loss_ll=7.8399, loss_ll_paf=11.5549, loss_ll_heat=4.12494, q=1000
[2018-07-10 15:47:59,203] [train] [INFO] epoch=7.00 step=55500, 10.7879 examples/sec lr=0.000033, loss=58.4741, loss_ll=10.4284, loss_ll_paf=17.3191, loss_ll_heat=3.53761, q=1000
[2018-07-10 15:50:28,298] [train] [INFO] epoch=7.00 step=55600, 10.7878 examples/sec lr=0.000033, loss=48.8168, loss_ll=9.22119, loss_ll_paf=14.7632, loss_ll_heat=3.67914, q=1000
[2018-07-10 15:52:58,455] [train] [INFO] epoch=7.00 step=55700, 10.7875 examples/sec lr=0.000033, loss=51.5086, loss_ll=9.41111, loss_ll_paf=14.2403, loss_ll_heat=4.58189, q=1000
[2018-07-10 15:55:27,866] [train] [INFO] epoch=7.00 step=55800, 10.7874 examples/sec lr=0.000033, loss=35.0104, loss_ll=6.11787, loss_ll_paf=9.25023, loss_ll_heat=2.9855, q=1000
[2018-07-10 15:57:58,095] [train] [INFO] epoch=7.00 step=55900, 10.7871 examples/sec lr=0.000033, loss=38.9614, loss_ll=6.96035, loss_ll_paf=10.6996, loss_ll_heat=3.2211, q=1000
[2018-07-10 16:00:29,395] [train] [INFO] epoch=7.00 step=56000, 10.7867 examples/sec lr=0.000033, loss=37.2739, loss_ll=6.71531, loss_ll_paf=10.37, loss_ll_heat=3.06062, q=1000
[2018-07-10 16:03:10,662] [train] [INFO] epoch=7.00 step=56100, 10.7851 examples/sec lr=0.000033, loss=31.384, loss_ll=5.2856, loss_ll_paf=7.27415, loss_ll_heat=3.29706, q=1000
[2018-07-10 16:05:40,732] [train] [INFO] epoch=7.00 step=56200, 10.7848 examples/sec lr=0.000033, loss=46.0975, loss_ll=8.30626, loss_ll_paf=12.5494, loss_ll_heat=4.06317, q=1000
[2018-07-10 16:08:08,050] [train] [INFO] epoch=7.00 step=56300, 10.7850 examples/sec lr=0.000033, loss=46.531, loss_ll=8.74113, loss_ll_paf=13.6633, loss_ll_heat=3.819, q=1000
[2018-07-10 16:10:35,811] [train] [INFO] epoch=7.00 step=56400, 10.7850 examples/sec lr=0.000033, loss=35.9611, loss_ll=6.2405, loss_ll_paf=9.02944, loss_ll_heat=3.45155, q=1000
[2018-07-10 16:13:05,296] [train] [INFO] epoch=7.00 step=56500, 10.7849 examples/sec lr=0.000033, loss=32.1302, loss_ll=5.832, loss_ll_paf=8.91448, loss_ll_heat=2.74952, q=1000
[2018-07-10 16:15:34,407] [train] [INFO] epoch=7.00 step=56600, 10.7848 examples/sec lr=0.000033, loss=49.659, loss_ll=8.78474, loss_ll_paf=12.9549, loss_ll_heat=4.61461, q=1000
[2018-07-10 16:18:04,168] [train] [INFO] epoch=7.00 step=56700, 10.7846 examples/sec lr=0.000033, loss=50.9814, loss_ll=9.56523, loss_ll_paf=15.6671, loss_ll_heat=3.46336, q=1000
[2018-07-10 16:20:32,003] [train] [INFO] epoch=7.00 step=56800, 10.7847 examples/sec lr=0.000033, loss=43.3726, loss_ll=7.85068, loss_ll_paf=11.7865, loss_ll_heat=3.91489, q=1000
[2018-07-10 16:23:00,997] [train] [INFO] epoch=7.00 step=56900, 10.7846 examples/sec lr=0.000033, loss=40.207, loss_ll=7.42388, loss_ll_paf=11.335, loss_ll_heat=3.51273, q=1000
[2018-07-10 16:25:30,712] [train] [INFO] epoch=7.00 step=57000, 10.7844 examples/sec lr=0.000033, loss=43.4558, loss_ll=7.76446, loss_ll_paf=12.182, loss_ll_heat=3.34697, q=1000
[2018-07-10 16:28:13,535] [train] [INFO] epoch=7.00 step=57100, 10.7826 examples/sec lr=0.000033, loss=38.8171, loss_ll=6.78129, loss_ll_paf=10.5052, loss_ll_heat=3.05733, q=1000
[2018-07-10 16:30:45,856] [train] [INFO] epoch=7.00 step=57200, 10.7821 examples/sec lr=0.000033, loss=36.6347, loss_ll=6.43846, loss_ll_paf=9.55217, loss_ll_heat=3.32475, q=1000
[2018-07-10 16:33:15,692] [train] [INFO] epoch=7.00 step=57300, 10.7819 examples/sec lr=0.000033, loss=42.8875, loss_ll=7.83453, loss_ll_paf=11.8119, loss_ll_heat=3.85719, q=1000
[2018-07-10 16:35:46,464] [train] [INFO] epoch=7.00 step=57400, 10.7816 examples/sec lr=0.000033, loss=33.68, loss_ll=5.5709, loss_ll_paf=7.70633, loss_ll_heat=3.43547, q=1000
[2018-07-10 16:38:18,545] [train] [INFO] epoch=7.00 step=57500, 10.7811 examples/sec lr=0.000033, loss=54.2178, loss_ll=10.1533, loss_ll_paf=16.2454, loss_ll_heat=4.06111, q=1000
[2018-07-10 16:40:48,975] [train] [INFO] epoch=7.00 step=57600, 10.7809 examples/sec lr=0.000033, loss=52.4536, loss_ll=9.57018, loss_ll_paf=14.717, loss_ll_heat=4.42338, q=1000
[2018-07-10 16:43:21,427] [train] [INFO] epoch=7.00 step=57700, 10.7804 examples/sec lr=0.000033, loss=51.4186, loss_ll=9.32295, loss_ll_paf=14.1717, loss_ll_heat=4.47418, q=1000
[2018-07-10 16:45:51,884] [train] [INFO] epoch=7.00 step=57800, 10.7801 examples/sec lr=0.000033, loss=32.3848, loss_ll=5.61106, loss_ll_paf=8.22363, loss_ll_heat=2.9985, q=1000
[2018-07-10 16:48:22,581] [train] [INFO] epoch=7.00 step=57900, 10.7798 examples/sec lr=0.000033, loss=30.7853, loss_ll=5.19166, loss_ll_paf=7.26077, loss_ll_heat=3.12255, q=1000
[2018-07-10 16:50:52,392] [train] [INFO] epoch=7.00 step=58000, 10.7797 examples/sec lr=0.000033, loss=36.1295, loss_ll=6.64519, loss_ll_paf=9.56163, loss_ll_heat=3.72875, q=1000
[2018-07-10 16:53:34,861] [train] [INFO] epoch=7.00 step=58100, 10.7779 examples/sec lr=0.000033, loss=38.7761, loss_ll=7.15352, loss_ll_paf=11.0996, loss_ll_heat=3.2074, q=1000
[2018-07-10 16:56:03,094] [train] [INFO] epoch=7.00 step=58200, 10.7779 examples/sec lr=0.000033, loss=45.656, loss_ll=8.19628, loss_ll_paf=12.6881, loss_ll_heat=3.70449, q=1000
[2018-07-10 16:58:31,710] [train] [INFO] epoch=7.00 step=58300, 10.7779 examples/sec lr=0.000033, loss=24.3612, loss_ll=4.3052, loss_ll_paf=5.57026, loss_ll_heat=3.04014, q=1000
[2018-07-10 17:01:01,294] [train] [INFO] epoch=7.00 step=58400, 10.7778 examples/sec lr=0.000033, loss=28.4566, loss_ll=4.58123, loss_ll_paf=6.17633, loss_ll_heat=2.98613, q=1000
[2018-07-10 17:03:31,266] [train] [INFO] epoch=7.00 step=58500, 10.7776 examples/sec lr=0.000033, loss=30.2324, loss_ll=5.11368, loss_ll_paf=7.23923, loss_ll_heat=2.98814, q=1000
[2018-07-10 17:06:00,070] [train] [INFO] epoch=7.00 step=58600, 10.7775 examples/sec lr=0.000033, loss=35.3331, loss_ll=6.369, loss_ll_paf=8.93702, loss_ll_heat=3.80099, q=1000
[2018-07-10 17:08:31,996] [train] [INFO] epoch=7.00 step=58700, 10.7771 examples/sec lr=0.000033, loss=32.6849, loss_ll=5.27838, loss_ll_paf=7.37994, loss_ll_heat=3.17683, q=1000
[2018-07-10 17:11:01,455] [train] [INFO] epoch=7.00 step=58800, 10.7770 examples/sec lr=0.000033, loss=38.466, loss_ll=6.9847, loss_ll_paf=10.9035, loss_ll_heat=3.06585, q=1000
[2018-07-10 17:13:28,192] [train] [INFO] epoch=7.00 step=58900, 10.7772 examples/sec lr=0.000033, loss=28.7942, loss_ll=4.93321, loss_ll_paf=7.06809, loss_ll_heat=2.79834, q=1000
[2018-07-10 17:15:58,877] [train] [INFO] epoch=7.00 step=59000, 10.7769 examples/sec lr=0.000033, loss=35.5531, loss_ll=5.80494, loss_ll_paf=8.80717, loss_ll_heat=2.8027, q=1000
[2018-07-10 17:18:42,182] [train] [INFO] epoch=7.00 step=59100, 10.7751 examples/sec lr=0.000033, loss=28.7294, loss_ll=4.81423, loss_ll_paf=7.08536, loss_ll_heat=2.54311, q=1000
[2018-07-10 17:21:10,549] [train] [INFO] epoch=7.00 step=59200, 10.7751 examples/sec lr=0.000033, loss=44.8431, loss_ll=8.4252, loss_ll_paf=12.5221, loss_ll_heat=4.32827, q=1000
[2018-07-10 17:23:39,919] [train] [INFO] epoch=7.00 step=59300, 10.7750 examples/sec lr=0.000033, loss=38.7217, loss_ll=6.87199, loss_ll_paf=10.1872, loss_ll_heat=3.55678, q=1000
[2018-07-10 17:26:07,919] [train] [INFO] epoch=7.00 step=59400, 10.7751 examples/sec lr=0.000033, loss=30.7391, loss_ll=5.6044, loss_ll_paf=7.30903, loss_ll_heat=3.89976, q=1000
[2018-07-10 17:28:39,402] [train] [INFO] epoch=7.00 step=59500, 10.7747 examples/sec lr=0.000033, loss=51.0646, loss_ll=9.20184, loss_ll_paf=14.4627, loss_ll_heat=3.94101, q=1000
[2018-07-10 17:31:04,908] [train] [INFO] epoch=7.00 step=59600, 10.7751 examples/sec lr=0.000033, loss=38.4974, loss_ll=6.62285, loss_ll_paf=10.1244, loss_ll_heat=3.12129, q=1000
[2018-07-10 17:33:35,027] [train] [INFO] epoch=7.00 step=59700, 10.7749 examples/sec lr=0.000033, loss=49.0672, loss_ll=8.71491, loss_ll_paf=13.2244, loss_ll_heat=4.2054, q=1000
[2018-07-10 17:36:03,593] [train] [INFO] epoch=7.00 step=59800, 10.7749 examples/sec lr=0.000033, loss=60.1523, loss_ll=11.2207, loss_ll_paf=18.6956, loss_ll_heat=3.74587, q=1000
[2018-07-10 17:38:35,930] [train] [INFO] epoch=7.00 step=59900, 10.7744 examples/sec lr=0.000033, loss=39.3261, loss_ll=7.51, loss_ll_paf=11.5953, loss_ll_heat=3.42473, q=1000
[2018-07-10 17:41:09,163] [train] [INFO] epoch=7.00 step=60000, 10.7738 examples/sec lr=0.000011, loss=51.8753, loss_ll=9.82675, loss_ll_paf=15.0448, loss_ll_heat=4.60868, q=1000
[2018-07-10 17:43:53,531] [train] [INFO] epoch=7.00 step=60100, 10.7719 examples/sec lr=0.000011, loss=44.8635, loss_ll=7.5941, loss_ll_paf=11.0865, loss_ll_heat=4.1017, q=1000
[2018-07-10 17:46:22,569] [train] [INFO] epoch=7.00 step=60200, 10.7719 examples/sec lr=0.000011, loss=54.7919, loss_ll=9.61909, loss_ll_paf=15.2888, loss_ll_heat=3.94941, q=1000
[2018-07-10 17:48:50,651] [train] [INFO] epoch=7.00 step=60300, 10.7719 examples/sec lr=0.000011, loss=37.4112, loss_ll=6.92359, loss_ll_paf=9.90877, loss_ll_heat=3.93842, q=1000
[2018-07-10 17:51:21,168] [train] [INFO] epoch=7.00 step=60400, 10.7717 examples/sec lr=0.000011, loss=31.0805, loss_ll=5.67047, loss_ll_paf=8.04703, loss_ll_heat=3.29391, q=1000
[2018-07-10 17:53:52,710] [train] [INFO] epoch=7.00 step=60500, 10.7713 examples/sec lr=0.000011, loss=44.0135, loss_ll=8.09461, loss_ll_paf=12.345, loss_ll_heat=3.84422, q=1000
[2018-07-10 17:56:21,124] [train] [INFO] epoch=7.00 step=60600, 10.7713 examples/sec lr=0.000011, loss=36.7485, loss_ll=6.62928, loss_ll_paf=9.44985, loss_ll_heat=3.80871, q=1000
[2018-07-10 17:58:50,233] [train] [INFO] epoch=7.00 step=60700, 10.7713 examples/sec lr=0.000011, loss=36.5484, loss_ll=6.09627, loss_ll_paf=9.48576, loss_ll_heat=2.70679, q=1000
[2018-07-10 18:01:19,269] [train] [INFO] epoch=7.00 step=60800, 10.7712 examples/sec lr=0.000011, loss=45.4043, loss_ll=8.05909, loss_ll_paf=12.1037, loss_ll_heat=4.01443, q=1000
[2018-07-10 18:03:47,796] [train] [INFO] epoch=8.00 step=60900, 10.7712 examples/sec lr=0.000011, loss=27.4501, loss_ll=4.67117, loss_ll_paf=6.84267, loss_ll_heat=2.49967, q=1000
[2018-07-10 18:06:17,530] [train] [INFO] epoch=8.00 step=61000, 10.7711 examples/sec lr=0.000011, loss=42.3963, loss_ll=7.44257, loss_ll_paf=11.5378, loss_ll_heat=3.34733, q=1000
[2018-07-10 18:08:57,926] [train] [INFO] epoch=8.00 step=61100, 10.7697 examples/sec lr=0.000011, loss=25.9123, loss_ll=4.6015, loss_ll_paf=6.82484, loss_ll_heat=2.37817, q=1000
[2018-07-10 18:11:26,486] [train] [INFO] epoch=8.00 step=61200, 10.7697 examples/sec lr=0.000011, loss=32.4472, loss_ll=5.71743, loss_ll_paf=7.97406, loss_ll_heat=3.4608, q=1000
[2018-07-10 18:13:56,038] [train] [INFO] epoch=8.00 step=61300, 10.7695 examples/sec lr=0.000011, loss=37.698, loss_ll=6.92482, loss_ll_paf=10.8031, loss_ll_heat=3.04658, q=1000
[2018-07-10 18:16:23,974] [train] [INFO] epoch=8.00 step=61400, 10.7696 examples/sec lr=0.000011, loss=43.2025, loss_ll=7.55835, loss_ll_paf=12.1979, loss_ll_heat=2.91876, q=1000
[2018-07-10 18:18:54,152] [train] [INFO] epoch=8.00 step=61500, 10.7694 examples/sec lr=0.000011, loss=29.2438, loss_ll=5.46593, loss_ll_paf=7.88164, loss_ll_heat=3.05023, q=1000
[2018-07-10 18:21:23,370] [train] [INFO] epoch=8.00 step=61600, 10.7694 examples/sec lr=0.000011, loss=46.7139, loss_ll=8.80867, loss_ll_paf=14.2611, loss_ll_heat=3.35625, q=1000
[2018-07-10 18:23:53,150] [train] [INFO] epoch=8.00 step=61700, 10.7692 examples/sec lr=0.000011, loss=42.1307, loss_ll=7.60834, loss_ll_paf=12.7574, loss_ll_heat=2.45924, q=1000
[2018-07-10 18:26:22,582] [train] [INFO] epoch=8.00 step=61800, 10.7691 examples/sec lr=0.000011, loss=31.8434, loss_ll=5.1428, loss_ll_paf=8.23298, loss_ll_heat=2.05262, q=1000
[2018-07-10 18:28:50,707] [train] [INFO] epoch=8.00 step=61900, 10.7692 examples/sec lr=0.000011, loss=48.4202, loss_ll=8.40501, loss_ll_paf=13.8648, loss_ll_heat=2.94524, q=1000
[2018-07-10 18:31:20,373] [train] [INFO] epoch=8.00 step=62000, 10.7690 examples/sec lr=0.000011, loss=40.5651, loss_ll=7.48079, loss_ll_paf=11.9664, loss_ll_heat=2.99519, q=1000
[2018-07-10 18:34:01,759] [train] [INFO] epoch=8.00 step=62100, 10.7675 examples/sec lr=0.000011, loss=29.3491, loss_ll=5.06089, loss_ll_paf=7.30741, loss_ll_heat=2.81437, q=1000
[2018-07-10 18:36:29,637] [train] [INFO] epoch=8.00 step=62200, 10.7676 examples/sec lr=0.000011, loss=27.481, loss_ll=5.03632, loss_ll_paf=7.17939, loss_ll_heat=2.89326, q=1000
[2018-07-10 18:38:56,369] [train] [INFO] epoch=8.00 step=62300, 10.7678 examples/sec lr=0.000011, loss=33.3998, loss_ll=6.07635, loss_ll_paf=8.90329, loss_ll_heat=3.2494, q=1000
[2018-07-10 18:41:23,555] [train] [INFO] epoch=8.00 step=62400, 10.7680 examples/sec lr=0.000011, loss=39.8667, loss_ll=6.95669, loss_ll_paf=10.7634, loss_ll_heat=3.14996, q=1000
[2018-07-10 18:43:51,050] [train] [INFO] epoch=8.00 step=62500, 10.7681 examples/sec lr=0.000011, loss=32.4677, loss_ll=5.41127, loss_ll_paf=7.40217, loss_ll_heat=3.42036, q=1000
[2018-07-10 18:46:19,443] [train] [INFO] epoch=8.00 step=62600, 10.7682 examples/sec lr=0.000011, loss=35.9464, loss_ll=6.56196, loss_ll_paf=9.64239, loss_ll_heat=3.48154, q=1000
[2018-07-10 18:48:50,414] [train] [INFO] epoch=8.00 step=62700, 10.7679 examples/sec lr=0.000011, loss=35.0838, loss_ll=6.13224, loss_ll_paf=9.2817, loss_ll_heat=2.98278, q=1000
[2018-07-10 18:51:20,798] [train] [INFO] epoch=8.00 step=62800, 10.7677 examples/sec lr=0.000011, loss=26.275, loss_ll=4.61844, loss_ll_paf=6.76395, loss_ll_heat=2.47294, q=1000
[2018-07-10 18:53:49,743] [train] [INFO] epoch=8.00 step=62900, 10.7676 examples/sec lr=0.000011, loss=44.6237, loss_ll=8.44059, loss_ll_paf=12.9789, loss_ll_heat=3.90231, q=1000
[2018-07-10 18:56:19,394] [train] [INFO] epoch=8.00 step=63000, 10.7675 examples/sec lr=0.000011, loss=29.2257, loss_ll=4.55939, loss_ll_paf=6.4542, loss_ll_heat=2.66457, q=1000
[2018-07-10 18:59:02,031] [train] [INFO] epoch=8.00 step=63100, 10.7659 examples/sec lr=0.000011, loss=27.2565, loss_ll=4.72396, loss_ll_paf=6.13791, loss_ll_heat=3.31002, q=1000
[2018-07-10 19:01:31,639] [train] [INFO] epoch=8.00 step=63200, 10.7658 examples/sec lr=0.000011, loss=47.3234, loss_ll=8.82989, loss_ll_paf=14.0194, loss_ll_heat=3.64041, q=1000
[2018-07-10 19:04:01,141] [train] [INFO] epoch=8.00 step=63300, 10.7657 examples/sec lr=0.000011, loss=32.0225, loss_ll=5.52104, loss_ll_paf=8.14226, loss_ll_heat=2.89981, q=1000
[2018-07-10 19:06:31,681] [train] [INFO] epoch=8.00 step=63400, 10.7655 examples/sec lr=0.000011, loss=37.5847, loss_ll=7.09645, loss_ll_paf=11.1026, loss_ll_heat=3.09035, q=1000
[2018-07-10 19:09:00,549] [train] [INFO] epoch=8.00 step=63500, 10.7654 examples/sec lr=0.000011, loss=37.2172, loss_ll=6.91752, loss_ll_paf=10.6589, loss_ll_heat=3.1761, q=1000
[2018-07-10 19:11:31,036] [train] [INFO] epoch=8.00 step=63600, 10.7652 examples/sec lr=0.000011, loss=26.5539, loss_ll=4.68449, loss_ll_paf=6.39777, loss_ll_heat=2.97122, q=1000
[2018-07-10 19:14:01,344] [train] [INFO] epoch=8.00 step=63700, 10.7650 examples/sec lr=0.000011, loss=28.0161, loss_ll=4.92734, loss_ll_paf=6.69999, loss_ll_heat=3.15469, q=1000
[2018-07-10 19:16:30,387] [train] [INFO] epoch=8.00 step=63800, 10.7650 examples/sec lr=0.000011, loss=43.1794, loss_ll=8.32399, loss_ll_paf=13.1793, loss_ll_heat=3.46866, q=1000
[2018-07-10 19:18:58,580] [train] [INFO] epoch=8.00 step=63900, 10.7650 examples/sec lr=0.000011, loss=23.2174, loss_ll=4.17398, loss_ll_paf=5.39949, loss_ll_heat=2.94847, q=1000
[2018-07-10 19:21:27,739] [train] [INFO] epoch=8.00 step=64000, 10.7650 examples/sec lr=0.000011, loss=27.9823, loss_ll=4.64393, loss_ll_paf=6.35939, loss_ll_heat=2.92847, q=1000
[2018-07-10 19:24:09,194] [train] [INFO] epoch=8.00 step=64100, 10.7635 examples/sec lr=0.000011, loss=44.8592, loss_ll=7.54859, loss_ll_paf=12.701, loss_ll_heat=2.39616, q=1000
[2018-07-10 19:26:39,801] [train] [INFO] epoch=8.00 step=64200, 10.7633 examples/sec lr=0.000011, loss=30.5124, loss_ll=5.14747, loss_ll_paf=7.5385, loss_ll_heat=2.75645, q=1000
[2018-07-10 19:29:08,594] [train] [INFO] epoch=8.00 step=64300, 10.7633 examples/sec lr=0.000011, loss=31.2618, loss_ll=5.5529, loss_ll_paf=7.82347, loss_ll_heat=3.28232, q=1000
[2018-07-10 19:31:40,211] [train] [INFO] epoch=8.00 step=64400, 10.7630 examples/sec lr=0.000011, loss=37.7553, loss_ll=6.783, loss_ll_paf=10.7064, loss_ll_heat=2.85962, q=1000
[2018-07-10 19:34:11,634] [train] [INFO] epoch=8.00 step=64500, 10.7626 examples/sec lr=0.000011, loss=44.2987, loss_ll=8.17177, loss_ll_paf=12.8673, loss_ll_heat=3.47621, q=1000
[2018-07-10 19:36:41,917] [train] [INFO] epoch=8.00 step=64600, 10.7625 examples/sec lr=0.000011, loss=37.0698, loss_ll=6.73368, loss_ll_paf=10.0357, loss_ll_heat=3.43161, q=1000
[2018-07-10 19:39:10,859] [train] [INFO] epoch=8.00 step=64700, 10.7624 examples/sec lr=0.000011, loss=43.8141, loss_ll=7.81741, loss_ll_paf=12.4327, loss_ll_heat=3.2021, q=1000
[2018-07-10 19:41:41,244] [train] [INFO] epoch=8.00 step=64800, 10.7622 examples/sec lr=0.000011, loss=33.0059, loss_ll=6.04546, loss_ll_paf=8.25179, loss_ll_heat=3.83913, q=1000
[2018-07-10 19:44:10,793] [train] [INFO] epoch=8.00 step=64900, 10.7621 examples/sec lr=0.000011, loss=30.3324, loss_ll=4.64499, loss_ll_paf=6.58949, loss_ll_heat=2.70049, q=1000
[2018-07-10 19:46:42,803] [train] [INFO] epoch=8.00 step=65000, 10.7618 examples/sec lr=0.000011, loss=39.7296, loss_ll=6.71099, loss_ll_paf=10.6951, loss_ll_heat=2.72691, q=1000
[2018-07-10 19:49:28,208] [train] [INFO] epoch=8.00 step=65100, 10.7599 examples/sec lr=0.000011, loss=47.7381, loss_ll=8.33977, loss_ll_paf=12.6845, loss_ll_heat=3.99507, q=1000
[2018-07-10 19:51:55,451] [train] [INFO] epoch=8.00 step=65200, 10.7601 examples/sec lr=0.000011, loss=38.2962, loss_ll=6.88641, loss_ll_paf=10.4022, loss_ll_heat=3.37063, q=1000
[2018-07-10 19:54:25,553] [train] [INFO] epoch=8.00 step=65300, 10.7599 examples/sec lr=0.000011, loss=38.3262, loss_ll=6.63748, loss_ll_paf=10.676, loss_ll_heat=2.59899, q=1000
[2018-07-10 19:56:53,783] [train] [INFO] epoch=8.00 step=65400, 10.7600 examples/sec lr=0.000011, loss=30.7951, loss_ll=4.99489, loss_ll_paf=7.38791, loss_ll_heat=2.60188, q=1000
[2018-07-10 19:59:22,845] [train] [INFO] epoch=8.00 step=65500, 10.7599 examples/sec lr=0.000011, loss=37.1062, loss_ll=6.77933, loss_ll_paf=9.84483, loss_ll_heat=3.71383, q=1000
[2018-07-10 20:01:54,865] [train] [INFO] epoch=8.00 step=65600, 10.7596 examples/sec lr=0.000011, loss=29.0887, loss_ll=4.6064, loss_ll_paf=6.3929, loss_ll_heat=2.81991, q=1000
[2018-07-10 20:04:25,809] [train] [INFO] epoch=8.00 step=65700, 10.7593 examples/sec lr=0.000011, loss=40.5321, loss_ll=7.46286, loss_ll_paf=11.7159, loss_ll_heat=3.20983, q=1000
[2018-07-10 20:06:54,736] [train] [INFO] epoch=8.00 step=65800, 10.7593 examples/sec lr=0.000011, loss=42.0246, loss_ll=7.28157, loss_ll_paf=11.2046, loss_ll_heat=3.3585, q=1000
[2018-07-10 20:09:24,701] [train] [INFO] epoch=8.00 step=65900, 10.7592 examples/sec lr=0.000011, loss=30.5411, loss_ll=5.00242, loss_ll_paf=6.93842, loss_ll_heat=3.06641, q=1000
[2018-07-10 20:11:55,330] [train] [INFO] epoch=8.00 step=66000, 10.7589 examples/sec lr=0.000011, loss=35.6027, loss_ll=6.12467, loss_ll_paf=9.02183, loss_ll_heat=3.22751, q=1000
[2018-07-10 20:14:38,099] [train] [INFO] epoch=8.00 step=66100, 10.7574 examples/sec lr=0.000011, loss=45.1928, loss_ll=8.03978, loss_ll_paf=13.0964, loss_ll_heat=2.98313, q=1000
[2018-07-10 20:17:05,960] [train] [INFO] epoch=8.00 step=66200, 10.7575 examples/sec lr=0.000011, loss=36.5355, loss_ll=6.35304, loss_ll_paf=9.62488, loss_ll_heat=3.0812, q=1000
[2018-07-10 20:19:36,477] [train] [INFO] epoch=8.00 step=66300, 10.7573 examples/sec lr=0.000011, loss=44.2169, loss_ll=7.96019, loss_ll_paf=12.6669, loss_ll_heat=3.25347, q=1000
[2018-07-10 20:22:03,144] [train] [INFO] epoch=8.00 step=66400, 10.7575 examples/sec lr=0.000011, loss=37.7597, loss_ll=6.6773, loss_ll_paf=9.96017, loss_ll_heat=3.39442, q=1000
[2018-07-10 20:24:34,159] [train] [INFO] epoch=8.00 step=66500, 10.7573 examples/sec lr=0.000011, loss=34.2594, loss_ll=5.68343, loss_ll_paf=8.00846, loss_ll_heat=3.3584, q=1000
[2018-07-10 20:27:01,789] [train] [INFO] epoch=8.00 step=66600, 10.7574 examples/sec lr=0.000011, loss=31.4243, loss_ll=5.56965, loss_ll_paf=7.60526, loss_ll_heat=3.53404, q=1000
[2018-07-10 20:29:32,998] [train] [INFO] epoch=8.00 step=66700, 10.7571 examples/sec lr=0.000011, loss=50.3853, loss_ll=9.51171, loss_ll_paf=15.2406, loss_ll_heat=3.78282, q=1000
[2018-07-10 20:32:02,635] [train] [INFO] epoch=8.00 step=66800, 10.7570 examples/sec lr=0.000011, loss=35.5705, loss_ll=6.40642, loss_ll_paf=9.70771, loss_ll_heat=3.10512, q=1000
[2018-07-10 20:34:30,461] [train] [INFO] epoch=8.00 step=66900, 10.7571 examples/sec lr=0.000011, loss=30.7355, loss_ll=5.21958, loss_ll_paf=6.74833, loss_ll_heat=3.69083, q=1000
[2018-07-10 20:37:00,612] [train] [INFO] epoch=8.00 step=67000, 10.7570 examples/sec lr=0.000011, loss=33.4415, loss_ll=5.78574, loss_ll_paf=8.65965, loss_ll_heat=2.91183, q=1000
[2018-07-10 20:39:42,151] [train] [INFO] epoch=8.00 step=67100, 10.7556 examples/sec lr=0.000011, loss=30.8306, loss_ll=5.39469, loss_ll_paf=7.34059, loss_ll_heat=3.44879, q=1000
[2018-07-10 20:42:13,506] [train] [INFO] epoch=8.00 step=67200, 10.7553 examples/sec lr=0.000011, loss=25.4366, loss_ll=4.18705, loss_ll_paf=5.44893, loss_ll_heat=2.92518, q=1000
[2018-07-10 20:44:41,746] [train] [INFO] epoch=8.00 step=67300, 10.7554 examples/sec lr=0.000011, loss=31.1385, loss_ll=5.71108, loss_ll_paf=8.2217, loss_ll_heat=3.20046, q=1000
[2018-07-10 20:47:11,737] [train] [INFO] epoch=8.00 step=67400, 10.7552 examples/sec lr=0.000011, loss=61.9593, loss_ll=10.6387, loss_ll_paf=16.4385, loss_ll_heat=4.83888, q=1000
[2018-07-10 20:49:44,269] [train] [INFO] epoch=8.00 step=67500, 10.7548 examples/sec lr=0.000011, loss=26.5401, loss_ll=4.2953, loss_ll_paf=6.00097, loss_ll_heat=2.58962, q=1000
[2018-07-10 20:52:16,954] [train] [INFO] epoch=8.00 step=67600, 10.7544 examples/sec lr=0.000011, loss=40.1118, loss_ll=7.55781, loss_ll_paf=11.263, loss_ll_heat=3.85262, q=1000
[2018-07-10 20:54:47,965] [train] [INFO] epoch=8.00 step=67700, 10.7542 examples/sec lr=0.000011, loss=29.6734, loss_ll=5.40627, loss_ll_paf=7.46766, loss_ll_heat=3.34488, q=1000
[2018-07-10 20:57:18,027] [train] [INFO] epoch=8.00 step=67800, 10.7541 examples/sec lr=0.000011, loss=36.7896, loss_ll=6.69863, loss_ll_paf=10.0392, loss_ll_heat=3.35803, q=1000
[2018-07-10 20:59:48,034] [train] [INFO] epoch=8.00 step=67900, 10.7539 examples/sec lr=0.000011, loss=29.5801, loss_ll=4.86487, loss_ll_paf=6.69998, loss_ll_heat=3.02976, q=1000
[2018-07-10 21:02:20,561] [train] [INFO] epoch=8.00 step=68000, 10.7535 examples/sec lr=0.000011, loss=38.5359, loss_ll=7.01922, loss_ll_paf=10.9235, loss_ll_heat=3.11493, q=1000
[2018-07-10 21:05:03,528] [train] [INFO] epoch=8.00 step=68100, 10.7520 examples/sec lr=0.000011, loss=33.2752, loss_ll=5.92088, loss_ll_paf=8.23104, loss_ll_heat=3.61073, q=1000
[2018-07-10 21:07:33,376] [train] [INFO] epoch=8.00 step=68200, 10.7519 examples/sec lr=0.000011, loss=39.9427, loss_ll=6.82182, loss_ll_paf=10.3715, loss_ll_heat=3.27216, q=1000
[2018-07-10 21:10:07,483] [train] [INFO] epoch=8.00 step=68300, 10.7513 examples/sec lr=0.000011, loss=34.8018, loss_ll=6.2064, loss_ll_paf=9.23355, loss_ll_heat=3.17926, q=1000
[2018-07-10 21:12:44,363] [train] [INFO] epoch=8.00 step=68400, 10.7505 examples/sec lr=0.000011, loss=26.9294, loss_ll=4.58032, loss_ll_paf=6.35901, loss_ll_heat=2.80162, q=1000
[2018-07-10 21:15:20,626] [train] [INFO] epoch=9.00 step=68500, 10.7497 examples/sec lr=0.000011, loss=39.9273, loss_ll=7.27494, loss_ll_paf=11.2123, loss_ll_heat=3.33755, q=1000
[2018-07-10 21:17:59,361] [train] [INFO] epoch=9.00 step=68600, 10.7487 examples/sec lr=0.000011, loss=28.4264, loss_ll=4.87822, loss_ll_paf=7.16126, loss_ll_heat=2.59517, q=1000
[2018-07-10 21:20:36,146] [train] [INFO] epoch=9.00 step=68700, 10.7478 examples/sec lr=0.000011, loss=27.2277, loss_ll=4.71617, loss_ll_paf=7.09482, loss_ll_heat=2.33752, q=1000
[2018-07-10 21:23:13,313] [train] [INFO] epoch=9.00 step=68800, 10.7470 examples/sec lr=0.000011, loss=33.7621, loss_ll=5.87034, loss_ll_paf=9.58242, loss_ll_heat=2.15827, q=1000
[2018-07-10 21:25:50,715] [train] [INFO] epoch=9.00 step=68900, 10.7461 examples/sec lr=0.000011, loss=22.483, loss_ll=3.83999, loss_ll_paf=5.4009, loss_ll_heat=2.27908, q=1000
[2018-07-10 21:28:29,433] [train] [INFO] epoch=9.00 step=69000, 10.7450 examples/sec lr=0.000011, loss=42.1239, loss_ll=6.88797, loss_ll_paf=9.92206, loss_ll_heat=3.85388, q=1000
[2018-07-10 21:31:20,267] [train] [INFO] epoch=9.00 step=69100, 10.7428 examples/sec lr=0.000011, loss=29.3207, loss_ll=5.06009, loss_ll_paf=7.04468, loss_ll_heat=3.0755, q=1000
[2018-07-10 21:33:56,732] [train] [INFO] epoch=9.00 step=69200, 10.7420 examples/sec lr=0.000011, loss=29.6111, loss_ll=5.58271, loss_ll_paf=8.35246, loss_ll_heat=2.81297, q=1000
[2018-07-10 21:36:34,923] [train] [INFO] epoch=9.00 step=69300, 10.7410 examples/sec lr=0.000011, loss=29.817, loss_ll=5.12522, loss_ll_paf=7.4681, loss_ll_heat=2.78234, q=1000
[2018-07-10 21:39:11,993] [train] [INFO] epoch=9.00 step=69400, 10.7402 examples/sec lr=0.000011, loss=31.7031, loss_ll=5.25304, loss_ll_paf=7.81025, loss_ll_heat=2.69583, q=1000
[2018-07-10 21:41:49,129] [train] [INFO] epoch=9.00 step=69500, 10.7393 examples/sec lr=0.000011, loss=25.2574, loss_ll=4.28892, loss_ll_paf=5.88555, loss_ll_heat=2.69229, q=1000
[2018-07-10 21:44:25,543] [train] [INFO] epoch=9.00 step=69600, 10.7386 examples/sec lr=0.000011, loss=25.6252, loss_ll=4.50554, loss_ll_paf=6.00626, loss_ll_heat=3.00483, q=1000
[2018-07-10 21:47:02,567] [train] [INFO] epoch=9.00 step=69700, 10.7377 examples/sec lr=0.000011, loss=42.3643, loss_ll=8.33391, loss_ll_paf=12.6818, loss_ll_heat=3.98606, q=1000
[2018-07-10 21:49:38,815] [train] [INFO] epoch=9.00 step=69800, 10.7370 examples/sec lr=0.000011, loss=43.0713, loss_ll=7.5598, loss_ll_paf=11.6155, loss_ll_heat=3.50413, q=1000
[2018-07-10 21:52:13,401] [train] [INFO] epoch=9.00 step=69900, 10.7364 examples/sec lr=0.000011, loss=27.7159, loss_ll=4.85667, loss_ll_paf=6.97403, loss_ll_heat=2.73931, q=1000
[2018-07-10 21:54:41,998] [train] [INFO] epoch=9.00 step=70000, 10.7364 examples/sec lr=0.000011, loss=27.7059, loss_ll=4.67231, loss_ll_paf=6.71056, loss_ll_heat=2.63405, q=1000
[2018-07-10 21:57:25,812] [train] [INFO] epoch=9.00 step=70100, 10.7349 examples/sec lr=0.000011, loss=29.781, loss_ll=5.2476, loss_ll_paf=7.54764, loss_ll_heat=2.94757, q=1000
[2018-07-10 21:59:55,269] [train] [INFO] epoch=9.00 step=70200, 10.7349 examples/sec lr=0.000011, loss=38.357, loss_ll=6.50806, loss_ll_paf=9.69885, loss_ll_heat=3.31727, q=1000
[2018-07-10 22:02:24,176] [train] [INFO] epoch=9.00 step=70300, 10.7349 examples/sec lr=0.000011, loss=54.5299, loss_ll=10.1455, loss_ll_paf=15.485, loss_ll_heat=4.80601, q=1000
[2018-07-10 22:04:52,773] [train] [INFO] epoch=9.00 step=70400, 10.7349 examples/sec lr=0.000011, loss=24.8148, loss_ll=3.81289, loss_ll_paf=5.17734, loss_ll_heat=2.44845, q=1000
[2018-07-10 22:07:21,548] [train] [INFO] epoch=9.00 step=70500, 10.7350 examples/sec lr=0.000011, loss=36.2047, loss_ll=6.53142, loss_ll_paf=9.69032, loss_ll_heat=3.37252, q=1000
[2018-07-10 22:09:49,142] [train] [INFO] epoch=9.00 step=70600, 10.7351 examples/sec lr=0.000011, loss=46.8442, loss_ll=8.47888, loss_ll_paf=13.9652, loss_ll_heat=2.99258, q=1000
[2018-07-10 22:12:19,905] [train] [INFO] epoch=9.00 step=70700, 10.7349 examples/sec lr=0.000011, loss=29.3525, loss_ll=5.37819, loss_ll_paf=8.01897, loss_ll_heat=2.7374, q=1000
[2018-07-10 22:14:51,921] [train] [INFO] epoch=9.00 step=70800, 10.7346 examples/sec lr=0.000011, loss=26.8299, loss_ll=4.50323, loss_ll_paf=6.52092, loss_ll_heat=2.48554, q=1000
[2018-07-10 22:17:22,617] [train] [INFO] epoch=9.00 step=70900, 10.7345 examples/sec lr=0.000011, loss=33.482, loss_ll=6.23906, loss_ll_paf=9.04315, loss_ll_heat=3.43497, q=1000
[2018-07-10 22:19:50,579] [train] [INFO] epoch=9.00 step=71000, 10.7346 examples/sec lr=0.000011, loss=35.897, loss_ll=5.98149, loss_ll_paf=8.74196, loss_ll_heat=3.22102, q=1000
[2018-07-10 22:22:33,048] [train] [INFO] epoch=9.00 step=71100, 10.7332 examples/sec lr=0.000011, loss=24.4814, loss_ll=4.27481, loss_ll_paf=6.1936, loss_ll_heat=2.35601, q=1000
[2018-07-10 22:25:03,011] [train] [INFO] epoch=9.00 step=71200, 10.7331 examples/sec lr=0.000011, loss=34.0548, loss_ll=5.8408, loss_ll_paf=8.21288, loss_ll_heat=3.46872, q=1000
[2018-07-10 22:27:33,115] [train] [INFO] epoch=9.00 step=71300, 10.7330 examples/sec lr=0.000011, loss=35.972, loss_ll=6.24591, loss_ll_paf=9.01568, loss_ll_heat=3.47614, q=1000
[2018-07-10 22:30:04,502] [train] [INFO] epoch=9.00 step=71400, 10.7328 examples/sec lr=0.000011, loss=25.1738, loss_ll=4.30826, loss_ll_paf=6.04994, loss_ll_heat=2.56658, q=1000
[2018-07-10 22:32:34,660] [train] [INFO] epoch=9.00 step=71500, 10.7327 examples/sec lr=0.000011, loss=45.8505, loss_ll=8.25678, loss_ll_paf=12.3071, loss_ll_heat=4.20649, q=1000
[2018-07-10 22:35:05,537] [train] [INFO] epoch=9.00 step=71600, 10.7325 examples/sec lr=0.000011, loss=38.7504, loss_ll=6.3569, loss_ll_paf=9.32738, loss_ll_heat=3.38642, q=1000
[2018-07-10 22:37:33,478] [train] [INFO] epoch=9.00 step=71700, 10.7326 examples/sec lr=0.000011, loss=30.2341, loss_ll=5.3, loss_ll_paf=7.46979, loss_ll_heat=3.13022, q=1000
[2018-07-10 22:40:04,159] [train] [INFO] epoch=9.00 step=71800, 10.7325 examples/sec lr=0.000011, loss=43.7408, loss_ll=7.74427, loss_ll_paf=12.4084, loss_ll_heat=3.08015, q=1000
[2018-07-10 22:42:34,199] [train] [INFO] epoch=9.00 step=71900, 10.7324 examples/sec lr=0.000011, loss=60.8911, loss_ll=11.055, loss_ll_paf=18.7127, loss_ll_heat=3.39727, q=1000
[2018-07-10 22:45:04,460] [train] [INFO] epoch=9.00 step=72000, 10.7322 examples/sec lr=0.000011, loss=29.4396, loss_ll=4.92356, loss_ll_paf=6.89196, loss_ll_heat=2.95515, q=1000
[2018-07-10 22:47:46,115] [train] [INFO] epoch=9.00 step=72100, 10.7310 examples/sec lr=0.000011, loss=43.203, loss_ll=7.88733, loss_ll_paf=12.2367, loss_ll_heat=3.53796, q=1000
[2018-07-10 22:50:16,390] [train] [INFO] epoch=9.00 step=72200, 10.7309 examples/sec lr=0.000011, loss=37.4822, loss_ll=6.92493, loss_ll_paf=10.7151, loss_ll_heat=3.13475, q=1000
[2018-07-10 22:52:44,514] [train] [INFO] epoch=9.00 step=72300, 10.7310 examples/sec lr=0.000011, loss=39.2457, loss_ll=6.72688, loss_ll_paf=10.4781, loss_ll_heat=2.97562, q=1000
[2018-07-10 22:55:13,930] [train] [INFO] epoch=9.00 step=72400, 10.7309 examples/sec lr=0.000011, loss=37.7608, loss_ll=6.66339, loss_ll_paf=10.1748, loss_ll_heat=3.15195, q=1000
[2018-07-10 22:57:43,817] [train] [INFO] epoch=9.00 step=72500, 10.7309 examples/sec lr=0.000011, loss=41.6467, loss_ll=7.32494, loss_ll_paf=12.0388, loss_ll_heat=2.61108, q=1000
[2018-07-10 23:00:14,306] [train] [INFO] epoch=9.00 step=72600, 10.7307 examples/sec lr=0.000011, loss=38.1114, loss_ll=6.77796, loss_ll_paf=11.0295, loss_ll_heat=2.52641, q=1000
[2018-07-10 23:02:44,017] [train] [INFO] epoch=9.00 step=72700, 10.7307 examples/sec lr=0.000011, loss=30.3485, loss_ll=5.34104, loss_ll_paf=7.8534, loss_ll_heat=2.82868, q=1000
[2018-07-10 23:05:14,500] [train] [INFO] epoch=9.00 step=72800, 10.7305 examples/sec lr=0.000011, loss=33.1258, loss_ll=5.7576, loss_ll_paf=7.89613, loss_ll_heat=3.61907, q=1000
[2018-07-10 23:07:44,828] [train] [INFO] epoch=9.00 step=72900, 10.7304 examples/sec lr=0.000011, loss=36.1276, loss_ll=6.45456, loss_ll_paf=10.232, loss_ll_heat=2.67716, q=1000
[2018-07-10 23:10:16,949] [train] [INFO] epoch=9.00 step=73000, 10.7301 examples/sec lr=0.000011, loss=34.743, loss_ll=5.84635, loss_ll_paf=8.33408, loss_ll_heat=3.35861, q=1000
[2018-07-10 23:13:00,291] [train] [INFO] epoch=9.00 step=73100, 10.7287 examples/sec lr=0.000011, loss=33.6394, loss_ll=5.91941, loss_ll_paf=8.59595, loss_ll_heat=3.24287, q=1000
[2018-07-10 23:15:30,937] [train] [INFO] epoch=9.00 step=73200, 10.7286 examples/sec lr=0.000011, loss=30.5641, loss_ll=5.48414, loss_ll_paf=8.14462, loss_ll_heat=2.82365, q=1000
[2018-07-10 23:18:00,639] [train] [INFO] epoch=9.00 step=73300, 10.7285 examples/sec lr=0.000011, loss=33.9356, loss_ll=6.32008, loss_ll_paf=9.69862, loss_ll_heat=2.94153, q=1000
[2018-07-10 23:20:34,830] [train] [INFO] epoch=9.00 step=73400, 10.7280 examples/sec lr=0.000011, loss=49.53, loss_ll=9.01738, loss_ll_paf=13.8211, loss_ll_heat=4.2137, q=1000
[2018-07-10 23:23:05,874] [train] [INFO] epoch=9.00 step=73500, 10.7278 examples/sec lr=0.000011, loss=42.0912, loss_ll=7.81494, loss_ll_paf=11.2969, loss_ll_heat=4.33303, q=1000
[2018-07-10 23:25:37,342] [train] [INFO] epoch=9.00 step=73600, 10.7276 examples/sec lr=0.000011, loss=43.6332, loss_ll=7.34421, loss_ll_paf=11.2225, loss_ll_heat=3.46594, q=1000
[2018-07-10 23:28:08,646] [train] [INFO] epoch=9.00 step=73700, 10.7274 examples/sec lr=0.000011, loss=41.0972, loss_ll=7.59146, loss_ll_paf=11.7195, loss_ll_heat=3.46343, q=1000
[2018-07-10 23:30:42,049] [train] [INFO] epoch=9.00 step=73800, 10.7270 examples/sec lr=0.000011, loss=24.6648, loss_ll=4.24784, loss_ll_paf=5.85191, loss_ll_heat=2.64378, q=1000
[2018-07-10 23:33:13,736] [train] [INFO] epoch=9.00 step=73900, 10.7267 examples/sec lr=0.000011, loss=29.843, loss_ll=4.75075, loss_ll_paf=6.98452, loss_ll_heat=2.51697, q=1000
[2018-07-10 23:35:45,322] [train] [INFO] epoch=9.00 step=74000, 10.7265 examples/sec lr=0.000011, loss=28.728, loss_ll=5.33136, loss_ll_paf=7.82301, loss_ll_heat=2.8397, q=1000
[2018-07-10 23:38:26,778] [train] [INFO] epoch=9.00 step=74100, 10.7253 examples/sec lr=0.000011, loss=47.9249, loss_ll=8.1927, loss_ll_paf=13.2147, loss_ll_heat=3.17071, q=1000
[2018-07-10 23:40:56,702] [train] [INFO] epoch=9.00 step=74200, 10.7252 examples/sec lr=0.000011, loss=45.5217, loss_ll=8.42031, loss_ll_paf=13.2346, loss_ll_heat=3.60599, q=1000
[2018-07-10 23:43:27,117] [train] [INFO] epoch=9.00 step=74300, 10.7251 examples/sec lr=0.000011, loss=35.8803, loss_ll=6.82932, loss_ll_paf=10.4233, loss_ll_heat=3.23534, q=1000
[2018-07-10 23:46:00,990] [train] [INFO] epoch=9.00 step=74400, 10.7247 examples/sec lr=0.000011, loss=21.2263, loss_ll=3.85874, loss_ll_paf=5.3748, loss_ll_heat=2.34267, q=1000
[2018-07-10 23:48:32,144] [train] [INFO] epoch=9.00 step=74500, 10.7245 examples/sec lr=0.000011, loss=31.3394, loss_ll=5.66743, loss_ll_paf=7.80261, loss_ll_heat=3.53225, q=1000
[2018-07-10 23:51:04,537] [train] [INFO] epoch=9.00 step=74600, 10.7242 examples/sec lr=0.000011, loss=33.4857, loss_ll=5.94558, loss_ll_paf=8.76232, loss_ll_heat=3.12884, q=1000
[2018-07-10 23:53:34,817] [train] [INFO] epoch=9.00 step=74700, 10.7241 examples/sec lr=0.000011, loss=33.7631, loss_ll=6.00133, loss_ll_paf=8.47007, loss_ll_heat=3.5326, q=1000
[2018-07-10 23:56:08,722] [train] [INFO] epoch=9.00 step=74800, 10.7236 examples/sec lr=0.000011, loss=33.1844, loss_ll=5.77871, loss_ll_paf=8.59865, loss_ll_heat=2.95877, q=1000
[2018-07-10 23:58:40,035] [train] [INFO] epoch=9.00 step=74900, 10.7234 examples/sec lr=0.000011, loss=25.6064, loss_ll=4.76017, loss_ll_paf=6.88637, loss_ll_heat=2.63397, q=1000
[2018-07-11 00:01:13,623] [train] [INFO] epoch=9.00 step=75000, 10.7230 examples/sec lr=0.000011, loss=42.9205, loss_ll=7.77027, loss_ll_paf=11.7243, loss_ll_heat=3.81628, q=1000
[2018-07-11 00:03:58,035] [train] [INFO] epoch=9.00 step=75100, 10.7215 examples/sec lr=0.000011, loss=64.8979, loss_ll=11.7462, loss_ll_paf=18.3026, loss_ll_heat=5.18977, q=1000
[2018-07-11 00:06:31,491] [train] [INFO] epoch=9.00 step=75200, 10.7211 examples/sec lr=0.000011, loss=42.8365, loss_ll=7.73823, loss_ll_paf=11.7091, loss_ll_heat=3.76733, q=1000
[2018-07-11 00:09:03,340] [train] [INFO] epoch=9.00 step=75300, 10.7209 examples/sec lr=0.000011, loss=39.6618, loss_ll=6.71093, loss_ll_paf=10.0458, loss_ll_heat=3.37605, q=1000
[2018-07-11 00:11:36,339] [train] [INFO] epoch=9.00 step=75400, 10.7205 examples/sec lr=0.000011, loss=36.7028, loss_ll=6.61702, loss_ll_paf=10.7637, loss_ll_heat=2.47034, q=1000
[2018-07-11 00:14:08,638] [train] [INFO] epoch=9.00 step=75500, 10.7202 examples/sec lr=0.000011, loss=39.5961, loss_ll=7.18924, loss_ll_paf=10.7644, loss_ll_heat=3.61404, q=1000
[2018-07-11 00:16:36,922] [train] [INFO] epoch=9.00 step=75600, 10.7203 examples/sec lr=0.000011, loss=37.0937, loss_ll=6.54447, loss_ll_paf=9.71966, loss_ll_heat=3.36927, q=1000
[2018-07-11 00:19:09,368] [train] [INFO] epoch=9.00 step=75700, 10.7200 examples/sec lr=0.000011, loss=29.3419, loss_ll=4.87951, loss_ll_paf=7.15108, loss_ll_heat=2.60794, q=1000
[2018-07-11 00:21:42,138] [train] [INFO] epoch=9.00 step=75800, 10.7197 examples/sec lr=0.000011, loss=53.49, loss_ll=9.97327, loss_ll_paf=16.0123, loss_ll_heat=3.93426, q=1000
[2018-07-11 00:24:13,153] [train] [INFO] epoch=9.00 step=75900, 10.7195 examples/sec lr=0.000011, loss=34.1001, loss_ll=5.96302, loss_ll_paf=9.32288, loss_ll_heat=2.60316, q=1000
[2018-07-11 00:26:44,231] [train] [INFO] epoch=9.00 step=76000, 10.7193 examples/sec lr=0.000011, loss=32.7241, loss_ll=5.50006, loss_ll_paf=7.64335, loss_ll_heat=3.35676, q=1000
[2018-07-11 00:29:29,104] [train] [INFO] epoch=10.00 step=76100, 10.7179 examples/sec lr=0.000011, loss=40.247, loss_ll=7.25947, loss_ll_paf=11.2323, loss_ll_heat=3.28665, q=1000
[2018-07-11 00:32:02,772] [train] [INFO] epoch=10.00 step=76200, 10.7175 examples/sec lr=0.000011, loss=37.0574, loss_ll=6.82313, loss_ll_paf=10.6958, loss_ll_heat=2.95045, q=1000
[2018-07-11 00:34:36,431] [train] [INFO] epoch=10.00 step=76300, 10.7170 examples/sec lr=0.000011, loss=36.9862, loss_ll=6.61296, loss_ll_paf=10.0811, loss_ll_heat=3.14487, q=1000
[2018-07-11 00:37:09,350] [train] [INFO] epoch=10.00 step=76400, 10.7167 examples/sec lr=0.000011, loss=33.7184, loss_ll=5.86858, loss_ll_paf=8.5898, loss_ll_heat=3.14736, q=1000
[2018-07-11 00:39:40,825] [train] [INFO] epoch=10.00 step=76500, 10.7165 examples/sec lr=0.000011, loss=38.927, loss_ll=7.15123, loss_ll_paf=10.3799, loss_ll_heat=3.92253, q=1000
[2018-07-11 00:42:14,134] [train] [INFO] epoch=10.00 step=76600, 10.7161 examples/sec lr=0.000011, loss=35.2806, loss_ll=6.02014, loss_ll_paf=8.36006, loss_ll_heat=3.68023, q=1000
[2018-07-11 00:44:48,029] [train] [INFO] epoch=10.00 step=76700, 10.7157 examples/sec lr=0.000011, loss=30.1534, loss_ll=4.96454, loss_ll_paf=7.18215, loss_ll_heat=2.74693, q=1000
[2018-07-11 00:47:21,055] [train] [INFO] epoch=10.00 step=76800, 10.7153 examples/sec lr=0.000011, loss=29.6879, loss_ll=5.05363, loss_ll_paf=7.20661, loss_ll_heat=2.90065, q=1000
[2018-07-11 00:49:53,489] [train] [INFO] epoch=10.00 step=76900, 10.7151 examples/sec lr=0.000011, loss=46.6647, loss_ll=8.77765, loss_ll_paf=12.7076, loss_ll_heat=4.84771, q=1000
[2018-07-11 00:52:29,204] [train] [INFO] epoch=10.00 step=77000, 10.7145 examples/sec lr=0.000011, loss=30.9132, loss_ll=5.48217, loss_ll_paf=7.98606, loss_ll_heat=2.97827, q=1000
[2018-07-11 00:55:09,786] [train] [INFO] epoch=10.00 step=77100, 10.7134 examples/sec lr=0.000011, loss=35.8071, loss_ll=6.54559, loss_ll_paf=10.0971, loss_ll_heat=2.99403, q=1000
[2018-07-11 00:57:42,720] [train] [INFO] epoch=10.00 step=77200, 10.7131 examples/sec lr=0.000011, loss=43.1761, loss_ll=7.75231, loss_ll_paf=12.7561, loss_ll_heat=2.74852, q=1000
[2018-07-11 01:00:18,059] [train] [INFO] epoch=10.00 step=77300, 10.7125 examples/sec lr=0.000011, loss=32.9341, loss_ll=5.67584, loss_ll_paf=8.67208, loss_ll_heat=2.67961, q=1000
[2018-07-11 01:02:52,549] [train] [INFO] epoch=10.00 step=77400, 10.7120 examples/sec lr=0.000011, loss=45.1906, loss_ll=7.74944, loss_ll_paf=11.6481, loss_ll_heat=3.85074, q=1000
[2018-07-11 01:05:24,099] [train] [INFO] epoch=10.00 step=77500, 10.7118 examples/sec lr=0.000011, loss=28.5819, loss_ll=5.1097, loss_ll_paf=7.66521, loss_ll_heat=2.5542, q=1000
[2018-07-11 01:07:56,789] [train] [INFO] epoch=10.00 step=77600, 10.7115 examples/sec lr=0.000011, loss=44.5449, loss_ll=8.08687, loss_ll_paf=12.8342, loss_ll_heat=3.33958, q=1000
[2018-07-11 01:10:29,931] [train] [INFO] epoch=10.00 step=77700, 10.7112 examples/sec lr=0.000011, loss=34.2914, loss_ll=5.65902, loss_ll_paf=8.56362, loss_ll_heat=2.75442, q=1000
[2018-07-11 01:13:03,100] [train] [INFO] epoch=10.00 step=77800, 10.7108 examples/sec lr=0.000011, loss=31.4314, loss_ll=5.29908, loss_ll_paf=7.74804, loss_ll_heat=2.85011, q=1000
[2018-07-11 01:15:37,319] [train] [INFO] epoch=10.00 step=77900, 10.7104 examples/sec lr=0.000011, loss=35.2591, loss_ll=6.21975, loss_ll_paf=9.94301, loss_ll_heat=2.4965, q=1000
[2018-07-11 01:18:10,458] [train] [INFO] epoch=10.00 step=78000, 10.7101 examples/sec lr=0.000011, loss=24.9991, loss_ll=4.30128, loss_ll_paf=5.53631, loss_ll_heat=3.06624, q=1000
[2018-07-11 01:20:58,005] [train] [INFO] epoch=10.00 step=78100, 10.7084 examples/sec lr=0.000011, loss=37.3316, loss_ll=6.64922, loss_ll_paf=10.2248, loss_ll_heat=3.07367, q=1000
[2018-07-11 01:23:31,660] [train] [INFO] epoch=10.00 step=78200, 10.7080 examples/sec lr=0.000011, loss=40.0784, loss_ll=7.13655, loss_ll_paf=10.6027, loss_ll_heat=3.67039, q=1000
[2018-07-11 01:26:04,885] [train] [INFO] epoch=10.00 step=78300, 10.7076 examples/sec lr=0.000011, loss=44.2497, loss_ll=7.7746, loss_ll_paf=13.021, loss_ll_heat=2.5282, q=1000
[2018-07-11 01:28:41,501] [train] [INFO] epoch=10.00 step=78400, 10.7070 examples/sec lr=0.000011, loss=28.5743, loss_ll=5.11083, loss_ll_paf=7.88052, loss_ll_heat=2.34114, q=1000
[2018-07-11 01:31:13,115] [train] [INFO] epoch=10.00 step=78500, 10.7068 examples/sec lr=0.000011, loss=28.3774, loss_ll=5.19383, loss_ll_paf=8.17094, loss_ll_heat=2.21672, q=1000
[2018-07-11 01:33:47,461] [train] [INFO] epoch=10.00 step=78600, 10.7063 examples/sec lr=0.000011, loss=42.6958, loss_ll=7.7853, loss_ll_paf=12.3029, loss_ll_heat=3.26774, q=1000
[2018-07-11 01:36:21,833] [train] [INFO] epoch=10.00 step=78700, 10.7059 examples/sec lr=0.000011, loss=34.9901, loss_ll=6.26305, loss_ll_paf=8.84448, loss_ll_heat=3.68162, q=1000
[2018-07-11 01:38:53,948] [train] [INFO] epoch=10.00 step=78800, 10.7057 examples/sec lr=0.000011, loss=22.891, loss_ll=4.24423, loss_ll_paf=6.02105, loss_ll_heat=2.46741, q=1000
[2018-07-11 01:41:24,508] [train] [INFO] epoch=10.00 step=78900, 10.7056 examples/sec lr=0.000011, loss=22.7918, loss_ll=4.21479, loss_ll_paf=6.38378, loss_ll_heat=2.0458, q=1000
[2018-07-11 01:43:59,104] [train] [INFO] epoch=10.00 step=79000, 10.7051 examples/sec lr=0.000011, loss=25.0278, loss_ll=4.26817, loss_ll_paf=5.70462, loss_ll_heat=2.83173, q=1000
[2018-07-11 01:46:46,180] [train] [INFO] epoch=10.00 step=79100, 10.7035 examples/sec lr=0.000011, loss=29.8074, loss_ll=5.39769, loss_ll_paf=7.60119, loss_ll_heat=3.19418, q=1000
[2018-07-11 01:49:17,667] [train] [INFO] epoch=10.00 step=79200, 10.7033 examples/sec lr=0.000011, loss=29.5334, loss_ll=4.99663, loss_ll_paf=8.04267, loss_ll_heat=1.9506, q=1000
[2018-07-11 01:51:51,238] [train] [INFO] epoch=10.00 step=79300, 10.7029 examples/sec lr=0.000011, loss=30.5498, loss_ll=5.25608, loss_ll_paf=7.23951, loss_ll_heat=3.27265, q=1000
[2018-07-11 01:54:25,790] [train] [INFO] epoch=10.00 step=79400, 10.7025 examples/sec lr=0.000011, loss=29.8258, loss_ll=5.02129, loss_ll_paf=7.3889, loss_ll_heat=2.65368, q=1000
[2018-07-11 01:56:57,174] [train] [INFO] epoch=10.00 step=79500, 10.7023 examples/sec lr=0.000011, loss=29.6615, loss_ll=5.5383, loss_ll_paf=8.06778, loss_ll_heat=3.00882, q=1000
[2018-07-11 01:59:31,935] [train] [INFO] epoch=10.00 step=79600, 10.7018 examples/sec lr=0.000011, loss=34.5848, loss_ll=5.72345, loss_ll_paf=8.11057, loss_ll_heat=3.33634, q=1000
[2018-07-11 02:02:05,497] [train] [INFO] epoch=10.00 step=79700, 10.7015 examples/sec lr=0.000011, loss=33.9442, loss_ll=5.50596, loss_ll_paf=7.77876, loss_ll_heat=3.23316, q=1000
[2018-07-11 02:04:37,661] [train] [INFO] epoch=10.00 step=79800, 10.7012 examples/sec lr=0.000011, loss=27.9215, loss_ll=4.8834, loss_ll_paf=6.80949, loss_ll_heat=2.95731, q=1000
[2018-07-11 02:07:12,998] [train] [INFO] epoch=10.00 step=79900, 10.7007 examples/sec lr=0.000011, loss=30.6173, loss_ll=5.29488, loss_ll_paf=7.82916, loss_ll_heat=2.76061, q=1000
[2018-07-11 02:09:47,967] [train] [INFO] epoch=10.00 step=80000, 10.7002 examples/sec lr=0.000011, loss=32.4686, loss_ll=5.91026, loss_ll_paf=8.62681, loss_ll_heat=3.19371, q=1000
[2018-07-11 02:12:33,670] [train] [INFO] epoch=10.00 step=80100, 10.6988 examples/sec lr=0.000011, loss=38.774, loss_ll=7.00689, loss_ll_paf=11.0313, loss_ll_heat=2.98247, q=1000
[2018-07-11 02:15:07,881] [train] [INFO] epoch=10.00 step=80200, 10.6984 examples/sec lr=0.000011, loss=24.0408, loss_ll=4.35146, loss_ll_paf=5.90655, loss_ll_heat=2.79636, q=1000
[2018-07-11 02:17:43,716] [train] [INFO] epoch=10.00 step=80300, 10.6978 examples/sec lr=0.000011, loss=38.195, loss_ll=6.53275, loss_ll_paf=10.108, loss_ll_heat=2.95751, q=1000
[2018-07-11 02:20:14,590] [train] [INFO] epoch=10.00 step=80400, 10.6977 examples/sec lr=0.000011, loss=30.5793, loss_ll=4.90365, loss_ll_paf=7.3, loss_ll_heat=2.5073, q=1000
[2018-07-11 02:22:47,981] [train] [INFO] epoch=10.00 step=80500, 10.6974 examples/sec lr=0.000011, loss=55.8678, loss_ll=10.0682, loss_ll_paf=15.9766, loss_ll_heat=4.15982, q=1000
[2018-07-11 02:25:21,075] [train] [INFO] epoch=10.00 step=80600, 10.6970 examples/sec lr=0.000011, loss=29.4516, loss_ll=4.72606, loss_ll_paf=6.51838, loss_ll_heat=2.93375, q=1000
[2018-07-11 02:27:54,340] [train] [INFO] epoch=10.00 step=80700, 10.6967 examples/sec lr=0.000011, loss=39.4779, loss_ll=6.93595, loss_ll_paf=10.7945, loss_ll_heat=3.07738, q=1000
[2018-07-11 02:30:28,983] [train] [INFO] epoch=10.00 step=80800, 10.6963 examples/sec lr=0.000011, loss=29.805, loss_ll=5.61449, loss_ll_paf=8.21476, loss_ll_heat=3.01422, q=1000
[2018-07-11 02:33:02,925] [train] [INFO] epoch=10.00 step=80900, 10.6959 examples/sec lr=0.000011, loss=41.7617, loss_ll=7.60836, loss_ll_paf=11.2675, loss_ll_heat=3.94917, q=1000
[2018-07-11 02:35:37,498] [train] [INFO] epoch=10.00 step=81000, 10.6954 examples/sec lr=0.000011, loss=40.7985, loss_ll=7.37401, loss_ll_paf=11.4076, loss_ll_heat=3.34039, q=1000
[2018-07-11 02:38:25,381] [train] [INFO] epoch=10.00 step=81100, 10.6938 examples/sec lr=0.000011, loss=31.6684, loss_ll=5.56478, loss_ll_paf=8.34301, loss_ll_heat=2.78654, q=1000
[2018-07-11 02:41:00,011] [train] [INFO] epoch=10.00 step=81200, 10.6934 examples/sec lr=0.000011, loss=38.6126, loss_ll=6.9822, loss_ll_paf=10.7933, loss_ll_heat=3.1711, q=1000
[2018-07-11 02:43:34,400] [train] [INFO] epoch=10.00 step=81300, 10.6930 examples/sec lr=0.000011, loss=34.0265, loss_ll=5.82366, loss_ll_paf=8.72932, loss_ll_heat=2.918, q=1000
[2018-07-11 02:46:08,803] [train] [INFO] epoch=10.00 step=81400, 10.6926 examples/sec lr=0.000011, loss=42.9838, loss_ll=7.42619, loss_ll_paf=10.8955, loss_ll_heat=3.95688, q=1000
[2018-07-11 02:48:42,740] [train] [INFO] epoch=10.00 step=81500, 10.6922 examples/sec lr=0.000011, loss=24.9273, loss_ll=4.26021, loss_ll_paf=6.14042, loss_ll_heat=2.38001, q=1000
[2018-07-11 02:51:18,082] [train] [INFO] epoch=10.00 step=81600, 10.6917 examples/sec lr=0.000011, loss=33.5936, loss_ll=6.14722, loss_ll_paf=8.03936, loss_ll_heat=4.25508, q=1000
[2018-07-11 02:53:51,270] [train] [INFO] epoch=10.00 step=81700, 10.6914 examples/sec lr=0.000011, loss=35.3288, loss_ll=6.35265, loss_ll_paf=9.98177, loss_ll_heat=2.72353, q=1000
[2018-07-11 02:56:23,926] [train] [INFO] epoch=10.00 step=81800, 10.6911 examples/sec lr=0.000011, loss=39.3411, loss_ll=7.09204, loss_ll_paf=10.8106, loss_ll_heat=3.37349, q=1000
[2018-07-11 02:58:58,300] [train] [INFO] epoch=10.00 step=81900, 10.6907 examples/sec lr=0.000011, loss=23.4819, loss_ll=3.89372, loss_ll_paf=5.56188, loss_ll_heat=2.22556, q=1000
[2018-07-11 03:01:33,605] [train] [INFO] epoch=10.00 step=82000, 10.6902 examples/sec lr=0.000011, loss=30.0573, loss_ll=5.16535, loss_ll_paf=7.1585, loss_ll_heat=3.1722, q=1000
[2018-07-11 03:04:19,722] [train] [INFO] epoch=10.00 step=82100, 10.6888 examples/sec lr=0.000011, loss=35.34, loss_ll=6.07178, loss_ll_paf=8.79637, loss_ll_heat=3.3472, q=1000
[2018-07-11 03:06:53,394] [train] [INFO] epoch=10.00 step=82200, 10.6884 examples/sec lr=0.000011, loss=27.9473, loss_ll=4.7613, loss_ll_paf=7.06653, loss_ll_heat=2.45607, q=1000
[2018-07-11 03:09:27,594] [train] [INFO] epoch=10.00 step=82300, 10.6880 examples/sec lr=0.000011, loss=27.8662, loss_ll=4.60612, loss_ll_paf=5.83359, loss_ll_heat=3.37864, q=1000
[2018-07-11 03:12:00,881] [train] [INFO] epoch=10.00 step=82400, 10.6877 examples/sec lr=0.000011, loss=34.8208, loss_ll=6.03737, loss_ll_paf=9.13536, loss_ll_heat=2.93938, q=1000
[2018-07-11 03:14:34,929] [train] [INFO] epoch=10.00 step=82500, 10.6873 examples/sec lr=0.000011, loss=44.517, loss_ll=7.91924, loss_ll_paf=13.1351, loss_ll_heat=2.70335, q=1000
[2018-07-11 03:17:10,256] [train] [INFO] epoch=10.00 step=82600, 10.6869 examples/sec lr=0.000011, loss=30.2487, loss_ll=5.27317, loss_ll_paf=8.08612, loss_ll_heat=2.46022, q=1000
[2018-07-11 03:19:45,836] [train] [INFO] epoch=10.00 step=82700, 10.6864 examples/sec lr=0.000011, loss=37.9937, loss_ll=6.63506, loss_ll_paf=10.066, loss_ll_heat=3.20412, q=1000
[2018-07-11 03:22:20,869] [train] [INFO] epoch=10.00 step=82800, 10.6859 examples/sec lr=0.000011, loss=31.8393, loss_ll=5.37951, loss_ll_paf=7.92489, loss_ll_heat=2.83412, q=1000
[2018-07-11 03:24:57,525] [train] [INFO] epoch=10.00 step=82900, 10.6853 examples/sec lr=0.000011, loss=33.3004, loss_ll=6.31836, loss_ll_paf=9.97237, loss_ll_heat=2.66435, q=1000
[2018-07-11 03:27:32,782] [train] [INFO] epoch=10.00 step=83000, 10.6848 examples/sec lr=0.000011, loss=46.7008, loss_ll=8.32479, loss_ll_paf=12.3656, loss_ll_heat=4.28396, q=1000
[2018-07-11 03:30:22,995] [train] [INFO] epoch=10.00 step=83100, 10.6831 examples/sec lr=0.000011, loss=30.2052, loss_ll=5.54222, loss_ll_paf=7.91641, loss_ll_heat=3.16803, q=1000
[2018-07-11 03:32:55,654] [train] [INFO] epoch=10.00 step=83200, 10.6828 examples/sec lr=0.000011, loss=42.9575, loss_ll=7.76052, loss_ll_paf=12.0507, loss_ll_heat=3.47035, q=1000
[2018-07-11 03:35:30,895] [train] [INFO] epoch=10.00 step=83300, 10.6824 examples/sec lr=0.000011, loss=30.3717, loss_ll=5.33591, loss_ll_paf=8.03356, loss_ll_heat=2.63826, q=1000
[2018-07-11 03:38:05,311] [train] [INFO] epoch=10.00 step=83400, 10.6820 examples/sec lr=0.000011, loss=35.344, loss_ll=5.78007, loss_ll_paf=8.65499, loss_ll_heat=2.90514, q=1000
[2018-07-11 03:40:41,200] [train] [INFO] epoch=10.00 step=83500, 10.6814 examples/sec lr=0.000011, loss=25.8017, loss_ll=4.81412, loss_ll_paf=6.88811, loss_ll_heat=2.74013, q=1000
[2018-07-11 03:43:15,667] [train] [INFO] epoch=10.00 step=83600, 10.6810 examples/sec lr=0.000011, loss=27.4359, loss_ll=4.94469, loss_ll_paf=7.22025, loss_ll_heat=2.66913, q=1000
[2018-07-11 03:45:48,884] [train] [INFO] epoch=11.00 step=83700, 10.6807 examples/sec lr=0.000011, loss=29.2194, loss_ll=4.95239, loss_ll_paf=7.06575, loss_ll_heat=2.83904, q=1000
[2018-07-11 03:48:21,790] [train] [INFO] epoch=11.00 step=83800, 10.6805 examples/sec lr=0.000011, loss=39.8925, loss_ll=7.37274, loss_ll_paf=11.2645, loss_ll_heat=3.48103, q=1000
[2018-07-11 03:50:57,854] [train] [INFO] epoch=11.00 step=83900, 10.6800 examples/sec lr=0.000011, loss=38.5638, loss_ll=6.46911, loss_ll_paf=9.8163, loss_ll_heat=3.12192, q=1000
[2018-07-11 03:53:35,236] [train] [INFO] epoch=11.00 step=84000, 10.6793 examples/sec lr=0.000011, loss=29.421, loss_ll=4.89909, loss_ll_paf=7.61586, loss_ll_heat=2.18231, q=1000
[2018-07-11 03:56:21,838] [train] [INFO] epoch=11.00 step=84100, 10.6779 examples/sec lr=0.000011, loss=31.0588, loss_ll=5.04349, loss_ll_paf=6.7398, loss_ll_heat=3.34718, q=1000
[2018-07-11 03:58:54,251] [train] [INFO] epoch=11.00 step=84200, 10.6777 examples/sec lr=0.000011, loss=26.0852, loss_ll=4.62295, loss_ll_paf=6.52579, loss_ll_heat=2.72011, q=1000
[2018-07-11 04:01:27,832] [train] [INFO] epoch=11.00 step=84300, 10.6774 examples/sec lr=0.000011, loss=52.7513, loss_ll=8.54839, loss_ll_paf=13.5896, loss_ll_heat=3.50721, q=1000
[2018-07-11 04:04:00,438] [train] [INFO] epoch=11.00 step=84400, 10.6771 examples/sec lr=0.000011, loss=29.4819, loss_ll=4.95164, loss_ll_paf=7.32794, loss_ll_heat=2.57534, q=1000
[2018-07-11 04:06:34,157] [train] [INFO] epoch=11.00 step=84500, 10.6768 examples/sec lr=0.000011, loss=34.1396, loss_ll=6.23347, loss_ll_paf=9.55648, loss_ll_heat=2.91045, q=1000
[2018-07-11 04:09:08,273] [train] [INFO] epoch=11.00 step=84600, 10.6764 examples/sec lr=0.000011, loss=42.5791, loss_ll=7.27877, loss_ll_paf=10.8157, loss_ll_heat=3.74183, q=1000
[2018-07-11 04:11:44,705] [train] [INFO] epoch=11.00 step=84700, 10.6759 examples/sec lr=0.000011, loss=26.9418, loss_ll=4.2544, loss_ll_paf=6.12547, loss_ll_heat=2.38332, q=1000
[2018-07-11 04:14:20,231] [train] [INFO] epoch=11.00 step=84800, 10.6754 examples/sec lr=0.000011, loss=30.2013, loss_ll=5.02019, loss_ll_paf=7.36417, loss_ll_heat=2.67621, q=1000
[2018-07-11 04:16:52,909] [train] [INFO] epoch=11.00 step=84900, 10.6752 examples/sec lr=0.000011, loss=26.9626, loss_ll=4.65319, loss_ll_paf=6.91527, loss_ll_heat=2.39111, q=1000
[2018-07-11 04:19:24,853] [train] [INFO] epoch=11.00 step=85000, 10.6750 examples/sec lr=0.000011, loss=35.8162, loss_ll=6.6991, loss_ll_paf=10.401, loss_ll_heat=2.99718, q=1000
[2018-07-11 04:22:12,425] [train] [INFO] epoch=11.00 step=85100, 10.6735 examples/sec lr=0.000011, loss=35.9685, loss_ll=6.33878, loss_ll_paf=9.97496, loss_ll_heat=2.7026, q=1000
[2018-07-11 04:24:45,591] [train] [INFO] epoch=11.00 step=85200, 10.6732 examples/sec lr=0.000011, loss=39.9935, loss_ll=6.70691, loss_ll_paf=10.1492, loss_ll_heat=3.26459, q=1000
[2018-07-11 04:27:21,310] [train] [INFO] epoch=11.00 step=85300, 10.6728 examples/sec lr=0.000011, loss=24.0987, loss_ll=4.1021, loss_ll_paf=5.29085, loss_ll_heat=2.91335, q=1000
[2018-07-11 04:29:56,336] [train] [INFO] epoch=11.00 step=85400, 10.6723 examples/sec lr=0.000011, loss=32.7974, loss_ll=6.023, loss_ll_paf=9.25234, loss_ll_heat=2.79366, q=1000
[2018-07-11 04:32:30,697] [train] [INFO] epoch=11.00 step=85500, 10.6720 examples/sec lr=0.000011, loss=29.8873, loss_ll=4.70289, loss_ll_paf=6.87779, loss_ll_heat=2.528, q=1000
[2018-07-11 04:35:05,066] [train] [INFO] epoch=11.00 step=85600, 10.6716 examples/sec lr=0.000011, loss=33.8451, loss_ll=6.16649, loss_ll_paf=9.20532, loss_ll_heat=3.12767, q=1000
[2018-07-11 04:37:38,442] [train] [INFO] epoch=11.00 step=85700, 10.6713 examples/sec lr=0.000011, loss=25.7628, loss_ll=4.51534, loss_ll_paf=6.5158, loss_ll_heat=2.51488, q=1000
[2018-07-11 04:40:11,162] [train] [INFO] epoch=11.00 step=85800, 10.6711 examples/sec lr=0.000011, loss=35.4706, loss_ll=6.73857, loss_ll_paf=10.1702, loss_ll_heat=3.30695, q=1000
[2018-07-11 04:42:42,570] [train] [INFO] epoch=11.00 step=85900, 10.6710 examples/sec lr=0.000011, loss=41.7157, loss_ll=7.12507, loss_ll_paf=10.9722, loss_ll_heat=3.27796, q=1000
[2018-07-11 04:45:15,796] [train] [INFO] epoch=11.00 step=86000, 10.6707 examples/sec lr=0.000011, loss=30.6578, loss_ll=5.70754, loss_ll_paf=8.54023, loss_ll_heat=2.87484, q=1000
[2018-07-11 04:48:02,808] [train] [INFO] epoch=11.00 step=86100, 10.6693 examples/sec lr=0.000011, loss=30.5355, loss_ll=5.37051, loss_ll_paf=7.6296, loss_ll_heat=3.11142, q=1000
[2018-07-11 04:50:36,491] [train] [INFO] epoch=11.00 step=86200, 10.6690 examples/sec lr=0.000011, loss=40.6578, loss_ll=7.53852, loss_ll_paf=12.0168, loss_ll_heat=3.06026, q=1000
[2018-07-11 04:53:09,918] [train] [INFO] epoch=11.00 step=86300, 10.6687 examples/sec lr=0.000011, loss=32.3533, loss_ll=5.56767, loss_ll_paf=8.13004, loss_ll_heat=3.0053, q=1000
[2018-07-11 04:55:44,689] [train] [INFO] epoch=11.00 step=86400, 10.6683 examples/sec lr=0.000011, loss=33.6362, loss_ll=5.88301, loss_ll_paf=8.91551, loss_ll_heat=2.85051, q=1000
[2018-07-11 04:58:17,441] [train] [INFO] epoch=11.00 step=86500, 10.6681 examples/sec lr=0.000011, loss=23.2088, loss_ll=3.72215, loss_ll_paf=5.26767, loss_ll_heat=2.17663, q=1000
[2018-07-11 05:00:51,959] [train] [INFO] epoch=11.00 step=86600, 10.6677 examples/sec lr=0.000011, loss=34.1532, loss_ll=5.91615, loss_ll_paf=8.33565, loss_ll_heat=3.49664, q=1000
[2018-07-11 05:03:24,061] [train] [INFO] epoch=11.00 step=86700, 10.6675 examples/sec lr=0.000011, loss=40.5065, loss_ll=7.02765, loss_ll_paf=10.8083, loss_ll_heat=3.24702, q=1000
[2018-07-11 05:05:56,879] [train] [INFO] epoch=11.00 step=86800, 10.6673 examples/sec lr=0.000011, loss=32.2919, loss_ll=6.00612, loss_ll_paf=8.65262, loss_ll_heat=3.35962, q=1000
[2018-07-11 05:08:29,474] [train] [INFO] epoch=11.00 step=86900, 10.6671 examples/sec lr=0.000011, loss=28.0702, loss_ll=5.39528, loss_ll_paf=8.12302, loss_ll_heat=2.66753, q=1000
[2018-07-11 05:11:03,520] [train] [INFO] epoch=11.00 step=87000, 10.6667 examples/sec lr=0.000011, loss=35.0295, loss_ll=6.38624, loss_ll_paf=9.69805, loss_ll_heat=3.07443, q=1000
[2018-07-11 05:13:50,041] [train] [INFO] epoch=11.00 step=87100, 10.6654 examples/sec lr=0.000011, loss=30.8545, loss_ll=5.30638, loss_ll_paf=7.48819, loss_ll_heat=3.12458, q=1000
[2018-07-11 05:16:21,380] [train] [INFO] epoch=11.00 step=87200, 10.6653 examples/sec lr=0.000011, loss=36.1852, loss_ll=6.53707, loss_ll_paf=10.51, loss_ll_heat=2.56414, q=1000
[2018-07-11 05:18:54,318] [train] [INFO] epoch=11.00 step=87300, 10.6650 examples/sec lr=0.000011, loss=30.7372, loss_ll=5.54459, loss_ll_paf=8.23668, loss_ll_heat=2.8525, q=1000
[2018-07-11 05:21:27,772] [train] [INFO] epoch=11.00 step=87400, 10.6648 examples/sec lr=0.000011, loss=50.6173, loss_ll=9.08741, loss_ll_paf=13.2651, loss_ll_heat=4.90969, q=1000
[2018-07-11 05:23:59,528] [train] [INFO] epoch=11.00 step=87500, 10.6646 examples/sec lr=0.000011, loss=37.3784, loss_ll=6.32854, loss_ll_paf=9.26706, loss_ll_heat=3.39002, q=1000
[2018-07-11 05:26:31,912] [train] [INFO] epoch=11.00 step=87600, 10.6644 examples/sec lr=0.000011, loss=47.051, loss_ll=8.21787, loss_ll_paf=13.0286, loss_ll_heat=3.40711, q=1000
[2018-07-11 05:29:04,817] [train] [INFO] epoch=11.00 step=87700, 10.6642 examples/sec lr=0.000011, loss=31.9024, loss_ll=5.97094, loss_ll_paf=9.15013, loss_ll_heat=2.79174, q=1000
[2018-07-11 05:31:38,803] [train] [INFO] epoch=11.00 step=87800, 10.6639 examples/sec lr=0.000011, loss=30.4612, loss_ll=5.22173, loss_ll_paf=7.65853, loss_ll_heat=2.78493, q=1000
[2018-07-11 05:34:13,025] [train] [INFO] epoch=11.00 step=87900, 10.6635 examples/sec lr=0.000011, loss=32.2088, loss_ll=5.82949, loss_ll_paf=8.61732, loss_ll_heat=3.04167, q=1000
[2018-07-11 05:36:43,187] [train] [INFO] epoch=11.00 step=88000, 10.6635 examples/sec lr=0.000011, loss=44.4764, loss_ll=7.82113, loss_ll_paf=12.1571, loss_ll_heat=3.48516, q=1000
[2018-07-11 05:39:30,681] [train] [INFO] epoch=11.00 step=88100, 10.6621 examples/sec lr=0.000011, loss=39.1755, loss_ll=6.82504, loss_ll_paf=10.6355, loss_ll_heat=3.01462, q=1000
[2018-07-11 05:42:03,128] [train] [INFO] epoch=11.00 step=88200, 10.6619 examples/sec lr=0.000011, loss=56.611, loss_ll=10.9036, loss_ll_paf=18.8571, loss_ll_heat=2.95001, q=1000
[2018-07-11 05:44:35,023] [train] [INFO] epoch=11.00 step=88300, 10.6618 examples/sec lr=0.000011, loss=32.2285, loss_ll=5.30982, loss_ll_paf=7.78759, loss_ll_heat=2.83205, q=1000
[2018-07-11 05:47:06,265] [train] [INFO] epoch=11.00 step=88400, 10.6617 examples/sec lr=0.000011, loss=25.9359, loss_ll=4.2804, loss_ll_paf=6.41683, loss_ll_heat=2.14398, q=1000
[2018-07-11 05:49:37,059] [train] [INFO] epoch=11.00 step=88500, 10.6616 examples/sec lr=0.000011, loss=34.0814, loss_ll=6.07401, loss_ll_paf=8.63022, loss_ll_heat=3.51781, q=1000
[2018-07-11 05:52:11,226] [train] [INFO] epoch=11.00 step=88600, 10.6613 examples/sec lr=0.000011, loss=29.1853, loss_ll=5.14104, loss_ll_paf=7.89107, loss_ll_heat=2.391, q=1000
[2018-07-11 05:54:44,005] [train] [INFO] epoch=11.00 step=88700, 10.6611 examples/sec lr=0.000011, loss=23.2672, loss_ll=4.03857, loss_ll_paf=5.44862, loss_ll_heat=2.62852, q=1000
[2018-07-11 05:57:16,714] [train] [INFO] epoch=11.00 step=88800, 10.6609 examples/sec lr=0.000011, loss=38.4847, loss_ll=6.96146, loss_ll_paf=10.9504, loss_ll_heat=2.97255, q=1000
[2018-07-11 05:59:49,203] [train] [INFO] epoch=11.00 step=88900, 10.6607 examples/sec lr=0.000011, loss=34.3168, loss_ll=6.12632, loss_ll_paf=9.11147, loss_ll_heat=3.14116, q=1000
[2018-07-11 06:02:21,093] [train] [INFO] epoch=11.00 step=89000, 10.6605 examples/sec lr=0.000011, loss=33.4988, loss_ll=6.20132, loss_ll_paf=8.54983, loss_ll_heat=3.85282, q=1000
[2018-07-11 06:05:07,945] [train] [INFO] epoch=11.00 step=89100, 10.6592 examples/sec lr=0.000011, loss=35.9752, loss_ll=6.38458, loss_ll_paf=9.77282, loss_ll_heat=2.99634, q=1000
[2018-07-11 06:07:40,466] [train] [INFO] epoch=11.00 step=89200, 10.6590 examples/sec lr=0.000011, loss=37.9193, loss_ll=6.81983, loss_ll_paf=10.7433, loss_ll_heat=2.89637, q=1000
[2018-07-11 06:10:13,242] [train] [INFO] epoch=11.00 step=89300, 10.6588 examples/sec lr=0.000011, loss=29.1762, loss_ll=4.84659, loss_ll_paf=6.83526, loss_ll_heat=2.85792, q=1000
[2018-07-11 06:12:44,371] [train] [INFO] epoch=11.00 step=89400, 10.6587 examples/sec lr=0.000011, loss=26.8076, loss_ll=4.34229, loss_ll_paf=6.866, loss_ll_heat=1.81857, q=1000
[2018-07-11 06:15:16,992] [train] [INFO] epoch=11.00 step=89500, 10.6585 examples/sec lr=0.000011, loss=39.6668, loss_ll=7.13478, loss_ll_paf=11.4495, loss_ll_heat=2.82008, q=1000
[2018-07-11 06:17:50,159] [train] [INFO] epoch=11.00 step=89600, 10.6583 examples/sec lr=0.000011, loss=25.4169, loss_ll=4.47849, loss_ll_paf=6.5084, loss_ll_heat=2.44858, q=1000
[2018-07-11 06:20:23,732] [train] [INFO] epoch=11.00 step=89700, 10.6580 examples/sec lr=0.000011, loss=33.3754, loss_ll=5.57396, loss_ll_paf=8.09742, loss_ll_heat=3.0505, q=1000
[2018-07-11 06:22:57,810] [train] [INFO] epoch=11.00 step=89800, 10.6577 examples/sec lr=0.000011, loss=20.5875, loss_ll=3.43833, loss_ll_paf=4.48788, loss_ll_heat=2.38878, q=1000
[2018-07-11 06:25:29,706] [train] [INFO] epoch=11.00 step=89900, 10.6576 examples/sec lr=0.000011, loss=39.3063, loss_ll=6.73125, loss_ll_paf=10.214, loss_ll_heat=3.24853, q=1000
[2018-07-11 06:28:04,487] [train] [INFO] epoch=11.00 step=90000, 10.6572 examples/sec lr=0.000004, loss=35.2861, loss_ll=6.31496, loss_ll_paf=9.301, loss_ll_heat=3.32891, q=1000
[2018-07-11 06:30:52,523] [train] [INFO] epoch=11.00 step=90100, 10.6558 examples/sec lr=0.000004, loss=28.6215, loss_ll=4.90596, loss_ll_paf=6.96302, loss_ll_heat=2.8489, q=1000
[2018-07-11 06:33:26,608] [train] [INFO] epoch=11.00 step=90200, 10.6555 examples/sec lr=0.000004, loss=40.6445, loss_ll=6.89411, loss_ll_paf=10.1592, loss_ll_heat=3.62906, q=1000
[2018-07-11 06:35:59,804] [train] [INFO] epoch=11.00 step=90300, 10.6552 examples/sec lr=0.000004, loss=38.5084, loss_ll=6.40133, loss_ll_paf=9.46515, loss_ll_heat=3.33752, q=1000
[2018-07-11 06:38:34,309] [train] [INFO] epoch=11.00 step=90400, 10.6549 examples/sec lr=0.000004, loss=33.7102, loss_ll=6.1939, loss_ll_paf=9.62959, loss_ll_heat=2.75822, q=1000
[2018-07-11 06:41:07,036] [train] [INFO] epoch=11.00 step=90500, 10.6547 examples/sec lr=0.000004, loss=36.1774, loss_ll=6.25813, loss_ll_paf=9.06416, loss_ll_heat=3.4521, q=1000
[2018-07-11 06:43:41,204] [train] [INFO] epoch=11.00 step=90600, 10.6544 examples/sec lr=0.000004, loss=35.6362, loss_ll=6.00879, loss_ll_paf=8.60533, loss_ll_heat=3.41226, q=1000
[2018-07-11 06:46:13,703] [train] [INFO] epoch=11.00 step=90700, 10.6542 examples/sec lr=0.000004, loss=41.0359, loss_ll=7.12446, loss_ll_paf=10.4812, loss_ll_heat=3.7677, q=1000
[2018-07-11 06:48:49,065] [train] [INFO] epoch=11.00 step=90800, 10.6538 examples/sec lr=0.000004, loss=30.933, loss_ll=5.49395, loss_ll_paf=8.32425, loss_ll_heat=2.66365, q=1000
[2018-07-11 06:51:20,782] [train] [INFO] epoch=11.00 step=90900, 10.6537 examples/sec lr=0.000004, loss=33.0737, loss_ll=5.81631, loss_ll_paf=9.01274, loss_ll_heat=2.61988, q=1000
[2018-07-11 06:53:51,376] [train] [INFO] epoch=11.00 step=91000, 10.6536 examples/sec lr=0.000004, loss=36.543, loss_ll=6.53213, loss_ll_paf=10.1446, loss_ll_heat=2.91966, q=1000
[2018-07-11 06:56:37,276] [train] [INFO] epoch=11.00 step=91100, 10.6524 examples/sec lr=0.000004, loss=29.3121, loss_ll=4.79159, loss_ll_paf=6.91499, loss_ll_heat=2.66819, q=1000
[2018-07-11 06:59:12,204] [train] [INFO] epoch=11.00 step=91200, 10.6520 examples/sec lr=0.000004, loss=35.5229, loss_ll=6.28039, loss_ll_paf=9.31874, loss_ll_heat=3.24203, q=1000
[2018-07-11 07:01:45,012] [train] [INFO] epoch=11.00 step=91300, 10.6518 examples/sec lr=0.000004, loss=24.6283, loss_ll=4.1922, loss_ll_paf=5.80996, loss_ll_heat=2.57445, q=1000
[2018-07-11 07:04:17,725] [train] [INFO] epoch=12.00 step=91400, 10.6516 examples/sec lr=0.000004, loss=28.6294, loss_ll=4.75883, loss_ll_paf=6.83665, loss_ll_heat=2.68101, q=1000
[2018-07-11 07:06:52,748] [train] [INFO] epoch=12.00 step=91500, 10.6513 examples/sec lr=0.000004, loss=22.5834, loss_ll=4.08249, loss_ll_paf=5.46638, loss_ll_heat=2.6986, q=1000
[2018-07-11 07:09:27,641] [train] [INFO] epoch=12.00 step=91600, 10.6509 examples/sec lr=0.000004, loss=33.9382, loss_ll=5.79137, loss_ll_paf=8.89951, loss_ll_heat=2.68322, q=1000
[2018-07-11 07:11:59,031] [train] [INFO] epoch=12.00 step=91700, 10.6508 examples/sec lr=0.000004, loss=32.4097, loss_ll=5.66828, loss_ll_paf=8.3943, loss_ll_heat=2.94227, q=1000
[2018-07-11 07:14:32,395] [train] [INFO] epoch=12.00 step=91800, 10.6506 examples/sec lr=0.000004, loss=28.729, loss_ll=4.83545, loss_ll_paf=6.56771, loss_ll_heat=3.10319, q=1000
[2018-07-11 07:17:01,904] [train] [INFO] epoch=12.00 step=91900, 10.6506 examples/sec lr=0.000004, loss=19.4264, loss_ll=3.30355, loss_ll_paf=4.36682, loss_ll_heat=2.24028, q=1000
[2018-07-11 07:19:35,823] [train] [INFO] epoch=12.00 step=92000, 10.6504 examples/sec lr=0.000004, loss=27.7929, loss_ll=4.59873, loss_ll_paf=6.91692, loss_ll_heat=2.28054, q=1000
[2018-07-11 07:22:20,758] [train] [INFO] epoch=12.00 step=92100, 10.6492 examples/sec lr=0.000004, loss=22.0401, loss_ll=3.58695, loss_ll_paf=5.19219, loss_ll_heat=1.9817, q=1000
[2018-07-11 07:24:53,509] [train] [INFO] epoch=12.00 step=92200, 10.6490 examples/sec lr=0.000004, loss=26.5716, loss_ll=4.60202, loss_ll_paf=6.37565, loss_ll_heat=2.82839, q=1000
[2018-07-11 07:27:24,119] [train] [INFO] epoch=12.00 step=92300, 10.6490 examples/sec lr=0.000004, loss=23.5626, loss_ll=4.16359, loss_ll_paf=5.87853, loss_ll_heat=2.44865, q=1000
[2018-07-11 07:29:56,787] [train] [INFO] epoch=12.00 step=92400, 10.6488 examples/sec lr=0.000004, loss=37.976, loss_ll=7.05406, loss_ll_paf=10.8812, loss_ll_heat=3.2269, q=1000
[2018-07-11 07:32:31,446] [train] [INFO] epoch=12.00 step=92500, 10.6485 examples/sec lr=0.000004, loss=28.1462, loss_ll=4.88969, loss_ll_paf=7.13495, loss_ll_heat=2.64443, q=1000
[2018-07-11 07:35:06,586] [train] [INFO] epoch=12.00 step=92600, 10.6481 examples/sec lr=0.000004, loss=42.9016, loss_ll=7.65359, loss_ll_paf=11.8807, loss_ll_heat=3.42646, q=1000
[2018-07-11 07:37:40,819] [train] [INFO] epoch=12.00 step=92700, 10.6478 examples/sec lr=0.000004, loss=40.9002, loss_ll=7.05047, loss_ll_paf=11.0135, loss_ll_heat=3.08748, q=1000
[2018-07-11 07:40:15,376] [train] [INFO] epoch=12.00 step=92800, 10.6475 examples/sec lr=0.000004, loss=30.7348, loss_ll=4.90933, loss_ll_paf=7.04014, loss_ll_heat=2.77853, q=1000
[2018-07-11 07:42:49,360] [train] [INFO] epoch=12.00 step=92900, 10.6472 examples/sec lr=0.000004, loss=33.4948, loss_ll=5.72439, loss_ll_paf=8.73688, loss_ll_heat=2.71191, q=1000
[2018-07-11 07:45:23,980] [train] [INFO] epoch=12.00 step=93000, 10.6469 examples/sec lr=0.000004, loss=26.2721, loss_ll=4.25467, loss_ll_paf=6.03767, loss_ll_heat=2.47167, q=1000
[2018-07-11 07:48:12,421] [train] [INFO] epoch=12.00 step=93100, 10.6455 examples/sec lr=0.000004, loss=38.0249, loss_ll=6.48604, loss_ll_paf=9.78286, loss_ll_heat=3.18923, q=1000
[2018-07-11 07:50:47,719] [train] [INFO] epoch=12.00 step=93200, 10.6451 examples/sec lr=0.000004, loss=29.9201, loss_ll=5.51333, loss_ll_paf=8.38738, loss_ll_heat=2.63929, q=1000
[2018-07-11 07:53:19,509] [train] [INFO] epoch=12.00 step=93300, 10.6450 examples/sec lr=0.000004, loss=31.5622, loss_ll=5.75676, loss_ll_paf=9.134, loss_ll_heat=2.37952, q=1000
[2018-07-11 07:55:53,051] [train] [INFO] epoch=12.00 step=93400, 10.6447 examples/sec lr=0.000004, loss=32.2979, loss_ll=5.4855, loss_ll_paf=8.31486, loss_ll_heat=2.65615, q=1000
[2018-07-11 07:58:26,932] [train] [INFO] epoch=12.00 step=93500, 10.6445 examples/sec lr=0.000004, loss=32.8521, loss_ll=5.97201, loss_ll_paf=9.04907, loss_ll_heat=2.89495, q=1000
[2018-07-11 08:00:58,260] [train] [INFO] epoch=12.00 step=93600, 10.6444 examples/sec lr=0.000004, loss=37.7251, loss_ll=7.19757, loss_ll_paf=11.2917, loss_ll_heat=3.10345, q=1000
[2018-07-11 08:03:32,040] [train] [INFO] epoch=12.00 step=93700, 10.6441 examples/sec lr=0.000004, loss=38.9048, loss_ll=6.25994, loss_ll_paf=8.98543, loss_ll_heat=3.53445, q=1000
[2018-07-11 08:06:06,321] [train] [INFO] epoch=12.00 step=93800, 10.6438 examples/sec lr=0.000004, loss=23.6634, loss_ll=4.01481, loss_ll_paf=5.60236, loss_ll_heat=2.42727, q=1000
[2018-07-11 08:08:41,821] [train] [INFO] epoch=12.00 step=93900, 10.6434 examples/sec lr=0.000004, loss=29.1496, loss_ll=4.56248, loss_ll_paf=6.89344, loss_ll_heat=2.23151, q=1000
[2018-07-11 08:11:17,114] [train] [INFO] epoch=12.00 step=94000, 10.6431 examples/sec lr=0.000004, loss=36.5887, loss_ll=6.08254, loss_ll_paf=9.05204, loss_ll_heat=3.11304, q=1000
[2018-07-11 08:14:04,762] [train] [INFO] epoch=12.00 step=94100, 10.6418 examples/sec lr=0.000004, loss=29.5962, loss_ll=5.19591, loss_ll_paf=7.43839, loss_ll_heat=2.95344, q=1000
[2018-07-11 08:16:37,807] [train] [INFO] epoch=12.00 step=94200, 10.6416 examples/sec lr=0.000004, loss=29.4914, loss_ll=5.07564, loss_ll_paf=7.5663, loss_ll_heat=2.58497, q=1000
[2018-07-11 08:19:12,550] [train] [INFO] epoch=12.00 step=94300, 10.6412 examples/sec lr=0.000004, loss=31.307, loss_ll=4.95581, loss_ll_paf=7.09183, loss_ll_heat=2.8198, q=1000
[2018-07-11 08:21:45,040] [train] [INFO] epoch=12.00 step=94400, 10.6411 examples/sec lr=0.000004, loss=29.3787, loss_ll=5.29767, loss_ll_paf=7.71368, loss_ll_heat=2.88166, q=1000
[2018-07-11 08:24:19,729] [train] [INFO] epoch=12.00 step=94500, 10.6407 examples/sec lr=0.000004, loss=21.1855, loss_ll=3.68074, loss_ll_paf=5.37332, loss_ll_heat=1.98817, q=1000
[2018-07-11 08:26:53,165] [train] [INFO] epoch=12.00 step=94600, 10.6405 examples/sec lr=0.000004, loss=33.1787, loss_ll=5.91397, loss_ll_paf=8.54053, loss_ll_heat=3.2874, q=1000
[2018-07-11 08:29:26,660] [train] [INFO] epoch=12.00 step=94700, 10.6403 examples/sec lr=0.000004, loss=32.597, loss_ll=5.73535, loss_ll_paf=8.9361, loss_ll_heat=2.53461, q=1000
[2018-07-11 08:32:01,033] [train] [INFO] epoch=12.00 step=94800, 10.6400 examples/sec lr=0.000004, loss=34.3696, loss_ll=6.20042, loss_ll_paf=8.9588, loss_ll_heat=3.44205, q=1000
[2018-07-11 08:34:32,296] [train] [INFO] epoch=12.00 step=94900, 10.6399 examples/sec lr=0.000004, loss=31.2639, loss_ll=5.45838, loss_ll_paf=8.08299, loss_ll_heat=2.83377, q=1000
[2018-07-11 08:37:08,027] [train] [INFO] epoch=12.00 step=95000, 10.6395 examples/sec lr=0.000004, loss=28.9393, loss_ll=5.23285, loss_ll_paf=7.69686, loss_ll_heat=2.76885, q=1000
[2018-07-11 08:39:51,279] [train] [INFO] epoch=12.00 step=95100, 10.6386 examples/sec lr=0.000004, loss=27.0439, loss_ll=4.63673, loss_ll_paf=7.36529, loss_ll_heat=1.90817, q=1000
[2018-07-11 08:42:24,915] [train] [INFO] epoch=12.00 step=95200, 10.6383 examples/sec lr=0.000004, loss=25.9295, loss_ll=4.2966, loss_ll_paf=6.42065, loss_ll_heat=2.17255, q=1000
[2018-07-11 08:44:57,624] [train] [INFO] epoch=12.00 step=95300, 10.6381 examples/sec lr=0.000004, loss=35.2162, loss_ll=5.64908, loss_ll_paf=8.39255, loss_ll_heat=2.90561, q=1000
[2018-07-11 08:47:31,472] [train] [INFO] epoch=12.00 step=95400, 10.6379 examples/sec lr=0.000004, loss=36.4724, loss_ll=6.12731, loss_ll_paf=9.26968, loss_ll_heat=2.98494, q=1000
[2018-07-11 08:50:07,081] [train] [INFO] epoch=12.00 step=95500, 10.6375 examples/sec lr=0.000004, loss=41.6494, loss_ll=7.75582, loss_ll_paf=12.2389, loss_ll_heat=3.27277, q=1000
[2018-07-11 08:52:40,909] [train] [INFO] epoch=12.00 step=95600, 10.6373 examples/sec lr=0.000004, loss=45.2927, loss_ll=7.63345, loss_ll_paf=12.5506, loss_ll_heat=2.71627, q=1000
[2018-07-11 08:55:15,956] [train] [INFO] epoch=12.00 step=95700, 10.6369 examples/sec lr=0.000004, loss=30.139, loss_ll=5.18959, loss_ll_paf=7.39204, loss_ll_heat=2.98715, q=1000
[2018-07-11 08:57:52,476] [train] [INFO] epoch=12.00 step=95800, 10.6365 examples/sec lr=0.000004, loss=39.2928, loss_ll=6.98583, loss_ll_paf=10.1741, loss_ll_heat=3.79755, q=1000
[2018-07-11 09:00:26,427] [train] [INFO] epoch=12.00 step=95900, 10.6362 examples/sec lr=0.000004, loss=29.7121, loss_ll=4.8571, loss_ll_paf=7.00675, loss_ll_heat=2.70744, q=1000
[2018-07-11 09:03:00,478] [train] [INFO] epoch=12.00 step=96000, 10.6359 examples/sec lr=0.000004, loss=28.626, loss_ll=4.89936, loss_ll_paf=6.77888, loss_ll_heat=3.01985, q=1000
[2018-07-11 09:05:45,048] [train] [INFO] epoch=12.00 step=96100, 10.6349 examples/sec lr=0.000004, loss=25.1361, loss_ll=4.21614, loss_ll_paf=5.78556, loss_ll_heat=2.64672, q=1000
[2018-07-11 09:08:18,497] [train] [INFO] epoch=12.00 step=96200, 10.6347 examples/sec lr=0.000004, loss=25.1069, loss_ll=4.38219, loss_ll_paf=6.71263, loss_ll_heat=2.05176, q=1000
[2018-07-11 09:10:52,334] [train] [INFO] epoch=12.00 step=96300, 10.6344 examples/sec lr=0.000004, loss=33.8037, loss_ll=5.95362, loss_ll_paf=8.22098, loss_ll_heat=3.68625, q=1000
[2018-07-11 09:13:24,965] [train] [INFO] epoch=12.00 step=96400, 10.6343 examples/sec lr=0.000004, loss=28.5951, loss_ll=4.80831, loss_ll_paf=6.41766, loss_ll_heat=3.19896, q=1000
[2018-07-11 09:15:58,017] [train] [INFO] epoch=12.00 step=96500, 10.6341 examples/sec lr=0.000004, loss=29.3167, loss_ll=5.50618, loss_ll_paf=7.50942, loss_ll_heat=3.50294, q=1000
[2018-07-11 09:18:31,961] [train] [INFO] epoch=12.00 step=96600, 10.6338 examples/sec lr=0.000004, loss=20.2723, loss_ll=3.11233, loss_ll_paf=4.15853, loss_ll_heat=2.06613, q=1000
[2018-07-11 09:21:05,529] [train] [INFO] epoch=12.00 step=96700, 10.6336 examples/sec lr=0.000004, loss=39.9177, loss_ll=6.92132, loss_ll_paf=11.1413, loss_ll_heat=2.70132, q=1000
[2018-07-11 09:23:38,483] [train] [INFO] epoch=12.00 step=96800, 10.6334 examples/sec lr=0.000004, loss=21.4623, loss_ll=3.63217, loss_ll_paf=4.58964, loss_ll_heat=2.6747, q=1000
[2018-07-11 09:26:11,243] [train] [INFO] epoch=12.00 step=96900, 10.6332 examples/sec lr=0.000004, loss=32.7447, loss_ll=5.34468, loss_ll_paf=8.0496, loss_ll_heat=2.63976, q=1000
[2018-07-11 09:28:45,842] [train] [INFO] epoch=12.00 step=97000, 10.6329 examples/sec lr=0.000004, loss=20.4191, loss_ll=3.48345, loss_ll_paf=4.24901, loss_ll_heat=2.71789, q=1000
[2018-07-11 09:31:35,361] [train] [INFO] epoch=12.00 step=97100, 10.6316 examples/sec lr=0.000004, loss=27.8049, loss_ll=4.79896, loss_ll_paf=7.42911, loss_ll_heat=2.1688, q=1000
[2018-07-11 09:34:09,428] [train] [INFO] epoch=12.00 step=97200, 10.6313 examples/sec lr=0.000004, loss=33.8845, loss_ll=6.08686, loss_ll_paf=8.74495, loss_ll_heat=3.42877, q=1000
[2018-07-11 09:36:42,892] [train] [INFO] epoch=12.00 step=97300, 10.6311 examples/sec lr=0.000004, loss=29.15, loss_ll=5.03007, loss_ll_paf=7.9035, loss_ll_heat=2.15665, q=1000
[2018-07-11 09:39:16,413] [train] [INFO] epoch=12.00 step=97400, 10.6309 examples/sec lr=0.000004, loss=22.4796, loss_ll=3.9478, loss_ll_paf=5.67078, loss_ll_heat=2.22482, q=1000
[2018-07-11 09:41:47,351] [train] [INFO] epoch=12.00 step=97500, 10.6308 examples/sec lr=0.000004, loss=50.6072, loss_ll=9.50266, loss_ll_paf=14.7614, loss_ll_heat=4.24392, q=1000
[2018-07-11 09:44:22,018] [train] [INFO] epoch=12.00 step=97600, 10.6305 examples/sec lr=0.000004, loss=40.3349, loss_ll=7.42396, loss_ll_paf=12.2185, loss_ll_heat=2.62943, q=1000
[2018-07-11 09:46:57,161] [train] [INFO] epoch=12.00 step=97700, 10.6302 examples/sec lr=0.000004, loss=58.3109, loss_ll=10.3129, loss_ll_paf=17.3334, loss_ll_heat=3.29242, q=1000
[2018-07-11 09:49:33,796] [train] [INFO] epoch=12.00 step=97800, 10.6298 examples/sec lr=0.000004, loss=37.0612, loss_ll=6.05634, loss_ll_paf=9.1805, loss_ll_heat=2.93218, q=1000
[2018-07-11 09:52:06,300] [train] [INFO] epoch=12.00 step=97900, 10.6296 examples/sec lr=0.000004, loss=29.2713, loss_ll=4.75148, loss_ll_paf=6.28297, loss_ll_heat=3.21999, q=1000
[2018-07-11 09:54:40,454] [train] [INFO] epoch=12.00 step=98000, 10.6294 examples/sec lr=0.000004, loss=33.2507, loss_ll=5.73835, loss_ll_paf=8.18133, loss_ll_heat=3.29537, q=1000
[2018-07-11 09:57:29,824] [train] [INFO] epoch=12.00 step=98100, 10.6280 examples/sec lr=0.000004, loss=41.0535, loss_ll=6.90861, loss_ll_paf=10.535, loss_ll_heat=3.28227, q=1000
[2018-07-11 10:00:04,870] [train] [INFO] epoch=12.00 step=98200, 10.6277 examples/sec lr=0.000004, loss=46.4188, loss_ll=8.31386, loss_ll_paf=13.0633, loss_ll_heat=3.56445, q=1000
[2018-07-11 10:02:39,563] [train] [INFO] epoch=12.00 step=98300, 10.6274 examples/sec lr=0.000004, loss=46.9517, loss_ll=8.97865, loss_ll_paf=12.9943, loss_ll_heat=4.96299, q=1000
[2018-07-11 10:05:13,135] [train] [INFO] epoch=12.00 step=98400, 10.6272 examples/sec lr=0.000004, loss=39.0627, loss_ll=6.85847, loss_ll_paf=10.6524, loss_ll_heat=3.06458, q=1000
[2018-07-11 10:07:47,945] [train] [INFO] epoch=12.00 step=98500, 10.6269 examples/sec lr=0.000004, loss=34.6128, loss_ll=6.22634, loss_ll_paf=10.0839, loss_ll_heat=2.36877, q=1000
[2018-07-11 10:10:23,206] [train] [INFO] epoch=12.00 step=98600, 10.6265 examples/sec lr=0.000004, loss=32.8084, loss_ll=6.04431, loss_ll_paf=9.02166, loss_ll_heat=3.06697, q=1000
[2018-07-11 10:12:57,750] [train] [INFO] epoch=12.00 step=98700, 10.6262 examples/sec lr=0.000004, loss=29.1913, loss_ll=5.42695, loss_ll_paf=7.35658, loss_ll_heat=3.49732, q=1000
[2018-07-11 10:15:31,623] [train] [INFO] epoch=12.00 step=98800, 10.6260 examples/sec lr=0.000004, loss=38.1291, loss_ll=5.69024, loss_ll_paf=8.56349, loss_ll_heat=2.817, q=1000
[2018-07-11 10:18:06,655] [train] [INFO] epoch=12.00 step=98900, 10.6257 examples/sec lr=0.000004, loss=25.597, loss_ll=4.51931, loss_ll_paf=5.98851, loss_ll_heat=3.0501, q=1000
[2018-07-11 10:20:40,659] [train] [INFO] epoch=13.00 step=99000, 10.6254 examples/sec lr=0.000004, loss=36.9621, loss_ll=6.76458, loss_ll_paf=11.2454, loss_ll_heat=2.28377, q=1000
[2018-07-11 10:23:27,689] [train] [INFO] epoch=13.00 step=99100, 10.6243 examples/sec lr=0.000004, loss=27.5646, loss_ll=4.51595, loss_ll_paf=6.05181, loss_ll_heat=2.98008, q=1000
[2018-07-11 10:26:03,964] [train] [INFO] epoch=13.00 step=99200, 10.6239 examples/sec lr=0.000004, loss=31.1304, loss_ll=4.88846, loss_ll_paf=7.11006, loss_ll_heat=2.66687, q=1000
[2018-07-11 10:28:36,559] [train] [INFO] epoch=13.00 step=99300, 10.6237 examples/sec lr=0.000004, loss=26.8065, loss_ll=4.58287, loss_ll_paf=6.26799, loss_ll_heat=2.89776, q=1000
[2018-07-11 10:31:11,496] [train] [INFO] epoch=13.00 step=99400, 10.6234 examples/sec lr=0.000004, loss=35.31, loss_ll=6.39966, loss_ll_paf=8.99878, loss_ll_heat=3.80054, q=1000
[2018-07-11 10:33:44,544] [train] [INFO] epoch=13.00 step=99500, 10.6232 examples/sec lr=0.000004, loss=35.2394, loss_ll=6.0381, loss_ll_paf=9.52024, loss_ll_heat=2.55595, q=1000
[2018-07-11 10:36:17,477] [train] [INFO] epoch=13.00 step=99600, 10.6231 examples/sec lr=0.000004, loss=33.1189, loss_ll=6.02351, loss_ll_paf=8.75862, loss_ll_heat=3.2884, q=1000
[2018-07-11 10:38:52,980] [train] [INFO] epoch=13.00 step=99700, 10.6227 examples/sec lr=0.000004, loss=28.4683, loss_ll=5.1054, loss_ll_paf=7.55308, loss_ll_heat=2.65772, q=1000
[2018-07-11 10:41:28,505] [train] [INFO] epoch=13.00 step=99800, 10.6224 examples/sec lr=0.000004, loss=42.8653, loss_ll=7.32561, loss_ll_paf=11.1685, loss_ll_heat=3.48277, q=1000
[2018-07-11 10:44:02,646] [train] [INFO] epoch=13.00 step=99900, 10.6221 examples/sec lr=0.000004, loss=42.8173, loss_ll=7.7922, loss_ll_paf=12.1703, loss_ll_heat=3.41414, q=1000
[2018-07-11 10:46:35,384] [train] [INFO] epoch=13.00 step=100000, 10.6220 examples/sec lr=0.000004, loss=37.9436, loss_ll=6.81993, loss_ll_paf=11.149, loss_ll_heat=2.49089, q=1000
[2018-07-11 10:49:24,792] [train] [INFO] epoch=13.00 step=100100, 10.6207 examples/sec lr=0.000004, loss=36.0758, loss_ll=6.35458, loss_ll_paf=9.7732, loss_ll_heat=2.93596, q=1000
[2018-07-11 10:51:59,510] [train] [INFO] epoch=13.00 step=100200, 10.6204 examples/sec lr=0.000004, loss=30.2127, loss_ll=5.27675, loss_ll_paf=7.6065, loss_ll_heat=2.94699, q=1000
[2018-07-11 10:54:37,675] [train] [INFO] epoch=13.00 step=100300, 10.6198 examples/sec lr=0.000004, loss=47.1557, loss_ll=8.10606, loss_ll_paf=13.4624, loss_ll_heat=2.74968, q=1000
[2018-07-11 10:57:19,963] [train] [INFO] epoch=13.00 step=100400, 10.6190 examples/sec lr=0.000004, loss=37.9342, loss_ll=6.19462, loss_ll_paf=9.91304, loss_ll_heat=2.4762, q=1000
[2018-07-11 11:00:01,875] [train] [INFO] epoch=13.00 step=100500, 10.6182 examples/sec lr=0.000004, loss=22.5455, loss_ll=3.9302, loss_ll_paf=5.69708, loss_ll_heat=2.16333, q=1000
[2018-07-11 11:02:42,880] [train] [INFO] epoch=13.00 step=100600, 10.6175 examples/sec lr=0.000004, loss=36.5619, loss_ll=6.65806, loss_ll_paf=10.1591, loss_ll_heat=3.15703, q=1000
[2018-07-11 11:05:23,660] [train] [INFO] epoch=13.00 step=100700, 10.6168 examples/sec lr=0.000004, loss=29.2009, loss_ll=5.07899, loss_ll_paf=6.67903, loss_ll_heat=3.47896, q=1000
[2018-07-11 11:08:07,325] [train] [INFO] epoch=13.00 step=100800, 10.6159 examples/sec lr=0.000004, loss=37.0455, loss_ll=6.77509, loss_ll_paf=10.5954, loss_ll_heat=2.95482, q=1000
[2018-07-11 11:10:50,241] [train] [INFO] epoch=13.00 step=100900, 10.6151 examples/sec lr=0.000004, loss=40.4513, loss_ll=6.98832, loss_ll_paf=10.0962, loss_ll_heat=3.88046, q=1000
[2018-07-11 11:13:32,459] [train] [INFO] epoch=13.00 step=101000, 10.6143 examples/sec lr=0.000004, loss=33.4149, loss_ll=6.01293, loss_ll_paf=8.9706, loss_ll_heat=3.05525, q=1000
[2018-07-11 11:16:28,793] [train] [INFO] epoch=13.00 step=101100, 10.6125 examples/sec lr=0.000004, loss=39.0259, loss_ll=7.03947, loss_ll_paf=11.1554, loss_ll_heat=2.92352, q=1000
[2018-07-11 11:19:10,283] [train] [INFO] epoch=13.00 step=101200, 10.6117 examples/sec lr=0.000004, loss=32.2968, loss_ll=5.82977, loss_ll_paf=8.6586, loss_ll_heat=3.00094, q=1000
[2018-07-11 11:21:52,328] [train] [INFO] epoch=13.00 step=101300, 10.6109 examples/sec lr=0.000004, loss=20.7683, loss_ll=3.71122, loss_ll_paf=4.8387, loss_ll_heat=2.58375, q=1000
[2018-07-11 11:24:32,394] [train] [INFO] epoch=13.00 step=101400, 10.6103 examples/sec lr=0.000004, loss=25.0742, loss_ll=4.19624, loss_ll_paf=5.60681, loss_ll_heat=2.78566, q=1000
[2018-07-11 11:27:13,102] [train] [INFO] epoch=13.00 step=101500, 10.6096 examples/sec lr=0.000004, loss=27.1856, loss_ll=4.64536, loss_ll_paf=7.17405, loss_ll_heat=2.11668, q=1000
[2018-07-11 11:29:53,183] [train] [INFO] epoch=13.00 step=101600, 10.6090 examples/sec lr=0.000004, loss=31.5723, loss_ll=5.68132, loss_ll_paf=7.813, loss_ll_heat=3.54963, q=1000
[2018-07-11 11:32:34,852] [train] [INFO] epoch=13.00 step=101700, 10.6082 examples/sec lr=0.000004, loss=30.4668, loss_ll=5.76641, loss_ll_paf=8.57034, loss_ll_heat=2.96248, q=1000
[2018-07-11 11:35:16,054] [train] [INFO] epoch=13.00 step=101800, 10.6075 examples/sec lr=0.000004, loss=33.1324, loss_ll=5.53398, loss_ll_paf=7.95662, loss_ll_heat=3.11134, q=1000
[2018-07-11 11:37:57,229] [train] [INFO] epoch=13.00 step=101900, 10.6068 examples/sec lr=0.000004, loss=24.5086, loss_ll=4.0496, loss_ll_paf=6.00352, loss_ll_heat=2.09569, q=1000
[2018-07-11 11:40:39,167] [train] [INFO] epoch=13.00 step=102000, 10.6060 examples/sec lr=0.000004, loss=23.1345, loss_ll=4.12407, loss_ll_paf=6.3467, loss_ll_heat=1.90143, q=1000
[2018-07-11 11:43:35,626] [train] [INFO] epoch=13.00 step=102100, 10.6043 examples/sec lr=0.000004, loss=35.7991, loss_ll=6.10987, loss_ll_paf=9.02779, loss_ll_heat=3.19195, q=1000
[2018-07-11 11:46:16,654] [train] [INFO] epoch=13.00 step=102200, 10.6036 examples/sec lr=0.000004, loss=32.2334, loss_ll=5.75383, loss_ll_paf=8.49344, loss_ll_heat=3.01421, q=1000
[2018-07-11 11:48:58,380] [train] [INFO] epoch=13.00 step=102300, 10.6028 examples/sec lr=0.000004, loss=32.4607, loss_ll=5.17251, loss_ll_paf=7.34317, loss_ll_heat=3.00185, q=1000
[2018-07-11 11:51:38,511] [train] [INFO] epoch=13.00 step=102400, 10.6022 examples/sec lr=0.000004, loss=25.4939, loss_ll=4.34283, loss_ll_paf=6.16101, loss_ll_heat=2.52464, q=1000
[2018-07-11 11:54:20,621] [train] [INFO] epoch=13.00 step=102500, 10.6014 examples/sec lr=0.000004, loss=39.7506, loss_ll=6.63942, loss_ll_paf=10.5906, loss_ll_heat=2.68819, q=1000
[2018-07-11 11:57:02,213] [train] [INFO] epoch=13.00 step=102600, 10.6007 examples/sec lr=0.000004, loss=32.4158, loss_ll=5.5318, loss_ll_paf=7.71853, loss_ll_heat=3.34508, q=1000
[2018-07-11 11:59:43,195] [train] [INFO] epoch=13.00 step=102700, 10.6000 examples/sec lr=0.000004, loss=36.0083, loss_ll=6.53976, loss_ll_paf=9.94089, loss_ll_heat=3.13864, q=1000
[2018-07-11 12:02:26,534] [train] [INFO] epoch=13.00 step=102800, 10.5992 examples/sec lr=0.000004, loss=38.6458, loss_ll=6.58274, loss_ll_paf=10.3177, loss_ll_heat=2.84783, q=1000
[2018-07-11 12:05:11,868] [train] [INFO] epoch=13.00 step=102900, 10.5982 examples/sec lr=0.000004, loss=41.9093, loss_ll=7.5607, loss_ll_paf=12.4414, loss_ll_heat=2.68002, q=1000
[2018-07-11 12:07:55,803] [train] [INFO] epoch=13.00 step=103000, 10.5973 examples/sec lr=0.000004, loss=36.6174, loss_ll=6.83407, loss_ll_paf=10.0923, loss_ll_heat=3.57582, q=1000
[2018-07-11 12:10:54,789] [train] [INFO] epoch=13.00 step=103100, 10.5954 examples/sec lr=0.000004, loss=36.5746, loss_ll=6.41763, loss_ll_paf=10.1593, loss_ll_heat=2.676, q=1000
[2018-07-11 12:13:39,642] [train] [INFO] epoch=13.00 step=103200, 10.5944 examples/sec lr=0.000004, loss=29.1052, loss_ll=4.84299, loss_ll_paf=6.78712, loss_ll_heat=2.89887, q=1000
[2018-07-11 12:16:22,791] [train] [INFO] epoch=13.00 step=103300, 10.5936 examples/sec lr=0.000004, loss=28.2029, loss_ll=5.28297, loss_ll_paf=7.66514, loss_ll_heat=2.90081, q=1000
[2018-07-11 12:19:08,092] [train] [INFO] epoch=13.00 step=103400, 10.5927 examples/sec lr=0.000004, loss=36.5409, loss_ll=6.6113, loss_ll_paf=10.7247, loss_ll_heat=2.49792, q=1000
[2018-07-11 12:21:51,157] [train] [INFO] epoch=13.00 step=103500, 10.5918 examples/sec lr=0.000004, loss=49.4238, loss_ll=9.36066, loss_ll_paf=15.9364, loss_ll_heat=2.78495, q=1000
[2018-07-11 12:24:32,343] [train] [INFO] epoch=13.00 step=103600, 10.5912 examples/sec lr=0.000004, loss=40.6173, loss_ll=6.87091, loss_ll_paf=9.86266, loss_ll_heat=3.87915, q=1000
[2018-07-11 12:27:16,865] [train] [INFO] epoch=13.00 step=103700, 10.5902 examples/sec lr=0.000004, loss=55.5638, loss_ll=10.2552, loss_ll_paf=17.0626, loss_ll_heat=3.44791, q=1000
[2018-07-11 12:30:01,503] [train] [INFO] epoch=13.00 step=103800, 10.5893 examples/sec lr=0.000004, loss=31.3587, loss_ll=5.68664, loss_ll_paf=8.13516, loss_ll_heat=3.23812, q=1000
[2018-07-11 12:32:42,657] [train] [INFO] epoch=13.00 step=103900, 10.5887 examples/sec lr=0.000004, loss=33.8384, loss_ll=6.08823, loss_ll_paf=8.94215, loss_ll_heat=3.23431, q=1000
[2018-07-11 12:35:26,496] [train] [INFO] epoch=13.00 step=104000, 10.5878 examples/sec lr=0.000004, loss=23.7243, loss_ll=3.89811, loss_ll_paf=5.60932, loss_ll_heat=2.1869, q=1000
[2018-07-11 12:38:26,537] [train] [INFO] epoch=13.00 step=104100, 10.5858 examples/sec lr=0.000004, loss=27.3087, loss_ll=5.06503, loss_ll_paf=7.59424, loss_ll_heat=2.53582, q=1000
[2018-07-11 12:41:11,856] [train] [INFO] epoch=13.00 step=104200, 10.5849 examples/sec lr=0.000004, loss=24.1355, loss_ll=4.13217, loss_ll_paf=5.46909, loss_ll_heat=2.79525, q=1000
[2018-07-11 12:43:57,256] [train] [INFO] epoch=13.00 step=104300, 10.5839 examples/sec lr=0.000004, loss=36.2356, loss_ll=6.42354, loss_ll_paf=11.2441, loss_ll_heat=1.603, q=1000
[2018-07-11 12:46:40,061] [train] [INFO] epoch=13.00 step=104400, 10.5832 examples/sec lr=0.000004, loss=31.1852, loss_ll=5.86705, loss_ll_paf=8.94155, loss_ll_heat=2.79254, q=1000
[2018-07-11 12:49:21,722] [train] [INFO] epoch=13.00 step=104500, 10.5825 examples/sec lr=0.000004, loss=35.5978, loss_ll=6.63891, loss_ll_paf=11.0042, loss_ll_heat=2.27361, q=1000
[2018-07-11 12:52:03,651] [train] [INFO] epoch=13.00 step=104600, 10.5817 examples/sec lr=0.000004, loss=36.1677, loss_ll=6.92291, loss_ll_paf=10.2755, loss_ll_heat=3.57029, q=1000
[2018-07-11 12:54:45,881] [train] [INFO] epoch=13.00 step=104700, 10.5810 examples/sec lr=0.000004, loss=25.2321, loss_ll=4.30939, loss_ll_paf=6.09349, loss_ll_heat=2.52528, q=1000
[2018-07-11 12:57:27,808] [train] [INFO] epoch=13.00 step=104800, 10.5803 examples/sec lr=0.000004, loss=35.7603, loss_ll=6.65345, loss_ll_paf=9.95615, loss_ll_heat=3.35076, q=1000
[2018-07-11 13:00:10,968] [train] [INFO] epoch=13.00 step=104900, 10.5795 examples/sec lr=0.000004, loss=44.5072, loss_ll=8.55801, loss_ll_paf=13.7663, loss_ll_heat=3.34968, q=1000
[2018-07-11 13:02:55,401] [train] [INFO] epoch=13.00 step=105000, 10.5786 examples/sec lr=0.000004, loss=30.7576, loss_ll=5.15028, loss_ll_paf=7.88923, loss_ll_heat=2.41134, q=1000
[2018-07-11 13:05:53,203] [train] [INFO] epoch=13.00 step=105100, 10.5768 examples/sec lr=0.000004, loss=41.4512, loss_ll=7.48755, loss_ll_paf=10.7607, loss_ll_heat=4.21436, q=1000
[2018-07-11 13:08:36,491] [train] [INFO] epoch=13.00 step=105200, 10.5760 examples/sec lr=0.000004, loss=26.3202, loss_ll=4.8494, loss_ll_paf=7.12822, loss_ll_heat=2.57059, q=1000
[2018-07-11 13:11:18,781] [train] [INFO] epoch=13.00 step=105300, 10.5753 examples/sec lr=0.000004, loss=36.0971, loss_ll=6.5011, loss_ll_paf=11.2894, loss_ll_heat=1.71275, q=1000
[2018-07-11 13:14:00,925] [train] [INFO] epoch=13.00 step=105400, 10.5746 examples/sec lr=0.000004, loss=24.3712, loss_ll=4.2679, loss_ll_paf=5.9991, loss_ll_heat=2.53669, q=1000
[2018-07-11 13:16:43,188] [train] [INFO] epoch=13.00 step=105500, 10.5739 examples/sec lr=0.000004, loss=32.1734, loss_ll=5.60203, loss_ll_paf=7.59645, loss_ll_heat=3.60761, q=1000
[2018-07-11 13:19:25,642] [train] [INFO] epoch=13.00 step=105600, 10.5731 examples/sec lr=0.000004, loss=32.0641, loss_ll=5.75029, loss_ll_paf=8.70304, loss_ll_heat=2.79754, q=1000
[2018-07-11 13:22:06,662] [train] [INFO] epoch=13.00 step=105700, 10.5725 examples/sec lr=0.000004, loss=27.4682, loss_ll=5.15022, loss_ll_paf=7.80643, loss_ll_heat=2.49401, q=1000
[2018-07-11 13:24:50,448] [train] [INFO] epoch=13.00 step=105800, 10.5717 examples/sec lr=0.000004, loss=31.0345, loss_ll=5.01267, loss_ll_paf=7.6793, loss_ll_heat=2.34604, q=1000
[2018-07-11 13:27:33,272] [train] [INFO] epoch=13.00 step=105900, 10.5709 examples/sec lr=0.000004, loss=22.21, loss_ll=3.86252, loss_ll_paf=5.83748, loss_ll_heat=1.88757, q=1000
[2018-07-11 13:30:15,728] [train] [INFO] epoch=13.00 step=106000, 10.5702 examples/sec lr=0.000004, loss=32.1122, loss_ll=4.98491, loss_ll_paf=7.22721, loss_ll_heat=2.7426, q=1000
[2018-07-11 13:33:12,738] [train] [INFO] epoch=13.00 step=106100, 10.5685 examples/sec lr=0.000004, loss=20.6477, loss_ll=3.62206, loss_ll_paf=5.22786, loss_ll_heat=2.01626, q=1000
[2018-07-11 13:35:54,655] [train] [INFO] epoch=13.00 step=106200, 10.5678 examples/sec lr=0.000004, loss=29.4498, loss_ll=5.39752, loss_ll_paf=8.28792, loss_ll_heat=2.50712, q=1000
[2018-07-11 13:38:36,258] [train] [INFO] epoch=13.00 step=106300, 10.5671 examples/sec lr=0.000004, loss=25.8657, loss_ll=4.36441, loss_ll_paf=6.08156, loss_ll_heat=2.64727, q=1000
[2018-07-11 13:41:16,767] [train] [INFO] epoch=13.00 step=106400, 10.5665 examples/sec lr=0.000004, loss=23.7362, loss_ll=4.16092, loss_ll_paf=6.00823, loss_ll_heat=2.31362, q=1000
[2018-07-11 13:43:58,257] [train] [INFO] epoch=13.00 step=106500, 10.5659 examples/sec lr=0.000004, loss=37.7318, loss_ll=6.23795, loss_ll_paf=9.25103, loss_ll_heat=3.22487, q=1000
[2018-07-11 13:46:37,632] [train] [INFO] epoch=14.00 step=106600, 10.5654 examples/sec lr=0.000004, loss=34.5816, loss_ll=6.05408, loss_ll_paf=9.59148, loss_ll_heat=2.51669, q=1000
[2018-07-11 13:49:18,441] [train] [INFO] epoch=14.00 step=106700, 10.5647 examples/sec lr=0.000004, loss=22.6973, loss_ll=3.91548, loss_ll_paf=5.61602, loss_ll_heat=2.21494, q=1000
[2018-07-11 13:51:59,488] [train] [INFO] epoch=14.00 step=106800, 10.5641 examples/sec lr=0.000004, loss=23.1712, loss_ll=3.77065, loss_ll_paf=5.47163, loss_ll_heat=2.06966, q=1000
[2018-07-11 13:54:43,107] [train] [INFO] epoch=14.00 step=106900, 10.5633 examples/sec lr=0.000004, loss=28.5786, loss_ll=4.427, loss_ll_paf=6.2244, loss_ll_heat=2.6296, q=1000
[2018-07-11 13:57:27,457] [train] [INFO] epoch=14.00 step=107000, 10.5625 examples/sec lr=0.000004, loss=24.7909, loss_ll=4.28761, loss_ll_paf=6.13513, loss_ll_heat=2.44009, q=1000
[2018-07-11 14:00:22,165] [train] [INFO] epoch=14.00 step=107100, 10.5610 examples/sec lr=0.000004, loss=32.5718, loss_ll=6.17506, loss_ll_paf=9.98225, loss_ll_heat=2.36788, q=1000
[2018-07-11 14:03:01,743] [train] [INFO] epoch=14.00 step=107200, 10.5604 examples/sec lr=0.000004, loss=40.4804, loss_ll=7.23827, loss_ll_paf=11.0584, loss_ll_heat=3.41813, q=1000
[2018-07-11 14:05:42,313] [train] [INFO] epoch=14.00 step=107300, 10.5599 examples/sec lr=0.000004, loss=35.6739, loss_ll=6.2001, loss_ll_paf=10.0175, loss_ll_heat=2.38272, q=1000
[2018-07-11 14:08:23,371] [train] [INFO] epoch=14.00 step=107400, 10.5592 examples/sec lr=0.000004, loss=22.8586, loss_ll=3.74117, loss_ll_paf=5.156, loss_ll_heat=2.32634, q=1000
[2018-07-11 14:11:02,649] [train] [INFO] epoch=14.00 step=107500, 10.5587 examples/sec lr=0.000004, loss=28.5649, loss_ll=4.5024, loss_ll_paf=6.54372, loss_ll_heat=2.46107, q=1000
[2018-07-11 14:13:44,563] [train] [INFO] epoch=14.00 step=107600, 10.5581 examples/sec lr=0.000004, loss=47.1375, loss_ll=8.3189, loss_ll_paf=12.9319, loss_ll_heat=3.7059, q=1000
[2018-07-11 14:16:23,736] [train] [INFO] epoch=14.00 step=107700, 10.5576 examples/sec lr=0.000004, loss=29.7622, loss_ll=5.29249, loss_ll_paf=8.77537, loss_ll_heat=1.80962, q=1000
[2018-07-11 14:19:04,603] [train] [INFO] epoch=14.00 step=107800, 10.5570 examples/sec lr=0.000004, loss=30.1589, loss_ll=5.06729, loss_ll_paf=7.69812, loss_ll_heat=2.43646, q=1000
[2018-07-11 14:21:42,965] [train] [INFO] epoch=14.00 step=107900, 10.5565 examples/sec lr=0.000004, loss=22.5213, loss_ll=4.16527, loss_ll_paf=5.67415, loss_ll_heat=2.65639, q=1000
[2018-07-11 14:24:25,427] [train] [INFO] epoch=14.00 step=108000, 10.5558 examples/sec lr=0.000004, loss=34.42, loss_ll=5.56188, loss_ll_paf=8.48404, loss_ll_heat=2.63972, q=1000
[2018-07-11 14:27:20,508] [train] [INFO] epoch=14.00 step=108100, 10.5543 examples/sec lr=0.000004, loss=31.5439, loss_ll=5.54083, loss_ll_paf=7.8915, loss_ll_heat=3.19016, q=1000
[2018-07-11 14:30:01,578] [train] [INFO] epoch=14.00 step=108200, 10.5537 examples/sec lr=0.000004, loss=38.5811, loss_ll=6.88191, loss_ll_paf=11.0034, loss_ll_heat=2.76046, q=1000
[2018-07-11 14:32:44,092] [train] [INFO] epoch=14.00 step=108300, 10.5530 examples/sec lr=0.000004, loss=28.5341, loss_ll=5.21326, loss_ll_paf=7.80977, loss_ll_heat=2.61676, q=1000
[2018-07-11 14:35:27,070] [train] [INFO] epoch=14.00 step=108400, 10.5523 examples/sec lr=0.000004, loss=43.9086, loss_ll=8.23141, loss_ll_paf=13.1121, loss_ll_heat=3.35071, q=1000
[2018-07-11 14:38:07,127] [train] [INFO] epoch=14.00 step=108500, 10.5517 examples/sec lr=0.000004, loss=29.0107, loss_ll=5.15197, loss_ll_paf=7.63962, loss_ll_heat=2.66432, q=1000
[2018-07-11 14:40:48,819] [train] [INFO] epoch=14.00 step=108600, 10.5511 examples/sec lr=0.000004, loss=37.0077, loss_ll=6.72655, loss_ll_paf=9.96854, loss_ll_heat=3.48455, q=1000
[2018-07-11 14:43:29,610] [train] [INFO] epoch=14.00 step=108700, 10.5505 examples/sec lr=0.000004, loss=16.5796, loss_ll=2.63279, loss_ll_paf=3.7925, loss_ll_heat=1.47309, q=1000
[2018-07-11 14:46:11,514] [train] [INFO] epoch=14.00 step=108800, 10.5498 examples/sec lr=0.000004, loss=22.2286, loss_ll=3.535, loss_ll_paf=4.65432, loss_ll_heat=2.41568, q=1000
[2018-07-11 14:48:53,818] [train] [INFO] epoch=14.00 step=108900, 10.5492 examples/sec lr=0.000004, loss=26.8577, loss_ll=4.97771, loss_ll_paf=7.35844, loss_ll_heat=2.59699, q=1000
[2018-07-11 14:51:35,841] [train] [INFO] epoch=14.00 step=109000, 10.5485 examples/sec lr=0.000004, loss=24.2943, loss_ll=4.01584, loss_ll_paf=5.65099, loss_ll_heat=2.38069, q=1000
[2018-07-11 14:54:30,168] [train] [INFO] epoch=14.00 step=109100, 10.5471 examples/sec lr=0.000004, loss=26.0399, loss_ll=4.66211, loss_ll_paf=6.78111, loss_ll_heat=2.5431, q=1000
[2018-07-11 14:57:13,875] [train] [INFO] epoch=14.00 step=109200, 10.5463 examples/sec lr=0.000004, loss=39.3553, loss_ll=7.13465, loss_ll_paf=11.6497, loss_ll_heat=2.61957, q=1000
[2018-07-11 14:59:55,236] [train] [INFO] epoch=14.00 step=109300, 10.5457 examples/sec lr=0.000004, loss=26.4291, loss_ll=4.77177, loss_ll_paf=7.14365, loss_ll_heat=2.39989, q=1000
[2018-07-11 15:02:37,481] [train] [INFO] epoch=14.00 step=109400, 10.5450 examples/sec lr=0.000004, loss=50.7416, loss_ll=9.30663, loss_ll_paf=15.8037, loss_ll_heat=2.80961, q=1000
[2018-07-11 15:05:20,267] [train] [INFO] epoch=14.00 step=109500, 10.5443 examples/sec lr=0.000004, loss=31.5874, loss_ll=5.45711, loss_ll_paf=8.04338, loss_ll_heat=2.87084, q=1000
[2018-07-11 15:08:03,978] [train] [INFO] epoch=14.00 step=109600, 10.5436 examples/sec lr=0.000004, loss=35.8765, loss_ll=6.52765, loss_ll_paf=10.0134, loss_ll_heat=3.04188, q=1000
[2018-07-11 15:10:46,304] [train] [INFO] epoch=14.00 step=109700, 10.5429 examples/sec lr=0.000004, loss=39.2423, loss_ll=6.95276, loss_ll_paf=10.5342, loss_ll_heat=3.37128, q=1000
[2018-07-11 15:13:29,717] [train] [INFO] epoch=14.00 step=109800, 10.5422 examples/sec lr=0.000004, loss=26.9735, loss_ll=4.87076, loss_ll_paf=7.18853, loss_ll_heat=2.55298, q=1000
[2018-07-11 15:16:12,667] [train] [INFO] epoch=14.00 step=109900, 10.5414 examples/sec lr=0.000004, loss=30.0004, loss_ll=5.2719, loss_ll_paf=7.9375, loss_ll_heat=2.60631, q=1000
[2018-07-11 15:18:54,693] [train] [INFO] epoch=14.00 step=110000, 10.5408 examples/sec lr=0.000004, loss=26.3367, loss_ll=4.5288, loss_ll_paf=6.63176, loss_ll_heat=2.42583, q=1000
[2018-07-11 15:21:52,513] [train] [INFO] epoch=14.00 step=110100, 10.5392 examples/sec lr=0.000004, loss=25.0977, loss_ll=4.11555, loss_ll_paf=5.8169, loss_ll_heat=2.41421, q=1000
[2018-07-11 15:24:37,311] [train] [INFO] epoch=14.00 step=110200, 10.5383 examples/sec lr=0.000004, loss=32.7417, loss_ll=6.1026, loss_ll_paf=9.27271, loss_ll_heat=2.93249, q=1000
[2018-07-11 15:27:18,375] [train] [INFO] epoch=14.00 step=110300, 10.5378 examples/sec lr=0.000004, loss=37.5738, loss_ll=6.91169, loss_ll_paf=10.1284, loss_ll_heat=3.695, q=1000
[2018-07-11 15:29:58,922] [train] [INFO] epoch=14.00 step=110400, 10.5372 examples/sec lr=0.000004, loss=34.1046, loss_ll=5.92087, loss_ll_paf=9.22073, loss_ll_heat=2.62101, q=1000
[2018-07-11 15:32:41,920] [train] [INFO] epoch=14.00 step=110500, 10.5365 examples/sec lr=0.000004, loss=26.9344, loss_ll=4.70176, loss_ll_paf=6.30341, loss_ll_heat=3.10011, q=1000
[2018-07-11 15:35:24,888] [train] [INFO] epoch=14.00 step=110600, 10.5358 examples/sec lr=0.000004, loss=27.4236, loss_ll=4.6833, loss_ll_paf=6.74336, loss_ll_heat=2.62325, q=1000
[2018-07-11 15:38:06,807] [train] [INFO] epoch=14.00 step=110700, 10.5352 examples/sec lr=0.000004, loss=36.3866, loss_ll=5.94363, loss_ll_paf=9.52518, loss_ll_heat=2.36207, q=1000
[2018-07-11 15:40:49,549] [train] [INFO] epoch=14.00 step=110800, 10.5345 examples/sec lr=0.000004, loss=30.9671, loss_ll=5.53227, loss_ll_paf=8.40439, loss_ll_heat=2.66016, q=1000
[2018-07-11 15:43:30,964] [train] [INFO] epoch=14.00 step=110900, 10.5339 examples/sec lr=0.000004, loss=29.8737, loss_ll=5.08983, loss_ll_paf=7.34733, loss_ll_heat=2.83232, q=1000
[2018-07-11 15:46:14,360] [train] [INFO] epoch=14.00 step=111000, 10.5332 examples/sec lr=0.000004, loss=18.2455, loss_ll=2.94424, loss_ll_paf=3.82365, loss_ll_heat=2.06483, q=1000
[2018-07-11 15:49:11,107] [train] [INFO] epoch=14.00 step=111100, 10.5316 examples/sec lr=0.000004, loss=34.2106, loss_ll=6.46246, loss_ll_paf=10.0057, loss_ll_heat=2.91926, q=1000
[2018-07-11 15:51:55,222] [train] [INFO] epoch=14.00 step=111200, 10.5309 examples/sec lr=0.000004, loss=25.0777, loss_ll=4.1562, loss_ll_paf=5.99316, loss_ll_heat=2.31924, q=1000
[2018-07-11 15:54:37,234] [train] [INFO] epoch=14.00 step=111300, 10.5302 examples/sec lr=0.000004, loss=22.3504, loss_ll=3.50165, loss_ll_paf=4.81166, loss_ll_heat=2.19165, q=1000
[2018-07-11 15:57:21,472] [train] [INFO] epoch=14.00 step=111400, 10.5295 examples/sec lr=0.000004, loss=39.7586, loss_ll=7.06128, loss_ll_paf=10.6731, loss_ll_heat=3.4495, q=1000
[2018-07-11 16:00:03,887] [train] [INFO] epoch=14.00 step=111500, 10.5288 examples/sec lr=0.000004, loss=27.1371, loss_ll=4.67069, loss_ll_paf=6.7043, loss_ll_heat=2.63708, q=1000
[2018-07-11 16:02:43,753] [train] [INFO] epoch=14.00 step=111600, 10.5283 examples/sec lr=0.000004, loss=40.7203, loss_ll=7.28978, loss_ll_paf=11.5342, loss_ll_heat=3.04537, q=1000
[2018-07-11 16:05:24,492] [train] [INFO] epoch=14.00 step=111700, 10.5278 examples/sec lr=0.000004, loss=37.3165, loss_ll=6.74215, loss_ll_paf=10.2344, loss_ll_heat=3.24991, q=1000
[2018-07-11 16:08:08,952] [train] [INFO] epoch=14.00 step=111800, 10.5270 examples/sec lr=0.000004, loss=46.4463, loss_ll=8.44183, loss_ll_paf=13.6439, loss_ll_heat=3.23972, q=1000
[2018-07-11 16:10:52,038] [train] [INFO] epoch=14.00 step=111900, 10.5263 examples/sec lr=0.000004, loss=33.1772, loss_ll=5.71632, loss_ll_paf=8.57268, loss_ll_heat=2.85997, q=1000
[2018-07-11 16:13:36,045] [train] [INFO] epoch=14.00 step=112000, 10.5256 examples/sec lr=0.000004, loss=28.5761, loss_ll=5.31121, loss_ll_paf=8.31871, loss_ll_heat=2.30371, q=1000
[2018-07-11 16:16:32,867] [train] [INFO] epoch=14.00 step=112100, 10.5241 examples/sec lr=0.000004, loss=38.2149, loss_ll=6.73546, loss_ll_paf=10.2133, loss_ll_heat=3.25763, q=1000
[2018-07-11 16:19:15,514] [train] [INFO] epoch=14.00 step=112200, 10.5234 examples/sec lr=0.000004, loss=31.9171, loss_ll=5.44236, loss_ll_paf=8.11368, loss_ll_heat=2.77104, q=1000
[2018-07-11 16:21:58,981] [train] [INFO] epoch=14.00 step=112300, 10.5227 examples/sec lr=0.000004, loss=25.5055, loss_ll=4.31911, loss_ll_paf=6.06362, loss_ll_heat=2.57459, q=1000
[2018-07-11 16:24:41,557] [train] [INFO] epoch=14.00 step=112400, 10.5221 examples/sec lr=0.000004, loss=24.7337, loss_ll=4.43428, loss_ll_paf=5.98489, loss_ll_heat=2.88368, q=1000
[2018-07-11 16:27:25,573] [train] [INFO] epoch=14.00 step=112500, 10.5213 examples/sec lr=0.000004, loss=26.9473, loss_ll=4.61333, loss_ll_paf=7.11622, loss_ll_heat=2.11044, q=1000
[2018-07-11 16:30:08,335] [train] [INFO] epoch=14.00 step=112600, 10.5207 examples/sec lr=0.000004, loss=26.9265, loss_ll=4.55965, loss_ll_paf=6.27894, loss_ll_heat=2.84037, q=1000
[2018-07-11 16:32:49,238] [train] [INFO] epoch=14.00 step=112700, 10.5201 examples/sec lr=0.000004, loss=25.5309, loss_ll=4.49551, loss_ll_paf=6.22322, loss_ll_heat=2.7678, q=1000
[2018-07-11 16:35:33,919] [train] [INFO] epoch=14.00 step=112800, 10.5194 examples/sec lr=0.000004, loss=30.5644, loss_ll=5.05467, loss_ll_paf=7.49828, loss_ll_heat=2.61107, q=1000
[2018-07-11 16:38:15,915] [train] [INFO] epoch=14.00 step=112900, 10.5187 examples/sec lr=0.000004, loss=30.9015, loss_ll=5.15372, loss_ll_paf=7.64089, loss_ll_heat=2.66654, q=1000
[2018-07-11 16:40:59,458] [train] [INFO] epoch=14.00 step=113000, 10.5180 examples/sec lr=0.000004, loss=26.9545, loss_ll=4.51047, loss_ll_paf=6.49815, loss_ll_heat=2.52279, q=1000
[2018-07-11 16:43:54,709] [train] [INFO] epoch=14.00 step=113100, 10.5166 examples/sec lr=0.000004, loss=39.6048, loss_ll=6.56613, loss_ll_paf=10.0858, loss_ll_heat=3.04651, q=1000
[2018-07-11 16:46:36,352] [train] [INFO] epoch=14.00 step=113200, 10.5161 examples/sec lr=0.000004, loss=24.1651, loss_ll=3.83192, loss_ll_paf=5.24106, loss_ll_heat=2.42279, q=1000
[2018-07-11 16:49:19,252] [train] [INFO] epoch=14.00 step=113300, 10.5154 examples/sec lr=0.000004, loss=25.9908, loss_ll=4.49015, loss_ll_paf=5.91543, loss_ll_heat=3.06486, q=1000
[2018-07-11 16:52:02,562] [train] [INFO] epoch=14.00 step=113400, 10.5147 examples/sec lr=0.000004, loss=32.8574, loss_ll=5.37036, loss_ll_paf=7.86098, loss_ll_heat=2.87974, q=1000
[2018-07-11 16:54:44,352] [train] [INFO] epoch=14.00 step=113500, 10.5141 examples/sec lr=0.000004, loss=37.868, loss_ll=6.88467, loss_ll_paf=10.1881, loss_ll_heat=3.58121, q=1000
[2018-07-11 16:57:25,849] [train] [INFO] epoch=14.00 step=113600, 10.5136 examples/sec lr=0.000004, loss=33.1875, loss_ll=5.61443, loss_ll_paf=8.11697, loss_ll_heat=3.11188, q=1000
[2018-07-11 17:00:08,831] [train] [INFO] epoch=14.00 step=113700, 10.5129 examples/sec lr=0.000004, loss=22.186, loss_ll=3.95077, loss_ll_paf=5.34595, loss_ll_heat=2.5556, q=1000
[2018-07-11 17:02:52,129] [train] [INFO] epoch=14.00 step=113800, 10.5122 examples/sec lr=0.000004, loss=46.9168, loss_ll=8.62359, loss_ll_paf=13.9067, loss_ll_heat=3.34049, q=1000
[2018-07-11 17:05:35,559] [train] [INFO] epoch=14.00 step=113900, 10.5116 examples/sec lr=0.000004, loss=27.1922, loss_ll=4.50312, loss_ll_paf=6.36912, loss_ll_heat=2.63713, q=1000
[2018-07-11 17:08:18,148] [train] [INFO] epoch=14.00 step=114000, 10.5109 examples/sec lr=0.000004, loss=24.798, loss_ll=4.33078, loss_ll_paf=6.17416, loss_ll_heat=2.48739, q=1000
[2018-07-11 17:11:15,219] [train] [INFO] epoch=14.00 step=114100, 10.5094 examples/sec lr=0.000004, loss=22.8788, loss_ll=3.88908, loss_ll_paf=5.60854, loss_ll_heat=2.16961, q=1000
[2018-07-11 17:13:58,822] [train] [INFO] epoch=15.00 step=114200, 10.5087 examples/sec lr=0.000004, loss=30.9971, loss_ll=5.65842, loss_ll_paf=9.04572, loss_ll_heat=2.27111, q=1000
[2018-07-11 17:16:41,053] [train] [INFO] epoch=15.00 step=114300, 10.5081 examples/sec lr=0.000004, loss=31.0297, loss_ll=5.23999, loss_ll_paf=7.81422, loss_ll_heat=2.66577, q=1000
[2018-07-11 17:19:25,388] [train] [INFO] epoch=15.00 step=114400, 10.5074 examples/sec lr=0.000004, loss=32.6317, loss_ll=5.64032, loss_ll_paf=8.68429, loss_ll_heat=2.59635, q=1000
[2018-07-11 17:22:07,977] [train] [INFO] epoch=15.00 step=114500, 10.5068 examples/sec lr=0.000004, loss=26.4385, loss_ll=3.9439, loss_ll_paf=5.88145, loss_ll_heat=2.00634, q=1000
[2018-07-11 17:24:50,934] [train] [INFO] epoch=15.00 step=114600, 10.5061 examples/sec lr=0.000004, loss=37.5747, loss_ll=6.68037, loss_ll_paf=9.57575, loss_ll_heat=3.78498, q=1000
[2018-07-11 17:27:33,303] [train] [INFO] epoch=15.00 step=114700, 10.5055 examples/sec lr=0.000004, loss=29.3642, loss_ll=5.33084, loss_ll_paf=7.69128, loss_ll_heat=2.9704, q=1000
[2018-07-11 17:30:17,191] [train] [INFO] epoch=15.00 step=114800, 10.5048 examples/sec lr=0.000004, loss=24.6728, loss_ll=4.08034, loss_ll_paf=5.37588, loss_ll_heat=2.78481, q=1000
[2018-07-11 17:33:00,448] [train] [INFO] epoch=15.00 step=114900, 10.5042 examples/sec lr=0.000004, loss=29.7912, loss_ll=5.13535, loss_ll_paf=7.75607, loss_ll_heat=2.51463, q=1000
[2018-07-11 17:35:45,380] [train] [INFO] epoch=15.00 step=115000, 10.5034 examples/sec lr=0.000004, loss=41.587, loss_ll=8.04833, loss_ll_paf=13.771, loss_ll_heat=2.3257, q=1000
[2018-07-11 17:38:43,483] [train] [INFO] epoch=15.00 step=115100, 10.5019 examples/sec lr=0.000004, loss=24.9357, loss_ll=4.34339, loss_ll_paf=6.13821, loss_ll_heat=2.54857, q=1000
[2018-07-11 17:41:27,391] [train] [INFO] epoch=15.00 step=115200, 10.5012 examples/sec lr=0.000004, loss=36.5352, loss_ll=6.56837, loss_ll_paf=10.1301, loss_ll_heat=3.00662, q=1000
[2018-07-11 17:44:10,416] [train] [INFO] epoch=15.00 step=115300, 10.5006 examples/sec lr=0.000004, loss=28.6707, loss_ll=5.09398, loss_ll_paf=6.78142, loss_ll_heat=3.40655, q=1000
[2018-07-11 17:46:51,985] [train] [INFO] epoch=15.00 step=115400, 10.5000 examples/sec lr=0.000004, loss=19.9608, loss_ll=3.38334, loss_ll_paf=4.41149, loss_ll_heat=2.3552, q=1000
[2018-07-11 17:49:32,214] [train] [INFO] epoch=15.00 step=115500, 10.4995 examples/sec lr=0.000004, loss=27.1216, loss_ll=5.01637, loss_ll_paf=7.41189, loss_ll_heat=2.62085, q=1000
[2018-07-11 17:52:15,809] [train] [INFO] epoch=15.00 step=115600, 10.4989 examples/sec lr=0.000004, loss=26.5143, loss_ll=4.61151, loss_ll_paf=6.00537, loss_ll_heat=3.21765, q=1000
[2018-07-11 17:54:58,057] [train] [INFO] epoch=15.00 step=115700, 10.4983 examples/sec lr=0.000004, loss=33.061, loss_ll=5.60123, loss_ll_paf=8.23942, loss_ll_heat=2.96304, q=1000
[2018-07-11 17:57:41,619] [train] [INFO] epoch=15.00 step=115800, 10.4976 examples/sec lr=0.000004, loss=31.9719, loss_ll=5.62597, loss_ll_paf=8.58611, loss_ll_heat=2.66583, q=1000
[2018-07-11 18:00:23,414] [train] [INFO] epoch=15.00 step=115900, 10.4971 examples/sec lr=0.000004, loss=36.9451, loss_ll=5.6616, loss_ll_paf=8.23762, loss_ll_heat=3.08558, q=1000
[2018-07-11 18:03:07,619] [train] [INFO] epoch=15.00 step=116000, 10.4964 examples/sec lr=0.000004, loss=35.9025, loss_ll=5.96251, loss_ll_paf=8.58013, loss_ll_heat=3.34489, q=1000
[2018-07-11 18:06:04,472] [train] [INFO] epoch=15.00 step=116100, 10.4949 examples/sec lr=0.000004, loss=33.6709, loss_ll=5.62658, loss_ll_paf=9.22355, loss_ll_heat=2.02961, q=1000
[2018-07-11 18:08:47,498] [train] [INFO] epoch=15.00 step=116200, 10.4943 examples/sec lr=0.000004, loss=29.9902, loss_ll=5.40769, loss_ll_paf=7.78382, loss_ll_heat=3.03156, q=1000
[2018-07-11 18:11:27,473] [train] [INFO] epoch=15.00 step=116300, 10.4938 examples/sec lr=0.000004, loss=23.0672, loss_ll=3.87289, loss_ll_paf=5.52754, loss_ll_heat=2.21823, q=1000
[2018-07-11 18:14:08,191] [train] [INFO] epoch=15.00 step=116400, 10.4934 examples/sec lr=0.000004, loss=31.2308, loss_ll=5.70265, loss_ll_paf=8.83237, loss_ll_heat=2.57293, q=1000
[2018-07-11 18:16:48,100] [train] [INFO] epoch=15.00 step=116500, 10.4929 examples/sec lr=0.000004, loss=41.7666, loss_ll=7.61699, loss_ll_paf=11.73, loss_ll_heat=3.50397, q=1000
[2018-07-11 18:19:30,688] [train] [INFO] epoch=15.00 step=116600, 10.4923 examples/sec lr=0.000004, loss=34.7924, loss_ll=5.9208, loss_ll_paf=8.53789, loss_ll_heat=3.30371, q=1000
[2018-07-11 18:22:09,574] [train] [INFO] epoch=15.00 step=116700, 10.4919 examples/sec lr=0.000004, loss=38.7277, loss_ll=7.40591, loss_ll_paf=11.9988, loss_ll_heat=2.81299, q=1000
[2018-07-11 18:24:52,726] [train] [INFO] epoch=15.00 step=116800, 10.4913 examples/sec lr=0.000004, loss=22.8003, loss_ll=3.99523, loss_ll_paf=5.29452, loss_ll_heat=2.69593, q=1000
[2018-07-11 18:27:36,297] [train] [INFO] epoch=15.00 step=116900, 10.4907 examples/sec lr=0.000004, loss=34.5619, loss_ll=6.52255, loss_ll_paf=10.2089, loss_ll_heat=2.83621, q=1000
[2018-07-11 18:30:20,246] [train] [INFO] epoch=15.00 step=117000, 10.4900 examples/sec lr=0.000004, loss=24.8564, loss_ll=4.43465, loss_ll_paf=6.29085, loss_ll_heat=2.57846, q=1000
[2018-07-11 18:33:18,372] [train] [INFO] epoch=15.00 step=117100, 10.4885 examples/sec lr=0.000004, loss=30.7141, loss_ll=5.10012, loss_ll_paf=7.72434, loss_ll_heat=2.47589, q=1000
[2018-07-11 18:36:00,948] [train] [INFO] epoch=15.00 step=117200, 10.4879 examples/sec lr=0.000004, loss=29.7132, loss_ll=5.01491, loss_ll_paf=7.3678, loss_ll_heat=2.66202, q=1000
[2018-07-11 18:38:42,456] [train] [INFO] epoch=15.00 step=117300, 10.4874 examples/sec lr=0.000004, loss=44.6942, loss_ll=8.04243, loss_ll_paf=12.7849, loss_ll_heat=3.30001, q=1000
[2018-07-11 18:41:23,963] [train] [INFO] epoch=15.00 step=117400, 10.4869 examples/sec lr=0.000004, loss=24.5365, loss_ll=4.19897, loss_ll_paf=6.1766, loss_ll_heat=2.22133, q=1000
[2018-07-11 18:44:05,943] [train] [INFO] epoch=15.00 step=117500, 10.4863 examples/sec lr=0.000004, loss=32.2716, loss_ll=5.43781, loss_ll_paf=8.14068, loss_ll_heat=2.73493, q=1000
[2018-07-11 18:46:48,384] [train] [INFO] epoch=15.00 step=117600, 10.4857 examples/sec lr=0.000004, loss=28.1462, loss_ll=4.97381, loss_ll_paf=7.80732, loss_ll_heat=2.14029, q=1000
[2018-07-11 18:49:29,236] [train] [INFO] epoch=15.00 step=117700, 10.4852 examples/sec lr=0.000004, loss=32.7124, loss_ll=5.64758, loss_ll_paf=8.16863, loss_ll_heat=3.12654, q=1000
[2018-07-11 18:52:10,352] [train] [INFO] epoch=15.00 step=117800, 10.4847 examples/sec lr=0.000004, loss=29.4364, loss_ll=5.14249, loss_ll_paf=7.7923, loss_ll_heat=2.49268, q=1000
[2018-07-11 18:54:50,219] [train] [INFO] epoch=15.00 step=117900, 10.4843 examples/sec lr=0.000004, loss=32.2651, loss_ll=5.53589, loss_ll_paf=8.22138, loss_ll_heat=2.8504, q=1000
[2018-07-11 18:57:32,458] [train] [INFO] epoch=15.00 step=118000, 10.4838 examples/sec lr=0.000004, loss=36.916, loss_ll=6.91474, loss_ll_paf=10.4301, loss_ll_heat=3.39941, q=1000
[2018-07-11 19:00:29,132] [train] [INFO] epoch=15.00 step=118100, 10.4824 examples/sec lr=0.000004, loss=27.9968, loss_ll=4.57178, loss_ll_paf=6.75222, loss_ll_heat=2.39134, q=1000
[2018-07-11 19:03:11,447] [train] [INFO] epoch=15.00 step=118200, 10.4818 examples/sec lr=0.000004, loss=44.1681, loss_ll=7.55099, loss_ll_paf=11.7526, loss_ll_heat=3.34934, q=1000
[2018-07-11 19:05:53,209] [train] [INFO] epoch=15.00 step=118300, 10.4813 examples/sec lr=0.000004, loss=47.5089, loss_ll=8.16483, loss_ll_paf=13.6592, loss_ll_heat=2.6705, q=1000
[2018-07-11 19:08:35,660] [train] [INFO] epoch=15.00 step=118400, 10.4807 examples/sec lr=0.000004, loss=37.2623, loss_ll=6.64851, loss_ll_paf=9.69292, loss_ll_heat=3.6041, q=1000
[2018-07-11 19:11:17,061] [train] [INFO] epoch=15.00 step=118500, 10.4802 examples/sec lr=0.000004, loss=34.9551, loss_ll=5.9218, loss_ll_paf=8.77529, loss_ll_heat=3.06832, q=1000
[2018-07-11 19:14:00,722] [train] [INFO] epoch=15.00 step=118600, 10.4796 examples/sec lr=0.000004, loss=39.7746, loss_ll=7.14647, loss_ll_paf=11.4815, loss_ll_heat=2.81146, q=1000
[2018-07-11 19:16:42,623] [train] [INFO] epoch=15.00 step=118700, 10.4790 examples/sec lr=0.000004, loss=30.8493, loss_ll=5.38291, loss_ll_paf=7.66508, loss_ll_heat=3.10075, q=1000
[2018-07-11 19:19:26,815] [train] [INFO] epoch=15.00 step=118800, 10.4784 examples/sec lr=0.000004, loss=36.724, loss_ll=6.84472, loss_ll_paf=10.1346, loss_ll_heat=3.55487, q=1000
[2018-07-11 19:22:07,898] [train] [INFO] epoch=15.00 step=118900, 10.4779 examples/sec lr=0.000004, loss=26.7397, loss_ll=4.38616, loss_ll_paf=6.38147, loss_ll_heat=2.39085, q=1000
[2018-07-11 19:24:51,644] [train] [INFO] epoch=15.00 step=119000, 10.4772 examples/sec lr=0.000004, loss=31.3841, loss_ll=5.04846, loss_ll_paf=7.07474, loss_ll_heat=3.02218, q=1000
[2018-07-11 19:27:47,126] [train] [INFO] epoch=15.00 step=119100, 10.4759 examples/sec lr=0.000004, loss=39.3176, loss_ll=7.14128, loss_ll_paf=11.3705, loss_ll_heat=2.91201, q=1000
[2018-07-11 19:30:26,750] [train] [INFO] epoch=15.00 step=119200, 10.4755 examples/sec lr=0.000004, loss=35.5426, loss_ll=6.13577, loss_ll_paf=9.5002, loss_ll_heat=2.77135, q=1000
[2018-07-11 19:33:09,689] [train] [INFO] epoch=15.00 step=119300, 10.4749 examples/sec lr=0.000004, loss=30.2478, loss_ll=5.25839, loss_ll_paf=7.87428, loss_ll_heat=2.6425, q=1000
[2018-07-11 19:35:49,673] [train] [INFO] epoch=15.00 step=119400, 10.4745 examples/sec lr=0.000004, loss=29.1054, loss_ll=5.25883, loss_ll_paf=8.11826, loss_ll_heat=2.39939, q=1000
[2018-07-11 19:38:31,849] [train] [INFO] epoch=15.00 step=119500, 10.4740 examples/sec lr=0.000004, loss=27.8946, loss_ll=4.7006, loss_ll_paf=7.08362, loss_ll_heat=2.31759, q=1000
[2018-07-11 19:41:15,528] [train] [INFO] epoch=15.00 step=119600, 10.4734 examples/sec lr=0.000004, loss=35.0457, loss_ll=6.49051, loss_ll_paf=9.76862, loss_ll_heat=3.21239, q=1000
[2018-07-11 19:43:56,144] [train] [INFO] epoch=15.00 step=119700, 10.4729 examples/sec lr=0.000004, loss=20.1251, loss_ll=3.74985, loss_ll_paf=4.92322, loss_ll_heat=2.57648, q=1000
[2018-07-11 19:46:39,212] [train] [INFO] epoch=15.00 step=119800, 10.4723 examples/sec lr=0.000004, loss=27.2855, loss_ll=4.11032, loss_ll_paf=5.69152, loss_ll_heat=2.52912, q=1000
[2018-07-11 19:49:20,637] [train] [INFO] epoch=15.00 step=119900, 10.4718 examples/sec lr=0.000004, loss=25.2758, loss_ll=3.96872, loss_ll_paf=5.52352, loss_ll_heat=2.41393, q=1000
[2018-07-11 19:52:03,749] [train] [INFO] epoch=15.00 step=120000, 10.4712 examples/sec lr=0.000001, loss=44.8162, loss_ll=8.09085, loss_ll_paf=12.578, loss_ll_heat=3.60368, q=1000
[2018-07-11 19:54:59,344] [train] [INFO] epoch=15.00 step=120100, 10.4699 examples/sec lr=0.000001, loss=30.4599, loss_ll=4.60674, loss_ll_paf=6.81619, loss_ll_heat=2.39729, q=1000
[2018-07-11 19:57:42,410] [train] [INFO] epoch=15.00 step=120200, 10.4694 examples/sec lr=0.000001, loss=30.6513, loss_ll=5.28054, loss_ll_paf=7.51097, loss_ll_heat=3.05011, q=1000
[2018-07-11 20:00:24,235] [train] [INFO] epoch=15.00 step=120300, 10.4688 examples/sec lr=0.000001, loss=41.7758, loss_ll=7.83072, loss_ll_paf=12.8835, loss_ll_heat=2.77797, q=1000
[2018-07-11 20:03:04,945] [train] [INFO] epoch=15.00 step=120400, 10.4684 examples/sec lr=0.000001, loss=29.9311, loss_ll=5.58234, loss_ll_paf=8.25277, loss_ll_heat=2.9119, q=1000
[2018-07-11 20:05:45,167] [train] [INFO] epoch=15.00 step=120500, 10.4680 examples/sec lr=0.000001, loss=35.7794, loss_ll=6.54626, loss_ll_paf=10.3142, loss_ll_heat=2.77835, q=1000
[2018-07-11 20:08:26,315] [train] [INFO] epoch=15.00 step=120600, 10.4675 examples/sec lr=0.000001, loss=23.4652, loss_ll=3.88249, loss_ll_paf=5.26094, loss_ll_heat=2.50404, q=1000
[2018-07-11 20:11:09,108] [train] [INFO] epoch=15.00 step=120700, 10.4669 examples/sec lr=0.000001, loss=32.6194, loss_ll=6.22372, loss_ll_paf=9.27114, loss_ll_heat=3.1763, q=1000
[2018-07-11 20:13:52,454] [train] [INFO] epoch=15.00 step=120800, 10.4663 examples/sec lr=0.000001, loss=35.4036, loss_ll=6.27643, loss_ll_paf=9.84356, loss_ll_heat=2.70931, q=1000
[2018-07-11 20:16:34,713] [train] [INFO] epoch=15.00 step=120900, 10.4658 examples/sec lr=0.000001, loss=27.6356, loss_ll=4.69148, loss_ll_paf=6.8371, loss_ll_heat=2.54587, q=1000
[2018-07-11 20:19:18,840] [train] [INFO] epoch=15.00 step=121000, 10.4652 examples/sec lr=0.000001, loss=29.4782, loss_ll=5.07323, loss_ll_paf=8.03557, loss_ll_heat=2.1109, q=1000
[2018-07-11 20:22:15,899] [train] [INFO] epoch=15.00 step=121100, 10.4638 examples/sec lr=0.000001, loss=26.6651, loss_ll=4.53597, loss_ll_paf=6.2404, loss_ll_heat=2.83155, q=1000
[2018-07-11 20:24:58,858] [train] [INFO] epoch=15.00 step=121200, 10.4632 examples/sec lr=0.000001, loss=30.2404, loss_ll=5.45855, loss_ll_paf=8.27689, loss_ll_heat=2.64022, q=1000
[2018-07-11 20:27:40,054] [train] [INFO] epoch=15.00 step=121300, 10.4628 examples/sec lr=0.000001, loss=29.6674, loss_ll=5.29523, loss_ll_paf=7.60375, loss_ll_heat=2.98671, q=1000
[2018-07-11 20:30:22,229] [train] [INFO] epoch=15.00 step=121400, 10.4623 examples/sec lr=0.000001, loss=32.1328, loss_ll=5.82953, loss_ll_paf=9.38182, loss_ll_heat=2.27725, q=1000
[2018-07-11 20:33:03,501] [train] [INFO] epoch=15.00 step=121500, 10.4618 examples/sec lr=0.000001, loss=32.1921, loss_ll=5.26536, loss_ll_paf=8.14784, loss_ll_heat=2.38287, q=1000
[2018-07-11 20:35:46,094] [train] [INFO] epoch=15.00 step=121600, 10.4612 examples/sec lr=0.000001, loss=26.6335, loss_ll=4.50894, loss_ll_paf=5.86186, loss_ll_heat=3.15602, q=1000
[2018-07-11 20:38:26,091] [train] [INFO] epoch=15.00 step=121700, 10.4608 examples/sec lr=0.000001, loss=35.6137, loss_ll=6.12896, loss_ll_paf=9.46858, loss_ll_heat=2.78934, q=1000
[2018-07-11 20:41:30,905] [train] [INFO] epoch=16.00 step=121800, 10.4591 examples/sec lr=0.000001, loss=19.2649, loss_ll=3.3631, loss_ll_paf=4.94593, loss_ll_heat=1.78026, q=1000
[2018-07-11 20:44:13,073] [train] [INFO] epoch=16.00 step=121900, 10.4585 examples/sec lr=0.000001, loss=32.5022, loss_ll=5.53469, loss_ll_paf=8.25073, loss_ll_heat=2.81865, q=1000
[2018-07-11 20:46:58,038] [train] [INFO] epoch=16.00 step=122000, 10.4579 examples/sec lr=0.000001, loss=42.9537, loss_ll=7.03073, loss_ll_paf=10.8387, loss_ll_heat=3.22273, q=1000
[2018-07-11 20:49:53,418] [train] [INFO] epoch=16.00 step=122100, 10.4566 examples/sec lr=0.000001, loss=34.8283, loss_ll=6.05664, loss_ll_paf=9.10813, loss_ll_heat=3.00515, q=1000
[2018-07-11 20:52:35,615] [train] [INFO] epoch=16.00 step=122200, 10.4561 examples/sec lr=0.000001, loss=37.1607, loss_ll=6.50304, loss_ll_paf=10.1563, loss_ll_heat=2.84975, q=1000
[2018-07-11 20:55:15,305] [train] [INFO] epoch=16.00 step=122300, 10.4557 examples/sec lr=0.000001, loss=30.0016, loss_ll=4.99983, loss_ll_paf=7.26852, loss_ll_heat=2.73113, q=1000
[2018-07-11 20:57:56,492] [train] [INFO] epoch=16.00 step=122400, 10.4553 examples/sec lr=0.000001, loss=32.9893, loss_ll=5.73268, loss_ll_paf=8.82188, loss_ll_heat=2.64348, q=1000
[2018-07-11 21:00:38,499] [train] [INFO] epoch=16.00 step=122500, 10.4548 examples/sec lr=0.000001, loss=22.3008, loss_ll=4.03832, loss_ll_paf=5.2379, loss_ll_heat=2.83873, q=1000
[2018-07-11 21:03:21,923] [train] [INFO] epoch=16.00 step=122600, 10.4542 examples/sec lr=0.000001, loss=31.7291, loss_ll=5.39111, loss_ll_paf=7.90534, loss_ll_heat=2.87688, q=1000
[2018-07-11 21:06:02,558] [train] [INFO] epoch=16.00 step=122700, 10.4538 examples/sec lr=0.000001, loss=64.1304, loss_ll=12.5417, loss_ll_paf=20.9707, loss_ll_heat=4.11281, q=1000
[2018-07-11 21:08:45,411] [train] [INFO] epoch=16.00 step=122800, 10.4532 examples/sec lr=0.000001, loss=31.1453, loss_ll=5.45318, loss_ll_paf=8.48986, loss_ll_heat=2.41649, q=1000
[2018-07-11 21:11:27,469] [train] [INFO] epoch=16.00 step=122900, 10.4527 examples/sec lr=0.000001, loss=32.1182, loss_ll=5.64031, loss_ll_paf=8.79418, loss_ll_heat=2.48643, q=1000
[2018-07-11 21:14:09,611] [train] [INFO] epoch=16.00 step=123000, 10.4522 examples/sec lr=0.000001, loss=31.6117, loss_ll=5.50536, loss_ll_paf=8.75855, loss_ll_heat=2.25216, q=1000
[2018-07-11 21:17:06,654] [train] [INFO] epoch=16.00 step=123100, 10.4509 examples/sec lr=0.000001, loss=35.3215, loss_ll=6.3961, loss_ll_paf=9.56724, loss_ll_heat=3.22496, q=1000
[2018-07-11 21:19:50,380] [train] [INFO] epoch=16.00 step=123200, 10.4503 examples/sec lr=0.000001, loss=40.5159, loss_ll=6.94838, loss_ll_paf=10.8234, loss_ll_heat=3.07337, q=1000
[2018-07-11 21:22:28,777] [train] [INFO] epoch=16.00 step=123300, 10.4500 examples/sec lr=0.000001, loss=32.0816, loss_ll=5.54104, loss_ll_paf=8.26912, loss_ll_heat=2.81296, q=1000
[2018-07-11 21:25:11,020] [train] [INFO] epoch=16.00 step=123400, 10.4495 examples/sec lr=0.000001, loss=26.7297, loss_ll=4.87909, loss_ll_paf=7.01042, loss_ll_heat=2.74776, q=1000
[2018-07-11 21:27:51,947] [train] [INFO] epoch=16.00 step=123500, 10.4491 examples/sec lr=0.000001, loss=20.6303, loss_ll=3.26235, loss_ll_paf=4.95603, loss_ll_heat=1.56867, q=1000
[2018-07-11 21:30:38,725] [train] [INFO] epoch=16.00 step=123600, 10.4483 examples/sec lr=0.000001, loss=27.5123, loss_ll=4.51331, loss_ll_paf=6.55792, loss_ll_heat=2.4687, q=1000
[2018-07-11 21:33:18,021] [train] [INFO] epoch=16.00 step=123700, 10.4480 examples/sec lr=0.000001, loss=32.6788, loss_ll=5.59394, loss_ll_paf=8.59608, loss_ll_heat=2.5918, q=1000
[2018-07-11 21:36:01,124] [train] [INFO] epoch=16.00 step=123800, 10.4474 examples/sec lr=0.000001, loss=27.2738, loss_ll=4.91105, loss_ll_paf=7.13591, loss_ll_heat=2.68619, q=1000
[2018-07-11 21:38:47,136] [train] [INFO] epoch=16.00 step=123900, 10.4467 examples/sec lr=0.000001, loss=16.8567, loss_ll=2.76348, loss_ll_paf=3.64456, loss_ll_heat=1.8824, q=1000
[2018-07-11 21:41:27,161] [train] [INFO] epoch=16.00 step=124000, 10.4464 examples/sec lr=0.000001, loss=38.6298, loss_ll=7.17505, loss_ll_paf=11.1797, loss_ll_heat=3.1704, q=1000
[2018-07-11 21:44:21,501] [train] [INFO] epoch=16.00 step=124100, 10.4452 examples/sec lr=0.000001, loss=33.825, loss_ll=6.1838, loss_ll_paf=9.45252, loss_ll_heat=2.91508, q=1000
[2018-07-11 21:46:59,075] [train] [INFO] epoch=16.00 step=124200, 10.4449 examples/sec lr=0.000001, loss=26.1527, loss_ll=4.51495, loss_ll_paf=6.51122, loss_ll_heat=2.51867, q=1000
[2018-07-11 21:49:38,737] [train] [INFO] epoch=16.00 step=124300, 10.4446 examples/sec lr=0.000001, loss=30.2942, loss_ll=5.43794, loss_ll_paf=8.00061, loss_ll_heat=2.87527, q=1000
[2018-07-11 21:52:24,358] [train] [INFO] epoch=16.00 step=124400, 10.4439 examples/sec lr=0.000001, loss=27.139, loss_ll=4.67062, loss_ll_paf=7.27562, loss_ll_heat=2.06562, q=1000
[2018-07-11 21:55:02,703] [train] [INFO] epoch=16.00 step=124500, 10.4436 examples/sec lr=0.000001, loss=21.3891, loss_ll=3.66861, loss_ll_paf=4.83364, loss_ll_heat=2.50359, q=1000
[2018-07-11 21:57:47,614] [train] [INFO] epoch=16.00 step=124600, 10.4430 examples/sec lr=0.000001, loss=29.4078, loss_ll=4.89972, loss_ll_paf=7.02257, loss_ll_heat=2.77686, q=1000
[2018-07-11 22:00:25,711] [train] [INFO] epoch=16.00 step=124700, 10.4427 examples/sec lr=0.000001, loss=33.9994, loss_ll=5.74702, loss_ll_paf=8.91706, loss_ll_heat=2.57698, q=1000
[2018-07-11 22:03:04,887] [train] [INFO] epoch=16.00 step=124800, 10.4424 examples/sec lr=0.000001, loss=29.1647, loss_ll=4.88178, loss_ll_paf=7.49431, loss_ll_heat=2.26925, q=1000
[2018-07-11 22:05:42,163] [train] [INFO] epoch=16.00 step=124900, 10.4422 examples/sec lr=0.000001, loss=29.6271, loss_ll=5.57603, loss_ll_paf=8.20237, loss_ll_heat=2.94969, q=1000
[2018-07-11 22:08:19,479] [train] [INFO] epoch=16.00 step=125000, 10.4420 examples/sec lr=0.000001, loss=46.2063, loss_ll=8.34168, loss_ll_paf=13.3405, loss_ll_heat=3.34286, q=1000
[2018-07-11 22:11:11,783] [train] [INFO] epoch=16.00 step=125100, 10.4409 examples/sec lr=0.000001, loss=25.9292, loss_ll=4.46245, loss_ll_paf=6.97544, loss_ll_heat=1.94947, q=1000
[2018-07-11 22:13:49,319] [train] [INFO] epoch=16.00 step=125200, 10.4407 examples/sec lr=0.000001, loss=23.2038, loss_ll=4.043, loss_ll_paf=5.56712, loss_ll_heat=2.51887, q=1000
[2018-07-11 22:16:26,200] [train] [INFO] epoch=16.00 step=125300, 10.4405 examples/sec lr=0.000001, loss=22.4666, loss_ll=3.63229, loss_ll_paf=5.32617, loss_ll_heat=1.9384, q=1000
[2018-07-11 22:19:04,994] [train] [INFO] epoch=16.00 step=125400, 10.4402 examples/sec lr=0.000001, loss=28.5726, loss_ll=4.8361, loss_ll_paf=7.43391, loss_ll_heat=2.2383, q=1000
[2018-07-11 22:21:40,770] [train] [INFO] epoch=16.00 step=125500, 10.4400 examples/sec lr=0.000001, loss=33.0771, loss_ll=5.53391, loss_ll_paf=7.7915, loss_ll_heat=3.27631, q=1000
[2018-07-11 22:24:18,430] [train] [INFO] epoch=16.00 step=125600, 10.4398 examples/sec lr=0.000001, loss=30.6838, loss_ll=5.12785, loss_ll_paf=7.67369, loss_ll_heat=2.58201, q=1000
[2018-07-11 22:26:57,501] [train] [INFO] epoch=16.00 step=125700, 10.4395 examples/sec lr=0.000001, loss=31.1313, loss_ll=5.42691, loss_ll_paf=7.79139, loss_ll_heat=3.06243, q=1000
[2018-07-11 22:29:36,789] [train] [INFO] epoch=16.00 step=125800, 10.4392 examples/sec lr=0.000001, loss=33.6961, loss_ll=6.10016, loss_ll_paf=8.64881, loss_ll_heat=3.55152, q=1000
[2018-07-11 22:32:15,361] [train] [INFO] epoch=16.00 step=125900, 10.4389 examples/sec lr=0.000001, loss=28.0843, loss_ll=4.59707, loss_ll_paf=6.78587, loss_ll_heat=2.40826, q=1000
[2018-07-11 22:34:54,413] [train] [INFO] epoch=16.00 step=126000, 10.4386 examples/sec lr=0.000001, loss=31.4083, loss_ll=5.66453, loss_ll_paf=9.03866, loss_ll_heat=2.2904, q=1000
[2018-07-11 22:37:47,020] [train] [INFO] epoch=16.00 step=126100, 10.4375 examples/sec lr=0.000001, loss=27.5575, loss_ll=5.23147, loss_ll_paf=7.97593, loss_ll_heat=2.48702, q=1000
[2018-07-11 22:40:25,910] [train] [INFO] epoch=16.00 step=126200, 10.4372 examples/sec lr=0.000001, loss=22.4504, loss_ll=3.76952, loss_ll_paf=5.13105, loss_ll_heat=2.40799, q=1000
[2018-07-11 22:43:04,252] [train] [INFO] epoch=16.00 step=126300, 10.4369 examples/sec lr=0.000001, loss=25.4312, loss_ll=4.45276, loss_ll_paf=6.66873, loss_ll_heat=2.23678, q=1000
[2018-07-11 22:45:42,155] [train] [INFO] epoch=16.00 step=126400, 10.4367 examples/sec lr=0.000001, loss=32.3279, loss_ll=5.07637, loss_ll_paf=7.79661, loss_ll_heat=2.35613, q=1000
[2018-07-11 22:48:20,517] [train] [INFO] epoch=16.00 step=126500, 10.4364 examples/sec lr=0.000001, loss=28.5109, loss_ll=5.15831, loss_ll_paf=6.80596, loss_ll_heat=3.51065, q=1000
[2018-07-11 22:50:59,487] [train] [INFO] epoch=16.00 step=126600, 10.4361 examples/sec lr=0.000001, loss=19.0335, loss_ll=3.44462, loss_ll_paf=4.6514, loss_ll_heat=2.23785, q=1000
[2018-07-11 22:53:38,650] [train] [INFO] epoch=16.00 step=126700, 10.4358 examples/sec lr=0.000001, loss=34.548, loss_ll=5.90699, loss_ll_paf=9.18044, loss_ll_heat=2.63355, q=1000
[2018-07-11 22:56:16,729] [train] [INFO] epoch=16.00 step=126800, 10.4356 examples/sec lr=0.000001, loss=38.9499, loss_ll=6.62124, loss_ll_paf=10.112, loss_ll_heat=3.13048, q=1000
[2018-07-11 22:58:53,701] [train] [INFO] epoch=16.00 step=126900, 10.4354 examples/sec lr=0.000001, loss=25.347, loss_ll=4.25341, loss_ll_paf=5.63234, loss_ll_heat=2.87448, q=1000
[2018-07-11 23:01:33,196] [train] [INFO] epoch=16.00 step=127000, 10.4350 examples/sec lr=0.000001, loss=35.1944, loss_ll=6.46567, loss_ll_paf=9.67798, loss_ll_heat=3.25336, q=1000
[2018-07-11 23:04:25,192] [train] [INFO] epoch=16.00 step=127100, 10.4340 examples/sec lr=0.000001, loss=21.8601, loss_ll=3.72144, loss_ll_paf=5.21041, loss_ll_heat=2.23246, q=1000
[2018-07-11 23:07:05,225] [train] [INFO] epoch=16.00 step=127200, 10.4337 examples/sec lr=0.000001, loss=29.6634, loss_ll=4.82713, loss_ll_paf=7.12033, loss_ll_heat=2.53393, q=1000
[2018-07-11 23:09:43,465] [train] [INFO] epoch=16.00 step=127300, 10.4334 examples/sec lr=0.000001, loss=38.2455, loss_ll=7.20194, loss_ll_paf=11.0176, loss_ll_heat=3.38631, q=1000
[2018-07-11 23:12:24,266] [train] [INFO] epoch=16.00 step=127400, 10.4330 examples/sec lr=0.000001, loss=36.1808, loss_ll=6.24511, loss_ll_paf=9.35114, loss_ll_heat=3.13908, q=1000
[2018-07-11 23:15:01,947] [train] [INFO] epoch=16.00 step=127500, 10.4328 examples/sec lr=0.000001, loss=40.0572, loss_ll=7.48326, loss_ll_paf=11.7692, loss_ll_heat=3.19734, q=1000
[2018-07-11 23:17:39,950] [train] [INFO] epoch=16.00 step=127600, 10.4325 examples/sec lr=0.000001, loss=26.8787, loss_ll=4.50298, loss_ll_paf=6.79276, loss_ll_heat=2.21319, q=1000
[2018-07-11 23:20:17,956] [train] [INFO] epoch=16.00 step=127700, 10.4323 examples/sec lr=0.000001, loss=44.9588, loss_ll=8.37334, loss_ll_paf=13.2391, loss_ll_heat=3.50761, q=1000
[2018-07-11 23:22:56,873] [train] [INFO] epoch=16.00 step=127800, 10.4320 examples/sec lr=0.000001, loss=31.9916, loss_ll=5.62817, loss_ll_paf=9.17169, loss_ll_heat=2.08465, q=1000
[2018-07-11 23:25:34,633] [train] [INFO] epoch=16.00 step=127900, 10.4318 examples/sec lr=0.000001, loss=29.9892, loss_ll=5.31076, loss_ll_paf=7.9361, loss_ll_heat=2.68542, q=1000
[2018-07-11 23:28:13,565] [train] [INFO] epoch=16.00 step=128000, 10.4315 examples/sec lr=0.000001, loss=26.6745, loss_ll=4.76989, loss_ll_paf=7.00686, loss_ll_heat=2.53292, q=1000
[2018-07-11 23:31:02,017] [train] [INFO] epoch=16.00 step=128100, 10.4307 examples/sec lr=0.000001, loss=32.4179, loss_ll=5.4244, loss_ll_paf=8.55808, loss_ll_heat=2.29072, q=1000
[2018-07-11 23:33:41,505] [train] [INFO] epoch=16.00 step=128200, 10.4303 examples/sec lr=0.000001, loss=32.0297, loss_ll=5.81826, loss_ll_paf=8.80124, loss_ll_heat=2.83528, q=1000
[2018-07-11 23:36:19,697] [train] [INFO] epoch=16.00 step=128300, 10.4301 examples/sec lr=0.000001, loss=30.3768, loss_ll=5.15028, loss_ll_paf=7.29628, loss_ll_heat=3.00429, q=1000
[2018-07-11 23:38:59,264] [train] [INFO] epoch=16.00 step=128400, 10.4298 examples/sec lr=0.000001, loss=41.8895, loss_ll=7.22485, loss_ll_paf=10.7288, loss_ll_heat=3.72095, q=1000
[2018-07-11 23:41:38,058] [train] [INFO] epoch=16.00 step=128500, 10.4295 examples/sec lr=0.000001, loss=30.2243, loss_ll=5.75894, loss_ll_paf=8.86997, loss_ll_heat=2.64791, q=1000
[2018-07-11 23:44:18,309] [train] [INFO] epoch=16.00 step=128600, 10.4291 examples/sec lr=0.000001, loss=27.218, loss_ll=4.25638, loss_ll_paf=5.73551, loss_ll_heat=2.77726, q=1000
[2018-07-11 23:46:57,680] [train] [INFO] epoch=16.00 step=128700, 10.4288 examples/sec lr=0.000001, loss=33.3861, loss_ll=6.06108, loss_ll_paf=9.89902, loss_ll_heat=2.22313, q=1000
[2018-07-11 23:49:37,150] [train] [INFO] epoch=16.00 step=128800, 10.4285 examples/sec lr=0.000001, loss=30.8661, loss_ll=5.32787, loss_ll_paf=8.19937, loss_ll_heat=2.45636, q=1000
[2018-07-11 23:52:15,373] [train] [INFO] epoch=16.00 step=128900, 10.4282 examples/sec lr=0.000001, loss=36.2831, loss_ll=6.32601, loss_ll_paf=10.1687, loss_ll_heat=2.48331, q=1000
[2018-07-11 23:54:54,804] [train] [INFO] epoch=16.00 step=129000, 10.4279 examples/sec lr=0.000001, loss=32.0128, loss_ll=5.63518, loss_ll_paf=8.36058, loss_ll_heat=2.90979, q=1000
[2018-07-11 23:57:46,215] [train] [INFO] epoch=16.00 step=129100, 10.4270 examples/sec lr=0.000001, loss=37.3962, loss_ll=6.64406, loss_ll_paf=10.3688, loss_ll_heat=2.91932, q=1000
[2018-07-12 00:00:24,320] [train] [INFO] epoch=16.00 step=129200, 10.4267 examples/sec lr=0.000001, loss=23.0982, loss_ll=3.99383, loss_ll_paf=5.49801, loss_ll_heat=2.48964, q=1000
[2018-07-12 00:03:03,189] [train] [INFO] epoch=16.00 step=129300, 10.4264 examples/sec lr=0.000001, loss=23.849, loss_ll=3.82342, loss_ll_paf=5.53224, loss_ll_heat=2.11461, q=1000
[2018-07-12 00:05:39,598] [train] [INFO] epoch=17.00 step=129400, 10.4263 examples/sec lr=0.000001, loss=29.7376, loss_ll=5.31132, loss_ll_paf=7.24931, loss_ll_heat=3.37334, q=1000
[2018-07-12 00:08:17,728] [train] [INFO] epoch=17.00 step=129500, 10.4260 examples/sec lr=0.000001, loss=27.7689, loss_ll=4.38927, loss_ll_paf=6.33673, loss_ll_heat=2.44182, q=1000
[2018-07-12 00:10:55,930] [train] [INFO] epoch=17.00 step=129600, 10.4258 examples/sec lr=0.000001, loss=45.0469, loss_ll=8.6411, loss_ll_paf=13.0548, loss_ll_heat=4.22744, q=1000
[2018-07-12 00:13:33,704] [train] [INFO] epoch=17.00 step=129700, 10.4256 examples/sec lr=0.000001, loss=30.6889, loss_ll=5.08267, loss_ll_paf=7.8864, loss_ll_heat=2.27894, q=1000
[2018-07-12 00:16:11,834] [train] [INFO] epoch=17.00 step=129800, 10.4253 examples/sec lr=0.000001, loss=25.9535, loss_ll=4.47473, loss_ll_paf=6.19567, loss_ll_heat=2.7538, q=1000
[2018-07-12 00:19:28,905] [train] [INFO] epoch=17.00 step=129900, 10.4230 examples/sec lr=0.000001, loss=29.5808, loss_ll=5.61801, loss_ll_paf=8.89317, loss_ll_heat=2.34286, q=1000
[2018-07-12 00:22:06,819] [train] [INFO] epoch=17.00 step=130000, 10.4228 examples/sec lr=0.000001, loss=41.1907, loss_ll=7.04264, loss_ll_paf=10.4422, loss_ll_heat=3.64307, q=1000
[2018-07-12 00:24:59,047] [train] [INFO] epoch=17.00 step=130100, 10.4218 examples/sec lr=0.000001, loss=37.2319, loss_ll=6.92234, loss_ll_paf=10.2874, loss_ll_heat=3.55725, q=1000
[2018-07-12 00:27:36,693] [train] [INFO] epoch=17.00 step=130200, 10.4216 examples/sec lr=0.000001, loss=31.099, loss_ll=5.6592, loss_ll_paf=8.82397, loss_ll_heat=2.49443, q=1000
[2018-07-12 00:30:14,008] [train] [INFO] epoch=17.00 step=130300, 10.4214 examples/sec lr=0.000001, loss=29.4418, loss_ll=4.98536, loss_ll_paf=7.4605, loss_ll_heat=2.51021, q=1000
[2018-07-12 00:32:52,328] [train] [INFO] epoch=17.00 step=130400, 10.4212 examples/sec lr=0.000001, loss=28.2554, loss_ll=4.67643, loss_ll_paf=6.99377, loss_ll_heat=2.35909, q=1000
[2018-07-12 00:35:29,566] [train] [INFO] epoch=17.00 step=130500, 10.4210 examples/sec lr=0.000001, loss=26.3758, loss_ll=4.57839, loss_ll_paf=6.75045, loss_ll_heat=2.40632, q=1000
[2018-07-12 00:38:05,957] [train] [INFO] epoch=17.00 step=130600, 10.4208 examples/sec lr=0.000001, loss=43.2159, loss_ll=7.89237, loss_ll_paf=12.9048, loss_ll_heat=2.8799, q=1000
[2018-07-12 00:40:43,928] [train] [INFO] epoch=17.00 step=130700, 10.4206 examples/sec lr=0.000001, loss=28.8019, loss_ll=5.05759, loss_ll_paf=7.7121, loss_ll_heat=2.40307, q=1000
[2018-07-12 00:43:22,576] [train] [INFO] epoch=17.00 step=130800, 10.4203 examples/sec lr=0.000001, loss=28.6054, loss_ll=4.78038, loss_ll_paf=7.14548, loss_ll_heat=2.41529, q=1000
[2018-07-12 00:45:59,653] [train] [INFO] epoch=17.00 step=130900, 10.4201 examples/sec lr=0.000001, loss=22.7444, loss_ll=4.01683, loss_ll_paf=5.9631, loss_ll_heat=2.07055, q=1000
[2018-07-12 00:48:36,708] [train] [INFO] epoch=17.00 step=131000, 10.4200 examples/sec lr=0.000001, loss=34.1818, loss_ll=5.91054, loss_ll_paf=9.25249, loss_ll_heat=2.56859, q=1000
[2018-07-12 00:51:27,488] [train] [INFO] epoch=17.00 step=131100, 10.4191 examples/sec lr=0.000001, loss=38.6871, loss_ll=6.55867, loss_ll_paf=10.3214, loss_ll_heat=2.79589, q=1000
[2018-07-12 00:54:05,617] [train] [INFO] epoch=17.00 step=131200, 10.4188 examples/sec lr=0.000001, loss=24.8884, loss_ll=4.36678, loss_ll_paf=6.51598, loss_ll_heat=2.21758, q=1000
[2018-07-12 00:56:43,607] [train] [INFO] epoch=17.00 step=131300, 10.4186 examples/sec lr=0.000001, loss=23.2362, loss_ll=3.70196, loss_ll_paf=5.45296, loss_ll_heat=1.95095, q=1000
[2018-07-12 00:59:20,933] [train] [INFO] epoch=17.00 step=131400, 10.4184 examples/sec lr=0.000001, loss=26.4121, loss_ll=3.97729, loss_ll_paf=5.73952, loss_ll_heat=2.21507, q=1000
[2018-07-12 01:01:57,548] [train] [INFO] epoch=17.00 step=131500, 10.4183 examples/sec lr=0.000001, loss=25.696, loss_ll=4.2691, loss_ll_paf=6.33427, loss_ll_heat=2.20394, q=1000
[2018-07-12 01:04:34,770] [train] [INFO] epoch=17.00 step=131600, 10.4181 examples/sec lr=0.000001, loss=31.3477, loss_ll=5.45397, loss_ll_paf=8.56298, loss_ll_heat=2.34496, q=1000
[2018-07-12 01:07:12,680] [train] [INFO] epoch=17.00 step=131700, 10.4178 examples/sec lr=0.000001, loss=26.8685, loss_ll=4.82883, loss_ll_paf=6.95339, loss_ll_heat=2.70427, q=1000
[2018-07-12 01:09:50,711] [train] [INFO] epoch=17.00 step=131800, 10.4176 examples/sec lr=0.000001, loss=41.7579, loss_ll=7.31841, loss_ll_paf=12.024, loss_ll_heat=2.61281, q=1000
[2018-07-12 01:12:28,231] [train] [INFO] epoch=17.00 step=131900, 10.4174 examples/sec lr=0.000001, loss=36.9286, loss_ll=6.32528, loss_ll_paf=9.18201, loss_ll_heat=3.46854, q=1000
[2018-07-12 01:15:06,053] [train] [INFO] epoch=17.00 step=132000, 10.4172 examples/sec lr=0.000001, loss=18.2355, loss_ll=2.92537, loss_ll_paf=3.80301, loss_ll_heat=2.04774, q=1000
[2018-07-12 01:17:56,789] [train] [INFO] epoch=17.00 step=132100, 10.4163 examples/sec lr=0.000001, loss=24.1391, loss_ll=4.14717, loss_ll_paf=5.89715, loss_ll_heat=2.39719, q=1000
[2018-07-12 01:20:36,143] [train] [INFO] epoch=17.00 step=132200, 10.4160 examples/sec lr=0.000001, loss=35.5777, loss_ll=6.26371, loss_ll_paf=9.67832, loss_ll_heat=2.8491, q=1000
[2018-07-12 01:23:14,034] [train] [INFO] epoch=17.00 step=132300, 10.4158 examples/sec lr=0.000001, loss=29.7852, loss_ll=5.43071, loss_ll_paf=8.29685, loss_ll_heat=2.56457, q=1000
[2018-07-12 01:25:51,438] [train] [INFO] epoch=17.00 step=132400, 10.4156 examples/sec lr=0.000001, loss=35.8473, loss_ll=5.75371, loss_ll_paf=8.45144, loss_ll_heat=3.05597, q=1000
[2018-07-12 01:28:28,659] [train] [INFO] epoch=17.00 step=132500, 10.4154 examples/sec lr=0.000001, loss=23.7859, loss_ll=3.7738, loss_ll_paf=5.28437, loss_ll_heat=2.26323, q=1000
[2018-07-12 01:31:07,843] [train] [INFO] epoch=17.00 step=132600, 10.4151 examples/sec lr=0.000001, loss=20.6429, loss_ll=3.66942, loss_ll_paf=4.79836, loss_ll_heat=2.54048, q=1000
[2018-07-12 01:33:45,852] [train] [INFO] epoch=17.00 step=132700, 10.4149 examples/sec lr=0.000001, loss=27.406, loss_ll=4.89527, loss_ll_paf=7.2115, loss_ll_heat=2.57904, q=1000
[2018-07-12 01:36:23,674] [train] [INFO] epoch=17.00 step=132800, 10.4147 examples/sec lr=0.000001, loss=26.6286, loss_ll=4.89293, loss_ll_paf=7.26845, loss_ll_heat=2.5174, q=1000
[2018-07-12 01:39:00,485] [train] [INFO] epoch=17.00 step=132900, 10.4145 examples/sec lr=0.000001, loss=28.9275, loss_ll=5.25754, loss_ll_paf=7.87069, loss_ll_heat=2.64439, q=1000
[2018-07-12 01:41:37,704] [train] [INFO] epoch=17.00 step=133000, 10.4144 examples/sec lr=0.000001, loss=27.9516, loss_ll=4.59067, loss_ll_paf=6.59094, loss_ll_heat=2.5904, q=1000
[2018-07-12 01:44:27,759] [train] [INFO] epoch=17.00 step=133100, 10.4135 examples/sec lr=0.000001, loss=37.4898, loss_ll=6.75407, loss_ll_paf=9.51329, loss_ll_heat=3.99484, q=1000
[2018-07-12 01:47:03,010] [train] [INFO] epoch=17.00 step=133200, 10.4134 examples/sec lr=0.000001, loss=40.3009, loss_ll=7.37591, loss_ll_paf=11.5298, loss_ll_heat=3.22205, q=1000
[2018-07-12 01:49:33,761] [train] [INFO] epoch=17.00 step=133300, 10.4136 examples/sec lr=0.000001, loss=26.6098, loss_ll=4.52768, loss_ll_paf=6.48627, loss_ll_heat=2.56909, q=1000
[2018-07-12 01:52:02,840] [train] [INFO] epoch=17.00 step=133400, 10.4138 examples/sec lr=0.000001, loss=36.0955, loss_ll=6.40431, loss_ll_paf=9.70625, loss_ll_heat=3.10238, q=1000
[2018-07-12 01:54:33,592] [train] [INFO] epoch=17.00 step=133500, 10.4140 examples/sec lr=0.000001, loss=23.9653, loss_ll=4.13496, loss_ll_paf=5.747, loss_ll_heat=2.52293, q=1000
[2018-07-12 01:57:03,396] [train] [INFO] epoch=17.00 step=133600, 10.4142 examples/sec lr=0.000001, loss=30.8269, loss_ll=5.77923, loss_ll_paf=9.07151, loss_ll_heat=2.48695, q=1000
[2018-07-12 01:59:32,413] [train] [INFO] epoch=17.00 step=133700, 10.4144 examples/sec lr=0.000001, loss=34.1861, loss_ll=6.04435, loss_ll_paf=9.29477, loss_ll_heat=2.79394, q=1000
[2018-07-12 02:02:03,450] [train] [INFO] epoch=17.00 step=133800, 10.4145 examples/sec lr=0.000001, loss=36.1349, loss_ll=6.13965, loss_ll_paf=9.0791, loss_ll_heat=3.20019, q=1000
[2018-07-12 02:04:33,155] [train] [INFO] epoch=17.00 step=133900, 10.4147 examples/sec lr=0.000001, loss=24.4251, loss_ll=4.08081, loss_ll_paf=5.67669, loss_ll_heat=2.48492, q=1000
[2018-07-12 02:07:04,417] [train] [INFO] epoch=17.00 step=134000, 10.4148 examples/sec lr=0.000001, loss=30.1081, loss_ll=5.23473, loss_ll_paf=7.86436, loss_ll_heat=2.60509, q=1000
[2018-07-12 02:09:46,380] [train] [INFO] epoch=17.00 step=134100, 10.4144 examples/sec lr=0.000001, loss=28.1498, loss_ll=4.79255, loss_ll_paf=6.88581, loss_ll_heat=2.69929, q=1000
[2018-07-12 02:12:16,074] [train] [INFO] epoch=17.00 step=134200, 10.4146 examples/sec lr=0.000001, loss=29.1698, loss_ll=5.07283, loss_ll_paf=7.03573, loss_ll_heat=3.10993, q=1000
[2018-07-12 02:14:45,524] [train] [INFO] epoch=17.00 step=134300, 10.4148 examples/sec lr=0.000001, loss=36.5764, loss_ll=6.90266, loss_ll_paf=10.066, loss_ll_heat=3.73928, q=1000
[2018-07-12 02:17:15,849] [train] [INFO] epoch=17.00 step=134400, 10.4150 examples/sec lr=0.000001, loss=33.2187, loss_ll=5.72724, loss_ll_paf=8.42429, loss_ll_heat=3.03018, q=1000
[2018-07-12 02:19:45,500] [train] [INFO] epoch=17.00 step=134500, 10.4152 examples/sec lr=0.000001, loss=31.5247, loss_ll=5.09222, loss_ll_paf=7.00356, loss_ll_heat=3.18089, q=1000
[2018-07-12 02:22:16,169] [train] [INFO] epoch=17.00 step=134600, 10.4153 examples/sec lr=0.000001, loss=43.054, loss_ll=7.18976, loss_ll_paf=10.8985, loss_ll_heat=3.48101, q=1000
[2018-07-12 02:24:47,274] [train] [INFO] epoch=17.00 step=134700, 10.4155 examples/sec lr=0.000001, loss=36.031, loss_ll=5.92728, loss_ll_paf=8.65955, loss_ll_heat=3.19502, q=1000
[2018-07-12 02:27:19,589] [train] [INFO] epoch=17.00 step=134800, 10.4155 examples/sec lr=0.000001, loss=27.1586, loss_ll=4.65866, loss_ll_paf=6.55007, loss_ll_heat=2.76725, q=1000
[2018-07-12 02:29:49,474] [train] [INFO] epoch=17.00 step=134900, 10.4157 examples/sec lr=0.000001, loss=26.0109, loss_ll=3.98495, loss_ll_paf=6.16321, loss_ll_heat=1.80669, q=1000
[2018-07-12 02:32:18,248] [train] [INFO] epoch=17.00 step=135000, 10.4160 examples/sec lr=0.000001, loss=23.5556, loss_ll=3.9763, loss_ll_paf=5.93996, loss_ll_heat=2.01264, q=1000
[2018-07-12 02:35:01,331] [train] [INFO] epoch=17.00 step=135100, 10.4155 examples/sec lr=0.000001, loss=20.5089, loss_ll=3.14683, loss_ll_paf=4.34294, loss_ll_heat=1.95072, q=1000
[2018-07-12 02:37:32,340] [train] [INFO] epoch=17.00 step=135200, 10.4156 examples/sec lr=0.000001, loss=24.5812, loss_ll=4.22289, loss_ll_paf=6.12143, loss_ll_heat=2.32435, q=1000
[2018-07-12 02:40:03,462] [train] [INFO] epoch=17.00 step=135300, 10.4158 examples/sec lr=0.000001, loss=34.9313, loss_ll=5.97282, loss_ll_paf=8.7744, loss_ll_heat=3.17125, q=1000
[2018-07-12 02:42:35,897] [train] [INFO] epoch=17.00 step=135400, 10.4158 examples/sec lr=0.000001, loss=23.3702, loss_ll=4.15205, loss_ll_paf=5.42628, loss_ll_heat=2.87781, q=1000
[2018-07-12 02:45:06,448] [train] [INFO] epoch=17.00 step=135500, 10.4160 examples/sec lr=0.000001, loss=25.1, loss_ll=4.21793, loss_ll_paf=5.84028, loss_ll_heat=2.59557, q=1000
[2018-07-12 02:47:36,385] [train] [INFO] epoch=17.00 step=135600, 10.4161 examples/sec lr=0.000001, loss=34.6605, loss_ll=5.94709, loss_ll_paf=9.46754, loss_ll_heat=2.42664, q=1000
[2018-07-12 02:50:08,820] [train] [INFO] epoch=17.00 step=135700, 10.4162 examples/sec lr=0.000001, loss=26.0855, loss_ll=4.78465, loss_ll_paf=7.02003, loss_ll_heat=2.54926, q=1000
[2018-07-12 02:52:40,544] [train] [INFO] epoch=17.00 step=135800, 10.4163 examples/sec lr=0.000001, loss=28.5296, loss_ll=4.76745, loss_ll_paf=6.45316, loss_ll_heat=3.08174, q=1000
[2018-07-12 02:55:10,351] [train] [INFO] epoch=17.00 step=135900, 10.4165 examples/sec lr=0.000001, loss=22.0508, loss_ll=3.72325, loss_ll_paf=5.46181, loss_ll_heat=1.98469, q=1000
[2018-07-12 02:57:40,335] [train] [INFO] epoch=17.00 step=136000, 10.4167 examples/sec lr=0.000001, loss=32.4161, loss_ll=5.6669, loss_ll_paf=8.77307, loss_ll_heat=2.56073, q=1000
[2018-07-12 03:00:24,717] [train] [INFO] epoch=17.00 step=136100, 10.4161 examples/sec lr=0.000001, loss=24.5013, loss_ll=4.10776, loss_ll_paf=5.27642, loss_ll_heat=2.93909, q=1000
[2018-07-12 03:02:54,326] [train] [INFO] epoch=17.00 step=136200, 10.4163 examples/sec lr=0.000001, loss=24.6755, loss_ll=4.37559, loss_ll_paf=6.31807, loss_ll_heat=2.43312, q=1000
[2018-07-12 03:05:23,740] [train] [INFO] epoch=17.00 step=136300, 10.4165 examples/sec lr=0.000001, loss=37.3788, loss_ll=5.74073, loss_ll_paf=8.451, loss_ll_heat=3.03046, q=1000
[2018-07-12 03:07:54,545] [train] [INFO] epoch=17.00 step=136400, 10.4167 examples/sec lr=0.000001, loss=25.6647, loss_ll=4.50522, loss_ll_paf=6.74093, loss_ll_heat=2.26951, q=1000
[2018-07-12 03:10:26,013] [train] [INFO] epoch=17.00 step=136500, 10.4168 examples/sec lr=0.000001, loss=31.9232, loss_ll=5.0094, loss_ll_paf=7.22631, loss_ll_heat=2.79249, q=1000
[2018-07-12 03:12:56,168] [train] [INFO] epoch=17.00 step=136600, 10.4170 examples/sec lr=0.000001, loss=33.7334, loss_ll=5.50747, loss_ll_paf=8.28885, loss_ll_heat=2.72609, q=1000
[2018-07-12 03:15:27,165] [train] [INFO] epoch=17.00 step=136700, 10.4171 examples/sec lr=0.000001, loss=30.2101, loss_ll=5.31534, loss_ll_paf=7.34926, loss_ll_heat=3.28141, q=1000
[2018-07-12 03:17:57,538] [train] [INFO] epoch=17.00 step=136800, 10.4172 examples/sec lr=0.000001, loss=36.4379, loss_ll=6.22496, loss_ll_paf=9.42497, loss_ll_heat=3.02494, q=1000
[2018-07-12 03:20:28,986] [train] [INFO] epoch=17.00 step=136900, 10.4174 examples/sec lr=0.000001, loss=18.7971, loss_ll=3.05484, loss_ll_paf=4.07589, loss_ll_heat=2.03379, q=1000
[2018-07-12 03:22:59,757] [train] [INFO] epoch=18.00 step=137000, 10.4175 examples/sec lr=0.000001, loss=23.9761, loss_ll=4.07955, loss_ll_paf=5.5706, loss_ll_heat=2.5885, q=1000
[2018-07-12 03:25:42,605] [train] [INFO] epoch=18.00 step=137100, 10.4170 examples/sec lr=0.000001, loss=32.351, loss_ll=5.56696, loss_ll_paf=8.25753, loss_ll_heat=2.8764, q=1000
[2018-07-12 03:28:14,016] [train] [INFO] epoch=18.00 step=137200, 10.4171 examples/sec lr=0.000001, loss=35.354, loss_ll=6.32372, loss_ll_paf=9.11298, loss_ll_heat=3.53445, q=1000
[2018-07-12 03:30:45,317] [train] [INFO] epoch=18.00 step=137300, 10.4173 examples/sec lr=0.000001, loss=30.4757, loss_ll=5.42999, loss_ll_paf=8.24684, loss_ll_heat=2.61313, q=1000
[2018-07-12 03:33:14,021] [train] [INFO] epoch=18.00 step=137400, 10.4175 examples/sec lr=0.000001, loss=23.0175, loss_ll=3.89345, loss_ll_paf=4.63369, loss_ll_heat=3.15321, q=1000
[2018-07-12 03:35:45,615] [train] [INFO] epoch=18.00 step=137500, 10.4176 examples/sec lr=0.000001, loss=34.6498, loss_ll=5.75878, loss_ll_paf=8.21612, loss_ll_heat=3.30144, q=1000
[2018-07-12 03:38:16,312] [train] [INFO] epoch=18.00 step=137600, 10.4177 examples/sec lr=0.000001, loss=20.855, loss_ll=3.69346, loss_ll_paf=4.73942, loss_ll_heat=2.64749, q=1000
[2018-07-12 03:40:48,628] [train] [INFO] epoch=18.00 step=137700, 10.4178 examples/sec lr=0.000001, loss=30.2289, loss_ll=5.07188, loss_ll_paf=7.25296, loss_ll_heat=2.8908, q=1000
[2018-07-12 03:43:18,227] [train] [INFO] epoch=18.00 step=137800, 10.4180 examples/sec lr=0.000001, loss=40.2059, loss_ll=6.79001, loss_ll_paf=10.5523, loss_ll_heat=3.02769, q=1000
[2018-07-12 03:45:49,807] [train] [INFO] epoch=18.00 step=137900, 10.4181 examples/sec lr=0.000001, loss=33.8377, loss_ll=5.90049, loss_ll_paf=9.09387, loss_ll_heat=2.70711, q=1000
[2018-07-12 03:48:21,298] [train] [INFO] epoch=18.00 step=138000, 10.4182 examples/sec lr=0.000001, loss=41.2628, loss_ll=7.40818, loss_ll_paf=11.8906, loss_ll_heat=2.92579, q=1000
[2018-07-12 03:51:05,863] [train] [INFO] epoch=18.00 step=138100, 10.4177 examples/sec lr=0.000001, loss=33.4964, loss_ll=5.67925, loss_ll_paf=8.53484, loss_ll_heat=2.82366, q=1000
[2018-07-12 03:53:37,481] [train] [INFO] epoch=18.00 step=138200, 10.4178 examples/sec lr=0.000001, loss=33.6534, loss_ll=5.97001, loss_ll_paf=9.03376, loss_ll_heat=2.90625, q=1000
[2018-07-12 03:56:10,203] [train] [INFO] epoch=18.00 step=138300, 10.4178 examples/sec lr=0.000001, loss=18.0847, loss_ll=3.06534, loss_ll_paf=3.93406, loss_ll_heat=2.19661, q=1000
[2018-07-12 03:58:42,093] [train] [INFO] epoch=18.00 step=138400, 10.4179 examples/sec lr=0.000001, loss=23.9575, loss_ll=4.25768, loss_ll_paf=6.10307, loss_ll_heat=2.41228, q=1000
[2018-07-12 04:01:12,251] [train] [INFO] epoch=18.00 step=138500, 10.4180 examples/sec lr=0.000001, loss=30.7449, loss_ll=5.50942, loss_ll_paf=8.45838, loss_ll_heat=2.56046, q=1000
[2018-07-12 04:03:43,689] [train] [INFO] epoch=18.00 step=138600, 10.4182 examples/sec lr=0.000001, loss=18.2217, loss_ll=3.08304, loss_ll_paf=4.33047, loss_ll_heat=1.83561, q=1000
[2018-07-12 04:06:14,297] [train] [INFO] epoch=18.00 step=138700, 10.4183 examples/sec lr=0.000001, loss=24.923, loss_ll=4.30959, loss_ll_paf=6.13549, loss_ll_heat=2.48368, q=1000
[2018-07-12 04:08:46,377] [train] [INFO] epoch=18.00 step=138800, 10.4184 examples/sec lr=0.000001, loss=27.5119, loss_ll=4.87416, loss_ll_paf=7.69105, loss_ll_heat=2.05727, q=1000
[2018-07-12 04:11:19,920] [train] [INFO] epoch=18.00 step=138900, 10.4184 examples/sec lr=0.000001, loss=35.4527, loss_ll=5.82896, loss_ll_paf=9.21128, loss_ll_heat=2.44664, q=1000
[2018-07-12 04:13:50,890] [train] [INFO] epoch=18.00 step=139000, 10.4185 examples/sec lr=0.000001, loss=26.1356, loss_ll=4.4452, loss_ll_paf=6.48665, loss_ll_heat=2.40375, q=1000
[2018-07-12 04:16:35,568] [train] [INFO] epoch=18.00 step=139100, 10.4180 examples/sec lr=0.000001, loss=31.776, loss_ll=5.09278, loss_ll_paf=6.96035, loss_ll_heat=3.2252, q=1000
[2018-07-12 04:19:07,308] [train] [INFO] epoch=18.00 step=139200, 10.4180 examples/sec lr=0.000001, loss=30.9481, loss_ll=5.19167, loss_ll_paf=7.84298, loss_ll_heat=2.54036, q=1000
[2018-07-12 04:21:39,561] [train] [INFO] epoch=18.00 step=139300, 10.4181 examples/sec lr=0.000001, loss=26.9655, loss_ll=4.8147, loss_ll_paf=7.66192, loss_ll_heat=1.96749, q=1000
[2018-07-12 04:24:09,612] [train] [INFO] epoch=18.00 step=139400, 10.4183 examples/sec lr=0.000001, loss=35.0066, loss_ll=5.80934, loss_ll_paf=8.77011, loss_ll_heat=2.84857, q=1000
[2018-07-12 04:26:41,170] [train] [INFO] epoch=18.00 step=139500, 10.4184 examples/sec lr=0.000001, loss=20.2913, loss_ll=3.1139, loss_ll_paf=4.5302, loss_ll_heat=1.6976, q=1000
[2018-07-12 04:29:13,755] [train] [INFO] epoch=18.00 step=139600, 10.4184 examples/sec lr=0.000001, loss=26.9614, loss_ll=4.74581, loss_ll_paf=7.48252, loss_ll_heat=2.00909, q=1000
[2018-07-12 04:31:46,634] [train] [INFO] epoch=18.00 step=139700, 10.4185 examples/sec lr=0.000001, loss=35.4582, loss_ll=6.02348, loss_ll_paf=9.93785, loss_ll_heat=2.10911, q=1000
[2018-07-12 04:34:18,454] [train] [INFO] epoch=18.00 step=139800, 10.4185 examples/sec lr=0.000001, loss=34.4861, loss_ll=6.45164, loss_ll_paf=9.91177, loss_ll_heat=2.99151, q=1000
[2018-07-12 04:36:49,842] [train] [INFO] epoch=18.00 step=139900, 10.4187 examples/sec lr=0.000001, loss=32.1424, loss_ll=5.45962, loss_ll_paf=8.70097, loss_ll_heat=2.21827, q=1000
[2018-07-12 04:39:21,621] [train] [INFO] epoch=18.00 step=140000, 10.4187 examples/sec lr=0.000001, loss=25.6183, loss_ll=4.38153, loss_ll_paf=6.31649, loss_ll_heat=2.44657, q=1000
[2018-07-12 04:42:05,610] [train] [INFO] epoch=18.00 step=140100, 10.4182 examples/sec lr=0.000001, loss=26.515, loss_ll=4.68345, loss_ll_paf=6.99207, loss_ll_heat=2.37483, q=1000
[2018-07-12 04:44:36,619] [train] [INFO] epoch=18.00 step=140200, 10.4184 examples/sec lr=0.000001, loss=27.9149, loss_ll=4.62217, loss_ll_paf=6.64236, loss_ll_heat=2.60199, q=1000
[2018-07-12 04:47:09,305] [train] [INFO] epoch=18.00 step=140300, 10.4184 examples/sec lr=0.000001, loss=29.5459, loss_ll=5.1711, loss_ll_paf=7.19195, loss_ll_heat=3.15026, q=1000
[2018-07-12 04:49:40,314] [train] [INFO] epoch=18.00 step=140400, 10.4185 examples/sec lr=0.000001, loss=27.6484, loss_ll=4.30212, loss_ll_paf=6.28577, loss_ll_heat=2.31846, q=1000
[2018-07-12 04:52:12,621] [train] [INFO] epoch=18.00 step=140500, 10.4186 examples/sec lr=0.000001, loss=24.8142, loss_ll=4.27118, loss_ll_paf=5.95995, loss_ll_heat=2.5824, q=1000
[2018-07-12 04:54:44,311] [train] [INFO] epoch=18.00 step=140600, 10.4187 examples/sec lr=0.000001, loss=26.3171, loss_ll=4.31738, loss_ll_paf=5.97299, loss_ll_heat=2.66177, q=1000
[2018-07-12 04:57:17,022] [train] [INFO] epoch=18.00 step=140700, 10.4187 examples/sec lr=0.000001, loss=21.946, loss_ll=3.61335, loss_ll_paf=4.8829, loss_ll_heat=2.3438, q=1000
[2018-07-12 04:59:47,187] [train] [INFO] epoch=18.00 step=140800, 10.4189 examples/sec lr=0.000001, loss=28.1644, loss_ll=5.03306, loss_ll_paf=7.20395, loss_ll_heat=2.86216, q=1000
[2018-07-12 05:02:19,222] [train] [INFO] epoch=18.00 step=140900, 10.4190 examples/sec lr=0.000001, loss=21.6503, loss_ll=3.7342, loss_ll_paf=5.31865, loss_ll_heat=2.14976, q=1000
[2018-07-12 05:04:50,119] [train] [INFO] epoch=18.00 step=141000, 10.4191 examples/sec lr=0.000001, loss=33.1583, loss_ll=5.82782, loss_ll_paf=8.6724, loss_ll_heat=2.98323, q=1000
[2018-07-12 05:07:32,637] [train] [INFO] epoch=18.00 step=141100, 10.4187 examples/sec lr=0.000001, loss=36.2518, loss_ll=6.53036, loss_ll_paf=10.3224, loss_ll_heat=2.7383, q=1000
[2018-07-12 05:10:03,653] [train] [INFO] epoch=18.00 step=141200, 10.4188 examples/sec lr=0.000001, loss=38.3405, loss_ll=7.12943, loss_ll_paf=11.5642, loss_ll_heat=2.69468, q=1000
[2018-07-12 05:12:35,663] [train] [INFO] epoch=18.00 step=141300, 10.4189 examples/sec lr=0.000001, loss=28.0402, loss_ll=4.89342, loss_ll_paf=7.14682, loss_ll_heat=2.64001, q=1000
[2018-07-12 05:15:06,053] [train] [INFO] epoch=18.00 step=141400, 10.4190 examples/sec lr=0.000001, loss=33.0069, loss_ll=5.92925, loss_ll_paf=9.22126, loss_ll_heat=2.63723, q=1000
[2018-07-12 05:17:37,504] [train] [INFO] epoch=18.00 step=141500, 10.4191 examples/sec lr=0.000001, loss=25.2461, loss_ll=4.28865, loss_ll_paf=6.22924, loss_ll_heat=2.34806, q=1000
[2018-07-12 05:20:08,397] [train] [INFO] epoch=18.00 step=141600, 10.4192 examples/sec lr=0.000001, loss=31.9859, loss_ll=5.79342, loss_ll_paf=8.35054, loss_ll_heat=3.23631, q=1000
[2018-07-12 05:22:39,556] [train] [INFO] epoch=18.00 step=141700, 10.4194 examples/sec lr=0.000001, loss=37.702, loss_ll=6.63732, loss_ll_paf=10.593, loss_ll_heat=2.68165, q=1000
[2018-07-12 05:25:10,316] [train] [INFO] epoch=18.00 step=141800, 10.4195 examples/sec lr=0.000001, loss=30.775, loss_ll=5.4112, loss_ll_paf=7.44815, loss_ll_heat=3.37425, q=1000
[2018-07-12 05:27:41,212] [train] [INFO] epoch=18.00 step=141900, 10.4196 examples/sec lr=0.000001, loss=30.5996, loss_ll=4.93292, loss_ll_paf=7.27695, loss_ll_heat=2.5889, q=1000
[2018-07-12 05:30:12,223] [train] [INFO] epoch=18.00 step=142000, 10.4197 examples/sec lr=0.000001, loss=38.206, loss_ll=6.78417, loss_ll_paf=10.2737, loss_ll_heat=3.29465, q=1000
[2018-07-12 05:32:55,493] [train] [INFO] epoch=18.00 step=142100, 10.4193 examples/sec lr=0.000001, loss=41.2697, loss_ll=7.51387, loss_ll_paf=11.5342, loss_ll_heat=3.49352, q=1000
[2018-07-12 05:35:26,063] [train] [INFO] epoch=18.00 step=142200, 10.4194 examples/sec lr=0.000001, loss=31.8186, loss_ll=5.18443, loss_ll_paf=7.79669, loss_ll_heat=2.57217, q=1000
[2018-07-12 05:37:57,870] [train] [INFO] epoch=18.00 step=142300, 10.4195 examples/sec lr=0.000001, loss=25.6246, loss_ll=4.09446, loss_ll_paf=5.91106, loss_ll_heat=2.27786, q=1000
[2018-07-12 05:40:27,992] [train] [INFO] epoch=18.00 step=142400, 10.4197 examples/sec lr=0.000001, loss=20.0266, loss_ll=3.44144, loss_ll_paf=4.90436, loss_ll_heat=1.97853, q=1000
[2018-07-12 05:43:00,905] [train] [INFO] epoch=18.00 step=142500, 10.4197 examples/sec lr=0.000001, loss=35.9869, loss_ll=6.69599, loss_ll_paf=10.1915, loss_ll_heat=3.20047, q=1000
[2018-07-12 05:45:32,898] [train] [INFO] epoch=18.00 step=142600, 10.4198 examples/sec lr=0.000001, loss=35.0721, loss_ll=5.96633, loss_ll_paf=9.11019, loss_ll_heat=2.82247, q=1000
[2018-07-12 05:48:04,709] [train] [INFO] epoch=18.00 step=142700, 10.4198 examples/sec lr=0.000001, loss=29.799, loss_ll=4.9087, loss_ll_paf=7.53239, loss_ll_heat=2.28502, q=1000
[2018-07-12 05:50:35,006] [train] [INFO] epoch=18.00 step=142800, 10.4200 examples/sec lr=0.000001, loss=51.7044, loss_ll=9.37297, loss_ll_paf=14.5754, loss_ll_heat=4.17053, q=1000
[2018-07-12 05:53:06,325] [train] [INFO] epoch=18.00 step=142900, 10.4201 examples/sec lr=0.000001, loss=23.33, loss_ll=3.83452, loss_ll_paf=5.0686, loss_ll_heat=2.60043, q=1000
[2018-07-12 05:55:37,469] [train] [INFO] epoch=18.00 step=143000, 10.4202 examples/sec lr=0.000001, loss=33.5331, loss_ll=6.30538, loss_ll_paf=9.9353, loss_ll_heat=2.67546, q=1000
[2018-07-12 05:58:20,243] [train] [INFO] epoch=18.00 step=143100, 10.4198 examples/sec lr=0.000001, loss=33.6512, loss_ll=5.84436, loss_ll_paf=9.06267, loss_ll_heat=2.62604, q=1000
[2018-07-12 06:00:50,474] [train] [INFO] epoch=18.00 step=143200, 10.4199 examples/sec lr=0.000001, loss=29.1063, loss_ll=4.68051, loss_ll_paf=6.3254, loss_ll_heat=3.03562, q=1000
[2018-07-12 06:03:22,350] [train] [INFO] epoch=18.00 step=143300, 10.4200 examples/sec lr=0.000001, loss=29.328, loss_ll=5.41005, loss_ll_paf=8.08035, loss_ll_heat=2.73975, q=1000
[2018-07-12 06:05:54,547] [train] [INFO] epoch=18.00 step=143400, 10.4201 examples/sec lr=0.000001, loss=28.9476, loss_ll=5.09872, loss_ll_paf=7.55352, loss_ll_heat=2.64392, q=1000
[2018-07-12 06:08:26,930] [train] [INFO] epoch=18.00 step=143500, 10.4201 examples/sec lr=0.000001, loss=30.4768, loss_ll=4.80601, loss_ll_paf=6.60347, loss_ll_heat=3.00856, q=1000
[2018-07-12 06:10:57,380] [train] [INFO] epoch=18.00 step=143600, 10.4203 examples/sec lr=0.000001, loss=38.625, loss_ll=6.5435, loss_ll_paf=9.473, loss_ll_heat=3.61401, q=1000
[2018-07-12 06:13:28,759] [train] [INFO] epoch=18.00 step=143700, 10.4204 examples/sec lr=0.000001, loss=28.7467, loss_ll=4.95833, loss_ll_paf=7.59327, loss_ll_heat=2.32339, q=1000
[2018-07-12 06:15:59,975] [train] [INFO] epoch=18.00 step=143800, 10.4205 examples/sec lr=0.000001, loss=27.0294, loss_ll=4.95063, loss_ll_paf=7.90981, loss_ll_heat=1.99146, q=1000
[2018-07-12 06:18:31,667] [train] [INFO] epoch=18.00 step=143900, 10.4206 examples/sec lr=0.000001, loss=28.2878, loss_ll=5.10933, loss_ll_paf=7.59777, loss_ll_heat=2.62089, q=1000
[2018-07-12 06:21:02,448] [train] [INFO] epoch=18.00 step=144000, 10.4207 examples/sec lr=0.000001, loss=30.4679, loss_ll=5.45119, loss_ll_paf=8.12493, loss_ll_heat=2.77745, q=1000
[2018-07-12 06:23:47,689] [train] [INFO] epoch=18.00 step=144100, 10.4202 examples/sec lr=0.000001, loss=29.0196, loss_ll=4.96618, loss_ll_paf=6.61944, loss_ll_heat=3.31291, q=1000
[2018-07-12 06:26:19,547] [train] [INFO] epoch=18.00 step=144200, 10.4202 examples/sec lr=0.000001, loss=39.8188, loss_ll=7.13401, loss_ll_paf=12.0456, loss_ll_heat=2.22242, q=1000
[2018-07-12 06:28:52,380] [train] [INFO] epoch=18.00 step=144300, 10.4203 examples/sec lr=0.000001, loss=23.9145, loss_ll=4.1647, loss_ll_paf=6.23021, loss_ll_heat=2.09919, q=1000
[2018-07-12 06:31:23,948] [train] [INFO] epoch=18.00 step=144400, 10.4204 examples/sec lr=0.000001, loss=26.602, loss_ll=5.05213, loss_ll_paf=7.73801, loss_ll_heat=2.36625, q=1000
[2018-07-12 06:33:55,663] [train] [INFO] epoch=18.00 step=144500, 10.4205 examples/sec lr=0.000001, loss=41.4367, loss_ll=7.06752, loss_ll_paf=10.399, loss_ll_heat=3.73604, q=1000
[2018-07-12 06:36:29,269] [train] [INFO] epoch=19.00 step=144600, 10.4205 examples/sec lr=0.000001, loss=27.3267, loss_ll=4.63377, loss_ll_paf=6.68947, loss_ll_heat=2.57807, q=1000
[2018-07-12 06:39:02,166] [train] [INFO] epoch=19.00 step=144700, 10.4205 examples/sec lr=0.000001, loss=30.4558, loss_ll=5.26785, loss_ll_paf=7.61696, loss_ll_heat=2.91875, q=1000
[2018-07-12 06:41:32,764] [train] [INFO] epoch=19.00 step=144800, 10.4206 examples/sec lr=0.000001, loss=26.6295, loss_ll=4.58304, loss_ll_paf=6.67796, loss_ll_heat=2.48813, q=1000
[2018-07-12 06:44:05,094] [train] [INFO] epoch=19.00 step=144900, 10.4207 examples/sec lr=0.000001, loss=38.0585, loss_ll=7.26577, loss_ll_paf=11.0794, loss_ll_heat=3.45213, q=1000
[2018-07-12 06:46:36,860] [train] [INFO] epoch=19.00 step=145000, 10.4208 examples/sec lr=0.000001, loss=25.912, loss_ll=4.65329, loss_ll_paf=6.56074, loss_ll_heat=2.74584, q=1000
[2018-07-12 06:49:22,589] [train] [INFO] epoch=19.00 step=145100, 10.4202 examples/sec lr=0.000001, loss=27.6857, loss_ll=4.9268, loss_ll_paf=7.02021, loss_ll_heat=2.8334, q=1000
[2018-07-12 06:51:57,735] [train] [INFO] epoch=19.00 step=145200, 10.4201 examples/sec lr=0.000001, loss=25.4056, loss_ll=4.08976, loss_ll_paf=5.66175, loss_ll_heat=2.51777, q=1000
[2018-07-12 06:54:29,496] [train] [INFO] epoch=19.00 step=145300, 10.4202 examples/sec lr=0.000001, loss=34.7615, loss_ll=6.09453, loss_ll_paf=8.51031, loss_ll_heat=3.67874, q=1000
[2018-07-12 06:56:58,882] [train] [INFO] epoch=19.00 step=145400, 10.4204 examples/sec lr=0.000001, loss=33.8626, loss_ll=5.93962, loss_ll_paf=8.98279, loss_ll_heat=2.89645, q=1000
[2018-07-12 06:59:30,417] [train] [INFO] epoch=19.00 step=145500, 10.4205 examples/sec lr=0.000001, loss=30.0137, loss_ll=4.86309, loss_ll_paf=7.8182, loss_ll_heat=1.90797, q=1000
[2018-07-12 07:02:02,099] [train] [INFO] epoch=19.00 step=145600, 10.4206 examples/sec lr=0.000001, loss=25.0855, loss_ll=4.34914, loss_ll_paf=6.25061, loss_ll_heat=2.44766, q=1000
[2018-07-12 07:04:33,676] [train] [INFO] epoch=19.00 step=145700, 10.4207 examples/sec lr=0.000001, loss=27.7585, loss_ll=4.9663, loss_ll_paf=7.47288, loss_ll_heat=2.45972, q=1000
[2018-07-12 07:07:06,469] [train] [INFO] epoch=19.00 step=145800, 10.4207 examples/sec lr=0.000001, loss=38.6438, loss_ll=6.73364, loss_ll_paf=9.78884, loss_ll_heat=3.67844, q=1000
[2018-07-12 07:09:36,999] [train] [INFO] epoch=19.00 step=145900, 10.4208 examples/sec lr=0.000001, loss=25.6008, loss_ll=4.17233, loss_ll_paf=5.96981, loss_ll_heat=2.37486, q=1000
[2018-07-12 07:12:10,188] [train] [INFO] epoch=19.00 step=146000, 10.4209 examples/sec lr=0.000001, loss=45.9845, loss_ll=8.67975, loss_ll_paf=14.1973, loss_ll_heat=3.16219, q=1000
[2018-07-12 07:14:57,103] [train] [INFO] epoch=19.00 step=146100, 10.4202 examples/sec lr=0.000001, loss=28.1445, loss_ll=4.64474, loss_ll_paf=6.25859, loss_ll_heat=3.03088, q=1000
[2018-07-12 07:17:28,789] [train] [INFO] epoch=19.00 step=146200, 10.4203 examples/sec lr=0.000001, loss=37.4655, loss_ll=6.54928, loss_ll_paf=9.93394, loss_ll_heat=3.16463, q=1000
[2018-07-12 07:20:00,708] [train] [INFO] epoch=19.00 step=146300, 10.4204 examples/sec lr=0.000001, loss=28.8734, loss_ll=4.8605, loss_ll_paf=7.51127, loss_ll_heat=2.20972, q=1000
[2018-07-12 07:22:33,745] [train] [INFO] epoch=19.00 step=146400, 10.4204 examples/sec lr=0.000001, loss=19.7412, loss_ll=3.33206, loss_ll_paf=4.12419, loss_ll_heat=2.53993, q=1000
[2018-07-12 07:25:05,192] [train] [INFO] epoch=19.00 step=146500, 10.4205 examples/sec lr=0.000001, loss=37.2616, loss_ll=6.1714, loss_ll_paf=9.70472, loss_ll_heat=2.63808, q=1000
[2018-07-12 07:27:38,643] [train] [INFO] epoch=19.00 step=146600, 10.4205 examples/sec lr=0.000001, loss=39.8677, loss_ll=7.0489, loss_ll_paf=10.9002, loss_ll_heat=3.19759, q=1000
[2018-07-12 07:30:12,868] [train] [INFO] epoch=19.00 step=146700, 10.4205 examples/sec lr=0.000001, loss=33.1653, loss_ll=5.75896, loss_ll_paf=8.83816, loss_ll_heat=2.67976, q=1000
[2018-07-12 07:32:45,455] [train] [INFO] epoch=19.00 step=146800, 10.4205 examples/sec lr=0.000001, loss=29.6872, loss_ll=5.3126, loss_ll_paf=7.92718, loss_ll_heat=2.69802, q=1000
[2018-07-12 07:35:19,017] [train] [INFO] epoch=19.00 step=146900, 10.4205 examples/sec lr=0.000001, loss=27.5849, loss_ll=4.71065, loss_ll_paf=7.18234, loss_ll_heat=2.23896, q=1000
[2018-07-12 07:37:52,623] [train] [INFO] epoch=19.00 step=147000, 10.4205 examples/sec lr=0.000001, loss=25.1748, loss_ll=4.5784, loss_ll_paf=6.34155, loss_ll_heat=2.81525, q=1000
[2018-07-12 07:40:42,308] [train] [INFO] epoch=19.00 step=147100, 10.4198 examples/sec lr=0.000001, loss=25.7865, loss_ll=4.79434, loss_ll_paf=6.74263, loss_ll_heat=2.84605, q=1000
[2018-07-12 07:43:15,799] [train] [INFO] epoch=19.00 step=147200, 10.4198 examples/sec lr=0.000001, loss=29.8093, loss_ll=5.20607, loss_ll_paf=7.29947, loss_ll_heat=3.11266, q=1000
[2018-07-12 07:45:49,066] [train] [INFO] epoch=19.00 step=147300, 10.4198 examples/sec lr=0.000001, loss=42.3492, loss_ll=7.46049, loss_ll_paf=12.2361, loss_ll_heat=2.68487, q=1000
[2018-07-12 07:48:23,176] [train] [INFO] epoch=19.00 step=147400, 10.4198 examples/sec lr=0.000001, loss=23.4592, loss_ll=4.03671, loss_ll_paf=5.61115, loss_ll_heat=2.46228, q=1000
[2018-07-12 07:50:56,855] [train] [INFO] epoch=19.00 step=147500, 10.4198 examples/sec lr=0.000001, loss=28.0288, loss_ll=4.91222, loss_ll_paf=7.59053, loss_ll_heat=2.2339, q=1000
[2018-07-12 07:53:30,590] [train] [INFO] epoch=19.00 step=147600, 10.4198 examples/sec lr=0.000001, loss=27.7096, loss_ll=4.92258, loss_ll_paf=7.2697, loss_ll_heat=2.57546, q=1000
[2018-07-12 07:56:03,840] [train] [INFO] epoch=19.00 step=147700, 10.4198 examples/sec lr=0.000001, loss=26.5803, loss_ll=4.75334, loss_ll_paf=7.00438, loss_ll_heat=2.50231, q=1000
[2018-07-12 07:58:37,123] [train] [INFO] epoch=19.00 step=147800, 10.4198 examples/sec lr=0.000001, loss=34.1966, loss_ll=6.21647, loss_ll_paf=9.59324, loss_ll_heat=2.8397, q=1000
[2018-07-12 08:01:11,229] [train] [INFO] epoch=19.00 step=147900, 10.4198 examples/sec lr=0.000001, loss=38.2829, loss_ll=6.76514, loss_ll_paf=10.8633, loss_ll_heat=2.66695, q=1000
[2018-07-12 08:03:45,879] [train] [INFO] epoch=19.00 step=148000, 10.4197 examples/sec lr=0.000001, loss=30.366, loss_ll=5.37286, loss_ll_paf=7.98341, loss_ll_heat=2.76232, q=1000
[2018-07-12 08:06:34,044] [train] [INFO] epoch=19.00 step=148100, 10.4190 examples/sec lr=0.000001, loss=41.4835, loss_ll=7.99866, loss_ll_paf=13.1102, loss_ll_heat=2.88709, q=1000
[2018-07-12 08:09:08,997] [train] [INFO] epoch=19.00 step=148200, 10.4190 examples/sec lr=0.000001, loss=35.3908, loss_ll=5.97145, loss_ll_paf=9.40831, loss_ll_heat=2.53458, q=1000
[2018-07-12 08:11:42,531] [train] [INFO] epoch=19.00 step=148300, 10.4190 examples/sec lr=0.000001, loss=27.1026, loss_ll=3.99428, loss_ll_paf=5.53406, loss_ll_heat=2.45449, q=1000
[2018-07-12 08:14:16,991] [train] [INFO] epoch=19.00 step=148400, 10.4189 examples/sec lr=0.000001, loss=33.5455, loss_ll=6.15335, loss_ll_paf=10.4762, loss_ll_heat=1.83046, q=1000
[2018-07-12 08:16:49,996] [train] [INFO] epoch=19.00 step=148500, 10.4190 examples/sec lr=0.000001, loss=27.8533, loss_ll=5.19252, loss_ll_paf=7.60517, loss_ll_heat=2.77987, q=1000
[2018-07-12 08:19:22,725] [train] [INFO] epoch=19.00 step=148600, 10.4190 examples/sec lr=0.000001, loss=22.9737, loss_ll=3.74236, loss_ll_paf=4.95437, loss_ll_heat=2.53035, q=1000
[2018-07-12 08:21:53,296] [train] [INFO] epoch=19.00 step=148700, 10.4191 examples/sec lr=0.000001, loss=24.382, loss_ll=4.2858, loss_ll_paf=5.78614, loss_ll_heat=2.78545, q=1000
[2018-07-12 08:24:27,812] [train] [INFO] epoch=19.00 step=148800, 10.4191 examples/sec lr=0.000001, loss=36.9134, loss_ll=6.17003, loss_ll_paf=9.10986, loss_ll_heat=3.23019, q=1000
[2018-07-12 08:27:03,045] [train] [INFO] epoch=19.00 step=148900, 10.4190 examples/sec lr=0.000001, loss=39.7648, loss_ll=6.52663, loss_ll_paf=10.5817, loss_ll_heat=2.47157, q=1000
[2018-07-12 08:29:35,494] [train] [INFO] epoch=19.00 step=149000, 10.4191 examples/sec lr=0.000001, loss=36.8218, loss_ll=6.26231, loss_ll_paf=9.78183, loss_ll_heat=2.7428, q=1000
[2018-07-12 08:32:24,259] [train] [INFO] epoch=19.00 step=149100, 10.4184 examples/sec lr=0.000001, loss=36.29, loss_ll=6.28517, loss_ll_paf=9.89993, loss_ll_heat=2.6704, q=1000
[2018-07-12 08:34:56,123] [train] [INFO] epoch=19.00 step=149200, 10.4185 examples/sec lr=0.000001, loss=39.8849, loss_ll=7.38094, loss_ll_paf=11.6833, loss_ll_heat=3.07854, q=1000
[2018-07-12 08:37:28,801] [train] [INFO] epoch=19.00 step=149300, 10.4185 examples/sec lr=0.000001, loss=29.3261, loss_ll=5.09643, loss_ll_paf=7.41776, loss_ll_heat=2.7751, q=1000
[2018-07-12 08:40:00,799] [train] [INFO] epoch=19.00 step=149400, 10.4186 examples/sec lr=0.000001, loss=27.2326, loss_ll=4.96914, loss_ll_paf=7.09321, loss_ll_heat=2.84507, q=1000
[2018-07-12 08:42:33,862] [train] [INFO] epoch=19.00 step=149500, 10.4186 examples/sec lr=0.000001, loss=21.817, loss_ll=3.71143, loss_ll_paf=4.95323, loss_ll_heat=2.46964, q=1000
[2018-07-12 08:45:06,316] [train] [INFO] epoch=19.00 step=149600, 10.4186 examples/sec lr=0.000001, loss=30.7234, loss_ll=5.34787, loss_ll_paf=7.80638, loss_ll_heat=2.88936, q=1000
[2018-07-12 08:47:40,846] [train] [INFO] epoch=19.00 step=149700, 10.4186 examples/sec lr=0.000001, loss=25.5638, loss_ll=4.31915, loss_ll_paf=5.66096, loss_ll_heat=2.97733, q=1000
[2018-07-12 08:50:14,316] [train] [INFO] epoch=19.00 step=149800, 10.4186 examples/sec lr=0.000001, loss=31.7743, loss_ll=5.70221, loss_ll_paf=8.19942, loss_ll_heat=3.205, q=1000
[2018-07-12 08:52:49,689] [train] [INFO] epoch=19.00 step=149900, 10.4185 examples/sec lr=0.000001, loss=28.2569, loss_ll=4.85612, loss_ll_paf=6.97931, loss_ll_heat=2.73294, q=1000
[2018-07-12 08:55:21,631] [train] [INFO] epoch=19.00 step=150000, 10.4186 examples/sec lr=0.000000, loss=17.4841, loss_ll=2.71492, loss_ll_paf=3.73977, loss_ll_heat=1.69007, q=1000
[2018-07-12 08:58:08,912] [train] [INFO] epoch=19.00 step=150100, 10.4180 examples/sec lr=0.000000, loss=25.6117, loss_ll=4.27264, loss_ll_paf=6.23633, loss_ll_heat=2.30896, q=1000
[2018-07-12 09:00:42,999] [train] [INFO] epoch=19.00 step=150200, 10.4180 examples/sec lr=0.000000, loss=23.2201, loss_ll=3.97663, loss_ll_paf=4.87263, loss_ll_heat=3.08063, q=1000
[2018-07-12 09:03:17,471] [train] [INFO] epoch=19.00 step=150300, 10.4179 examples/sec lr=0.000000, loss=20.1193, loss_ll=3.46699, loss_ll_paf=4.63247, loss_ll_heat=2.30151, q=1000
[2018-07-12 09:05:52,327] [train] [INFO] epoch=19.00 step=150400, 10.4179 examples/sec lr=0.000000, loss=25.4294, loss_ll=4.45914, loss_ll_paf=6.27287, loss_ll_heat=2.64542, q=1000
[2018-07-12 09:08:24,977] [train] [INFO] epoch=19.00 step=150500, 10.4179 examples/sec lr=0.000000, loss=40.6173, loss_ll=7.09968, loss_ll_paf=11.3715, loss_ll_heat=2.82787, q=1000
[2018-07-12 09:10:58,569] [train] [INFO] epoch=19.00 step=150600, 10.4179 examples/sec lr=0.000000, loss=37.9404, loss_ll=6.9447, loss_ll_paf=10.9035, loss_ll_heat=2.98594, q=1000
[2018-07-12 09:13:32,082] [train] [INFO] epoch=19.00 step=150700, 10.4179 examples/sec lr=0.000000, loss=38.7834, loss_ll=7.04098, loss_ll_paf=10.9434, loss_ll_heat=3.13859, q=1000
[2018-07-12 09:16:07,156] [train] [INFO] epoch=19.00 step=150800, 10.4178 examples/sec lr=0.000000, loss=24.6434, loss_ll=4.65646, loss_ll_paf=6.43926, loss_ll_heat=2.87367, q=1000
[2018-07-12 09:18:42,936] [train] [INFO] epoch=19.00 step=150900, 10.4177 examples/sec lr=0.000000, loss=36.3183, loss_ll=6.4507, loss_ll_paf=10.6325, loss_ll_heat=2.26891, q=1000
[2018-07-12 09:21:17,542] [train] [INFO] epoch=19.00 step=151000, 10.4177 examples/sec lr=0.000000, loss=20.7217, loss_ll=3.28671, loss_ll_paf=4.27916, loss_ll_heat=2.29425, q=1000
[2018-07-12 09:24:08,114] [train] [INFO] epoch=19.00 step=151100, 10.4169 examples/sec lr=0.000000, loss=26.0748, loss_ll=4.47568, loss_ll_paf=6.59838, loss_ll_heat=2.35299, q=1000
[2018-07-12 09:26:40,272] [train] [INFO] epoch=19.00 step=151200, 10.4170 examples/sec lr=0.000000, loss=39.7162, loss_ll=7.35376, loss_ll_paf=11.5169, loss_ll_heat=3.1906, q=1000
[2018-07-12 09:29:14,528] [train] [INFO] epoch=19.00 step=151300, 10.4170 examples/sec lr=0.000000, loss=24.4531, loss_ll=3.96129, loss_ll_paf=5.65171, loss_ll_heat=2.27086, q=1000
[2018-07-12 09:31:47,379] [train] [INFO] epoch=19.00 step=151400, 10.4170 examples/sec lr=0.000000, loss=29.6555, loss_ll=5.3299, loss_ll_paf=7.51715, loss_ll_heat=3.14265, q=1000
[2018-07-12 09:34:22,148] [train] [INFO] epoch=19.00 step=151500, 10.4169 examples/sec lr=0.000000, loss=28.6698, loss_ll=5.58797, loss_ll_paf=8.57923, loss_ll_heat=2.59671, q=1000
[2018-07-12 09:36:56,836] [train] [INFO] epoch=19.00 step=151600, 10.4169 examples/sec lr=0.000000, loss=32.2091, loss_ll=5.76431, loss_ll_paf=9.32601, loss_ll_heat=2.20261, q=1000
[2018-07-12 09:39:32,991] [train] [INFO] epoch=19.00 step=151700, 10.4168 examples/sec lr=0.000000, loss=20.7574, loss_ll=3.68641, loss_ll_paf=5.02166, loss_ll_heat=2.35117, q=1000
[2018-07-12 09:42:07,128] [train] [INFO] epoch=19.00 step=151800, 10.4168 examples/sec lr=0.000000, loss=25.2766, loss_ll=4.39368, loss_ll_paf=6.96639, loss_ll_heat=1.82097, q=1000
[2018-07-12 09:44:42,091] [train] [INFO] epoch=19.00 step=151900, 10.4167 examples/sec lr=0.000000, loss=25.4305, loss_ll=4.39003, loss_ll_paf=6.01383, loss_ll_heat=2.76622, q=1000
[2018-07-12 09:47:15,348] [train] [INFO] epoch=19.00 step=152000, 10.4167 examples/sec lr=0.000000, loss=25.9512, loss_ll=4.34016, loss_ll_paf=6.01634, loss_ll_heat=2.66398, q=1000
[2018-07-12 09:50:02,842] [train] [INFO] epoch=19.00 step=152100, 10.4161 examples/sec lr=0.000000, loss=30.2904, loss_ll=5.32481, loss_ll_paf=7.55103, loss_ll_heat=3.0986, q=1000
[2018-07-12 09:52:36,920] [train] [INFO] epoch=20.00 step=152200, 10.4161 examples/sec lr=0.000000, loss=35.9286, loss_ll=6.58232, loss_ll_paf=10.1083, loss_ll_heat=3.05632, q=1000
[2018-07-12 09:55:11,064] [train] [INFO] epoch=20.00 step=152300, 10.4160 examples/sec lr=0.000000, loss=27.6764, loss_ll=4.80538, loss_ll_paf=7.42363, loss_ll_heat=2.18714, q=1000
[2018-07-12 09:57:45,174] [train] [INFO] epoch=20.00 step=152400, 10.4160 examples/sec lr=0.000000, loss=38.1443, loss_ll=6.40644, loss_ll_paf=10.5317, loss_ll_heat=2.28115, q=1000
[2018-07-12 10:00:20,031] [train] [INFO] epoch=20.00 step=152500, 10.4160 examples/sec lr=0.000000, loss=30.5279, loss_ll=5.47889, loss_ll_paf=7.85063, loss_ll_heat=3.10715, q=1000
[2018-07-12 10:02:56,009] [train] [INFO] epoch=20.00 step=152600, 10.4159 examples/sec lr=0.000000, loss=26.6268, loss_ll=4.95704, loss_ll_paf=7.1612, loss_ll_heat=2.75287, q=1000
[2018-07-12 10:05:31,867] [train] [INFO] epoch=20.00 step=152700, 10.4158 examples/sec lr=0.000000, loss=41.5507, loss_ll=7.42375, loss_ll_paf=11.7337, loss_ll_heat=3.1138, q=1000
[2018-07-12 10:08:05,818] [train] [INFO] epoch=20.00 step=152800, 10.4158 examples/sec lr=0.000000, loss=29.7157, loss_ll=4.79856, loss_ll_paf=7.32588, loss_ll_heat=2.27124, q=1000
[2018-07-12 10:10:41,662] [train] [INFO] epoch=20.00 step=152900, 10.4157 examples/sec lr=0.000000, loss=29.6728, loss_ll=5.11377, loss_ll_paf=7.58234, loss_ll_heat=2.6452, q=1000
[2018-07-12 10:13:15,865] [train] [INFO] epoch=20.00 step=153000, 10.4156 examples/sec lr=0.000000, loss=23.9769, loss_ll=4.14735, loss_ll_paf=5.84456, loss_ll_heat=2.45014, q=1000
[2018-07-12 10:16:03,844] [train] [INFO] epoch=20.00 step=153100, 10.4150 examples/sec lr=0.000000, loss=53.5905, loss_ll=9.7433, loss_ll_paf=16.8161, loss_ll_heat=2.67046, q=1000
[2018-07-12 10:18:37,050] [train] [INFO] epoch=20.00 step=153200, 10.4150 examples/sec lr=0.000000, loss=22.5411, loss_ll=3.97797, loss_ll_paf=5.67707, loss_ll_heat=2.27886, q=1000
[2018-07-12 10:21:11,494] [train] [INFO] epoch=20.00 step=153300, 10.4150 examples/sec lr=0.000000, loss=42.3002, loss_ll=7.48573, loss_ll_paf=11.5282, loss_ll_heat=3.44325, q=1000
[2018-07-12 10:23:44,984] [train] [INFO] epoch=20.00 step=153400, 10.4150 examples/sec lr=0.000000, loss=23.9987, loss_ll=4.1073, loss_ll_paf=5.83951, loss_ll_heat=2.3751, q=1000
[2018-07-12 10:26:20,299] [train] [INFO] epoch=20.00 step=153500, 10.4149 examples/sec lr=0.000000, loss=39.8138, loss_ll=7.13527, loss_ll_paf=11.6017, loss_ll_heat=2.66886, q=1000
[2018-07-12 10:28:55,350] [train] [INFO] epoch=20.00 step=153600, 10.4148 examples/sec lr=0.000000, loss=32.5242, loss_ll=5.53488, loss_ll_paf=8.25798, loss_ll_heat=2.81179, q=1000
[2018-07-12 10:31:30,124] [train] [INFO] epoch=20.00 step=153700, 10.4148 examples/sec lr=0.000000, loss=18.0345, loss_ll=3.17305, loss_ll_paf=4.51944, loss_ll_heat=1.82666, q=1000
[2018-07-12 10:34:04,095] [train] [INFO] epoch=20.00 step=153800, 10.4148 examples/sec lr=0.000000, loss=31.6688, loss_ll=5.27712, loss_ll_paf=7.95569, loss_ll_heat=2.59856, q=1000
[2018-07-12 10:36:36,741] [train] [INFO] epoch=20.00 step=153900, 10.4148 examples/sec lr=0.000000, loss=26.4856, loss_ll=4.65016, loss_ll_paf=6.95374, loss_ll_heat=2.34657, q=1000
[2018-07-12 10:39:09,294] [train] [INFO] epoch=20.00 step=154000, 10.4149 examples/sec lr=0.000000, loss=34.5344, loss_ll=5.64558, loss_ll_paf=8.46064, loss_ll_heat=2.83052, q=1000
[2018-07-12 10:41:56,174] [train] [INFO] epoch=20.00 step=154100, 10.4143 examples/sec lr=0.000000, loss=20.2759, loss_ll=3.29738, loss_ll_paf=4.28373, loss_ll_heat=2.31103, q=1000
[2018-07-12 10:44:28,142] [train] [INFO] epoch=20.00 step=154200, 10.4144 examples/sec lr=0.000000, loss=30.1207, loss_ll=5.44946, loss_ll_paf=8.48549, loss_ll_heat=2.41344, q=1000
[2018-07-12 10:47:02,345] [train] [INFO] epoch=20.00 step=154300, 10.4143 examples/sec lr=0.000000, loss=21.6345, loss_ll=3.6714, loss_ll_paf=5.13234, loss_ll_heat=2.21045, q=1000
[2018-07-12 10:49:34,831] [train] [INFO] epoch=20.00 step=154400, 10.4144 examples/sec lr=0.000000, loss=38.1168, loss_ll=6.66377, loss_ll_paf=10.6393, loss_ll_heat=2.6882, q=1000
[2018-07-12 10:52:09,033] [train] [INFO] epoch=20.00 step=154500, 10.4144 examples/sec lr=0.000000, loss=29.7901, loss_ll=5.01141, loss_ll_paf=7.46855, loss_ll_heat=2.55426, q=1000
[2018-07-12 10:54:41,326] [train] [INFO] epoch=20.00 step=154600, 10.4144 examples/sec lr=0.000000, loss=28.0008, loss_ll=4.72105, loss_ll_paf=7.45604, loss_ll_heat=1.98606, q=1000
[2018-07-12 10:57:15,110] [train] [INFO] epoch=20.00 step=154700, 10.4144 examples/sec lr=0.000000, loss=40.6297, loss_ll=7.1924, loss_ll_paf=11.9644, loss_ll_heat=2.42037, q=1000
[2018-07-12 10:59:49,132] [train] [INFO] epoch=20.00 step=154800, 10.4144 examples/sec lr=0.000000, loss=30.2457, loss_ll=4.95252, loss_ll_paf=7.67295, loss_ll_heat=2.2321, q=1000
[2018-07-12 11:02:24,072] [train] [INFO] epoch=20.00 step=154900, 10.4143 examples/sec lr=0.000000, loss=28.3238, loss_ll=5.05473, loss_ll_paf=7.70942, loss_ll_heat=2.40004, q=1000
[2018-07-12 11:04:56,943] [train] [INFO] epoch=20.00 step=155000, 10.4144 examples/sec lr=0.000000, loss=30.4177, loss_ll=5.36453, loss_ll_paf=8.69364, loss_ll_heat=2.03543, q=1000
[2018-07-12 11:07:46,628] [train] [INFO] epoch=20.00 step=155100, 10.4137 examples/sec lr=0.000000, loss=31.5967, loss_ll=5.69639, loss_ll_paf=8.35679, loss_ll_heat=3.03598, q=1000
[2018-07-12 11:10:17,025] [train] [INFO] epoch=20.00 step=155200, 10.4138 examples/sec lr=0.000000, loss=35.8477, loss_ll=6.41807, loss_ll_paf=10.0154, loss_ll_heat=2.82077, q=1000
[2018-07-12 11:12:50,646] [train] [INFO] epoch=20.00 step=155300, 10.4138 examples/sec lr=0.000000, loss=31.8175, loss_ll=5.46671, loss_ll_paf=8.08718, loss_ll_heat=2.84623, q=1000
[2018-07-12 11:15:24,287] [train] [INFO] epoch=20.00 step=155400, 10.4138 examples/sec lr=0.000000, loss=22.6593, loss_ll=3.73372, loss_ll_paf=4.89567, loss_ll_heat=2.57177, q=1000
[2018-07-12 11:17:58,300] [train] [INFO] epoch=20.00 step=155500, 10.4138 examples/sec lr=0.000000, loss=33.6197, loss_ll=5.85381, loss_ll_paf=9.62423, loss_ll_heat=2.08338, q=1000
[2018-07-12 11:20:31,508] [train] [INFO] epoch=20.00 step=155600, 10.4138 examples/sec lr=0.000000, loss=31.2705, loss_ll=5.44916, loss_ll_paf=7.55545, loss_ll_heat=3.34287, q=1000
[2018-07-12 11:23:05,399] [train] [INFO] epoch=20.00 step=155700, 10.4138 examples/sec lr=0.000000, loss=38.6597, loss_ll=6.7536, loss_ll_paf=10.9938, loss_ll_heat=2.51338, q=1000
[2018-07-12 11:25:38,408] [train] [INFO] epoch=20.00 step=155800, 10.4138 examples/sec lr=0.000000, loss=28.1384, loss_ll=4.80421, loss_ll_paf=6.88094, loss_ll_heat=2.72748, q=1000
[2018-07-12 11:28:10,670] [train] [INFO] epoch=20.00 step=155900, 10.4139 examples/sec lr=0.000000, loss=25.9874, loss_ll=4.58511, loss_ll_paf=6.86236, loss_ll_heat=2.30787, q=1000
[2018-07-12 11:30:43,544] [train] [INFO] epoch=20.00 step=156000, 10.4139 examples/sec lr=0.000000, loss=40.1995, loss_ll=7.05759, loss_ll_paf=11.5396, loss_ll_heat=2.57557, q=1000
[2018-07-12 11:33:30,646] [train] [INFO] epoch=20.00 step=156100, 10.4133 examples/sec lr=0.000000, loss=40.7467, loss_ll=7.19601, loss_ll_paf=11.3317, loss_ll_heat=3.06035, q=1000
[2018-07-12 11:36:01,510] [train] [INFO] epoch=20.00 step=156200, 10.4135 examples/sec lr=0.000000, loss=26.7295, loss_ll=4.30679, loss_ll_paf=5.96603, loss_ll_heat=2.64754, q=1000
[2018-07-12 11:38:34,896] [train] [INFO] epoch=20.00 step=156300, 10.4135 examples/sec lr=0.000000, loss=34.5475, loss_ll=5.06778, loss_ll_paf=7.77079, loss_ll_heat=2.36477, q=1000
[2018-07-12 11:41:07,769] [train] [INFO] epoch=20.00 step=156400, 10.4135 examples/sec lr=0.000000, loss=38.4361, loss_ll=7.37071, loss_ll_paf=12.5263, loss_ll_heat=2.21511, q=1000
[2018-07-12 11:43:41,413] [train] [INFO] epoch=20.00 step=156500, 10.4135 examples/sec lr=0.000000, loss=33.6022, loss_ll=5.84068, loss_ll_paf=8.60183, loss_ll_heat=3.07953, q=1000
[2018-07-12 11:46:13,753] [train] [INFO] epoch=20.00 step=156600, 10.4136 examples/sec lr=0.000000, loss=32.3999, loss_ll=5.85151, loss_ll_paf=8.59661, loss_ll_heat=3.10642, q=1000
[2018-07-12 11:48:46,809] [train] [INFO] epoch=20.00 step=156700, 10.4136 examples/sec lr=0.000000, loss=28.7604, loss_ll=5.13574, loss_ll_paf=7.14697, loss_ll_heat=3.12451, q=1000
[2018-07-12 11:51:20,124] [train] [INFO] epoch=20.00 step=156800, 10.4136 examples/sec lr=0.000000, loss=30.0313, loss_ll=4.7283, loss_ll_paf=7.07121, loss_ll_heat=2.3854, q=1000
[2018-07-12 11:53:54,688] [train] [INFO] epoch=20.00 step=156900, 10.4136 examples/sec lr=0.000000, loss=31.2692, loss_ll=5.64761, loss_ll_paf=8.39463, loss_ll_heat=2.90058, q=1000
[2018-07-12 11:56:29,436] [train] [INFO] epoch=20.00 step=157000, 10.4135 examples/sec lr=0.000000, loss=28.4913, loss_ll=4.68402, loss_ll_paf=6.72059, loss_ll_heat=2.64746, q=1000
[2018-07-12 11:59:18,411] [train] [INFO] epoch=20.00 step=157100, 10.4129 examples/sec lr=0.000000, loss=22.8918, loss_ll=3.52724, loss_ll_paf=4.92247, loss_ll_heat=2.13201, q=1000
[2018-07-12 12:01:48,750] [train] [INFO] epoch=20.00 step=157200, 10.4130 examples/sec lr=0.000000, loss=29.2084, loss_ll=5.03304, loss_ll_paf=7.3298, loss_ll_heat=2.73628, q=1000
[2018-07-12 12:04:21,829] [train] [INFO] epoch=20.00 step=157300, 10.4130 examples/sec lr=0.000000, loss=38.9428, loss_ll=6.59714, loss_ll_paf=10.2369, loss_ll_heat=2.95738, q=1000
[2018-07-12 12:06:54,413] [train] [INFO] epoch=20.00 step=157400, 10.4131 examples/sec lr=0.000000, loss=33.0989, loss_ll=5.52526, loss_ll_paf=8.69627, loss_ll_heat=2.35424, q=1000
[2018-07-12 12:10:47,701] [train] [INFO] epoch=20.00 step=157500, 10.4096 examples/sec lr=0.000000, loss=30.6566, loss_ll=5.14163, loss_ll_paf=7.95584, loss_ll_heat=2.32743, q=1000
[2018-07-12 12:13:20,358] [train] [INFO] epoch=20.00 step=157600, 10.4097 examples/sec lr=0.000000, loss=29.5512, loss_ll=5.08452, loss_ll_paf=7.552, loss_ll_heat=2.61703, q=1000
[2018-07-12 12:15:52,178] [train] [INFO] epoch=20.00 step=157700, 10.4098 examples/sec lr=0.000000, loss=24.4914, loss_ll=4.30626, loss_ll_paf=6.16477, loss_ll_heat=2.44775, q=1000
[2018-07-12 12:18:23,798] [train] [INFO] epoch=20.00 step=157800, 10.4099 examples/sec lr=0.000000, loss=29.2187, loss_ll=5.27751, loss_ll_paf=7.88941, loss_ll_heat=2.66561, q=1000
[2018-07-12 12:20:54,440] [train] [INFO] epoch=20.00 step=157900, 10.4100 examples/sec lr=0.000000, loss=29.3214, loss_ll=5.07218, loss_ll_paf=7.06032, loss_ll_heat=3.08404, q=1000
[2018-07-12 12:23:27,423] [train] [INFO] epoch=20.00 step=158000, 10.4100 examples/sec lr=0.000000, loss=37.388, loss_ll=6.71562, loss_ll_paf=10.7162, loss_ll_heat=2.71503, q=1000
[2018-07-12 12:26:13,057] [train] [INFO] epoch=20.00 step=158100, 10.4095 examples/sec lr=0.000000, loss=31.2268, loss_ll=5.61116, loss_ll_paf=8.39558, loss_ll_heat=2.82674, q=1000
[2018-07-12 12:28:43,734] [train] [INFO] epoch=20.00 step=158200, 10.4096 examples/sec lr=0.000000, loss=40.395, loss_ll=7.51752, loss_ll_paf=11.5467, loss_ll_heat=3.48833, q=1000
[2018-07-12 12:31:14,642] [train] [INFO] epoch=20.00 step=158300, 10.4098 examples/sec lr=0.000000, loss=34.9521, loss_ll=6.54485, loss_ll_paf=10.1763, loss_ll_heat=2.91338, q=1000
[2018-07-12 12:33:45,704] [train] [INFO] epoch=20.00 step=158400, 10.4099 examples/sec lr=0.000000, loss=35.1069, loss_ll=6.58762, loss_ll_paf=9.6543, loss_ll_heat=3.52093, q=1000
[2018-07-12 12:36:17,607] [train] [INFO] epoch=20.00 step=158500, 10.4099 examples/sec lr=0.000000, loss=22.001, loss_ll=3.91029, loss_ll_paf=5.47229, loss_ll_heat=2.3483, q=1000
[2018-07-12 12:38:50,658] [train] [INFO] epoch=20.00 step=158600, 10.4100 examples/sec lr=0.000000, loss=28.1714, loss_ll=5.04968, loss_ll_paf=7.2444, loss_ll_heat=2.85496, q=1000
[2018-07-12 12:41:23,114] [train] [INFO] epoch=20.00 step=158700, 10.4100 examples/sec lr=0.000000, loss=29.0853, loss_ll=4.76807, loss_ll_paf=6.6947, loss_ll_heat=2.84144, q=1000
[2018-07-12 12:43:53,869] [train] [INFO] epoch=20.00 step=158800, 10.4102 examples/sec lr=0.000000, loss=31.2332, loss_ll=5.59453, loss_ll_paf=8.24347, loss_ll_heat=2.94558, q=1000
[2018-07-12 12:46:26,798] [train] [INFO] epoch=20.00 step=158900, 10.4102 examples/sec lr=0.000000, loss=28.7508, loss_ll=5.25802, loss_ll_paf=8.1269, loss_ll_heat=2.38914, q=1000
[2018-07-12 12:48:57,713] [train] [INFO] epoch=20.00 step=159000, 10.4103 examples/sec lr=0.000000, loss=31.0948, loss_ll=5.39463, loss_ll_paf=8.72344, loss_ll_heat=2.06582, q=1000
[2018-07-12 12:51:45,522] [train] [INFO] epoch=20.00 step=159100, 10.4097 examples/sec lr=0.000000, loss=31.8185, loss_ll=5.96152, loss_ll_paf=9.24673, loss_ll_heat=2.67631, q=1000
[2018-07-12 12:54:18,144] [train] [INFO] epoch=20.00 step=159200, 10.4097 examples/sec lr=0.000000, loss=19.5728, loss_ll=3.00312, loss_ll_paf=4.06595, loss_ll_heat=1.9403, q=1000
[2018-07-12 12:56:50,736] [train] [INFO] epoch=20.00 step=159300, 10.4098 examples/sec lr=0.000000, loss=27.5871, loss_ll=4.82822, loss_ll_paf=7.26206, loss_ll_heat=2.39439, q=1000
[2018-07-12 12:59:24,893] [train] [INFO] epoch=20.00 step=159400, 10.4098 examples/sec lr=0.000000, loss=23.2144, loss_ll=4.23009, loss_ll_paf=5.8489, loss_ll_heat=2.61128, q=1000
[2018-07-12 13:01:57,089] [train] [INFO] epoch=20.00 step=159500, 10.4098 examples/sec lr=0.000000, loss=35.3229, loss_ll=6.28918, loss_ll_paf=9.58199, loss_ll_heat=2.99638, q=1000
[2018-07-12 13:04:29,916] [train] [INFO] epoch=20.00 step=159600, 10.4099 examples/sec lr=0.000000, loss=35.794, loss_ll=6.32047, loss_ll_paf=9.59368, loss_ll_heat=3.04727, q=1000
[2018-07-12 13:07:01,941] [train] [INFO] epoch=20.00 step=159700, 10.4099 examples/sec lr=0.000000, loss=29.8153, loss_ll=5.18834, loss_ll_paf=7.97341, loss_ll_heat=2.40328, q=1000
[2018-07-12 13:09:35,008] [train] [INFO] epoch=21.00 step=159800, 10.4100 examples/sec lr=0.000000, loss=26.3969, loss_ll=4.52078, loss_ll_paf=6.63238, loss_ll_heat=2.40919, q=1000
[2018-07-12 13:12:05,246] [train] [INFO] epoch=21.00 step=159900, 10.4101 examples/sec lr=0.000000, loss=23.8232, loss_ll=4.04104, loss_ll_paf=5.33546, loss_ll_heat=2.74662, q=1000
[2018-07-12 13:14:38,094] [train] [INFO] epoch=21.00 step=160000, 10.4102 examples/sec lr=0.000000, loss=23.2497, loss_ll=3.7182, loss_ll_paf=5.20336, loss_ll_heat=2.23305, q=1000
[2018-07-12 13:17:24,624] [train] [INFO] epoch=21.00 step=160100, 10.4096 examples/sec lr=0.000000, loss=29.607, loss_ll=5.07082, loss_ll_paf=7.06359, loss_ll_heat=3.07804, q=1000
[2018-07-12 13:19:55,344] [train] [INFO] epoch=21.00 step=160200, 10.4097 examples/sec lr=0.000000, loss=31.266, loss_ll=5.95659, loss_ll_paf=9.91346, loss_ll_heat=1.99972, q=1000
[2018-07-12 13:22:28,537] [train] [INFO] epoch=21.00 step=160300, 10.4098 examples/sec lr=0.000000, loss=28.2856, loss_ll=5.14469, loss_ll_paf=8.05599, loss_ll_heat=2.23339, q=1000
[2018-07-12 13:25:01,505] [train] [INFO] epoch=21.00 step=160400, 10.4098 examples/sec lr=0.000000, loss=41.4348, loss_ll=6.8689, loss_ll_paf=10.0998, loss_ll_heat=3.63803, q=1000
[2018-07-12 13:27:34,269] [train] [INFO] epoch=21.00 step=160500, 10.4098 examples/sec lr=0.000000, loss=25.7331, loss_ll=4.59075, loss_ll_paf=6.76415, loss_ll_heat=2.41735, q=1000
[2018-07-12 13:30:08,279] [train] [INFO] epoch=21.00 step=160600, 10.4098 examples/sec lr=0.000000, loss=28.9171, loss_ll=5.01695, loss_ll_paf=8.06953, loss_ll_heat=1.96437, q=1000
[2018-07-12 13:32:39,517] [train] [INFO] epoch=21.00 step=160700, 10.4099 examples/sec lr=0.000000, loss=26.0653, loss_ll=4.33766, loss_ll_paf=5.98965, loss_ll_heat=2.68568, q=1000
[2018-07-12 13:35:14,125] [train] [INFO] epoch=21.00 step=160800, 10.4099 examples/sec lr=0.000000, loss=27.8244, loss_ll=4.67244, loss_ll_paf=6.57825, loss_ll_heat=2.76663, q=1000
[2018-07-12 13:37:48,378] [train] [INFO] epoch=21.00 step=160900, 10.4099 examples/sec lr=0.000000, loss=40.3242, loss_ll=7.46377, loss_ll_paf=12.3953, loss_ll_heat=2.53223, q=1000
[2018-07-12 13:40:21,940] [train] [INFO] epoch=21.00 step=161000, 10.4099 examples/sec lr=0.000000, loss=24.3938, loss_ll=3.9132, loss_ll_paf=5.68983, loss_ll_heat=2.13657, q=1000
[2018-07-12 13:43:10,730] [train] [INFO] epoch=21.00 step=161100, 10.4092 examples/sec lr=0.000000, loss=29.8531, loss_ll=5.45912, loss_ll_paf=8.53074, loss_ll_heat=2.38749, q=1000
[2018-07-12 13:45:44,382] [train] [INFO] epoch=21.00 step=161200, 10.4092 examples/sec lr=0.000000, loss=33.4961, loss_ll=5.54566, loss_ll_paf=8.59182, loss_ll_heat=2.4995, q=1000
[2018-07-12 13:48:17,309] [train] [INFO] epoch=21.00 step=161300, 10.4093 examples/sec lr=0.000000, loss=28.6965, loss_ll=4.78115, loss_ll_paf=6.7767, loss_ll_heat=2.7856, q=1000
[2018-07-12 13:50:51,173] [train] [INFO] epoch=21.00 step=161400, 10.4093 examples/sec lr=0.000000, loss=32.6924, loss_ll=6.19498, loss_ll_paf=9.50051, loss_ll_heat=2.88944, q=1000
[2018-07-12 13:53:25,374] [train] [INFO] epoch=21.00 step=161500, 10.4092 examples/sec lr=0.000000, loss=31.7817, loss_ll=5.56416, loss_ll_paf=8.95633, loss_ll_heat=2.17199, q=1000
[2018-07-12 13:55:59,209] [train] [INFO] epoch=21.00 step=161600, 10.4092 examples/sec lr=0.000000, loss=32.8022, loss_ll=5.63979, loss_ll_paf=8.58857, loss_ll_heat=2.69101, q=1000
[2018-07-12 13:58:33,157] [train] [INFO] epoch=21.00 step=161700, 10.4092 examples/sec lr=0.000000, loss=31.3813, loss_ll=5.87588, loss_ll_paf=9.13303, loss_ll_heat=2.61874, q=1000
[2018-07-12 14:01:06,059] [train] [INFO] epoch=21.00 step=161800, 10.4093 examples/sec lr=0.000000, loss=46.5565, loss_ll=8.15082, loss_ll_paf=12.8452, loss_ll_heat=3.45645, q=1000
[2018-07-12 14:03:39,118] [train] [INFO] epoch=21.00 step=161900, 10.4093 examples/sec lr=0.000000, loss=22.4448, loss_ll=3.95908, loss_ll_paf=5.04794, loss_ll_heat=2.87022, q=1000
[2018-07-12 14:06:14,152] [train] [INFO] epoch=21.00 step=162000, 10.4092 examples/sec lr=0.000000, loss=35.9386, loss_ll=6.57323, loss_ll_paf=10.8751, loss_ll_heat=2.27137, q=1000
[2018-07-12 14:09:04,534] [train] [INFO] epoch=21.00 step=162100, 10.4085 examples/sec lr=0.000000, loss=22.5893, loss_ll=3.40534, loss_ll_paf=4.9309, loss_ll_heat=1.87978, q=1000
[2018-07-12 14:11:47,950] [train] [INFO] epoch=21.00 step=162200, 10.4081 examples/sec lr=0.000000, loss=30.8291, loss_ll=5.80458, loss_ll_paf=8.71695, loss_ll_heat=2.89222, q=1000
[2018-07-12 14:14:40,100] [train] [INFO] epoch=21.00 step=162300, 10.4074 examples/sec lr=0.000000, loss=32.5712, loss_ll=5.801, loss_ll_paf=8.70348, loss_ll_heat=2.89852, q=1000
[2018-07-12 14:17:22,147] [train] [INFO] epoch=21.00 step=162400, 10.4070 examples/sec lr=0.000000, loss=22.2095, loss_ll=3.63444, loss_ll_paf=5.72427, loss_ll_heat=1.54461, q=1000
[2018-07-12 14:20:02,356] [train] [INFO] epoch=21.00 step=162500, 10.4067 examples/sec lr=0.000000, loss=29.8412, loss_ll=5.18672, loss_ll_paf=7.2941, loss_ll_heat=3.07935, q=1000
[2018-07-12 14:22:41,533] [train] [INFO] epoch=21.00 step=162600, 10.4065 examples/sec lr=0.000000, loss=43.934, loss_ll=8.18654, loss_ll_paf=12.5987, loss_ll_heat=3.77441, q=1000
[2018-07-12 14:25:15,436] [train] [INFO] epoch=21.00 step=162700, 10.4065 examples/sec lr=0.000000, loss=39.5995, loss_ll=7.09733, loss_ll_paf=11.0887, loss_ll_heat=3.10599, q=1000
[2018-07-12 14:27:47,477] [train] [INFO] epoch=21.00 step=162800, 10.4066 examples/sec lr=0.000000, loss=39.4, loss_ll=7.02304, loss_ll_paf=11.5226, loss_ll_heat=2.52345, q=1000
[2018-07-12 14:30:24,845] [train] [INFO] epoch=21.00 step=162900, 10.4064 examples/sec lr=0.000000, loss=24.4097, loss_ll=4.18825, loss_ll_paf=5.90135, loss_ll_heat=2.47516, q=1000
[2018-07-12 14:32:59,812] [train] [INFO] epoch=21.00 step=163000, 10.4064 examples/sec lr=0.000000, loss=21.3664, loss_ll=3.70926, loss_ll_paf=4.95982, loss_ll_heat=2.4587, q=1000
