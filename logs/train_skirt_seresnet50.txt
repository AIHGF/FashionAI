/usr/lib64/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
[2018-07-12 16:01:37,136] [train] [INFO] define model+
[2018-07-12 16:01:37,141] [pose_dataset] [INFO] dataflow img_path=/home/shy/projects/tf-openpose/data/
[2018-07-12 16:01:40,185] [pose_dataset] [INFO] /home/shy/projects/tf-openpose/data/train/train_bak.csv dataset 12546
[32m[0712 16:01:40 @parallel.py:178][0m [MultiProcessPrefetchData] Will fork a dataflow more than one times. This assumes the datapoints are i.i.d.
[32m[0712 16:01:40 @parallel.py:178][0m [MultiProcessPrefetchData] Will fork a dataflow more than one times. This assumes the datapoints are i.i.d.
[2018-07-12 16:01:40,953] [pose_dataset] [INFO] dataflow img_path=/home/shy/projects/tf-openpose/data/
[2018-07-12 16:01:42,551] [pose_dataset] [INFO] /home/shy/projects/tf-openpose/data/train/val_bak.csv dataset 1394
[32m[0712 16:01:42 @parallel.py:178][0m [MultiProcessPrefetchData] Will fork a dataflow more than one times. This assumes the datapoints are i.i.d.
[32m[0712 16:01:42 @parallel.py:178][0m [MultiProcessPrefetchData] Will fork a dataflow more than one times. This assumes the datapoints are i.i.d.
[2018-07-12 16:01:43,257] [train] [INFO] tensorboard val image: 12
[2018-07-12 16:01:43,257] [train] [INFO] Tensor("fifo_queue_Dequeue:0", shape=(16, 368, 368, 3), dtype=float32, device=/device:GPU:0)
[2018-07-12 16:01:43,258] [train] [INFO] Tensor("fifo_queue_Dequeue:1", shape=(16, 46, 46, 5), dtype=float32, device=/device:GPU:0)
[2018-07-12 16:01:43,258] [train] [INFO] Tensor("fifo_queue_Dequeue:2", shape=(16, 46, 46, 10), dtype=float32, device=/device:GPU:0)
[2018-07-12 16:03:01,995] [train] [INFO] define model-
2018-07-12 16:03:04.481025: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-07-12 16:03:04.481080: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-07-12 16:03:04.481087: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-07-12 16:03:04.481093: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-07-12 16:03:04.481099: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2018-07-12 16:03:12.546841: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: 
name: GeForce GTX 1080 Ti
major: 6 minor: 1 memoryClockRate (GHz) 1.582
pciBusID 0000:04:00.0
Total memory: 10.92GiB
Free memory: 10.76GiB
2018-07-12 16:03:12.770852: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x563245f0 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.
2018-07-12 16:03:12.772246: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 1 with properties: 
name: GeForce GTX 1080 Ti
major: 6 minor: 1 memoryClockRate (GHz) 1.582
pciBusID 0000:05:00.0
Total memory: 10.92GiB
Free memory: 10.76GiB
2018-07-12 16:03:12.981653: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x7568dff0 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.
2018-07-12 16:03:12.982988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 2 with properties: 
name: GeForce GTX 1080 Ti
major: 6 minor: 1 memoryClockRate (GHz) 1.582
pciBusID 0000:06:00.0
Total memory: 10.92GiB
Free memory: 10.76GiB
2018-07-12 16:03:13.195285: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x756ec9a0 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.
2018-07-12 16:03:13.196539: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 3 with properties: 
name: GeForce GTX 1080 Ti
major: 6 minor: 1 memoryClockRate (GHz) 1.582
pciBusID 0000:07:00.0
Total memory: 10.92GiB
Free memory: 10.76GiB
2018-07-12 16:03:13.404654: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x5627de50 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.
2018-07-12 16:03:13.405972: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 4 with properties: 
name: GeForce GTX 1080 Ti
major: 6 minor: 1 memoryClockRate (GHz) 1.582
pciBusID 0000:08:00.0
Total memory: 10.92GiB
Free memory: 10.76GiB
2018-07-12 16:03:13.617862: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x561d1bc0 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.
2018-07-12 16:03:13.619159: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 5 with properties: 
name: GeForce GTX 1080 Ti
major: 6 minor: 1 memoryClockRate (GHz) 1.582
pciBusID 0000:0b:00.0
Total memory: 10.92GiB
Free memory: 10.76GiB
2018-07-12 16:03:13.858866: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x562328e0 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.
2018-07-12 16:03:13.860927: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 6 with properties: 
name: GeForce GTX 1080 Ti
major: 6 minor: 1 memoryClockRate (GHz) 1.582
pciBusID 0000:0c:00.0
Total memory: 10.92GiB
Free memory: 10.76GiB
2018-07-12 16:03:14.140567: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x5625f9d0 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.
2018-07-12 16:03:14.142886: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 7 with properties: 
name: GeForce GTX 1080 Ti
major: 6 minor: 1 memoryClockRate (GHz) 1.582
pciBusID 0000:0d:00.0
Total memory: 10.92GiB
Free memory: 10.76GiB
2018-07-12 16:03:14.188535: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 1 2 3 4 5 6 7 
2018-07-12 16:03:14.188581: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y Y Y Y Y Y Y Y 
2018-07-12 16:03:14.188592: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 1:   Y Y Y Y Y Y Y Y 
2018-07-12 16:03:14.188600: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 2:   Y Y Y Y Y Y Y Y 
2018-07-12 16:03:14.188608: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 3:   Y Y Y Y Y Y Y Y 
2018-07-12 16:03:14.188616: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 4:   Y Y Y Y Y Y Y Y 
2018-07-12 16:03:14.188624: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 5:   Y Y Y Y Y Y Y Y 
2018-07-12 16:03:14.188640: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 6:   Y Y Y Y Y Y Y Y 
2018-07-12 16:03:14.188649: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 7:   Y Y Y Y Y Y Y Y 
2018-07-12 16:03:14.188840: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:04:00.0)
2018-07-12 16:03:14.188854: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:05:00.0)
2018-07-12 16:03:14.188863: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:2) -> (device: 2, name: GeForce GTX 1080 Ti, pci bus id: 0000:06:00.0)
2018-07-12 16:03:14.188871: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:3) -> (device: 3, name: GeForce GTX 1080 Ti, pci bus id: 0000:07:00.0)
2018-07-12 16:03:14.188896: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:4) -> (device: 4, name: GeForce GTX 1080 Ti, pci bus id: 0000:08:00.0)
2018-07-12 16:03:14.188905: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:5) -> (device: 5, name: GeForce GTX 1080 Ti, pci bus id: 0000:0b:00.0)
2018-07-12 16:03:14.188913: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:6) -> (device: 6, name: GeForce GTX 1080 Ti, pci bus id: 0000:0c:00.0)
2018-07-12 16:03:14.188920: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:7) -> (device: 7, name: GeForce GTX 1080 Ti, pci bus id: 0000:0d:00.0)
[2018-07-12 16:03:16,163] [train] [INFO] model weights initialization
[2018-07-12 16:04:03,834] [train] [INFO] Restore pretrained weights from /home/shy/projects/tf-openpose/models/numpy/se_resnet50.npy ...
conv4_1_3x3/bn/gamma   0 / 225
conv4_2_1x1_increase/bn/beta   1 / 225
conv2_1_1x1_increase/bn/gamma   2 / 225
conv4_1_1x1_proj/kernel   3 / 225
conv5_2_1x1_reduce/kernel   4 / 225
conv3_2_3x3/bn/gamma   5 / 225
conv2_3_3x3/bn/gamma   6 / 225
conv3_4_1x1_reduce/bn/beta   7 / 225
conv5_2_1x1_down/kernel   8 / 225
conv3_2_1x1_down/kernel   9 / 225
conv3_4_1x1_increase/bn/beta   10 / 225
conv2_3_3x3/bn/beta   11 / 225
conv5_1_1x1_down/bias   12 / 225
conv4_4_3x3/bn/beta   13 / 225
conv2_1_1x1_proj/bn/gamma   14 / 225
conv3_2_1x1_down/bias   15 / 225
conv4_6_1x1_up/kernel   16 / 225
conv4_4_1x1_reduce/bn/gamma   17 / 225
conv5_2_1x1_reduce/bn/gamma   18 / 225
conv3_4_3x3/kernel   19 / 225
conv1/7x7_s2/bn/gamma   20 / 225
conv4_6_1x1_up/bias   21 / 225
conv3_3_1x1_down/kernel   22 / 225
conv4_1_1x1_reduce/kernel   23 / 225
conv2_2_3x3/kernel   24 / 225
conv2_1_3x3/kernel   25 / 225
conv5_3_1x1_down/kernel   26 / 225
conv2_1_1x1_reduce/kernel   27 / 225
conv3_2_1x1_reduce/kernel   28 / 225
conv4_3_3x3/bn/gamma   29 / 225
conv4_2_1x1_up/bias   30 / 225
conv5_1_1x1_increase/kernel   31 / 225
dense/bias   32 / 225
conv4_4_1x1_reduce/bn/beta   33 / 225
conv3_1_1x1_proj/bn/gamma   34 / 225
conv5_1_1x1_proj/kernel   35 / 225
conv5_1_3x3/bn/beta   36 / 225
conv5_1_1x1_up/kernel   37 / 225
conv4_3_1x1_reduce/kernel   38 / 225
conv4_6_1x1_reduce/bn/beta   39 / 225
conv5_3_1x1_increase/bn/beta   40 / 225
conv4_4_1x1_increase/kernel   41 / 225
conv3_4_1x1_increase/kernel   42 / 225
conv3_3_1x1_down/bias   43 / 225
conv3_2_3x3/kernel   44 / 225
conv5_3_1x1_increase/kernel   45 / 225
conv5_3_3x3/bn/beta   46 / 225
conv5_3_1x1_reduce/bn/beta   47 / 225
conv4_6_1x1_increase/bn/beta   48 / 225
conv2_2_1x1_reduce/bn/beta   49 / 225
conv5_2_3x3/bn/gamma   50 / 225
conv2_1_1x1_increase/bn/beta   51 / 225
conv4_1_1x1_up/kernel   52 / 225
conv3_2_1x1_up/kernel   53 / 225
conv5_1_1x1_reduce/bn/beta   54 / 225
conv4_4_1x1_increase/bn/gamma   55 / 225
conv4_2_3x3/bn/gamma   56 / 225
conv4_3_1x1_down/kernel   57 / 225
conv4_1_3x3/bn/beta   58 / 225
conv3_4_1x1_increase/bn/gamma   59 / 225
conv3_3_1x1_reduce/bn/beta   60 / 225
conv4_1_3x3/kernel   61 / 225
conv4_3_3x3/bn/beta   62 / 225
conv4_1_1x1_proj/bn/beta   63 / 225
conv2_3_1x1_up/kernel   64 / 225
conv4_5_1x1_up/bias   65 / 225
conv4_2_1x1_up/kernel   66 / 225
conv4_1_1x1_increase/bn/gamma   67 / 225
conv3_4_3x3/bn/gamma   68 / 225
conv5_1_1x1_reduce/bn/gamma   69 / 225
conv3_1_3x3/bn/gamma   70 / 225
conv3_2_1x1_up/bias   71 / 225
conv2_1_1x1_down/bias   72 / 225
conv2_2_3x3/bn/gamma   73 / 225
conv2_1_1x1_up/kernel   74 / 225
conv4_2_1x1_reduce/bn/gamma   75 / 225
conv4_1_1x1_reduce/bn/beta   76 / 225
conv3_3_1x1_up/kernel   77 / 225
conv3_1_1x1_proj/bn/beta   78 / 225
conv5_1_3x3/bn/gamma   79 / 225
conv3_1_1x1_reduce/bn/beta   80 / 225
conv5_3_3x3/kernel   81 / 225
conv3_4_1x1_down/kernel   82 / 225
conv4_2_1x1_reduce/kernel   83 / 225
conv4_3_3x3/kernel   84 / 225
conv3_4_1x1_down/bias   85 / 225
conv2_3_1x1_reduce/bn/gamma   86 / 225
conv4_4_3x3/bn/gamma   87 / 225
conv1/7x7_s2/kernel   88 / 225
conv3_3_1x1_reduce/bn/gamma   89 / 225
conv4_1_1x1_up/bias   90 / 225
conv2_1_1x1_increase/kernel   91 / 225
conv5_1_1x1_down/kernel   92 / 225
conv4_5_3x3/bn/gamma   93 / 225
conv4_6_3x3/kernel   94 / 225
conv4_2_1x1_down/bias   95 / 225
conv4_3_1x1_increase/bn/gamma   96 / 225
conv4_2_3x3/kernel   97 / 225
conv4_2_1x1_reduce/bn/beta   98 / 225
conv3_3_1x1_reduce/kernel   99 / 225
conv4_4_1x1_reduce/kernel   100 / 225
conv5_3_1x1_reduce/bn/gamma   101 / 225
conv2_2_1x1_down/bias   102 / 225
conv4_6_1x1_reduce/kernel   103 / 225
conv2_3_1x1_reduce/bn/beta   104 / 225
conv3_1_1x1_increase/bn/beta   105 / 225
conv4_3_1x1_reduce/bn/beta   106 / 225
conv4_6_1x1_reduce/bn/gamma   107 / 225
conv3_1_1x1_down/kernel   108 / 225
conv3_3_1x1_increase/bn/gamma   109 / 225
conv5_3_1x1_increase/bn/gamma   110 / 225
conv2_1_3x3/bn/gamma   111 / 225
conv2_3_3x3/kernel   112 / 225
conv4_4_1x1_up/bias   113 / 225
conv4_2_1x1_increase/kernel   114 / 225
conv3_4_1x1_reduce/bn/gamma   115 / 225
conv2_2_3x3/bn/beta   11[2018-07-12 18:18:50,375] [train] [INFO] Restore pretrained weights...Done
[2018-07-12 18:18:50,376] [train] [INFO] prepare file writer
[2018-07-12 18:19:13,814] [train] [INFO] prepare coordinator
[2018-07-12 18:19:14,832] [train] [INFO] Training Started.
[2018-07-12 18:24:58,523] [train] [INFO] epoch=0.00 step=100, 4.6554 examples/sec lr=0.000100, loss=142.385, loss_ll=29.1389, loss_ll_paf=49.4087, loss_ll_heat=8.86909, q=625
[2018-07-12 18:27:44,773] [train] [INFO] epoch=0.00 step=200, 6.2753 examples/sec lr=0.000100, loss=134.984, loss_ll=26.8107, loss_ll_paf=47.3836, loss_ll_heat=6.23782, q=1000
[2018-07-12 18:30:22,082] [train] [INFO] epoch=0.00 step=300, 7.1937 examples/sec lr=0.000100, loss=131.309, loss_ll=25.5366, loss_ll_paf=44.2608, loss_ll_heat=6.81243, q=1000
[2018-07-12 18:32:59,110] [train] [INFO] epoch=0.00 step=400, 7.7644 examples/sec lr=0.000100, loss=170.329, loss_ll=33.9171, loss_ll_paf=61.0469, loss_ll_heat=6.78741, q=1000
[2018-07-12 18:35:37,498] [train] [INFO] epoch=0.00 step=500, 8.1411 examples/sec lr=0.000100, loss=142.559, loss_ll=28.105, loss_ll_paf=48.7937, loss_ll_heat=7.41635, q=1000
[2018-07-12 18:38:14,343] [train] [INFO] epoch=0.00 step=600, 8.4247 examples/sec lr=0.000100, loss=84.7482, loss_ll=16.7754, loss_ll_paf=28.3183, loss_ll_heat=5.23249, q=1000
[2018-07-12 18:40:51,218] [train] [INFO] epoch=0.00 step=700, 8.6394 examples/sec lr=0.000100, loss=106.55, loss_ll=20.9163, loss_ll_paf=36.9358, loss_ll_heat=4.89668, q=1000
[2018-07-12 18:43:28,923] [train] [INFO] epoch=0.00 step=800, 8.8028 examples/sec lr=0.000100, loss=95.4033, loss_ll=19.1651, loss_ll_paf=32.5113, loss_ll_heat=5.81892, q=1000
[2018-07-12 18:46:07,549] [train] [INFO] epoch=0.00 step=900, 8.9290 examples/sec lr=0.000100, loss=107.098, loss_ll=21.5361, loss_ll_paf=36.378, loss_ll_heat=6.69423, q=1000
[2018-07-12 18:48:42,615] [train] [INFO] epoch=0.00 step=1000, 9.0509 examples/sec lr=0.000100, loss=130.772, loss_ll=25.2159, loss_ll_paf=45.9755, loss_ll_heat=4.45633, q=1000
[2018-07-12 18:52:29,853] [train] [INFO] epoch=0.00 step=1100, 8.8220 examples/sec lr=0.000100, loss=95.1187, loss_ll=18.8226, loss_ll_paf=33.5305, loss_ll_heat=4.11463, q=1000
[2018-07-12 18:55:10,546] [train] [INFO] epoch=0.00 step=1200, 8.9066 examples/sec lr=0.000100, loss=83.3665, loss_ll=16.4348, loss_ll_paf=29.3309, loss_ll_heat=3.53861, q=1000
[2018-07-12 18:57:52,817] [train] [INFO] epoch=0.00 step=1300, 8.9733 examples/sec lr=0.000100, loss=63.6215, loss_ll=12.3135, loss_ll_paf=21, loss_ll_heat=3.62698, q=1000
[2018-07-12 19:00:31,874] [train] [INFO] epoch=0.00 step=1400, 9.0431 examples/sec lr=0.000100, loss=58.02, loss_ll=11.5645, loss_ll_paf=19.9708, loss_ll_heat=3.1582, q=1000
[2018-07-12 19:03:12,593] [train] [INFO] epoch=0.00 step=1500, 9.0986 examples/sec lr=0.000100, loss=86.5097, loss_ll=16.961, loss_ll_paf=30.4714, loss_ll_heat=3.45056, q=1000
[2018-07-12 19:05:52,067] [train] [INFO] epoch=0.00 step=1600, 9.1519 examples/sec lr=0.000100, loss=96.6677, loss_ll=19.0401, loss_ll_paf=33.7967, loss_ll_heat=4.28341, q=1000
[2018-07-12 19:08:31,619] [train] [INFO] epoch=0.00 step=1700, 9.1992 examples/sec lr=0.000100, loss=67.1774, loss_ll=12.4948, loss_ll_paf=21.7055, loss_ll_heat=3.28402, q=1000
[2018-07-12 19:11:07,629] [train] [INFO] epoch=0.00 step=1800, 9.2521 examples/sec lr=0.000100, loss=50.5121, loss_ll=9.49703, loss_ll_paf=16.5861, loss_ll_heat=2.40794, q=1000
[2018-07-12 19:13:45,637] [train] [INFO] epoch=0.00 step=1900, 9.2944 examples/sec lr=0.000100, loss=51.4609, loss_ll=10.1522, loss_ll_paf=17.9738, loss_ll_heat=2.3305, q=1000
[2018-07-12 19:16:21,614] [train] [INFO] epoch=0.00 step=2000, 9.3382 examples/sec lr=0.000100, loss=93.2017, loss_ll=18.163, loss_ll_paf=33.2235, loss_ll_heat=3.10251, q=1000
[2018-07-12 19:19:50,881] [train] [INFO] epoch=0.00 step=2100, 9.2408 examples/sec lr=0.000100, loss=78.1882, loss_ll=14.0258, loss_ll_paf=25.8964, loss_ll_heat=2.15529, q=1000
[2018-07-12 19:22:31,286] [train] [INFO] epoch=0.00 step=2200, 9.2718 examples/sec lr=0.000100, loss=84.8732, loss_ll=15.8915, loss_ll_paf=29.4742, loss_ll_heat=2.30877, q=1000
[2018-07-12 19:25:09,796] [train] [INFO] epoch=0.00 step=2300, 9.3048 examples/sec lr=0.000100, loss=68.7848, loss_ll=12.1649, loss_ll_paf=22.4658, loss_ll_heat=1.86403, q=1000
[2018-07-12 19:27:55,024] [train] [INFO] epoch=0.00 step=2400, 9.3200 examples/sec lr=0.000100, loss=98.3632, loss_ll=18.4858, loss_ll_paf=34.5229, loss_ll_heat=2.44875, q=1000
[2018-07-12 19:30:58,833] [train] [INFO] epoch=0.00 step=2500, 9.2937 examples/sec lr=0.000100, loss=101.427, loss_ll=20.035, loss_ll_paf=35.776, loss_ll_heat=4.29389, q=1000
[2018-07-12 19:33:46,818] [train] [INFO] epoch=0.00 step=2600, 9.3024 examples/sec lr=0.000100, loss=67.6094, loss_ll=12.3107, loss_ll_paf=21.8977, loss_ll_heat=2.72379, q=1000
[2018-07-12 19:37:05,606] [train] [INFO] epoch=0.00 step=2700, 9.2490 examples/sec lr=0.000100, loss=48.8477, loss_ll=8.86112, loss_ll_paf=15.8689, loss_ll_heat=1.85338, q=1000
[2018-07-12 19:39:50,774] [train] [INFO] epoch=0.00 step=2800, 9.2640 examples/sec lr=0.000100, loss=51.1579, loss_ll=8.43278, loss_ll_paf=14.327, loss_ll_heat=2.53857, q=1000
[2018-07-12 19:42:59,573] [train] [INFO] epoch=0.00 step=2900, 9.2343 examples/sec lr=0.000100, loss=64.1902, loss_ll=11.602, loss_ll_paf=21.0028, loss_ll_heat=2.20115, q=1000
[2018-07-12 19:46:00,464] [train] [INFO] epoch=0.00 step=3000, 9.2208 examples/sec lr=0.000100, loss=96.9215, loss_ll=18.0532, loss_ll_paf=33.4205, loss_ll_heat=2.68596, q=1000
[2018-07-12 19:49:20,408] [train] [INFO] epoch=0.00 step=3100, 9.1757 examples/sec lr=0.000100, loss=52.8711, loss_ll=9.52542, loss_ll_paf=16.7454, loss_ll_heat=2.30549, q=1000
[2018-07-12 19:52:20,773] [train] [INFO] epoch=0.00 step=3200, 9.1659 examples/sec lr=0.000100, loss=81.0475, loss_ll=15.8718, loss_ll_paf=29.3598, loss_ll_heat=2.38368, q=1000
[2018-07-12 19:55:18,156] [train] [INFO] epoch=0.00 step=3300, 9.1614 examples/sec lr=0.000100, loss=44.6654, loss_ll=8.11767, loss_ll_paf=14.2802, loss_ll_heat=1.95517, q=1000
[2018-07-12 19:58:04,546] [train] [INFO] epoch=0.00 step=3400, 9.1741 examples/sec lr=0.000100, loss=66.7902, loss_ll=12.4722, loss_ll_paf=22.6473, loss_ll_heat=2.29712, q=1000
[2018-07-12 20:00:47,187] [train] [INFO] epoch=0.00 step=3500, 9.1919 examples/sec lr=0.000100, loss=73.4636, loss_ll=13.9369, loss_ll_paf=24.8322, loss_ll_heat=3.04156, q=1000
[2018-07-12 20:03:23,534] [train] [INFO] epoch=0.00 step=3600, 9.2179 examples/sec lr=0.000100, loss=46.7585, loss_ll=8.41657, loss_ll_paf=15.1084, loss_ll_heat=1.72472, q=1000
[2018-07-12 20:05:58,733] [train] [INFO] epoch=0.00 step=3700, 9.2444 examples/sec lr=0.000100, loss=56.5629, loss_ll=10.6105, loss_ll_paf=18.7304, loss_ll_heat=2.49051, q=1000
[2018-07-12 20:08:36,462] [train] [INFO] epoch=0.00 step=3800, 9.2660 examples/sec lr=0.000100, loss=55.9002, loss_ll=9.52138, loss_ll_paf=17.4837, loss_ll_heat=1.5591, q=1000
[2018-07-12 20:11:13,634] [train] [INFO] epoch=0.00 step=3900, 9.2874 examples/sec lr=0.000100, loss=49.9841, loss_ll=9.5246, loss_ll_paf=17.4672, loss_ll_heat=1.58204, q=1000
[2018-07-12 20:13:50,082] [train] [INFO] epoch=0.00 step=4000, 9.3088 examples/sec lr=0.000100, loss=47.3675, loss_ll=8.67368, loss_ll_paf=15.0742, loss_ll_heat=2.27312, q=1000
[2018-07-12 20:16:37,905] [train] [INFO] epoch=0.00 step=4100, 9.3141 examples/sec lr=0.000100, loss=62.5347, loss_ll=11.6664, loss_ll_paf=21.2725, loss_ll_heat=2.06038, q=1000
[2018-07-12 20:19:14,090] [train] [INFO] epoch=0.00 step=4200, 9.3343 examples/sec lr=0.000100, loss=60.1803, loss_ll=10.8643, loss_ll_paf=18.9184, loss_ll_heat=2.81017, q=1000
[2018-07-12 20:21:50,220] [train] [INFO] epoch=0.00 step=4300, 9.3537 examples/sec lr=0.000100, loss=53.32, loss_ll=9.85373, loss_ll_paf=17.6828, loss_ll_heat=2.02466, q=1000
[2018-07-12 20:24:27,119] [train] [INFO] epoch=0.00 step=4400, 9.3713 examples/sec lr=0.000100, loss=48.6387, loss_ll=8.2708, loss_ll_paf=15.0552, loss_ll_heat=1.48637, q=1000
[2018-07-12 20:27:03,578] [train] [INFO] epoch=0.00 step=4500, 9.3888 examples/sec lr=0.000100, loss=56.092, loss_ll=10.5096, loss_ll_paf=19.05, loss_ll_heat=1.96926, q=1000
[2018-07-12 20:29:40,781] [train] [INFO] epoch=0.00 step=4600, 9.4046 examples/sec lr=0.000100, loss=38.9401, loss_ll=7.31732, loss_ll_paf=12.4034, loss_ll_heat=2.23121, q=1000
[2018-07-12 20:32:16,975] [train] [INFO] epoch=0.00 step=4700, 9.4210 examples/sec lr=0.000100, loss=77.6819, loss_ll=13.8764, loss_ll_paf=26.079, loss_ll_heat=1.67382, q=1000
[2018-07-12 20:34:51,297] [train] [INFO] epoch=0.00 step=4800, 9.4390 examples/sec lr=0.000100, loss=60.6283, loss_ll=11.3667, loss_ll_paf=21.0808, loss_ll_heat=1.6526, q=1000
[2018-07-12 20:37:34,452] [train] [INFO] epoch=0.00 step=4900, 9.4462 examples/sec lr=0.000100, loss=42.0717, loss_ll=7.94067, loss_ll_paf=14.1297, loss_ll_heat=1.75169, q=1000
[2018-07-12 20:40:18,046] [train] [INFO] epoch=0.00 step=5000, 9.4527 examples/sec lr=0.000100, loss=40.9895, loss_ll=7.63217, loss_ll_paf=13.4108, loss_ll_heat=1.85353, q=1000
[2018-07-12 20:43:11,938] [train] [INFO] epoch=0.00 step=5100, 9.4476 examples/sec lr=0.000100, loss=29.5116, loss_ll=5.22634, loss_ll_paf=9.16575, loss_ll_heat=1.28694, q=1000
[2018-07-12 20:45:48,516] [train] [INFO] epoch=0.00 step=5200, 9.4613 examples/sec lr=0.000100, loss=60.921, loss_ll=11.5084, loss_ll_paf=20.4776, loss_ll_heat=2.53928, q=1000
[2018-07-12 20:48:26,139] [train] [INFO] epoch=0.00 step=5300, 9.4735 examples/sec lr=0.000100, loss=39.1972, loss_ll=7.54486, loss_ll_paf=12.96, loss_ll_heat=2.12973, q=1000
[2018-07-12 20:51:03,251] [train] [INFO] epoch=0.00 step=5400, 9.4857 examples/sec lr=0.000100, loss=77.1292, loss_ll=14.1975, loss_ll_paf=25.4219, loss_ll_heat=2.97303, q=1000
[2018-07-12 20:53:40,751] [train] [INFO] epoch=0.00 step=5500, 9.4972 examples/sec lr=0.000100, loss=51.3926, loss_ll=9.67387, loss_ll_paf=17.6749, loss_ll_heat=1.6728, q=1000
[2018-07-12 20:56:15,412] [train] [INFO] epoch=0.00 step=5600, 9.5111 examples/sec lr=0.000100, loss=58.1274, loss_ll=10.6776, loss_ll_paf=18.89, loss_ll_heat=2.46533, q=1000
[2018-07-12 20:58:53,599] [train] [INFO] epoch=0.00 step=5700, 9.5211 examples/sec lr=0.000100, loss=58.2483, loss_ll=11.6177, loss_ll_paf=20.1531, loss_ll_heat=3.08232, q=1000
[2018-07-12 21:01:30,348] [train] [INFO] epoch=0.00 step=5800, 9.5321 examples/sec lr=0.000100, loss=28.9114, loss_ll=4.69196, loss_ll_paf=8.09036, loss_ll_heat=1.29357, q=1000
[2018-07-12 21:04:07,758] [train] [INFO] epoch=0.00 step=5900, 9.5422 examples/sec lr=0.000100, loss=51.5373, loss_ll=9.63266, loss_ll_paf=17.3857, loss_ll_heat=1.8796, q=1000
[2018-07-12 21:06:44,733] [train] [INFO] epoch=0.00 step=6000, 9.5523 examples/sec lr=0.000100, loss=79.3048, loss_ll=15.0757, loss_ll_paf=27.579, loss_ll_heat=2.57237, q=1000
[2018-07-12 21:09:36,471] [train] [INFO] epoch=0.00 step=6100, 9.5484 examples/sec lr=0.000100, loss=48.6252, loss_ll=8.49111, loss_ll_paf=14.9625, loss_ll_heat=2.01976, q=1000
[2018-07-12 21:12:10,728] [train] [INFO] epoch=0.00 step=6200, 9.5606 examples/sec lr=0.000100, loss=46.4215, loss_ll=8.28148, loss_ll_paf=14.7298, loss_ll_heat=1.83321, q=1000
[2018-07-12 21:14:49,155] [train] [INFO] epoch=0.00 step=6300, 9.5687 examples/sec lr=0.000100, loss=79.6081, loss_ll=14.9321, loss_ll_paf=26.8684, loss_ll_heat=2.99578, q=1000
[2018-07-12 21:17:27,014] [train] [INFO] epoch=0.00 step=6400, 9.5771 examples/sec lr=0.000100, loss=40.1409, loss_ll=8.14657, loss_ll_paf=14.3475, loss_ll_heat=1.94567, q=1000
[2018-07-12 21:20:03,491] [train] [INFO] epoch=0.00 step=6500, 9.5864 examples/sec lr=0.000100, loss=44.0783, loss_ll=8.42286, loss_ll_paf=15.0849, loss_ll_heat=1.76082, q=1000
[2018-07-12 21:22:40,438] [train] [INFO] epoch=0.00 step=6600, 9.5951 examples/sec lr=0.000100, loss=47.012, loss_ll=8.71042, loss_ll_paf=15.2703, loss_ll_heat=2.15054, q=1000
[2018-07-12 21:25:15,214] [train] [INFO] epoch=0.00 step=6700, 9.6054 examples/sec lr=0.000100, loss=33.3431, loss_ll=5.92753, loss_ll_paf=10.7076, loss_ll_heat=1.14749, q=1000
[2018-07-12 21:27:51,901] [train] [INFO] epoch=0.00 step=6800, 9.6138 examples/sec lr=0.000100, loss=59.1785, loss_ll=11.009, loss_ll_paf=20.2656, loss_ll_heat=1.75252, q=1000
[2018-07-12 21:30:29,033] [train] [INFO] epoch=0.00 step=6900, 9.6216 examples/sec lr=0.000100, loss=28.0036, loss_ll=4.95467, loss_ll_paf=8.08667, loss_ll_heat=1.82268, q=1000
[2018-07-12 21:33:06,592] [train] [INFO] epoch=0.00 step=7000, 9.6288 examples/sec lr=0.000100, loss=56.7755, loss_ll=10.6941, loss_ll_paf=19.9862, loss_ll_heat=1.40193, q=1000
[2018-07-12 21:35:57,030] [train] [INFO] epoch=0.00 step=7100, 9.6253 examples/sec lr=0.000100, loss=53.4925, loss_ll=9.13165, loss_ll_paf=16.383, loss_ll_heat=1.88026, q=1000
[2018-07-12 21:38:36,856] [train] [INFO] epoch=0.00 step=7200, 9.6305 examples/sec lr=0.000100, loss=49.8578, loss_ll=8.7166, loss_ll_paf=15.9041, loss_ll_heat=1.52916, q=1000
[2018-07-12 21:41:15,218] [train] [INFO] epoch=0.00 step=7300, 9.6367 examples/sec lr=0.000100, loss=34.3585, loss_ll=6.10077, loss_ll_paf=10.994, loss_ll_heat=1.20754, q=1000
[2018-07-12 21:43:52,167] [train] [INFO] epoch=0.00 step=7400, 9.6438 examples/sec lr=0.000100, loss=44.9398, loss_ll=8.30562, loss_ll_paf=15.542, loss_ll_heat=1.06929, q=1000
[2018-07-12 21:46:27,704] [train] [INFO] epoch=0.00 step=7500, 9.6518 examples/sec lr=0.000100, loss=28.0225, loss_ll=4.50019, loss_ll_paf=7.79249, loss_ll_heat=1.2079, q=1000
[2018-07-12 21:49:03,771] [train] [INFO] epoch=0.00 step=7600, 9.6593 examples/sec lr=0.000100, loss=35.8301, loss_ll=6.46926, loss_ll_paf=11.7782, loss_ll_heat=1.1603, q=1000
[2018-07-12 21:51:41,151] [train] [INFO] epoch=1.00 step=7700, 9.6655 examples/sec lr=0.000100, loss=62.6058, loss_ll=10.9659, loss_ll_paf=20.1885, loss_ll_heat=1.74327, q=1000
[2018-07-12 21:54:17,505] [train] [INFO] epoch=1.00 step=7800, 9.6724 examples/sec lr=0.000100, loss=69.6804, loss_ll=12.981, loss_ll_paf=24.1895, loss_ll_heat=1.77259, q=1000
[2018-07-12 21:56:55,042] [train] [INFO] epoch=1.00 step=7900, 9.6783 examples/sec lr=0.000100, loss=54.5409, loss_ll=10.1401, loss_ll_paf=19.1084, loss_ll_heat=1.1717, q=1000
[2018-07-12 21:59:32,003] [train] [INFO] epoch=1.00 step=8000, 9.6844 examples/sec lr=0.000100, loss=59.6435, loss_ll=10.7188, loss_ll_paf=19.2528, loss_ll_heat=2.18486, q=1000
[2018-07-12 22:02:19,945] [train] [INFO] epoch=1.00 step=8100, 9.6824 examples/sec lr=0.000100, loss=54.6632, loss_ll=10.3617, loss_ll_paf=19.1288, loss_ll_heat=1.59461, q=1000
[2018-07-12 22:04:57,762] [train] [INFO] epoch=1.00 step=8200, 9.6877 examples/sec lr=0.000100, loss=41.7563, loss_ll=6.7702, loss_ll_paf=12.3286, loss_ll_heat=1.21185, q=1000
[2018-07-12 22:07:34,633] [train] [INFO] epoch=1.00 step=8300, 9.6936 examples/sec lr=0.000100, loss=47.1192, loss_ll=8.64074, loss_ll_paf=15.6681, loss_ll_heat=1.61336, q=1000
[2018-07-12 22:10:13,901] [train] [INFO] epoch=1.00 step=8400, 9.6976 examples/sec lr=0.000100, loss=69.5716, loss_ll=12.7466, loss_ll_paf=22.994, loss_ll_heat=2.49916, q=1000
[2018-07-12 22:12:50,240] [train] [INFO] epoch=1.00 step=8500, 9.7036 examples/sec lr=0.000100, loss=58.7841, loss_ll=10.4477, loss_ll_paf=19.1515, loss_ll_heat=1.74403, q=1000
[2018-07-12 22:15:27,504] [train] [INFO] epoch=1.00 step=8600, 9.7088 examples/sec lr=0.000100, loss=35.3248, loss_ll=6.12101, loss_ll_paf=10.987, loss_ll_heat=1.25506, q=1000
[2018-07-12 22:18:04,632] [train] [INFO] epoch=1.00 step=8700, 9.7140 examples/sec lr=0.000100, loss=40.4117, loss_ll=7.094, loss_ll_paf=12.3803, loss_ll_heat=1.8077, q=1000
[2018-07-12 22:20:41,820] [train] [INFO] epoch=1.00 step=8800, 9.7191 examples/sec lr=0.000100, loss=62.7916, loss_ll=11.0266, loss_ll_paf=19.9469, loss_ll_heat=2.10634, q=1000
[2018-07-12 22:23:17,683] [train] [INFO] epoch=1.00 step=8900, 9.7249 examples/sec lr=0.000100, loss=34.3842, loss_ll=5.16916, loss_ll_paf=9.18884, loss_ll_heat=1.14947, q=1000
[2018-07-12 22:25:56,079] [train] [INFO] epoch=1.00 step=9000, 9.7289 examples/sec lr=0.000100, loss=34.9023, loss_ll=6.05532, loss_ll_paf=11.0419, loss_ll_heat=1.06874, q=1000
[2018-07-12 22:28:49,331] [train] [INFO] epoch=1.00 step=9100, 9.7232 examples/sec lr=0.000100, loss=78.5609, loss_ll=15.6136, loss_ll_paf=29.1249, loss_ll_heat=2.10222, q=1000
[2018-07-12 22:31:27,909] [train] [INFO] epoch=1.00 step=9200, 9.7270 examples/sec lr=0.000100, loss=35.7715, loss_ll=6.67029, loss_ll_paf=11.3287, loss_ll_heat=2.01188, q=1000
[2018-07-12 22:34:05,149] [train] [INFO] epoch=1.00 step=9300, 9.7317 examples/sec lr=0.000100, loss=45.3114, loss_ll=7.79656, loss_ll_paf=14.0711, loss_ll_heat=1.52202, q=1000
[2018-07-12 22:36:42,837] [train] [INFO] epoch=1.00 step=9400, 9.7359 examples/sec lr=0.000100, loss=40.1154, loss_ll=7.60721, loss_ll_paf=14.1705, loss_ll_heat=1.0439, q=1000
[2018-07-12 22:39:19,283] [train] [INFO] epoch=1.00 step=9500, 9.7408 examples/sec lr=0.000100, loss=44.6818, loss_ll=7.60478, loss_ll_paf=13.9636, loss_ll_heat=1.24593, q=1000
[2018-07-12 22:41:57,051] [train] [INFO] epoch=1.00 step=9600, 9.7448 examples/sec lr=0.000100, loss=44.9021, loss_ll=7.93866, loss_ll_paf=14.0334, loss_ll_heat=1.84396, q=1000
[2018-07-12 22:44:34,503] [train] [INFO] epoch=1.00 step=9700, 9.7489 examples/sec lr=0.000100, loss=52.7234, loss_ll=9.38328, loss_ll_paf=17.2253, loss_ll_heat=1.54122, q=1000
[2018-07-12 22:47:13,022] [train] [INFO] epoch=1.00 step=9800, 9.7523 examples/sec lr=0.000100, loss=39.9839, loss_ll=7.08116, loss_ll_paf=12.715, loss_ll_heat=1.44728, q=1000
[2018-07-12 22:49:50,713] [train] [INFO] epoch=1.00 step=9900, 9.7562 examples/sec lr=0.000100, loss=52.5083, loss_ll=10.0265, loss_ll_paf=18.7526, loss_ll_heat=1.30047, q=1000
[2018-07-12 22:52:27,550] [train] [INFO] epoch=1.00 step=10000, 9.7604 examples/sec lr=0.000100, loss=35.6684, loss_ll=5.56659, loss_ll_paf=10.0121, loss_ll_heat=1.12111, q=1000
[2018-07-12 22:55:16,789] [train] [INFO] epoch=1.00 step=10100, 9.7573 examples/sec lr=0.000100, loss=42.6868, loss_ll=7.84858, loss_ll_paf=14.4041, loss_ll_heat=1.2931, q=1000
[2018-07-12 22:57:54,389] [train] [INFO] epoch=1.00 step=10200, 9.7610 examples/sec lr=0.000100, loss=47.5267, loss_ll=8.09709, loss_ll_paf=14.5805, loss_ll_heat=1.61373, q=1000
[2018-07-12 23:00:32,067] [train] [INFO] epoch=1.00 step=10300, 9.7646 examples/sec lr=0.000100, loss=65.4944, loss_ll=12.8223, loss_ll_paf=23.2797, loss_ll_heat=2.36483, q=1000
[2018-07-12 23:03:10,683] [train] [INFO] epoch=1.00 step=10400, 9.7676 examples/sec lr=0.000100, loss=32.1844, loss_ll=5.11563, loss_ll_paf=8.86051, loss_ll_heat=1.37074, q=1000
[2018-07-12 23:05:48,201] [train] [INFO] epoch=1.00 step=10500, 9.7712 examples/sec lr=0.000100, loss=33.6082, loss_ll=5.50923, loss_ll_paf=10.1338, loss_ll_heat=0.884702, q=1000
[2018-07-12 23:08:25,807] [train] [INFO] epoch=1.00 step=10600, 9.7747 examples/sec lr=0.000100, loss=27.5477, loss_ll=4.71246, loss_ll_paf=8.43844, loss_ll_heat=0.986481, q=1000
[2018-07-12 23:11:04,071] [train] [INFO] epoch=1.00 step=10700, 9.7777 examples/sec lr=0.000100, loss=38.1177, loss_ll=6.54754, loss_ll_paf=11.2899, loss_ll_heat=1.80518, q=1000
[2018-07-12 23:13:41,931] [train] [INFO] epoch=1.00 step=10800, 9.7809 examples/sec lr=0.000100, loss=33.3679, loss_ll=5.87322, loss_ll_paf=10.0874, loss_ll_heat=1.6591, q=1000
[2018-07-12 23:16:18,310] [train] [INFO] epoch=1.00 step=10900, 9.7848 examples/sec lr=0.000100, loss=41.6571, loss_ll=7.18022, loss_ll_paf=13.0902, loss_ll_heat=1.27028, q=1000
[2018-07-12 23:18:56,865] [train] [INFO] epoch=1.00 step=11000, 9.7875 examples/sec lr=0.000100, loss=52.4238, loss_ll=9.33254, loss_ll_paf=17.3092, loss_ll_heat=1.35588, q=1000
[2018-07-12 23:21:47,983] [train] [INFO] epoch=1.00 step=11100, 9.7834 examples/sec lr=0.000100, loss=58.6985, loss_ll=11.4149, loss_ll_paf=21.2141, loss_ll_heat=1.61561, q=1000
[2018-07-12 23:24:26,070] [train] [INFO] epoch=1.00 step=11200, 9.7863 examples/sec lr=0.000100, loss=61.8645, loss_ll=11.8812, loss_ll_paf=21.3766, loss_ll_heat=2.38576, q=1000
[2018-07-12 23:27:02,250] [train] [INFO] epoch=1.00 step=11300, 9.7902 examples/sec lr=0.000100, loss=51.365, loss_ll=9.23478, loss_ll_paf=16.2404, loss_ll_heat=2.22912, q=1000
[2018-07-12 23:29:41,359] [train] [INFO] epoch=1.00 step=11400, 9.7925 examples/sec lr=0.000100, loss=52.0611, loss_ll=8.94441, loss_ll_paf=16.0092, loss_ll_heat=1.87966, q=1000
[2018-07-12 23:32:19,380] [train] [INFO] epoch=1.00 step=11500, 9.7953 examples/sec lr=0.000100, loss=40.8121, loss_ll=7.04418, loss_ll_paf=12.9842, loss_ll_heat=1.10412, q=1000
[2018-07-12 23:34:53,985] [train] [INFO] epoch=1.00 step=11600, 9.7998 examples/sec lr=0.000100, loss=35.3522, loss_ll=6.30828, loss_ll_paf=11.1211, loss_ll_heat=1.49549, q=1000
[2018-07-12 23:37:31,012] [train] [INFO] epoch=1.00 step=11700, 9.8030 examples/sec lr=0.000100, loss=55.0525, loss_ll=10.1569, loss_ll_paf=18.6521, loss_ll_heat=1.66168, q=1000
[2018-07-12 23:40:09,979] [train] [INFO] epoch=1.00 step=11800, 9.8052 examples/sec lr=0.000100, loss=38.1569, loss_ll=6.50893, loss_ll_paf=11.9447, loss_ll_heat=1.07317, q=1000
[2018-07-12 23:42:48,451] [train] [INFO] epoch=1.00 step=11900, 9.8075 examples/sec lr=0.000100, loss=33.4789, loss_ll=5.61573, loss_ll_paf=9.93242, loss_ll_heat=1.29904, q=1000
[2018-07-12 23:45:26,170] [train] [INFO] epoch=1.00 step=12000, 9.8103 examples/sec lr=0.000100, loss=19.7522, loss_ll=3.26977, loss_ll_paf=5.69451, loss_ll_heat=0.845029, q=1000
[2018-07-12 23:48:16,655] [train] [INFO] epoch=1.00 step=12100, 9.8066 examples/sec lr=0.000100, loss=42.2465, loss_ll=7.22581, loss_ll_paf=13.1296, loss_ll_heat=1.32199, q=1000
[2018-07-12 23:50:54,965] [train] [INFO] epoch=1.00 step=12200, 9.8090 examples/sec lr=0.000100, loss=58.7645, loss_ll=11.0684, loss_ll_paf=20.103, loss_ll_heat=2.03379, q=1000
[2018-07-12 23:53:32,232] [train] [INFO] epoch=1.00 step=12300, 9.8118 examples/sec lr=0.000100, loss=38.2016, loss_ll=6.7746, loss_ll_paf=12.187, loss_ll_heat=1.36218, q=1000
[2018-07-12 23:56:11,548] [train] [INFO] epoch=1.00 step=12400, 9.8137 examples/sec lr=0.000100, loss=46.6279, loss_ll=8.63505, loss_ll_paf=15.8834, loss_ll_heat=1.38673, q=1000
[2018-07-12 23:58:48,780] [train] [INFO] epoch=1.00 step=12500, 9.8165 examples/sec lr=0.000100, loss=64.0188, loss_ll=12.592, loss_ll_paf=23.2029, loss_ll_heat=1.98105, q=1000
[2018-07-13 00:01:26,768] [train] [INFO] epoch=1.00 step=12600, 9.8189 examples/sec lr=0.000100, loss=60.4726, loss_ll=10.3611, loss_ll_paf=18.4923, loss_ll_heat=2.22995, q=1000
[2018-07-13 00:04:06,131] [train] [INFO] epoch=1.00 step=12700, 9.8206 examples/sec lr=0.000100, loss=19.1661, loss_ll=3.03526, loss_ll_paf=5.30392, loss_ll_heat=0.766594, q=1000
[2018-07-13 00:06:45,557] [train] [INFO] epoch=1.00 step=12800, 9.8222 examples/sec lr=0.000100, loss=47.1283, loss_ll=8.29579, loss_ll_paf=15.2842, loss_ll_heat=1.30741, q=1000
[2018-07-13 00:09:22,940] [train] [INFO] epoch=1.00 step=12900, 9.8248 examples/sec lr=0.000100, loss=48.1887, loss_ll=9.11037, loss_ll_paf=16.8528, loss_ll_heat=1.36796, q=1000
[2018-07-13 00:12:01,657] [train] [INFO] epoch=1.00 step=13000, 9.8267 examples/sec lr=0.000100, loss=36.6707, loss_ll=7.26935, loss_ll_paf=13.4047, loss_ll_heat=1.13404, q=1000
[2018-07-13 00:14:56,386] [train] [INFO] epoch=1.00 step=13100, 9.8212 examples/sec lr=0.000100, loss=40.1163, loss_ll=7.10116, loss_ll_paf=12.9116, loss_ll_heat=1.29076, q=1000
[2018-07-13 00:17:34,360] [train] [INFO] epoch=1.00 step=13200, 9.8235 examples/sec lr=0.000100, loss=33.9637, loss_ll=6.36033, loss_ll_paf=11.5691, loss_ll_heat=1.15155, q=1000
[2018-07-13 00:20:11,391] [train] [INFO] epoch=1.00 step=13300, 9.8261 examples/sec lr=0.000100, loss=37.8775, loss_ll=6.87262, loss_ll_paf=12.2578, loss_ll_heat=1.48748, q=1000
[2018-07-13 00:22:49,014] [train] [INFO] epoch=1.00 step=13400, 9.8285 examples/sec lr=0.000100, loss=36.013, loss_ll=6.57046, loss_ll_paf=12.0097, loss_ll_heat=1.13126, q=1000
[2018-07-13 00:25:26,829] [train] [INFO] epoch=1.00 step=13500, 9.8307 examples/sec lr=0.000100, loss=58.7569, loss_ll=10.429, loss_ll_paf=19.0687, loss_ll_heat=1.78929, q=1000
[2018-07-13 00:28:04,177] [train] [INFO] epoch=1.00 step=13600, 9.8331 examples/sec lr=0.000100, loss=38.5335, loss_ll=6.79894, loss_ll_paf=12.446, loss_ll_heat=1.15191, q=1000
[2018-07-13 00:30:40,814] [train] [INFO] epoch=1.00 step=13700, 9.8358 examples/sec lr=0.000100, loss=44.8781, loss_ll=7.96954, loss_ll_paf=14.7317, loss_ll_heat=1.20739, q=1000
[2018-07-13 00:33:18,057] [train] [INFO] epoch=1.00 step=13800, 9.8382 examples/sec lr=0.000100, loss=33.1975, loss_ll=5.71936, loss_ll_paf=9.71431, loss_ll_heat=1.72442, q=1000
[2018-07-13 00:35:55,609] [train] [INFO] epoch=1.00 step=13900, 9.8404 examples/sec lr=0.000100, loss=47.3887, loss_ll=9.6194, loss_ll_paf=17.4937, loss_ll_heat=1.74505, q=1000
[2018-07-13 00:38:39,504] [train] [INFO] epoch=1.00 step=14000, 9.8398 examples/sec lr=0.000100, loss=48.5668, loss_ll=8.99108, loss_ll_paf=16.4259, loss_ll_heat=1.55621, q=1000
[2018-07-13 00:41:32,878] [train] [INFO] epoch=1.00 step=14100, 9.8352 examples/sec lr=0.000100, loss=61.3393, loss_ll=11.4776, loss_ll_paf=20.9189, loss_ll_heat=2.03631, q=1000
[2018-07-13 00:44:09,430] [train] [INFO] epoch=1.00 step=14200, 9.8378 examples/sec lr=0.000100, loss=66.0251, loss_ll=12.3752, loss_ll_paf=22.8532, loss_ll_heat=1.89718, q=1000
[2018-07-13 00:46:46,478] [train] [INFO] epoch=1.00 step=14300, 9.8402 examples/sec lr=0.000100, loss=35.1704, loss_ll=6.67092, loss_ll_paf=12.0426, loss_ll_heat=1.29925, q=1000
[2018-07-13 00:49:24,420] [train] [INFO] epoch=1.00 step=14400, 9.8421 examples/sec lr=0.000100, loss=51.3215, loss_ll=9.41727, loss_ll_paf=17.8739, loss_ll_heat=0.960668, q=1000
[2018-07-13 00:52:00,882] [train] [INFO] epoch=1.00 step=14500, 9.8447 examples/sec lr=0.000100, loss=47.0131, loss_ll=8.39256, loss_ll_paf=15.6223, loss_ll_heat=1.16278, q=1000
[2018-07-13 00:54:37,887] [train] [INFO] epoch=1.00 step=14600, 9.8470 examples/sec lr=0.000100, loss=35.9968, loss_ll=6.59631, loss_ll_paf=11.6304, loss_ll_heat=1.56226, q=1000
[2018-07-13 00:57:14,401] [train] [INFO] epoch=1.00 step=14700, 9.8494 examples/sec lr=0.000100, loss=40.3941, loss_ll=7.287, loss_ll_paf=13.4135, loss_ll_heat=1.16045, q=1000
[2018-07-13 00:59:51,348] [train] [INFO] epoch=1.00 step=14800, 9.8517 examples/sec lr=0.000100, loss=34.5771, loss_ll=6.78651, loss_ll_paf=11.9478, loss_ll_heat=1.62526, q=1000
[2018-07-13 01:02:28,686] [train] [INFO] epoch=1.00 step=14900, 9.8537 examples/sec lr=0.000100, loss=40.0743, loss_ll=6.87205, loss_ll_paf=12.3283, loss_ll_heat=1.41578, q=1000
[2018-07-13 01:05:04,304] [train] [INFO] epoch=1.00 step=15000, 9.8565 examples/sec lr=0.000100, loss=34.7117, loss_ll=6.7213, loss_ll_paf=12.3772, loss_ll_heat=1.06542, q=1000
[2018-07-13 01:07:55,437] [train] [INFO] epoch=1.00 step=15100, 9.8529 examples/sec lr=0.000100, loss=24.1169, loss_ll=4.472, loss_ll_paf=7.56, loss_ll_heat=1.38399, q=1000
[2018-07-13 01:10:29,466] [train] [INFO] epoch=1.00 step=15200, 9.8563 examples/sec lr=0.000100, loss=41.8323, loss_ll=7.97683, loss_ll_paf=14.3891, loss_ll_heat=1.56455, q=1000
[2018-07-13 01:13:06,430] [train] [INFO] epoch=2.00 step=15300, 9.8584 examples/sec lr=0.000100, loss=35.5765, loss_ll=6.66987, loss_ll_paf=12.1767, loss_ll_heat=1.16303, q=1000
[2018-07-13 01:15:42,666] [train] [INFO] epoch=2.00 step=15400, 9.8608 examples/sec lr=0.000100, loss=71.4572, loss_ll=12.7497, loss_ll_paf=23.7979, loss_ll_heat=1.70163, q=1000
[2018-07-13 01:18:19,990] [train] [INFO] epoch=2.00 step=15500, 9.8627 examples/sec lr=0.000100, loss=50.6237, loss_ll=8.80293, loss_ll_paf=16.5963, loss_ll_heat=1.00957, q=1000
[2018-07-13 01:20:57,123] [train] [INFO] epoch=2.00 step=15600, 9.8647 examples/sec lr=0.000100, loss=34.3848, loss_ll=5.9518, loss_ll_paf=10.7751, loss_ll_heat=1.12848, q=1000
[2018-07-13 01:23:34,018] [train] [INFO] epoch=2.00 step=15700, 9.8668 examples/sec lr=0.000100, loss=34.3054, loss_ll=5.93985, loss_ll_paf=10.3698, loss_ll_heat=1.5099, q=1000
[2018-07-13 01:26:10,568] [train] [INFO] epoch=2.00 step=15800, 9.8689 examples/sec lr=0.000100, loss=68.0158, loss_ll=12.3413, loss_ll_paf=22.4927, loss_ll_heat=2.18987, q=1000
[2018-07-13 01:28:47,703] [train] [INFO] epoch=2.00 step=15900, 9.8708 examples/sec lr=0.000100, loss=22.0498, loss_ll=3.84102, loss_ll_paf=6.09009, loss_ll_heat=1.59195, q=1000
[2018-07-13 01:31:22,767] [train] [INFO] epoch=2.00 step=16000, 9.8735 examples/sec lr=0.000100, loss=29.2213, loss_ll=4.63679, loss_ll_paf=8.07811, loss_ll_heat=1.19548, q=1000
[2018-07-13 01:34:11,371] [train] [INFO] epoch=2.00 step=16100, 9.8710 examples/sec lr=0.000100, loss=45.2326, loss_ll=8.66386, loss_ll_paf=15.9511, loss_ll_heat=1.37666, q=1000
[2018-07-13 01:36:47,505] [train] [INFO] epoch=2.00 step=16200, 9.8733 examples/sec lr=0.000100, loss=36.2181, loss_ll=6.0177, loss_ll_paf=10.8909, loss_ll_heat=1.14455, q=1000
[2018-07-13 01:39:24,241] [train] [INFO] epoch=2.00 step=16300, 9.8753 examples/sec lr=0.000100, loss=41.5253, loss_ll=7.14111, loss_ll_paf=13.0248, loss_ll_heat=1.25743, q=1000
[2018-07-13 01:42:00,762] [train] [INFO] epoch=2.00 step=16400, 9.8773 examples/sec lr=0.000100, loss=63.2935, loss_ll=12.3845, loss_ll_paf=22.6069, loss_ll_heat=2.16204, q=1000
[2018-07-13 01:44:42,989] [train] [INFO] epoch=2.00 step=16500, 9.8772 examples/sec lr=0.000100, loss=35.874, loss_ll=6.30093, loss_ll_paf=10.8798, loss_ll_heat=1.72209, q=1000
[2018-07-13 01:47:26,433] [train] [INFO] epoch=2.00 step=16600, 9.8767 examples/sec lr=0.000100, loss=21.1042, loss_ll=3.84255, loss_ll_paf=6.85745, loss_ll_heat=0.827656, q=1000
[2018-07-13 01:50:03,250] [train] [INFO] epoch=2.00 step=16700, 9.8786 examples/sec lr=0.000100, loss=39.8817, loss_ll=7.1482, loss_ll_paf=12.6896, loss_ll_heat=1.60679, q=1000
[2018-07-13 01:52:39,810] [train] [INFO] epoch=2.00 step=16800, 9.8805 examples/sec lr=0.000100, loss=36.8573, loss_ll=6.45631, loss_ll_paf=11.7048, loss_ll_heat=1.20777, q=1000
[2018-07-13 01:55:15,766] [train] [INFO] epoch=2.00 step=16900, 9.8827 examples/sec lr=0.000100, loss=26.1934, loss_ll=4.37393, loss_ll_paf=7.83995, loss_ll_heat=0.907908, q=1000
[2018-07-13 01:57:50,472] [train] [INFO] epoch=2.00 step=17000, 9.8853 examples/sec lr=0.000100, loss=50.7046, loss_ll=9.44734, loss_ll_paf=17.4154, loss_ll_heat=1.47925, q=1000
[2018-07-13 02:00:40,410] [train] [INFO] epoch=2.00 step=17100, 9.8824 examples/sec lr=0.000100, loss=55.6237, loss_ll=10.1175, loss_ll_paf=19.255, loss_ll_heat=0.979985, q=1000
[2018-07-13 02:03:17,815] [train] [INFO] epoch=2.00 step=17200, 9.8840 examples/sec lr=0.000100, loss=35.1305, loss_ll=5.87382, loss_ll_paf=10.6476, loss_ll_heat=1.10006, q=1000
[2018-07-13 02:05:55,408] [train] [INFO] epoch=2.00 step=17300, 9.8855 examples/sec lr=0.000100, loss=55.32, loss_ll=10.3218, loss_ll_paf=19.317, loss_ll_heat=1.32651, q=1000
[2018-07-13 02:08:32,340] [train] [INFO] epoch=2.00 step=17400, 9.8872 examples/sec lr=0.000100, loss=28.9613, loss_ll=5.04199, loss_ll_paf=8.81323, loss_ll_heat=1.27074, q=1000
[2018-07-13 02:11:09,674] [train] [INFO] epoch=2.00 step=17500, 9.8888 examples/sec lr=0.000100, loss=59.866, loss_ll=10.9713, loss_ll_paf=20.6041, loss_ll_heat=1.33852, q=1000
[2018-07-13 02:13:46,617] [train] [INFO] epoch=2.00 step=17600, 9.8905 examples/sec lr=0.000100, loss=18.8549, loss_ll=2.88686, loss_ll_paf=5.01291, loss_ll_heat=0.760813, q=1000
[2018-07-13 02:16:21,181] [train] [INFO] epoch=2.00 step=17700, 9.8930 examples/sec lr=0.000100, loss=30.4957, loss_ll=5.86993, loss_ll_paf=10.3765, loss_ll_heat=1.36334, q=1000
[2018-07-13 02:18:59,660] [train] [INFO] epoch=2.00 step=17800, 9.8941 examples/sec lr=0.000100, loss=48.9822, loss_ll=8.82906, loss_ll_paf=15.8874, loss_ll_heat=1.77076, q=1000
[2018-07-13 02:21:38,515] [train] [INFO] epoch=2.00 step=17900, 9.8951 examples/sec lr=0.000100, loss=30.9232, loss_ll=5.35085, loss_ll_paf=9.76306, loss_ll_heat=0.938646, q=1000
[2018-07-13 02:24:16,647] [train] [INFO] epoch=2.00 step=18000, 9.8963 examples/sec lr=0.000100, loss=43.8649, loss_ll=6.56072, loss_ll_paf=11.914, loss_ll_heat=1.20748, q=1000
[2018-07-13 02:27:07,042] [train] [INFO] epoch=2.00 step=18100, 9.8933 examples/sec lr=0.000100, loss=70.8631, loss_ll=13.1144, loss_ll_paf=24.5855, loss_ll_heat=1.64327, q=1000
[2018-07-13 02:29:42,851] [train] [INFO] epoch=2.00 step=18200, 9.8953 examples/sec lr=0.000100, loss=26.6057, loss_ll=4.78088, loss_ll_paf=8.48473, loss_ll_heat=1.07704, q=1000
[2018-07-13 02:32:21,496] [train] [INFO] epoch=2.00 step=18300, 9.8964 examples/sec lr=0.000100, loss=41.3203, loss_ll=7.19678, loss_ll_paf=13.2794, loss_ll_heat=1.11416, q=1000
[2018-07-13 02:34:57,740] [train] [INFO] epoch=2.00 step=18400, 9.8982 examples/sec lr=0.000100, loss=38.7246, loss_ll=6.75803, loss_ll_paf=11.7349, loss_ll_heat=1.78112, q=1000
[2018-07-13 02:37:34,429] [train] [INFO] epoch=2.00 step=18500, 9.8998 examples/sec lr=0.000100, loss=52.5654, loss_ll=10.2322, loss_ll_paf=18.3657, loss_ll_heat=2.09873, q=1000
[2018-07-13 02:40:12,531] [train] [INFO] epoch=2.00 step=18600, 9.9010 examples/sec lr=0.000100, loss=24.834, loss_ll=4.12136, loss_ll_paf=6.81032, loss_ll_heat=1.4324, q=1000
[2018-07-13 02:42:49,113] [train] [INFO] epoch=2.00 step=18700, 9.9026 examples/sec lr=0.000100, loss=37.4601, loss_ll=6.17771, loss_ll_paf=11.3135, loss_ll_heat=1.04194, q=1000
[2018-07-13 02:45:33,797] [train] [INFO] epoch=2.00 step=18800, 9.9016 examples/sec lr=0.000100, loss=39.6645, loss_ll=7.218, loss_ll_paf=13.1728, loss_ll_heat=1.26321, q=1000
[2018-07-13 02:48:17,716] [train] [INFO] epoch=2.00 step=18900, 9.9008 examples/sec lr=0.000100, loss=60.261, loss_ll=10.5813, loss_ll_paf=19.2319, loss_ll_heat=1.93075, q=1000
[2018-07-13 02:50:54,805] [train] [INFO] epoch=2.00 step=19000, 9.9023 examples/sec lr=0.000100, loss=42.2466, loss_ll=8.18911, loss_ll_paf=15.1346, loss_ll_heat=1.2436, q=1000
[2018-07-13 02:53:44,811] [train] [INFO] epoch=2.00 step=19100, 9.8996 examples/sec lr=0.000100, loss=27.344, loss_ll=4.51302, loss_ll_paf=8.16854, loss_ll_heat=0.857494, q=1000
[2018-07-13 02:56:23,565] [train] [INFO] epoch=2.00 step=19200, 9.9005 examples/sec lr=0.000100, loss=33.5654, loss_ll=6.01524, loss_ll_paf=10.5312, loss_ll_heat=1.49924, q=1000
[2018-07-13 02:59:01,278] [train] [INFO] epoch=2.00 step=19300, 9.9017 examples/sec lr=0.000100, loss=33.6743, loss_ll=6.18001, loss_ll_paf=10.4285, loss_ll_heat=1.93153, q=1000
[2018-07-13 03:01:38,860] [train] [INFO] epoch=2.00 step=19400, 9.9030 examples/sec lr=0.000100, loss=34.4007, loss_ll=6.11209, loss_ll_paf=10.7948, loss_ll_heat=1.4294, q=1000
[2018-07-13 03:04:17,347] [train] [INFO] epoch=2.00 step=19500, 9.9040 examples/sec lr=0.000100, loss=48.1799, loss_ll=8.66013, loss_ll_paf=16.2368, loss_ll_heat=1.08342, q=1000
[2018-07-13 03:06:56,137] [train] [INFO] epoch=2.00 step=19600, 9.9048 examples/sec lr=0.000100, loss=32.7226, loss_ll=5.44152, loss_ll_paf=9.71173, loss_ll_heat=1.17132, q=1000
[2018-07-13 03:09:34,719] [train] [INFO] epoch=2.00 step=19700, 9.9058 examples/sec lr=0.000100, loss=40.6906, loss_ll=7.99858, loss_ll_paf=14.3592, loss_ll_heat=1.638, q=1000
[2018-07-13 03:12:14,052] [train] [INFO] epoch=2.00 step=19800, 9.9064 examples/sec lr=0.000100, loss=31.1601, loss_ll=5.93543, loss_ll_paf=10.4509, loss_ll_heat=1.41994, q=1000
[2018-07-13 03:14:53,925] [train] [INFO] epoch=2.00 step=19900, 9.9069 examples/sec lr=0.000100, loss=42.3698, loss_ll=7.68837, loss_ll_paf=13.8054, loss_ll_heat=1.57133, q=1000
[2018-07-13 03:17:31,481] [train] [INFO] epoch=2.00 step=20000, 9.9081 examples/sec lr=0.000100, loss=46.4654, loss_ll=8.39994, loss_ll_paf=14.7761, loss_ll_heat=2.02383, q=1000
[2018-07-13 03:20:21,995] [train] [INFO] epoch=2.00 step=20100, 9.9054 examples/sec lr=0.000100, loss=46.891, loss_ll=8.7179, loss_ll_paf=16.1088, loss_ll_heat=1.32704, q=1000
[2018-07-13 03:22:58,923] [train] [INFO] epoch=2.00 step=20200, 9.9068 examples/sec lr=0.000100, loss=34.6564, loss_ll=6.44576, loss_ll_paf=12.0892, loss_ll_heat=0.802362, q=1000
[2018-07-13 03:25:35,991] [train] [INFO] epoch=2.00 step=20300, 9.9081 examples/sec lr=0.000100, loss=30.3449, loss_ll=5.05783, loss_ll_paf=9.16431, loss_ll_heat=0.951357, q=1000
[2018-07-13 03:28:12,362] [train] [INFO] epoch=2.00 step=20400, 9.9097 examples/sec lr=0.000100, loss=20.0988, loss_ll=3.27528, loss_ll_paf=5.55958, loss_ll_heat=0.990983, q=1000
[2018-07-13 03:30:49,731] [train] [INFO] epoch=2.00 step=20500, 9.9109 examples/sec lr=0.000100, loss=44.3289, loss_ll=8.37335, loss_ll_paf=14.6614, loss_ll_heat=2.08533, q=1000
[2018-07-13 03:33:25,934] [train] [INFO] epoch=2.00 step=20600, 9.9125 examples/sec lr=0.000100, loss=47.3382, loss_ll=8.44829, loss_ll_paf=15.4287, loss_ll_heat=1.46792, q=1000
[2018-07-13 03:36:02,995] [train] [INFO] epoch=2.00 step=20700, 9.9137 examples/sec lr=0.000100, loss=32.6613, loss_ll=5.5441, loss_ll_paf=9.55364, loss_ll_heat=1.53456, q=1000
[2018-07-13 03:38:39,668] [train] [INFO] epoch=2.00 step=20800, 9.9151 examples/sec lr=0.000100, loss=30.383, loss_ll=6.22366, loss_ll_paf=11.3271, loss_ll_heat=1.12021, q=1000
[2018-07-13 03:41:17,870] [train] [INFO] epoch=2.00 step=20900, 9.9161 examples/sec lr=0.000100, loss=23.6642, loss_ll=4.47439, loss_ll_paf=8.2478, loss_ll_heat=0.700978, q=1000
[2018-07-13 03:43:55,801] [train] [INFO] epoch=2.00 step=21000, 9.9171 examples/sec lr=0.000100, loss=40.2036, loss_ll=7.9749, loss_ll_paf=13.7865, loss_ll_heat=2.16328, q=1000
[2018-07-13 03:46:48,374] [train] [INFO] epoch=2.00 step=21100, 9.9138 examples/sec lr=0.000100, loss=46.114, loss_ll=8.70888, loss_ll_paf=15.4727, loss_ll_heat=1.94505, q=1000
[2018-07-13 03:49:26,476] [train] [INFO] epoch=2.00 step=21200, 9.9148 examples/sec lr=0.000100, loss=53.0365, loss_ll=9.85118, loss_ll_paf=17.8328, loss_ll_heat=1.86955, q=1000
[2018-07-13 03:52:03,589] [train] [INFO] epoch=2.00 step=21300, 9.9160 examples/sec lr=0.000100, loss=52.5658, loss_ll=10.2239, loss_ll_paf=19.3415, loss_ll_heat=1.10624, q=1000
[2018-07-13 03:54:41,430] [train] [INFO] epoch=2.00 step=21400, 9.9170 examples/sec lr=0.000100, loss=31.7362, loss_ll=5.43791, loss_ll_paf=9.69151, loss_ll_heat=1.18432, q=1000
[2018-07-13 03:57:18,607] [train] [INFO] epoch=2.00 step=21500, 9.9182 examples/sec lr=0.000100, loss=46.778, loss_ll=8.52431, loss_ll_paf=15.1358, loss_ll_heat=1.91283, q=1000
[2018-07-13 03:59:54,395] [train] [INFO] epoch=2.00 step=21600, 9.9198 examples/sec lr=0.000100, loss=43.4984, loss_ll=7.3039, loss_ll_paf=12.8489, loss_ll_heat=1.75887, q=1000
[2018-07-13 04:02:34,563] [train] [INFO] epoch=2.00 step=21700, 9.9201 examples/sec lr=0.000100, loss=31.3919, loss_ll=5.39127, loss_ll_paf=9.57293, loss_ll_heat=1.20962, q=1000
[2018-07-13 04:05:13,670] [train] [INFO] epoch=2.00 step=21800, 9.9207 examples/sec lr=0.000100, loss=35.8154, loss_ll=6.96219, loss_ll_paf=12.7301, loss_ll_heat=1.19431, q=1000
[2018-07-13 04:07:54,518] [train] [INFO] epoch=2.00 step=21900, 9.9208 examples/sec lr=0.000100, loss=18.0124, loss_ll=3.09998, loss_ll_paf=5.24876, loss_ll_heat=0.951194, q=1000
[2018-07-13 04:10:35,483] [train] [INFO] epoch=2.00 step=22000, 9.9209 examples/sec lr=0.000100, loss=40.6993, loss_ll=7.60759, loss_ll_paf=14.0615, loss_ll_heat=1.15373, q=1000
[2018-07-13 04:13:28,796] [train] [INFO] epoch=2.00 step=22100, 9.9176 examples/sec lr=0.000100, loss=24.0759, loss_ll=4.05263, loss_ll_paf=7.17932, loss_ll_heat=0.925954, q=1000
[2018-07-13 04:16:12,740] [train] [INFO] epoch=2.00 step=22200, 9.9168 examples/sec lr=0.000100, loss=29.4132, loss_ll=5.02637, loss_ll_paf=9.21235, loss_ll_heat=0.84038, q=1000
[2018-07-13 04:18:56,086] [train] [INFO] epoch=2.00 step=22300, 9.9163 examples/sec lr=0.000100, loss=43.8182, loss_ll=8.35053, loss_ll_paf=15.337, loss_ll_heat=1.36403, q=1000
[2018-07-13 04:21:43,262] [train] [INFO] epoch=2.00 step=22400, 9.9147 examples/sec lr=0.000100, loss=16.5041, loss_ll=2.99881, loss_ll_paf=4.99715, loss_ll_heat=1.00046, q=1000
[2018-07-13 04:24:26,587] [train] [INFO] epoch=2.00 step=22500, 9.9141 examples/sec lr=0.000100, loss=23.8838, loss_ll=3.99529, loss_ll_paf=6.94179, loss_ll_heat=1.04879, q=1000
[2018-07-13 04:27:11,570] [train] [INFO] epoch=2.00 step=22600, 9.9132 examples/sec lr=0.000100, loss=41.4953, loss_ll=7.49259, loss_ll_paf=13.2296, loss_ll_heat=1.75562, q=1000
[2018-07-13 04:29:55,614] [train] [INFO] epoch=2.00 step=22700, 9.9125 examples/sec lr=0.000100, loss=56.821, loss_ll=10.4013, loss_ll_paf=19.136, loss_ll_heat=1.66661, q=1000
[2018-07-13 04:32:39,336] [train] [INFO] epoch=2.00 step=22800, 9.9118 examples/sec lr=0.000100, loss=28.9194, loss_ll=5.4646, loss_ll_paf=9.54779, loss_ll_heat=1.38141, q=1000
[2018-07-13 04:35:16,714] [train] [INFO] epoch=3.00 step=22900, 9.9129 examples/sec lr=0.000100, loss=15.6803, loss_ll=2.51577, loss_ll_paf=4.02075, loss_ll_heat=1.01078, q=1000
[2018-07-13 04:37:58,298] [train] [INFO] epoch=3.00 step=23000, 9.9129 examples/sec lr=0.000100, loss=36.801, loss_ll=6.70289, loss_ll_paf=11.6659, loss_ll_heat=1.73992, q=1000
[2018-07-13 04:40:51,651] [train] [INFO] epoch=3.00 step=23100, 9.9097 examples/sec lr=0.000100, loss=35.7966, loss_ll=6.43371, loss_ll_paf=11.86, loss_ll_heat=1.00742, q=1000
[2018-07-13 04:43:30,990] [train] [INFO] epoch=3.00 step=23200, 9.9103 examples/sec lr=0.000100, loss=33.773, loss_ll=6.14542, loss_ll_paf=11.4688, loss_ll_heat=0.821994, q=1000
[2018-07-13 04:46:12,988] [train] [INFO] epoch=3.00 step=23300, 9.9101 examples/sec lr=0.000100, loss=38.7591, loss_ll=6.41369, loss_ll_paf=11.741, loss_ll_heat=1.08634, q=1000
[2018-07-13 04:48:51,632] [train] [INFO] epoch=3.00 step=23400, 9.9108 examples/sec lr=0.000100, loss=58.3745, loss_ll=10.8128, loss_ll_paf=19.7069, loss_ll_heat=1.91873, q=1000
[2018-07-13 04:51:31,466] [train] [INFO] epoch=3.00 step=23500, 9.9113 examples/sec lr=0.000100, loss=50.7335, loss_ll=9.29744, loss_ll_paf=17.4577, loss_ll_heat=1.13721, q=1000
[2018-07-13 04:54:16,872] [train] [INFO] epoch=3.00 step=23600, 9.9102 examples/sec lr=0.000100, loss=47.4018, loss_ll=8.88849, loss_ll_paf=16.4864, loss_ll_heat=1.29054, q=1000
[2018-07-13 04:57:02,057] [train] [INFO] epoch=3.00 step=23700, 9.9093 examples/sec lr=0.000100, loss=34.5814, loss_ll=6.39892, loss_ll_paf=11.5078, loss_ll_heat=1.29006, q=1000
[2018-07-13 04:59:46,221] [train] [INFO] epoch=3.00 step=23800, 9.9086 examples/sec lr=0.000100, loss=46.1777, loss_ll=8.04228, loss_ll_paf=14.1719, loss_ll_heat=1.91267, q=1000
[2018-07-13 05:02:30,094] [train] [INFO] epoch=3.00 step=23900, 9.9080 examples/sec lr=0.000100, loss=25.1789, loss_ll=4.27224, loss_ll_paf=7.83049, loss_ll_heat=0.713983, q=1000
[2018-07-13 05:05:15,265] [train] [INFO] epoch=3.00 step=24000, 9.9070 examples/sec lr=0.000100, loss=33.7341, loss_ll=5.89105, loss_ll_paf=11.0104, loss_ll_heat=0.771753, q=1000
[2018-07-13 05:08:08,369] [train] [INFO] epoch=3.00 step=24100, 9.9041 examples/sec lr=0.000100, loss=51.8461, loss_ll=8.93482, loss_ll_paf=16.4729, loss_ll_heat=1.39671, q=1000
[2018-07-13 05:10:47,204] [train] [INFO] epoch=3.00 step=24200, 9.9047 examples/sec lr=0.000100, loss=66.1508, loss_ll=12.8361, loss_ll_paf=24.006, loss_ll_heat=1.66629, q=1000
[2018-07-13 05:13:24,991] [train] [INFO] epoch=3.00 step=24300, 9.9057 examples/sec lr=0.000100, loss=35.1608, loss_ll=5.86978, loss_ll_paf=10.2652, loss_ll_heat=1.4744, q=1000
[2018-07-13 05:16:04,708] [train] [INFO] epoch=3.00 step=24400, 9.9061 examples/sec lr=0.000100, loss=45.0175, loss_ll=8.54095, loss_ll_paf=15.3574, loss_ll_heat=1.72449, q=1000
[2018-07-13 05:18:43,402] [train] [INFO] epoch=3.00 step=24500, 9.9069 examples/sec lr=0.000100, loss=35.8984, loss_ll=6.23131, loss_ll_paf=11.6453, loss_ll_heat=0.817298, q=1000
[2018-07-13 05:21:23,578] [train] [INFO] epoch=3.00 step=24600, 9.9072 examples/sec lr=0.000100, loss=16.9898, loss_ll=3.18849, loss_ll_paf=5.72046, loss_ll_heat=0.656516, q=1000
[2018-07-13 05:24:03,485] [train] [INFO] epoch=3.00 step=24700, 9.9076 examples/sec lr=0.000100, loss=29.0881, loss_ll=4.9491, loss_ll_paf=8.67097, loss_ll_heat=1.22722, q=1000
[2018-07-13 05:26:46,676] [train] [INFO] epoch=3.00 step=24800, 9.9072 examples/sec lr=0.000100, loss=41.8441, loss_ll=7.65114, loss_ll_paf=13.9825, loss_ll_heat=1.31978, q=1000
[2018-07-13 05:29:30,952] [train] [INFO] epoch=3.00 step=24900, 9.9065 examples/sec lr=0.000100, loss=37.3564, loss_ll=6.49341, loss_ll_paf=11.6849, loss_ll_heat=1.30189, q=1000
[2018-07-13 05:32:15,176] [train] [INFO] epoch=3.00 step=25000, 9.9058 examples/sec lr=0.000100, loss=22.8313, loss_ll=3.84164, loss_ll_paf=6.81015, loss_ll_heat=0.873121, q=1000
[2018-07-13 05:35:16,903] [train] [INFO] epoch=3.00 step=25100, 9.9009 examples/sec lr=0.000100, loss=32.9925, loss_ll=6.19952, loss_ll_paf=11.3124, loss_ll_heat=1.08664, q=1000
[2018-07-13 05:38:01,193] [train] [INFO] epoch=3.00 step=25200, 9.9002 examples/sec lr=0.000100, loss=27.321, loss_ll=4.50968, loss_ll_paf=8.04573, loss_ll_heat=0.97363, q=1000
[2018-07-13 05:40:45,424] [train] [INFO] epoch=3.00 step=25300, 9.8996 examples/sec lr=0.000100, loss=50.0289, loss_ll=8.47658, loss_ll_paf=15.4454, loss_ll_heat=1.50774, q=1000
[2018-07-13 05:43:27,316] [train] [INFO] epoch=3.00 step=25400, 9.8995 examples/sec lr=0.000100, loss=35.1525, loss_ll=6.03393, loss_ll_paf=10.7897, loss_ll_heat=1.27817, q=1000
[2018-07-13 05:46:13,960] [train] [INFO] epoch=3.00 step=25500, 9.8983 examples/sec lr=0.000100, loss=31.2345, loss_ll=5.72401, loss_ll_paf=10.1369, loss_ll_heat=1.31111, q=1000
[2018-07-13 05:49:00,071] [train] [INFO] epoch=3.00 step=25600, 9.8972 examples/sec lr=0.000100, loss=28.3989, loss_ll=5.14392, loss_ll_paf=9.32608, loss_ll_heat=0.961768, q=1000
[2018-07-13 05:51:45,392] [train] [INFO] epoch=3.00 step=25700, 9.8964 examples/sec lr=0.000100, loss=34.4664, loss_ll=6.89684, loss_ll_paf=12.6343, loss_ll_heat=1.15938, q=1000
[2018-07-13 05:54:29,993] [train] [INFO] epoch=3.00 step=25800, 9.8957 examples/sec lr=0.000100, loss=30.1898, loss_ll=5.23907, loss_ll_paf=9.49793, loss_ll_heat=0.980218, q=1000
[2018-07-13 05:57:14,931] [train] [INFO] epoch=3.00 step=25900, 9.8949 examples/sec lr=0.000100, loss=24.3103, loss_ll=4.52925, loss_ll_paf=7.52915, loss_ll_heat=1.52934, q=1000
[2018-07-13 05:59:59,533] [train] [INFO] epoch=3.00 step=26000, 9.8942 examples/sec lr=0.000100, loss=24.0261, loss_ll=4.16278, loss_ll_paf=7.33604, loss_ll_heat=0.989528, q=1000
[2018-07-13 06:02:58,590] [train] [INFO] epoch=3.00 step=26100, 9.8902 examples/sec lr=0.000100, loss=29.1639, loss_ll=5.01972, loss_ll_paf=8.67843, loss_ll_heat=1.36101, q=1000
[2018-07-13 06:05:42,624] [train] [INFO] epoch=3.00 step=26200, 9.8896 examples/sec lr=0.000100, loss=34.2315, loss_ll=6.21391, loss_ll_paf=11.6492, loss_ll_heat=0.778579, q=1000
[2018-07-13 06:08:20,882] [train] [INFO] epoch=3.00 step=26300, 9.8905 examples/sec lr=0.000100, loss=39.0302, loss_ll=7.11805, loss_ll_paf=13.2617, loss_ll_heat=0.97437, q=1000
[2018-07-13 06:10:59,468] [train] [INFO] epoch=3.00 step=26400, 9.8912 examples/sec lr=0.000100, loss=39.3007, loss_ll=6.59315, loss_ll_paf=12.0829, loss_ll_heat=1.10337, q=1000
[2018-07-13 06:13:39,165] [train] [INFO] epoch=3.00 step=26500, 9.8917 examples/sec lr=0.000100, loss=34.2527, loss_ll=6.15442, loss_ll_paf=10.5926, loss_ll_heat=1.71622, q=1000
[2018-07-13 06:16:17,880] [train] [INFO] epoch=3.00 step=26600, 9.8924 examples/sec lr=0.000100, loss=45.7517, loss_ll=8.3874, loss_ll_paf=15.4749, loss_ll_heat=1.29992, q=1000
[2018-07-13 06:18:54,470] [train] [INFO] epoch=3.00 step=26700, 9.8936 examples/sec lr=0.000100, loss=26.4941, loss_ll=4.21941, loss_ll_paf=7.52471, loss_ll_heat=0.91411, q=1000
[2018-07-13 06:21:32,709] [train] [INFO] epoch=3.00 step=26800, 9.8943 examples/sec lr=0.000100, loss=30.6688, loss_ll=5.40151, loss_ll_paf=9.77456, loss_ll_heat=1.02846, q=1000
[2018-07-13 06:24:11,136] [train] [INFO] epoch=3.00 step=26900, 9.8951 examples/sec lr=0.000100, loss=33.7843, loss_ll=6.53664, loss_ll_paf=12.3876, loss_ll_heat=0.685698, q=1000
[2018-07-13 06:26:49,472] [train] [INFO] epoch=3.00 step=27000, 9.8959 examples/sec lr=0.000100, loss=66.6181, loss_ll=12.4339, loss_ll_paf=23.713, loss_ll_heat=1.15485, q=1000
[2018-07-13 06:29:44,069] [train] [INFO] epoch=3.00 step=27100, 9.8929 examples/sec lr=0.000100, loss=27.5154, loss_ll=4.6091, loss_ll_paf=8.00507, loss_ll_heat=1.21312, q=1000
[2018-07-13 06:32:21,089] [train] [INFO] epoch=3.00 step=27200, 9.8940 examples/sec lr=0.000100, loss=14.4493, loss_ll=2.40908, loss_ll_paf=4.17892, loss_ll_heat=0.639241, q=1000
[2018-07-13 06:34:59,088] [train] [INFO] epoch=3.00 step=27300, 9.8948 examples/sec lr=0.000100, loss=35.6617, loss_ll=6.35506, loss_ll_paf=11.5689, loss_ll_heat=1.14126, q=1000
[2018-07-13 06:37:38,327] [train] [INFO] epoch=3.00 step=27400, 9.8954 examples/sec lr=0.000100, loss=30.5259, loss_ll=5.96412, loss_ll_paf=10.955, loss_ll_heat=0.973223, q=1000
[2018-07-13 06:40:16,621] [train] [INFO] epoch=3.00 step=27500, 9.8961 examples/sec lr=0.000100, loss=30.678, loss_ll=5.3744, loss_ll_paf=9.55715, loss_ll_heat=1.19165, q=1000
[2018-07-13 06:42:54,518] [train] [INFO] epoch=3.00 step=27600, 9.8970 examples/sec lr=0.000100, loss=45.9484, loss_ll=8.2096, loss_ll_paf=14.9098, loss_ll_heat=1.50938, q=1000
[2018-07-13 06:45:32,438] [train] [INFO] epoch=3.00 step=27700, 9.8978 examples/sec lr=0.000100, loss=37.5625, loss_ll=6.06548, loss_ll_paf=10.8628, loss_ll_heat=1.26818, q=1000
[2018-07-13 06:48:09,738] [train] [INFO] epoch=3.00 step=27800, 9.8988 examples/sec lr=0.000100, loss=32.7109, loss_ll=6.14055, loss_ll_paf=10.8183, loss_ll_heat=1.46278, q=1000
[2018-07-13 06:50:43,365] [train] [INFO] epoch=3.00 step=27900, 9.9005 examples/sec lr=0.000100, loss=35.0133, loss_ll=6.50952, loss_ll_paf=11.8633, loss_ll_heat=1.15569, q=1000
[2018-07-13 06:53:23,840] [train] [INFO] epoch=3.00 step=28000, 9.9008 examples/sec lr=0.000100, loss=17.1154, loss_ll=2.6028, loss_ll_paf=4.3702, loss_ll_heat=0.835393, q=1000
[2018-07-13 06:56:18,763] [train] [INFO] epoch=3.00 step=28100, 9.8979 examples/sec lr=0.000100, loss=34.0956, loss_ll=5.78703, loss_ll_paf=10.3326, loss_ll_heat=1.24143, q=1000
[2018-07-13 06:58:59,926] [train] [INFO] epoch=3.00 step=28200, 9.8980 examples/sec lr=0.000100, loss=47.3135, loss_ll=8.50319, loss_ll_paf=15.2024, loss_ll_heat=1.80398, q=1000
[2018-07-13 07:01:39,073] [train] [INFO] epoch=3.00 step=28300, 9.8985 examples/sec lr=0.000100, loss=28.6887, loss_ll=5.163, loss_ll_paf=9.31167, loss_ll_heat=1.01433, q=1000
[2018-07-13 07:04:20,482] [train] [INFO] epoch=3.00 step=28400, 9.8986 examples/sec lr=0.000100, loss=52.0322, loss_ll=9.54562, loss_ll_paf=17.5203, loss_ll_heat=1.57093, q=1000
[2018-07-13 07:07:00,113] [train] [INFO] epoch=3.00 step=28500, 9.8990 examples/sec lr=0.000100, loss=22.0096, loss_ll=3.55657, loss_ll_paf=6.47786, loss_ll_heat=0.635268, q=1000
[2018-07-13 07:09:41,154] [train] [INFO] epoch=3.00 step=28600, 9.8991 examples/sec lr=0.000100, loss=30.7436, loss_ll=5.23307, loss_ll_paf=9.84435, loss_ll_heat=0.621794, q=1000
[2018-07-13 07:12:21,292] [train] [INFO] epoch=3.00 step=28700, 9.8994 examples/sec lr=0.000100, loss=30.2016, loss_ll=4.96298, loss_ll_paf=9.1989, loss_ll_heat=0.727066, q=1000
[2018-07-13 07:15:00,334] [train] [INFO] epoch=3.00 step=28800, 9.9000 examples/sec lr=0.000100, loss=16.1656, loss_ll=2.50008, loss_ll_paf=4.30055, loss_ll_heat=0.69961, q=1000
[2018-07-13 07:17:44,982] [train] [INFO] epoch=3.00 step=28900, 9.8993 examples/sec lr=0.000100, loss=31.8511, loss_ll=5.9022, loss_ll_paf=10.3972, loss_ll_heat=1.40721, q=1000
[2018-07-13 07:20:28,946] [train] [INFO] epoch=3.00 step=29000, 9.8989 examples/sec lr=0.000100, loss=44.1849, loss_ll=8.15637, loss_ll_paf=15.1421, loss_ll_heat=1.17064, q=1000
[2018-07-13 07:23:29,067] [train] [INFO] epoch=3.00 step=29100, 9.8950 examples/sec lr=0.000100, loss=35.6044, loss_ll=6.39351, loss_ll_paf=11.3078, loss_ll_heat=1.47923, q=1000
[2018-07-13 07:26:07,200] [train] [INFO] epoch=3.00 step=29200, 9.8957 examples/sec lr=0.000100, loss=36.3273, loss_ll=6.62914, loss_ll_paf=12.1695, loss_ll_heat=1.08875, q=1000
[2018-07-13 07:28:46,220] [train] [INFO] epoch=3.00 step=29300, 9.8963 examples/sec lr=0.000100, loss=26.8594, loss_ll=4.61625, loss_ll_paf=8.4215, loss_ll_heat=0.811008, q=1000
[2018-07-13 07:31:25,313] [train] [INFO] epoch=3.00 step=29400, 9.8968 examples/sec lr=0.000100, loss=51.2233, loss_ll=8.93058, loss_ll_paf=16.883, loss_ll_heat=0.978128, q=1000
[2018-07-13 07:34:04,269] [train] [INFO] epoch=3.00 step=29500, 9.8974 examples/sec lr=0.000100, loss=25.7171, loss_ll=3.92846, loss_ll_paf=7.01859, loss_ll_heat=0.838335, q=1000
[2018-07-13 07:36:40,943] [train] [INFO] epoch=3.00 step=29600, 9.8984 examples/sec lr=0.000100, loss=21.0203, loss_ll=3.67441, loss_ll_paf=6.46952, loss_ll_heat=0.879291, q=1000
[2018-07-13 07:39:19,639] [train] [INFO] epoch=3.00 step=29700, 9.8990 examples/sec lr=0.000100, loss=35.0754, loss_ll=6.61648, loss_ll_paf=11.7409, loss_ll_heat=1.4921, q=1000
[2018-07-13 07:41:57,230] [train] [INFO] epoch=3.00 step=29800, 9.8998 examples/sec lr=0.000100, loss=34.2529, loss_ll=5.4446, loss_ll_paf=9.86945, loss_ll_heat=1.01976, q=1000
[2018-07-13 07:44:36,914] [train] [INFO] epoch=3.00 step=29900, 9.9002 examples/sec lr=0.000100, loss=32.6333, loss_ll=5.97199, loss_ll_paf=10.6791, loss_ll_heat=1.26486, q=1000
[2018-07-13 07:47:15,314] [train] [INFO] epoch=3.00 step=30000, 9.9009 examples/sec lr=0.000033, loss=22.9312, loss_ll=4.05716, loss_ll_paf=7.05834, loss_ll_heat=1.05599, q=1000
[2018-07-13 07:50:07,429] [train] [INFO] epoch=3.00 step=30100, 9.8988 examples/sec lr=0.000033, loss=19.9102, loss_ll=3.63821, loss_ll_paf=6.01337, loss_ll_heat=1.26304, q=1000
[2018-07-13 07:52:44,675] [train] [INFO] epoch=3.00 step=30200, 9.8996 examples/sec lr=0.000033, loss=19.0567, loss_ll=3.24942, loss_ll_paf=5.75935, loss_ll_heat=0.739478, q=1000
[2018-07-13 07:55:22,180] [train] [INFO] epoch=3.00 step=30300, 9.9005 examples/sec lr=0.000033, loss=33.3966, loss_ll=6.43779, loss_ll_paf=11.5911, loss_ll_heat=1.28453, q=1000
[2018-07-13 07:57:57,604] [train] [INFO] epoch=3.00 step=30400, 9.9017 examples/sec lr=0.000033, loss=20.0142, loss_ll=3.70286, loss_ll_paf=6.68173, loss_ll_heat=0.724001, q=1000
[2018-07-13 08:00:35,251] [train] [INFO] epoch=4.00 step=30500, 9.9025 examples/sec lr=0.000033, loss=35.9305, loss_ll=6.43662, loss_ll_paf=11.9011, loss_ll_heat=0.972184, q=1000
[2018-07-13 08:03:12,531] [train] [INFO] epoch=4.00 step=30600, 9.9034 examples/sec lr=0.000033, loss=21.6417, loss_ll=3.3887, loss_ll_paf=6.14966, loss_ll_heat=0.627745, q=1000
[2018-07-13 08:05:50,369] [train] [INFO] epoch=4.00 step=30700, 9.9041 examples/sec lr=0.000033, loss=41.6167, loss_ll=7.55868, loss_ll_paf=14.073, loss_ll_heat=1.04431, q=1000
[2018-07-13 08:08:27,756] [train] [INFO] epoch=4.00 step=30800, 9.9049 examples/sec lr=0.000033, loss=20.775, loss_ll=3.32006, loss_ll_paf=5.90491, loss_ll_heat=0.735209, q=1000
[2018-07-13 08:11:06,605] [train] [INFO] epoch=4.00 step=30900, 9.9055 examples/sec lr=0.000033, loss=33.7088, loss_ll=6.80099, loss_ll_paf=11.7559, loss_ll_heat=1.84611, q=1000
[2018-07-13 08:13:45,065] [train] [INFO] epoch=4.00 step=31000, 9.9061 examples/sec lr=0.000033, loss=15.5963, loss_ll=2.64079, loss_ll_paf=4.678, loss_ll_heat=0.603577, q=1000
[2018-07-13 08:16:35,461] [train] [INFO] epoch=4.00 step=31100, 9.9043 examples/sec lr=0.000033, loss=32.3209, loss_ll=6.25542, loss_ll_paf=11.5019, loss_ll_heat=1.0089, q=1000
[2018-07-13 08:19:13,970] [train] [INFO] epoch=4.00 step=31200, 9.9049 examples/sec lr=0.000033, loss=13.8894, loss_ll=2.43702, loss_ll_paf=4.32927, loss_ll_heat=0.544765, q=1000
[2018-07-13 08:21:50,718] [train] [INFO] epoch=4.00 step=31300, 9.9059 examples/sec lr=0.000033, loss=29.8082, loss_ll=5.38945, loss_ll_paf=9.73837, loss_ll_heat=1.04052, q=1000
[2018-07-13 08:24:25,878] [train] [INFO] epoch=4.00 step=31400, 9.9071 examples/sec lr=0.000033, loss=29.9337, loss_ll=5.21694, loss_ll_paf=9.3977, loss_ll_heat=1.03618, q=1000
[2018-07-13 08:27:04,344] [train] [INFO] epoch=4.00 step=31500, 9.9077 examples/sec lr=0.000033, loss=30.5185, loss_ll=5.21576, loss_ll_paf=9.64976, loss_ll_heat=0.781753, q=1000
[2018-07-13 08:29:41,015] [train] [INFO] epoch=4.00 step=31600, 9.9086 examples/sec lr=0.000033, loss=15.7656, loss_ll=2.71154, loss_ll_paf=4.82555, loss_ll_heat=0.597528, q=1000
[2018-07-13 08:32:21,413] [train] [INFO] epoch=4.00 step=31700, 9.9088 examples/sec lr=0.000033, loss=25.5112, loss_ll=4.49121, loss_ll_paf=8.09713, loss_ll_heat=0.885285, q=1000
[2018-07-13 08:34:58,589] [train] [INFO] epoch=4.00 step=31800, 9.9097 examples/sec lr=0.000033, loss=21.9928, loss_ll=3.67935, loss_ll_paf=6.78185, loss_ll_heat=0.576839, q=1000
[2018-07-13 08:37:35,863] [train] [INFO] epoch=4.00 step=31900, 9.9105 examples/sec lr=0.000033, loss=35.5795, loss_ll=6.30754, loss_ll_paf=11.1238, loss_ll_heat=1.49133, q=1000
[2018-07-13 08:40:11,613] [train] [INFO] epoch=4.00 step=32000, 9.9116 examples/sec lr=0.000033, loss=38.832, loss_ll=7.31125, loss_ll_paf=13.6194, loss_ll_heat=1.00312, q=1000
[2018-07-13 08:43:03,168] [train] [INFO] epoch=4.00 step=32100, 9.9096 examples/sec lr=0.000033, loss=25.4723, loss_ll=4.45052, loss_ll_paf=8.07932, loss_ll_heat=0.821725, q=1000
[2018-07-13 08:45:40,436] [train] [INFO] epoch=4.00 step=32200, 9.9104 examples/sec lr=0.000033, loss=39.4293, loss_ll=7.3623, loss_ll_paf=13.917, loss_ll_heat=0.807562, q=1000
[2018-07-13 08:48:17,759] [train] [INFO] epoch=4.00 step=32300, 9.9112 examples/sec lr=0.000033, loss=8.53048, loss_ll=1.35454, loss_ll_paf=1.94117, loss_ll_heat=0.767918, q=1000
[2018-07-13 08:50:54,924] [train] [INFO] epoch=4.00 step=32400, 9.9120 examples/sec lr=0.000033, loss=27.901, loss_ll=4.83744, loss_ll_paf=8.54363, loss_ll_heat=1.13125, q=1000
[2018-07-13 08:53:32,028] [train] [INFO] epoch=4.00 step=32500, 9.9128 examples/sec lr=0.000033, loss=51.3016, loss_ll=9.93999, loss_ll_paf=18.6503, loss_ll_heat=1.22972, q=1000
[2018-07-13 08:56:09,236] [train] [INFO] epoch=4.00 step=32600, 9.9136 examples/sec lr=0.000033, loss=22.0987, loss_ll=3.47077, loss_ll_paf=6.07437, loss_ll_heat=0.867164, q=1000
[2018-07-13 08:58:45,932] [train] [INFO] epoch=4.00 step=32700, 9.9145 examples/sec lr=0.000033, loss=34.299, loss_ll=6.72963, loss_ll_paf=12.5312, loss_ll_heat=0.92807, q=1000
[2018-07-13 09:01:22,273] [train] [INFO] epoch=4.00 step=32800, 9.9155 examples/sec lr=0.000033, loss=17.5827, loss_ll=3.23718, loss_ll_paf=5.42296, loss_ll_heat=1.0514, q=1000
[2018-07-13 09:04:00,113] [train] [INFO] epoch=4.00 step=32900, 9.9161 examples/sec lr=0.000033, loss=22.5405, loss_ll=4.02421, loss_ll_paf=7.29118, loss_ll_heat=0.757244, q=1000
[2018-07-13 09:06:35,774] [train] [INFO] epoch=4.00 step=33000, 9.9172 examples/sec lr=0.000033, loss=24.8906, loss_ll=3.89625, loss_ll_paf=7.31149, loss_ll_heat=0.481016, q=1000
[2018-07-13 09:09:25,583] [train] [INFO] epoch=4.00 step=33100, 9.9156 examples/sec lr=0.000033, loss=25.1856, loss_ll=4.58535, loss_ll_paf=7.66233, loss_ll_heat=1.50837, q=1000
[2018-07-13 09:12:02,105] [train] [INFO] epoch=4.00 step=33200, 9.9165 examples/sec lr=0.000033, loss=24.2283, loss_ll=4.32113, loss_ll_paf=7.55161, loss_ll_heat=1.09066, q=1000
[2018-07-13 09:14:39,864] [train] [INFO] epoch=4.00 step=33300, 9.9172 examples/sec lr=0.000033, loss=30.2255, loss_ll=5.75292, loss_ll_paf=10.4663, loss_ll_heat=1.03957, q=1000
[2018-07-13 09:17:16,067] [train] [INFO] epoch=4.00 step=33400, 9.9181 examples/sec lr=0.000033, loss=51.068, loss_ll=10.0242, loss_ll_paf=19.047, loss_ll_heat=1.00137, q=1000
[2018-07-13 09:19:54,283] [train] [INFO] epoch=4.00 step=33500, 9.9187 examples/sec lr=0.000033, loss=14.526, loss_ll=2.22082, loss_ll_paf=3.88595, loss_ll_heat=0.555686, q=1000
[2018-07-13 09:22:32,330] [train] [INFO] epoch=4.00 step=33600, 9.9193 examples/sec lr=0.000033, loss=40.1728, loss_ll=7.9531, loss_ll_paf=15.0581, loss_ll_heat=0.848069, q=1000
[2018-07-13 09:25:09,622] [train] [INFO] epoch=4.00 step=33700, 9.9200 examples/sec lr=0.000033, loss=42.8843, loss_ll=7.74252, loss_ll_paf=14.4929, loss_ll_heat=0.992132, q=1000
[2018-07-13 09:27:47,015] [train] [INFO] epoch=4.00 step=33800, 9.9207 examples/sec lr=0.000033, loss=29.4461, loss_ll=5.46901, loss_ll_paf=9.97681, loss_ll_heat=0.961205, q=1000
[2018-07-13 09:30:26,624] [train] [INFO] epoch=4.00 step=33900, 9.9210 examples/sec lr=0.000033, loss=11.7216, loss_ll=2.07121, loss_ll_paf=3.4601, loss_ll_heat=0.682328, q=1000
[2018-07-13 09:33:04,074] [train] [INFO] epoch=4.00 step=34000, 9.9217 examples/sec lr=0.000033, loss=21.4863, loss_ll=3.99657, loss_ll_paf=7.17862, loss_ll_heat=0.81452, q=1000
[2018-07-13 09:35:54,001] [train] [INFO] epoch=4.00 step=34100, 9.9202 examples/sec lr=0.000033, loss=34.3027, loss_ll=6.04531, loss_ll_paf=11.1256, loss_ll_heat=0.964995, q=1000
[2018-07-13 09:38:31,275] [train] [INFO] epoch=4.00 step=34200, 9.9209 examples/sec lr=0.000033, loss=25.2659, loss_ll=3.80213, loss_ll_paf=6.56948, loss_ll_heat=1.03479, q=1000
[2018-07-13 09:41:04,743] [train] [INFO] epoch=4.00 step=34300, 9.9223 examples/sec lr=0.000033, loss=29.2745, loss_ll=5.33903, loss_ll_paf=9.5499, loss_ll_heat=1.12816, q=1000
[2018-07-13 09:43:37,914] [train] [INFO] epoch=4.00 step=34400, 9.9237 examples/sec lr=0.000033, loss=31.1394, loss_ll=5.29299, loss_ll_paf=9.79882, loss_ll_heat=0.787162, q=1000
[2018-07-13 09:46:08,351] [train] [INFO] epoch=4.00 step=34500, 9.9256 examples/sec lr=0.000033, loss=18.6845, loss_ll=3.00165, loss_ll_paf=5.0567, loss_ll_heat=0.946602, q=1000
[2018-07-13 09:48:40,862] [train] [INFO] epoch=4.00 step=34600, 9.9272 examples/sec lr=0.000033, loss=42.1108, loss_ll=6.91113, loss_ll_paf=12.5737, loss_ll_heat=1.24859, q=1000
[2018-07-13 09:51:12,408] [train] [INFO] epoch=4.00 step=34700, 9.9289 examples/sec lr=0.000033, loss=29.9874, loss_ll=5.34586, loss_ll_paf=9.14873, loss_ll_heat=1.543, q=1000
[2018-07-13 09:53:43,253] [train] [INFO] epoch=4.00 step=34800, 9.9307 examples/sec lr=0.000033, loss=16.8833, loss_ll=3.12424, loss_ll_paf=5.28294, loss_ll_heat=0.965541, q=1000
[2018-07-13 09:56:14,316] [train] [INFO] epoch=4.00 step=34900, 9.9325 examples/sec lr=0.000033, loss=25.3629, loss_ll=4.64546, loss_ll_paf=8.51378, loss_ll_heat=0.777139, q=1000
[2018-07-13 09:58:43,740] [train] [INFO] epoch=4.00 step=35000, 9.9346 examples/sec lr=0.000033, loss=20.3207, loss_ll=3.20475, loss_ll_paf=5.80106, loss_ll_heat=0.608439, q=1000
[2018-07-13 10:01:31,901] [train] [INFO] epoch=4.00 step=35100, 9.9333 examples/sec lr=0.000033, loss=31.0616, loss_ll=5.25036, loss_ll_paf=9.2043, loss_ll_heat=1.29641, q=1000
[2018-07-13 10:04:04,509] [train] [INFO] epoch=4.00 step=35200, 9.9348 examples/sec lr=0.000033, loss=31.6616, loss_ll=5.29878, loss_ll_paf=9.44218, loss_ll_heat=1.15538, q=1000
[2018-07-13 10:06:35,600] [train] [INFO] epoch=4.00 step=35300, 9.9365 examples/sec lr=0.000033, loss=29.194, loss_ll=5.07604, loss_ll_paf=9.25292, loss_ll_heat=0.899151, q=1000
[2018-07-13 10:09:06,981] [train] [INFO] epoch=4.00 step=35400, 9.9382 examples/sec lr=0.000033, loss=49.5081, loss_ll=9.61935, loss_ll_paf=18.1538, loss_ll_heat=1.08494, q=1000
[2018-07-13 10:11:37,112] [train] [INFO] epoch=4.00 step=35500, 9.9401 examples/sec lr=0.000033, loss=8.21437, loss_ll=1.30414, loss_ll_paf=2.01519, loss_ll_heat=0.593083, q=1000
[2018-07-13 10:14:07,315] [train] [INFO] epoch=4.00 step=35600, 9.9420 examples/sec lr=0.000033, loss=21.726, loss_ll=3.44463, loss_ll_paf=6.24311, loss_ll_heat=0.646152, q=1000
[2018-07-13 10:16:39,319] [train] [INFO] epoch=4.00 step=35700, 9.9435 examples/sec lr=0.000033, loss=13.0382, loss_ll=2.10884, loss_ll_paf=3.80981, loss_ll_heat=0.407864, q=1000
[2018-07-13 10:19:09,241] [train] [INFO] epoch=4.00 step=35800, 9.9454 examples/sec lr=0.000033, loss=27.0163, loss_ll=4.43882, loss_ll_paf=8.22111, loss_ll_heat=0.656519, q=1000
[2018-07-13 10:21:42,717] [train] [INFO] epoch=4.00 step=35900, 9.9467 examples/sec lr=0.000033, loss=18.4397, loss_ll=3.24948, loss_ll_paf=5.7069, loss_ll_heat=0.792069, q=1000
[2018-07-13 10:24:12,518] [train] [INFO] epoch=4.00 step=36000, 9.9486 examples/sec lr=0.000033, loss=28.3467, loss_ll=5.23221, loss_ll_paf=9.86778, loss_ll_heat=0.596643, q=1000
[2018-07-13 10:27:23,707] [train] [INFO] epoch=4.00 step=36100, 9.9434 examples/sec lr=0.000033, loss=34.1647, loss_ll=6.36545, loss_ll_paf=12.1425, loss_ll_heat=0.588359, q=1000
[2018-07-13 10:30:02,311] [train] [INFO] epoch=4.00 step=36200, 9.9438 examples/sec lr=0.000033, loss=22.0011, loss_ll=3.93798, loss_ll_paf=7.05803, loss_ll_heat=0.817922, q=1000
[2018-07-13 10:32:56,074] [train] [INFO] epoch=4.00 step=36300, 9.9416 examples/sec lr=0.000033, loss=30.6801, loss_ll=5.69024, loss_ll_paf=10.633, loss_ll_heat=0.747488, q=1000
[2018-07-13 10:35:51,798] [train] [INFO] epoch=4.00 step=36400, 9.9391 examples/sec lr=0.000033, loss=37.1342, loss_ll=6.34899, loss_ll_paf=11.7581, loss_ll_heat=0.939908, q=1000
[2018-07-13 10:38:45,992] [train] [INFO] epoch=4.00 step=36500, 9.9368 examples/sec lr=0.000033, loss=9.94689, loss_ll=1.50835, loss_ll_paf=2.43506, loss_ll_heat=0.581652, q=1000
[2018-07-13 10:41:38,873] [train] [INFO] epoch=4.00 step=36600, 9.9348 examples/sec lr=0.000033, loss=41.8934, loss_ll=7.44319, loss_ll_paf=13.3504, loss_ll_heat=1.53595, q=1000
[2018-07-13 10:44:29,732] [train] [INFO] epoch=4.00 step=36700, 9.9332 examples/sec lr=0.000033, loss=41.0915, loss_ll=8.08287, loss_ll_paf=14.4337, loss_ll_heat=1.73202, q=1000
[2018-07-13 10:47:11,843] [train] [INFO] epoch=4.00 step=36800, 9.9330 examples/sec lr=0.000033, loss=18.878, loss_ll=3.4324, loss_ll_paf=6.21105, loss_ll_heat=0.653752, q=1000
[2018-07-13 10:49:53,620] [train] [INFO] epoch=4.00 step=36900, 9.9329 examples/sec lr=0.000033, loss=32.1865, loss_ll=5.67488, loss_ll_paf=10.2258, loss_ll_heat=1.12395, q=1000
[2018-07-13 10:52:36,621] [train] [INFO] epoch=4.00 step=37000, 9.9326 examples/sec lr=0.000033, loss=14.124, loss_ll=2.50416, loss_ll_paf=4.21762, loss_ll_heat=0.790707, q=1000
[2018-07-13 10:55:25,093] [train] [INFO] epoch=4.00 step=37100, 9.9314 examples/sec lr=0.000033, loss=35.8091, loss_ll=6.46227, loss_ll_paf=12.1682, loss_ll_heat=0.756284, q=1000
[2018-07-13 10:58:04,223] [train] [INFO] epoch=4.00 step=37200, 9.9317 examples/sec lr=0.000033, loss=38.2984, loss_ll=7.25176, loss_ll_paf=13.1415, loss_ll_heat=1.36199, q=1000
[2018-07-13 11:00:51,768] [train] [INFO] epoch=4.00 step=37300, 9.9306 examples/sec lr=0.000033, loss=27.8854, loss_ll=4.81342, loss_ll_paf=8.69627, loss_ll_heat=0.930569, q=1000
[2018-07-13 11:03:36,790] [train] [INFO] epoch=4.00 step=37400, 9.9300 examples/sec lr=0.000033, loss=43.35, loss_ll=8.40386, loss_ll_paf=15.3244, loss_ll_heat=1.48326, q=1000
[2018-07-13 11:06:20,637] [train] [INFO] epoch=4.00 step=37500, 9.9295 examples/sec lr=0.000033, loss=15.1281, loss_ll=2.4786, loss_ll_paf=3.98171, loss_ll_heat=0.975499, q=1000
[2018-07-13 11:08:49,760] [train] [INFO] epoch=4.00 step=37600, 9.9315 examples/sec lr=0.000033, loss=16.6964, loss_ll=3.07074, loss_ll_paf=5.25981, loss_ll_heat=0.881676, q=1000
[2018-07-13 11:11:20,989] [train] [INFO] epoch=4.00 step=37700, 9.9331 examples/sec lr=0.000033, loss=21.7081, loss_ll=3.23978, loss_ll_paf=5.62323, loss_ll_heat=0.856335, q=1000
[2018-07-13 11:13:53,150] [train] [INFO] epoch=4.00 step=37800, 9.9346 examples/sec lr=0.000033, loss=17.7469, loss_ll=3.18287, loss_ll_paf=5.40359, loss_ll_heat=0.96214, q=1000
[2018-07-13 11:16:22,163] [train] [INFO] epoch=4.00 step=37900, 9.9365 examples/sec lr=0.000033, loss=27.1173, loss_ll=4.88665, loss_ll_paf=9.0594, loss_ll_heat=0.713909, q=1000
[2018-07-13 11:18:54,461] [train] [INFO] epoch=4.00 step=38000, 9.9379 examples/sec lr=0.000033, loss=13.8955, loss_ll=2.29331, loss_ll_paf=3.85033, loss_ll_heat=0.736287, q=1000
[2018-07-13 11:21:37,188] [train] [INFO] epoch=5.00 step=38100, 9.9377 examples/sec lr=0.000033, loss=18.9831, loss_ll=3.45926, loss_ll_paf=5.96371, loss_ll_heat=0.954799, q=1000
[2018-07-13 11:24:08,487] [train] [INFO] epoch=5.00 step=38200, 9.9392 examples/sec lr=0.000033, loss=7.67425, loss_ll=1.34905, loss_ll_paf=2.19306, loss_ll_heat=0.505041, q=1000
[2018-07-13 11:26:43,757] [train] [INFO] epoch=5.00 step=38300, 9.9402 examples/sec lr=0.000033, loss=33.2283, loss_ll=6.1798, loss_ll_paf=11.1529, loss_ll_heat=1.20667, q=1000
[2018-07-13 11:29:20,760] [train] [INFO] epoch=5.00 step=38400, 9.9408 examples/sec lr=0.000033, loss=12.8422, loss_ll=2.0272, loss_ll_paf=3.34853, loss_ll_heat=0.705876, q=1000
[2018-07-13 11:31:53,805] [train] [INFO] epoch=5.00 step=38500, 9.9421 examples/sec lr=0.000033, loss=12.2167, loss_ll=2.12646, loss_ll_paf=3.67308, loss_ll_heat=0.579842, q=1000
[2018-07-13 11:34:28,101] [train] [INFO] epoch=5.00 step=38600, 9.9431 examples/sec lr=0.000033, loss=18.0663, loss_ll=3.20496, loss_ll_paf=5.74937, loss_ll_heat=0.660555, q=1000
[2018-07-13 11:36:57,507] [train] [INFO] epoch=5.00 step=38700, 9.9450 examples/sec lr=0.000033, loss=7.16229, loss_ll=1.17459, loss_ll_paf=1.72646, loss_ll_heat=0.622724, q=1000
[2018-07-13 11:39:26,588] [train] [INFO] epoch=5.00 step=38800, 9.9468 examples/sec lr=0.000033, loss=24.483, loss_ll=4.25384, loss_ll_paf=7.77958, loss_ll_heat=0.728093, q=1000
[2018-07-13 11:41:54,341] [train] [INFO] epoch=5.00 step=38900, 9.9489 examples/sec lr=0.000033, loss=19.6252, loss_ll=3.02684, loss_ll_paf=5.48337, loss_ll_heat=0.570315, q=1000
[2018-07-13 11:44:25,676] [train] [INFO] epoch=5.00 step=39000, 9.9504 examples/sec lr=0.000033, loss=19.0775, loss_ll=3.67441, loss_ll_paf=6.4421, loss_ll_heat=0.906723, q=1000
[2018-07-13 11:47:13,675] [train] [INFO] epoch=5.00 step=39100, 9.9493 examples/sec lr=0.000033, loss=17.941, loss_ll=2.9541, loss_ll_paf=5.18917, loss_ll_heat=0.719023, q=1000
[2018-07-13 11:49:44,990] [train] [INFO] epoch=5.00 step=39200, 9.9508 examples/sec lr=0.000033, loss=39.3628, loss_ll=7.69627, loss_ll_paf=14.3482, loss_ll_heat=1.04435, q=1000
[2018-07-13 11:52:17,720] [train] [INFO] epoch=5.00 step=39300, 9.9521 examples/sec lr=0.000033, loss=9.98288, loss_ll=1.49633, loss_ll_paf=2.49816, loss_ll_heat=0.494502, q=1000
[2018-07-13 11:54:48,960] [train] [INFO] epoch=5.00 step=39400, 9.9536 examples/sec lr=0.000033, loss=27.7772, loss_ll=5.28535, loss_ll_paf=9.3042, loss_ll_heat=1.26649, q=1000
[2018-07-13 11:57:22,027] [train] [INFO] epoch=5.00 step=39500, 9.9548 examples/sec lr=0.000033, loss=33.5183, loss_ll=6.20086, loss_ll_paf=11.449, loss_ll_heat=0.952774, q=1000
[2018-07-13 11:59:57,693] [train] [INFO] epoch=5.00 step=39600, 9.9556 examples/sec lr=0.000033, loss=32.9082, loss_ll=6.20655, loss_ll_paf=11.4481, loss_ll_heat=0.965018, q=1000
[2018-07-13 12:02:28,877] [train] [INFO] epoch=5.00 step=39700, 9.9570 examples/sec lr=0.000033, loss=29.2596, loss_ll=5.37645, loss_ll_paf=10.2044, loss_ll_heat=0.548502, q=1000
[2018-07-13 12:04:59,228] [train] [INFO] epoch=5.00 step=39800, 9.9587 examples/sec lr=0.000033, loss=15.691, loss_ll=2.49835, loss_ll_paf=4.40577, loss_ll_heat=0.590936, q=1000
[2018-07-13 12:07:28,410] [train] [INFO] epoch=5.00 step=39900, 9.9604 examples/sec lr=0.000033, loss=22.4654, loss_ll=3.95122, loss_ll_paf=6.86939, loss_ll_heat=1.03305, q=1000
[2018-07-13 12:10:00,512] [train] [INFO] epoch=5.00 step=40000, 9.9618 examples/sec lr=0.000033, loss=24.5268, loss_ll=4.38453, loss_ll_paf=7.86774, loss_ll_heat=0.901324, q=1000
[2018-07-13 12:12:48,528] [train] [INFO] epoch=5.00 step=40100, 9.9606 examples/sec lr=0.000033, loss=13.0621, loss_ll=2.01284, loss_ll_paf=3.37728, loss_ll_heat=0.648391, q=1000
[2018-07-13 12:15:18,191] [train] [INFO] epoch=5.00 step=40200, 9.9623 examples/sec lr=0.000033, loss=34.3666, loss_ll=7.06707, loss_ll_paf=13.3611, loss_ll_heat=0.773039, q=1000
[2018-07-13 12:17:48,413] [train] [INFO] epoch=5.00 step=40300, 9.9639 examples/sec lr=0.000033, loss=32.0682, loss_ll=6.15519, loss_ll_paf=10.963, loss_ll_heat=1.34734, q=1000
[2018-07-13 12:20:17,603] [train] [INFO] epoch=5.00 step=40400, 9.9657 examples/sec lr=0.000033, loss=28.3121, loss_ll=4.74346, loss_ll_paf=8.72036, loss_ll_heat=0.766558, q=1000
[2018-07-13 12:22:48,068] [train] [INFO] epoch=5.00 step=40500, 9.9672 examples/sec lr=0.000033, loss=27.3112, loss_ll=5.31769, loss_ll_paf=9.22964, loss_ll_heat=1.40574, q=1000
[2018-07-13 12:25:16,679] [train] [INFO] epoch=5.00 step=40600, 9.9690 examples/sec lr=0.000033, loss=27.1208, loss_ll=4.73723, loss_ll_paf=8.32211, loss_ll_heat=1.15235, q=1000
[2018-07-13 12:27:47,267] [train] [INFO] epoch=5.00 step=40700, 9.9705 examples/sec lr=0.000033, loss=28.0146, loss_ll=5.13887, loss_ll_paf=9.44624, loss_ll_heat=0.8315, q=1000
[2018-07-13 12:30:16,997] [train] [INFO] epoch=5.00 step=40800, 9.9722 examples/sec lr=0.000033, loss=19.0108, loss_ll=3.57078, loss_ll_paf=6.39575, loss_ll_heat=0.745812, q=1000
[2018-07-13 12:32:47,719] [train] [INFO] epoch=5.00 step=40900, 9.9737 examples/sec lr=0.000033, loss=23.4668, loss_ll=4.43155, loss_ll_paf=7.92415, loss_ll_heat=0.938949, q=1000
[2018-07-13 12:35:15,073] [train] [INFO] epoch=5.00 step=41000, 9.9756 examples/sec lr=0.000033, loss=20.0666, loss_ll=3.59345, loss_ll_paf=6.63165, loss_ll_heat=0.555245, q=1000
[2018-07-13 12:37:57,765] [train] [INFO] epoch=5.00 step=41100, 9.9753 examples/sec lr=0.000033, loss=22.9508, loss_ll=4.24608, loss_ll_paf=7.89401, loss_ll_heat=0.598151, q=1000
[2018-07-13 12:40:27,404] [train] [INFO] epoch=5.00 step=41200, 9.9769 examples/sec lr=0.000033, loss=12.0612, loss_ll=2.17599, loss_ll_paf=3.38338, loss_ll_heat=0.968598, q=1000
[2018-07-13 12:42:57,988] [train] [INFO] epoch=5.00 step=41300, 9.9784 examples/sec lr=0.000033, loss=29.6681, loss_ll=5.55563, loss_ll_paf=10.122, loss_ll_heat=0.989279, q=1000
[2018-07-13 12:45:32,424] [train] [INFO] epoch=5.00 step=41400, 9.9793 examples/sec lr=0.000033, loss=15.406, loss_ll=3.05221, loss_ll_paf=5.53769, loss_ll_heat=0.566727, q=1000
[2018-07-13 12:48:05,419] [train] [INFO] epoch=5.00 step=41500, 9.9804 examples/sec lr=0.000033, loss=18.3597, loss_ll=3.01258, loss_ll_paf=5.16734, loss_ll_heat=0.857809, q=1000
[2018-07-13 12:50:37,753] [train] [INFO] epoch=5.00 step=41600, 9.9816 examples/sec lr=0.000033, loss=36.3534, loss_ll=6.41297, loss_ll_paf=11.5023, loss_ll_heat=1.32365, q=1000
[2018-07-13 12:53:07,221] [train] [INFO] epoch=5.00 step=41700, 9.9832 examples/sec lr=0.000033, loss=29.5036, loss_ll=5.44558, loss_ll_paf=9.69106, loss_ll_heat=1.20011, q=1000
[2018-07-13 12:55:40,919] [train] [INFO] epoch=5.00 step=41800, 9.9842 examples/sec lr=0.000033, loss=32.9998, loss_ll=6.17632, loss_ll_paf=11.257, loss_ll_heat=1.09567, q=1000
[2018-07-13 12:58:10,844] [train] [INFO] epoch=5.00 step=41900, 9.9857 examples/sec lr=0.000033, loss=39.1189, loss_ll=7.433, loss_ll_paf=13.3645, loss_ll_heat=1.5015, q=1000
[2018-07-13 13:00:42,545] [train] [INFO] epoch=5.00 step=42000, 9.9870 examples/sec lr=0.000033, loss=42.7203, loss_ll=7.98807, loss_ll_paf=15.185, loss_ll_heat=0.791166, q=1000
[2018-07-13 13:03:23,250] [train] [INFO] epoch=5.00 step=42100, 9.9869 examples/sec lr=0.000033, loss=12.8802, loss_ll=2.21783, loss_ll_paf=3.88243, loss_ll_heat=0.553234, q=1000
[2018-07-13 13:05:52,072] [train] [INFO] epoch=5.00 step=42200, 9.9886 examples/sec lr=0.000033, loss=31.1465, loss_ll=5.34274, loss_ll_paf=9.97611, loss_ll_heat=0.709383, q=1000
[2018-07-13 13:08:20,472] [train] [INFO] epoch=5.00 step=42300, 9.9903 examples/sec lr=0.000033, loss=13.9525, loss_ll=2.45262, loss_ll_paf=4.39664, loss_ll_heat=0.508595, q=1000
[2018-07-13 13:10:50,593] [train] [INFO] epoch=5.00 step=42400, 9.9918 examples/sec lr=0.000033, loss=12.5171, loss_ll=1.95695, loss_ll_paf=3.37388, loss_ll_heat=0.540009, q=1000
[2018-07-13 13:13:18,979] [train] [INFO] epoch=5.00 step=42500, 9.9935 examples/sec lr=0.000033, loss=32.8367, loss_ll=5.84408, loss_ll_paf=11.0603, loss_ll_heat=0.627873, q=1000
[2018-07-13 13:15:49,010] [train] [INFO] epoch=5.00 step=42600, 9.9950 examples/sec lr=0.000033, loss=32.6515, loss_ll=6.0854, loss_ll_paf=11.473, loss_ll_heat=0.697751, q=1000
[2018-07-13 13:18:23,376] [train] [INFO] epoch=5.00 step=42700, 9.9958 examples/sec lr=0.000033, loss=38.4055, loss_ll=7.18061, loss_ll_paf=13.4474, loss_ll_heat=0.913828, q=1000
[2018-07-13 13:20:51,544] [train] [INFO] epoch=5.00 step=42800, 9.9976 examples/sec lr=0.000033, loss=17.078, loss_ll=2.51919, loss_ll_paf=4.29638, loss_ll_heat=0.742002, q=1000
[2018-07-13 13:23:20,081] [train] [INFO] epoch=5.00 step=42900, 9.9992 examples/sec lr=0.000033, loss=30.2846, loss_ll=5.22424, loss_ll_paf=9.82769, loss_ll_heat=0.620797, q=1000
[2018-07-13 13:25:48,942] [train] [INFO] epoch=5.00 step=43000, 10.0009 examples/sec lr=0.000033, loss=12.0914, loss_ll=1.98731, loss_ll_paf=3.63054, loss_ll_heat=0.344079, q=1000
[2018-07-13 13:28:31,687] [train] [INFO] epoch=5.00 step=43100, 10.0005 examples/sec lr=0.000033, loss=24.1858, loss_ll=4.34867, loss_ll_paf=7.99327, loss_ll_heat=0.704073, q=1000
[2018-07-13 13:31:01,773] [train] [INFO] epoch=5.00 step=43200, 10.0019 examples/sec lr=0.000033, loss=15.8436, loss_ll=2.46329, loss_ll_paf=4.25533, loss_ll_heat=0.671247, q=1000
[2018-07-13 13:33:30,808] [train] [INFO] epoch=5.00 step=43300, 10.0035 examples/sec lr=0.000033, loss=24.5494, loss_ll=4.18501, loss_ll_paf=7.34163, loss_ll_heat=1.0284, q=1000
[2018-07-13 13:36:05,354] [train] [INFO] epoch=5.00 step=43400, 10.0042 examples/sec lr=0.000033, loss=33.6595, loss_ll=6.22243, loss_ll_paf=11.6361, loss_ll_heat=0.808786, q=1000
[2018-07-13 13:38:35,262] [train] [INFO] epoch=5.00 step=43500, 10.0057 examples/sec lr=0.000033, loss=16.8492, loss_ll=2.72316, loss_ll_paf=4.94439, loss_ll_heat=0.501924, q=1000
[2018-07-13 13:41:06,864] [train] [INFO] epoch=5.00 step=43600, 10.0069 examples/sec lr=0.000033, loss=14.0724, loss_ll=2.04093, loss_ll_paf=3.57613, loss_ll_heat=0.505724, q=1000
[2018-07-13 13:43:35,997] [train] [INFO] epoch=5.00 step=43700, 10.0084 examples/sec lr=0.000033, loss=23.1459, loss_ll=3.68101, loss_ll_paf=6.77227, loss_ll_heat=0.58976, q=1000
[2018-07-13 13:46:10,388] [train] [INFO] epoch=5.00 step=43800, 10.0092 examples/sec lr=0.000033, loss=25.2957, loss_ll=4.74761, loss_ll_paf=8.84291, loss_ll_heat=0.652321, q=1000
[2018-07-13 13:48:37,618] [train] [INFO] epoch=5.00 step=43900, 10.0110 examples/sec lr=0.000033, loss=31.4706, loss_ll=6.11651, loss_ll_paf=11.1228, loss_ll_heat=1.11023, q=1000
[2018-07-13 13:51:06,951] [train] [INFO] epoch=5.00 step=44000, 10.0125 examples/sec lr=0.000033, loss=13.6402, loss_ll=2.39815, loss_ll_paf=4.26883, loss_ll_heat=0.527461, q=1000
[2018-07-13 13:53:48,320] [train] [INFO] epoch=5.00 step=44100, 10.0123 examples/sec lr=0.000033, loss=21.5192, loss_ll=3.78924, loss_ll_paf=6.62668, loss_ll_heat=0.951798, q=1000
[2018-07-13 13:56:17,307] [train] [INFO] epoch=5.00 step=44200, 10.0138 examples/sec lr=0.000033, loss=10.6021, loss_ll=1.87567, loss_ll_paf=3.39934, loss_ll_heat=0.351991, q=1000
[2018-07-13 13:58:47,943] [train] [INFO] epoch=5.00 step=44300, 10.0151 examples/sec lr=0.000033, loss=16.7327, loss_ll=3.10567, loss_ll_paf=5.56844, loss_ll_heat=0.642904, q=1000
[2018-07-13 14:01:20,119] [train] [INFO] epoch=5.00 step=44400, 10.0162 examples/sec lr=0.000033, loss=20.3903, loss_ll=3.52398, loss_ll_paf=6.42981, loss_ll_heat=0.618153, q=1000
[2018-07-13 14:03:48,008] [train] [INFO] epoch=5.00 step=44500, 10.0178 examples/sec lr=0.000033, loss=6.99107, loss_ll=1.21832, loss_ll_paf=1.84446, loss_ll_heat=0.59218, q=1000
[2018-07-13 14:06:18,246] [train] [INFO] epoch=5.00 step=44600, 10.0192 examples/sec lr=0.000033, loss=19.8799, loss_ll=3.36072, loss_ll_paf=5.82588, loss_ll_heat=0.895552, q=1000
[2018-07-13 14:08:52,145] [train] [INFO] epoch=5.00 step=44700, 10.0200 examples/sec lr=0.000033, loss=27.3171, loss_ll=4.64322, loss_ll_paf=8.14028, loss_ll_heat=1.14616, q=1000
[2018-07-13 14:11:25,158] [train] [INFO] epoch=5.00 step=44800, 10.0209 examples/sec lr=0.000033, loss=34.528, loss_ll=6.36454, loss_ll_paf=12.0569, loss_ll_heat=0.672135, q=1000
[2018-07-13 14:13:53,501] [train] [INFO] epoch=5.00 step=44900, 10.0225 examples/sec lr=0.000033, loss=13.9042, loss_ll=2.35695, loss_ll_paf=4.09141, loss_ll_heat=0.622485, q=1000
[2018-07-13 14:16:19,833] [train] [INFO] epoch=5.00 step=45000, 10.0244 examples/sec lr=0.000033, loss=39.6747, loss_ll=7.44999, loss_ll_paf=13.8508, loss_ll_heat=1.04915, q=1000
[2018-07-13 14:18:57,602] [train] [INFO] epoch=5.00 step=45100, 10.0246 examples/sec lr=0.000033, loss=17.067, loss_ll=2.39638, loss_ll_paf=4.19129, loss_ll_heat=0.601483, q=1000
[2018-07-13 14:21:27,550] [train] [INFO] epoch=5.00 step=45200, 10.0260 examples/sec lr=0.000033, loss=25.6811, loss_ll=4.65746, loss_ll_paf=8.32409, loss_ll_heat=0.990827, q=1000
[2018-07-13 14:23:58,036] [train] [INFO] epoch=5.00 step=45300, 10.0272 examples/sec lr=0.000033, loss=8.07212, loss_ll=1.44702, loss_ll_paf=2.41484, loss_ll_heat=0.479205, q=1000
[2018-07-13 14:26:31,226] [train] [INFO] epoch=5.00 step=45400, 10.0281 examples/sec lr=0.000033, loss=28.1031, loss_ll=5.34695, loss_ll_paf=9.90956, loss_ll_heat=0.784351, q=1000
[2018-07-13 14:28:59,199] [train] [INFO] epoch=5.00 step=45500, 10.0297 examples/sec lr=0.000033, loss=24.9758, loss_ll=4.69415, loss_ll_paf=8.52366, loss_ll_heat=0.864634, q=1000
[2018-07-13 14:31:29,537] [train] [INFO] epoch=5.00 step=45600, 10.0310 examples/sec lr=0.000033, loss=22.6828, loss_ll=4.41229, loss_ll_paf=7.93419, loss_ll_heat=0.890398, q=1000
[2018-07-13 14:33:57,025] [train] [INFO] epoch=6.00 step=45700, 10.0326 examples/sec lr=0.000033, loss=17.0527, loss_ll=2.95016, loss_ll_paf=5.22493, loss_ll_heat=0.675383, q=1000
[2018-07-13 14:36:27,278] [train] [INFO] epoch=6.00 step=45800, 10.0339 examples/sec lr=0.000033, loss=14.4773, loss_ll=2.27552, loss_ll_paf=3.70411, loss_ll_heat=0.846925, q=1000
[2018-07-13 14:38:55,935] [train] [INFO] epoch=6.00 step=45900, 10.0354 examples/sec lr=0.000033, loss=13.7262, loss_ll=1.92708, loss_ll_paf=3.25757, loss_ll_heat=0.59659, q=1000
[2018-07-13 14:41:25,644] [train] [INFO] epoch=6.00 step=46000, 10.0367 examples/sec lr=0.000033, loss=15.398, loss_ll=2.45107, loss_ll_paf=4.3583, loss_ll_heat=0.543843, q=1000
[2018-07-13 14:44:09,543] [train] [INFO] epoch=6.00 step=46100, 10.0361 examples/sec lr=0.000033, loss=27.1242, loss_ll=4.76777, loss_ll_paf=8.73929, loss_ll_heat=0.796245, q=1000
[2018-07-13 14:46:44,643] [train] [INFO] epoch=6.00 step=46200, 10.0367 examples/sec lr=0.000033, loss=16.4946, loss_ll=3.03625, loss_ll_paf=5.23909, loss_ll_heat=0.8334, q=1000
[2018-07-13 14:49:13,058] [train] [INFO] epoch=6.00 step=46300, 10.0382 examples/sec lr=0.000033, loss=17.8736, loss_ll=2.83946, loss_ll_paf=5.19798, loss_ll_heat=0.480934, q=1000
[2018-07-13 14:51:44,095] [train] [INFO] epoch=6.00 step=46400, 10.0393 examples/sec lr=0.000033, loss=19.3674, loss_ll=3.35622, loss_ll_paf=6.29823, loss_ll_heat=0.414204, q=1000
[2018-07-13 14:54:13,242] [train] [INFO] epoch=6.00 step=46500, 10.0407 examples/sec lr=0.000033, loss=20.6303, loss_ll=3.45841, loss_ll_paf=5.9117, loss_ll_heat=1.00511, q=1000
[2018-07-13 14:56:43,460] [train] [INFO] epoch=6.00 step=46600, 10.0419 examples/sec lr=0.000033, loss=9.01641, loss_ll=1.38797, loss_ll_paf=2.31231, loss_ll_heat=0.463638, q=1000
[2018-07-13 14:59:11,962] [train] [INFO] epoch=6.00 step=46700, 10.0434 examples/sec lr=0.000033, loss=19.5699, loss_ll=3.57574, loss_ll_paf=6.72461, loss_ll_heat=0.426867, q=1000
[2018-07-13 15:01:42,953] [train] [INFO] epoch=6.00 step=46800, 10.0445 examples/sec lr=0.000033, loss=37.226, loss_ll=6.50917, loss_ll_paf=12.1966, loss_ll_heat=0.821693, q=1000
[2018-07-13 15:04:11,547] [train] [INFO] epoch=6.00 step=46900, 10.0460 examples/sec lr=0.000033, loss=17.6989, loss_ll=2.83959, loss_ll_paf=5.14726, loss_ll_heat=0.531927, q=1000
[2018-07-13 15:06:42,258] [train] [INFO] epoch=6.00 step=47000, 10.0471 examples/sec lr=0.000033, loss=15.954, loss_ll=2.83324, loss_ll_paf=4.80614, loss_ll_heat=0.860348, q=1000
[2018-07-13 15:09:23,023] [train] [INFO] epoch=6.00 step=47100, 10.0469 examples/sec lr=0.000033, loss=23.3486, loss_ll=4.40042, loss_ll_paf=8.32288, loss_ll_heat=0.477959, q=1000
[2018-07-13 15:11:55,248] [train] [INFO] epoch=6.00 step=47200, 10.0478 examples/sec lr=0.000033, loss=44.359, loss_ll=8.76075, loss_ll_paf=16.2146, loss_ll_heat=1.3069, q=1000
[2018-07-13 15:14:27,050] [train] [INFO] epoch=6.00 step=47300, 10.0488 examples/sec lr=0.000033, loss=24.3721, loss_ll=4.5422, loss_ll_paf=8.47712, loss_ll_heat=0.607287, q=1000
[2018-07-13 15:16:59,288] [train] [INFO] epoch=6.00 step=47400, 10.0498 examples/sec lr=0.000033, loss=42.2216, loss_ll=8.0653, loss_ll_paf=15.1383, loss_ll_heat=0.99233, q=1000
[2018-07-13 15:19:28,394] [train] [INFO] epoch=6.00 step=47500, 10.0511 examples/sec lr=0.000033, loss=14.7734, loss_ll=2.51702, loss_ll_paf=4.06814, loss_ll_heat=0.965891, q=1000
[2018-07-13 15:22:02,238] [train] [INFO] epoch=6.00 step=47600, 10.0518 examples/sec lr=0.000033, loss=33.3125, loss_ll=6.11625, loss_ll_paf=11.7522, loss_ll_heat=0.480275, q=1000
[2018-07-13 15:24:30,075] [train] [INFO] epoch=6.00 step=47700, 10.0533 examples/sec lr=0.000033, loss=30.1753, loss_ll=5.7716, loss_ll_paf=10.2716, loss_ll_heat=1.27156, q=1000
[2018-07-13 15:27:03,622] [train] [INFO] epoch=6.00 step=47800, 10.0541 examples/sec lr=0.000033, loss=12.9402, loss_ll=2.12595, loss_ll_paf=3.71087, loss_ll_heat=0.541033, q=1000
[2018-07-13 15:29:30,916] [train] [INFO] epoch=6.00 step=47900, 10.0556 examples/sec lr=0.000033, loss=38.403, loss_ll=7.16603, loss_ll_paf=12.8842, loss_ll_heat=1.44785, q=1000
[2018-07-13 15:32:00,575] [train] [INFO] epoch=6.00 step=48000, 10.0569 examples/sec lr=0.000033, loss=12.623, loss_ll=2.04689, loss_ll_paf=3.77166, loss_ll_heat=0.322121, q=1000
[2018-07-13 15:34:47,917] [train] [INFO] epoch=6.00 step=48100, 10.0558 examples/sec lr=0.000033, loss=23.4099, loss_ll=4.27392, loss_ll_paf=7.62387, loss_ll_heat=0.923961, q=1000
[2018-07-13 15:37:17,423] [train] [INFO] epoch=6.00 step=48200, 10.0570 examples/sec lr=0.000033, loss=11.9519, loss_ll=2.07942, loss_ll_paf=3.66967, loss_ll_heat=0.489165, q=1000
[2018-07-13 15:39:45,158] [train] [INFO] epoch=6.00 step=48300, 10.0585 examples/sec lr=0.000033, loss=53.5889, loss_ll=10.3253, loss_ll_paf=19.8181, loss_ll_heat=0.8325, q=1000
[2018-07-13 15:42:15,887] [train] [INFO] epoch=6.00 step=48400, 10.0596 examples/sec lr=0.000033, loss=29.0577, loss_ll=5.32664, loss_ll_paf=10.089, loss_ll_heat=0.564305, q=1000
[2018-07-13 15:44:50,808] [train] [INFO] epoch=6.00 step=48500, 10.0602 examples/sec lr=0.000033, loss=28.9523, loss_ll=4.99763, loss_ll_paf=9.45842, loss_ll_heat=0.536834, q=1000
[2018-07-13 15:47:19,432] [train] [INFO] epoch=6.00 step=48600, 10.0615 examples/sec lr=0.000033, loss=29.0689, loss_ll=5.44524, loss_ll_paf=9.61314, loss_ll_heat=1.27735, q=1000
[2018-07-13 15:49:47,292] [train] [INFO] epoch=6.00 step=48700, 10.0630 examples/sec lr=0.000033, loss=24.2228, loss_ll=4.30939, loss_ll_paf=7.64754, loss_ll_heat=0.971235, q=1000
[2018-07-13 15:52:15,593] [train] [INFO] epoch=6.00 step=48800, 10.0644 examples/sec lr=0.000033, loss=16.5863, loss_ll=2.82842, loss_ll_paf=5.10272, loss_ll_heat=0.554119, q=1000
[2018-07-13 15:54:54,645] [train] [INFO] epoch=6.00 step=48900, 10.0643 examples/sec lr=0.000033, loss=15.8475, loss_ll=3.01976, loss_ll_paf=5.31418, loss_ll_heat=0.725336, q=1000
[2018-07-13 15:57:58,781] [train] [INFO] epoch=6.00 step=49000, 10.0611 examples/sec lr=0.000033, loss=30.2456, loss_ll=5.59635, loss_ll_paf=10.2178, loss_ll_heat=0.974915, q=1000
[2018-07-13 16:01:07,241] [train] [INFO] epoch=6.00 step=49100, 10.0573 examples/sec lr=0.000033, loss=19.0485, loss_ll=3.52822, loss_ll_paf=6.37479, loss_ll_heat=0.681659, q=1000
[2018-07-13 16:03:58,729] [train] [INFO] epoch=6.00 step=49200, 10.0557 examples/sec lr=0.000033, loss=23.9092, loss_ll=4.20298, loss_ll_paf=7.60684, loss_ll_heat=0.799119, q=1000
[2018-07-13 16:06:50,626] [train] [INFO] epoch=6.00 step=49300, 10.0541 examples/sec lr=0.000033, loss=18.3022, loss_ll=3.14246, loss_ll_paf=5.7968, loss_ll_heat=0.48812, q=1000
[2018-07-13 16:09:42,738] [train] [INFO] epoch=6.00 step=49400, 10.0524 examples/sec lr=0.000033, loss=21.2568, loss_ll=3.38468, loss_ll_paf=6.39279, loss_ll_heat=0.376577, q=1000
[2018-07-13 16:12:30,704] [train] [INFO] epoch=6.00 step=49500, 10.0513 examples/sec lr=0.000033, loss=30.6214, loss_ll=5.64267, loss_ll_paf=10.5018, loss_ll_heat=0.783543, q=1000
[2018-07-13 16:15:01,193] [train] [INFO] epoch=6.00 step=49600, 10.0524 examples/sec lr=0.000033, loss=20.5572, loss_ll=3.52695, loss_ll_paf=6.43662, loss_ll_heat=0.617277, q=1000
[2018-07-13 16:18:03,780] [train] [INFO] epoch=6.00 step=49700, 10.0494 examples/sec lr=0.000033, loss=22.4554, loss_ll=4.05782, loss_ll_paf=7.76716, loss_ll_heat=0.348482, q=1000
[2018-07-13 16:20:58,396] [train] [INFO] epoch=6.00 step=49800, 10.0475 examples/sec lr=0.000033, loss=18.4279, loss_ll=3.39671, loss_ll_paf=5.92666, loss_ll_heat=0.866752, q=1000
[2018-07-13 16:23:49,758] [train] [INFO] epoch=6.00 step=49900, 10.0459 examples/sec lr=0.000033, loss=27.6225, loss_ll=5.08074, loss_ll_paf=9.15821, loss_ll_heat=1.00327, q=1000
[2018-07-13 16:26:38,314] [train] [INFO] epoch=6.00 step=50000, 10.0448 examples/sec lr=0.000033, loss=14.3381, loss_ll=2.42371, loss_ll_paf=4.24429, loss_ll_heat=0.603136, q=1000
[2018-07-13 16:29:42,807] [train] [INFO] epoch=6.00 step=50100, 10.0416 examples/sec lr=0.000033, loss=21.398, loss_ll=4.05987, loss_ll_paf=7.55285, loss_ll_heat=0.566892, q=1000
[2018-07-13 16:32:32,607] [train] [INFO] epoch=6.00 step=50200, 10.0403 examples/sec lr=0.000033, loss=31.6354, loss_ll=5.67793, loss_ll_paf=10.7995, loss_ll_heat=0.556329, q=1000
[2018-07-13 16:35:13,733] [train] [INFO] epoch=6.00 step=50300, 10.0401 examples/sec lr=0.000033, loss=27.8995, loss_ll=5.06207, loss_ll_paf=9.28294, loss_ll_heat=0.841196, q=1000
[2018-07-13 16:37:55,348] [train] [INFO] epoch=6.00 step=50400, 10.0398 examples/sec lr=0.000033, loss=33.0574, loss_ll=6.46308, loss_ll_paf=12.0681, loss_ll_heat=0.858059, q=1000
[2018-07-13 16:40:39,698] [train] [INFO] epoch=6.00 step=50500, 10.0392 examples/sec lr=0.000033, loss=30.2291, loss_ll=5.87553, loss_ll_paf=10.9927, loss_ll_heat=0.758404, q=1000
[2018-07-13 16:43:11,312] [train] [INFO] epoch=6.00 step=50600, 10.0401 examples/sec lr=0.000033, loss=6.73739, loss_ll=1.14561, loss_ll_paf=2.00823, loss_ll_heat=0.282988, q=1000
[2018-07-13 16:45:41,016] [train] [INFO] epoch=6.00 step=50700, 10.0413 examples/sec lr=0.000033, loss=25.3148, loss_ll=4.56261, loss_ll_paf=8.45342, loss_ll_heat=0.671814, q=1000
[2018-07-13 16:48:10,606] [train] [INFO] epoch=6.00 step=50800, 10.0425 examples/sec lr=0.000033, loss=31.5671, loss_ll=5.76078, loss_ll_paf=10.2962, loss_ll_heat=1.22532, q=1000
[2018-07-13 16:50:39,673] [train] [INFO] epoch=6.00 step=50900, 10.0438 examples/sec lr=0.000033, loss=19.864, loss_ll=3.45519, loss_ll_paf=6.29408, loss_ll_heat=0.616306, q=1000
[2018-07-13 16:53:09,590] [train] [INFO] epoch=6.00 step=51000, 10.0450 examples/sec lr=0.000033, loss=34.0057, loss_ll=6.13447, loss_ll_paf=11.5814, loss_ll_heat=0.687525, q=1000
[2018-07-13 16:55:49,724] [train] [INFO] epoch=6.00 step=51100, 10.0449 examples/sec lr=0.000033, loss=18.5365, loss_ll=3.11399, loss_ll_paf=5.53983, loss_ll_heat=0.688147, q=1000
[2018-07-13 16:58:19,965] [train] [INFO] epoch=6.00 step=51200, 10.0460 examples/sec lr=0.000033, loss=11.5683, loss_ll=2.06357, loss_ll_paf=3.6885, loss_ll_heat=0.438639, q=1000
[2018-07-13 17:00:47,369] [train] [INFO] epoch=6.00 step=51300, 10.0474 examples/sec lr=0.000033, loss=22.4925, loss_ll=4.29292, loss_ll_paf=7.8675, loss_ll_heat=0.718346, q=1000
[2018-07-13 17:03:18,205] [train] [INFO] epoch=6.00 step=51400, 10.0485 examples/sec lr=0.000033, loss=40.7446, loss_ll=7.65463, loss_ll_paf=13.8745, loss_ll_heat=1.43478, q=1000
[2018-07-13 17:05:50,183] [train] [INFO] epoch=6.00 step=51500, 10.0494 examples/sec lr=0.000033, loss=17.7179, loss_ll=3.06314, loss_ll_paf=5.63098, loss_ll_heat=0.495306, q=1000
[2018-07-13 17:08:16,873] [train] [INFO] epoch=6.00 step=51600, 10.0509 examples/sec lr=0.000033, loss=14.6747, loss_ll=2.31289, loss_ll_paf=4.1822, loss_ll_heat=0.443584, q=1000
[2018-07-13 17:10:51,278] [train] [INFO] epoch=6.00 step=51700, 10.0515 examples/sec lr=0.000033, loss=20.8634, loss_ll=3.02772, loss_ll_paf=5.41466, loss_ll_heat=0.640779, q=1000
[2018-07-13 17:13:21,104] [train] [INFO] epoch=6.00 step=51800, 10.0526 examples/sec lr=0.000033, loss=30.5433, loss_ll=5.46418, loss_ll_paf=9.92979, loss_ll_heat=0.998571, q=1000
[2018-07-13 17:15:52,202] [train] [INFO] epoch=6.00 step=51900, 10.0536 examples/sec lr=0.000033, loss=30.9297, loss_ll=6.07436, loss_ll_paf=11.0001, loss_ll_heat=1.14862, q=1000
[2018-07-13 17:18:21,004] [train] [INFO] epoch=6.00 step=52000, 10.0548 examples/sec lr=0.000033, loss=20.2764, loss_ll=3.51106, loss_ll_paf=6.53898, loss_ll_heat=0.483131, q=1000
[2018-07-13 17:21:03,493] [train] [INFO] epoch=6.00 step=52100, 10.0544 examples/sec lr=0.000033, loss=11.2441, loss_ll=1.89904, loss_ll_paf=3.23718, loss_ll_heat=0.560908, q=1000
[2018-07-13 17:24:07,828] [train] [INFO] epoch=6.00 step=52200, 10.0514 examples/sec lr=0.000033, loss=11.2532, loss_ll=1.5817, loss_ll_paf=2.77943, loss_ll_heat=0.383973, q=1000
[2018-07-13 17:26:55,828] [train] [INFO] epoch=6.00 step=52300, 10.0503 examples/sec lr=0.000033, loss=15.227, loss_ll=2.50532, loss_ll_paf=4.58138, loss_ll_heat=0.429265, q=1000
[2018-07-13 17:29:45,541] [train] [INFO] epoch=6.00 step=52400, 10.0491 examples/sec lr=0.000033, loss=23.2628, loss_ll=4.24733, loss_ll_paf=8.05937, loss_ll_heat=0.435296, q=1000
[2018-07-13 17:32:36,037] [train] [INFO] epoch=6.00 step=52500, 10.0477 examples/sec lr=0.000033, loss=30.5362, loss_ll=5.3387, loss_ll_paf=9.42359, loss_ll_heat=1.2538, q=1000
[2018-07-13 17:35:26,533] [train] [INFO] epoch=6.00 step=52600, 10.0464 examples/sec lr=0.000033, loss=21.1215, loss_ll=2.96928, loss_ll_paf=5.37126, loss_ll_heat=0.567298, q=1000
[2018-07-13 17:38:12,863] [train] [INFO] epoch=6.00 step=52700, 10.0455 examples/sec lr=0.000033, loss=11.6519, loss_ll=2.05842, loss_ll_paf=3.69697, loss_ll_heat=0.419881, q=1000
[2018-07-13 17:40:53,770] [train] [INFO] epoch=6.00 step=52800, 10.0453 examples/sec lr=0.000033, loss=45.0567, loss_ll=8.58206, loss_ll_paf=15.6748, loss_ll_heat=1.48934, q=1000
[2018-07-13 17:43:35,359] [train] [INFO] epoch=6.00 step=52900, 10.0450 examples/sec lr=0.000033, loss=26.4904, loss_ll=4.67407, loss_ll_paf=8.59242, loss_ll_heat=0.755726, q=1000
[2018-07-13 17:46:08,936] [train] [INFO] epoch=6.00 step=53000, 10.0457 examples/sec lr=0.000033, loss=27.5205, loss_ll=4.59091, loss_ll_paf=8.66491, loss_ll_heat=0.516899, q=1000
[2018-07-13 17:48:52,739] [train] [INFO] epoch=6.00 step=53100, 10.0452 examples/sec lr=0.000033, loss=11.5775, loss_ll=2.34733, loss_ll_paf=4.00065, loss_ll_heat=0.69401, q=1000
[2018-07-13 17:51:21,215] [train] [INFO] epoch=6.00 step=53200, 10.0465 examples/sec lr=0.000033, loss=17.1905, loss_ll=3.19178, loss_ll_paf=5.73859, loss_ll_heat=0.644966, q=1000
[2018-07-13 17:53:51,341] [train] [INFO] epoch=7.00 step=53300, 10.0475 examples/sec lr=0.000033, loss=29.1107, loss_ll=5.81459, loss_ll_paf=10.2755, loss_ll_heat=1.35373, q=1000
[2018-07-13 17:56:19,123] [train] [INFO] epoch=7.00 step=53400, 10.0489 examples/sec lr=0.000033, loss=22.9706, loss_ll=4.22782, loss_ll_paf=7.8075, loss_ll_heat=0.648135, q=1000
[2018-07-13 17:58:46,954] [train] [INFO] epoch=7.00 step=53500, 10.0502 examples/sec lr=0.000033, loss=28.0804, loss_ll=4.8024, loss_ll_paf=8.89897, loss_ll_heat=0.705826, q=1000
[2018-07-13 18:01:16,284] [train] [INFO] epoch=7.00 step=53600, 10.0514 examples/sec lr=0.000033, loss=20.6493, loss_ll=3.95581, loss_ll_paf=7.24974, loss_ll_heat=0.661882, q=1000
[2018-07-13 18:03:44,136] [train] [INFO] epoch=7.00 step=53700, 10.0527 examples/sec lr=0.000033, loss=14.011, loss_ll=2.25405, loss_ll_paf=3.8507, loss_ll_heat=0.657403, q=1000
[2018-07-13 18:06:15,719] [train] [INFO] epoch=7.00 step=53800, 10.0536 examples/sec lr=0.000033, loss=30.9652, loss_ll=5.7547, loss_ll_paf=9.99811, loss_ll_heat=1.5113, q=1000
[2018-07-13 18:08:45,090] [train] [INFO] epoch=7.00 step=53900, 10.0548 examples/sec lr=0.000033, loss=24.7816, loss_ll=4.44152, loss_ll_paf=8.01135, loss_ll_heat=0.871694, q=1000
[2018-07-13 18:11:15,128] [train] [INFO] epoch=7.00 step=54000, 10.0558 examples/sec lr=0.000033, loss=35.5597, loss_ll=6.57881, loss_ll_paf=12.3902, loss_ll_heat=0.767431, q=1000
[2018-07-13 18:13:55,817] [train] [INFO] epoch=7.00 step=54100, 10.0556 examples/sec lr=0.000033, loss=19.0405, loss_ll=3.53855, loss_ll_paf=6.57253, loss_ll_heat=0.504566, q=1000
[2018-07-13 18:16:23,687] [train] [INFO] epoch=7.00 step=54200, 10.0570 examples/sec lr=0.000033, loss=8.33264, loss_ll=1.32369, loss_ll_paf=2.29166, loss_ll_heat=0.355719, q=1000
[2018-07-13 18:19:14,915] [train] [INFO] epoch=7.00 step=54300, 10.0555 examples/sec lr=0.000033, loss=14.6905, loss_ll=2.6045, loss_ll_paf=4.62103, loss_ll_heat=0.587964, q=1000
[2018-07-13 18:21:44,324] [train] [INFO] epoch=7.00 step=54400, 10.0567 examples/sec lr=0.000033, loss=16.8214, loss_ll=2.8748, loss_ll_paf=5.3901, loss_ll_heat=0.359511, q=1000
[2018-07-13 18:24:45,709] [train] [INFO] epoch=7.00 step=54500, 10.0541 examples/sec lr=0.000033, loss=11.1226, loss_ll=1.96114, loss_ll_paf=3.58949, loss_ll_heat=0.332797, q=1000
[2018-07-13 18:27:44,924] [train] [INFO] epoch=7.00 step=54600, 10.0518 examples/sec lr=0.000033, loss=30.9016, loss_ll=5.48664, loss_ll_paf=10.6869, loss_ll_heat=0.286394, q=1000
[2018-07-13 18:30:31,964] [train] [INFO] epoch=7.00 step=54700, 10.0509 examples/sec lr=0.000033, loss=39.9756, loss_ll=7.70432, loss_ll_paf=14.436, loss_ll_heat=0.972622, q=1000
[2018-07-13 18:33:32,004] [train] [INFO] epoch=7.00 step=54800, 10.0485 examples/sec lr=0.000033, loss=17.9306, loss_ll=3.08765, loss_ll_paf=5.625, loss_ll_heat=0.550307, q=1000
[2018-07-13 18:36:32,739] [train] [INFO] epoch=7.00 step=54900, 10.0460 examples/sec lr=0.000033, loss=20.4441, loss_ll=3.57772, loss_ll_paf=6.42463, loss_ll_heat=0.730809, q=1000
[2018-07-13 18:39:24,300] [train] [INFO] epoch=7.00 step=55000, 10.0446 examples/sec lr=0.000033, loss=41.9011, loss_ll=8.30137, loss_ll_paf=15.6111, loss_ll_heat=0.991602, q=1000
[2018-07-13 18:42:27,653] [train] [INFO] epoch=7.00 step=55100, 10.0418 examples/sec lr=0.000033, loss=29.1867, loss_ll=5.16732, loss_ll_paf=9.58122, loss_ll_heat=0.753411, q=1000
[2018-07-13 18:45:15,944] [train] [INFO] epoch=7.00 step=55200, 10.0408 examples/sec lr=0.000033, loss=17.8994, loss_ll=3.44305, loss_ll_paf=6.35377, loss_ll_heat=0.532335, q=1000
[2018-07-13 18:48:03,305] [train] [INFO] epoch=7.00 step=55300, 10.0399 examples/sec lr=0.000033, loss=21.2588, loss_ll=3.12852, loss_ll_paf=5.74005, loss_ll_heat=0.516993, q=1000
[2018-07-13 18:50:40,308] [train] [INFO] epoch=7.00 step=55400, 10.0402 examples/sec lr=0.000033, loss=31.3349, loss_ll=5.93058, loss_ll_paf=11.102, loss_ll_heat=0.759199, q=1000
[2018-07-13 18:53:23,686] [train] [INFO] epoch=7.00 step=55500, 10.0397 examples/sec lr=0.000033, loss=17.7959, loss_ll=3.18986, loss_ll_paf=5.6501, loss_ll_heat=0.729618, q=1000
[2018-07-13 18:55:56,477] [train] [INFO] epoch=7.00 step=55600, 10.0404 examples/sec lr=0.000033, loss=21.1722, loss_ll=3.9386, loss_ll_paf=7.2054, loss_ll_heat=0.671801, q=1000
[2018-07-13 18:58:30,390] [train] [INFO] epoch=7.00 step=55700, 10.0411 examples/sec lr=0.000033, loss=23.4313, loss_ll=4.30989, loss_ll_paf=8.11494, loss_ll_heat=0.504832, q=1000
[2018-07-13 19:01:04,819] [train] [INFO] epoch=7.00 step=55800, 10.0416 examples/sec lr=0.000033, loss=15.0362, loss_ll=2.70831, loss_ll_paf=4.62912, loss_ll_heat=0.787504, q=1000
[2018-07-13 19:03:36,733] [train] [INFO] epoch=7.00 step=55900, 10.0425 examples/sec lr=0.000033, loss=30.0609, loss_ll=5.31172, loss_ll_paf=9.67287, loss_ll_heat=0.950566, q=1000
[2018-07-13 19:06:09,121] [train] [INFO] epoch=7.00 step=56000, 10.0432 examples/sec lr=0.000033, loss=17.7782, loss_ll=3.0189, loss_ll_paf=5.35914, loss_ll_heat=0.678653, q=1000
[2018-07-13 19:09:33,173] [train] [INFO] epoch=7.00 step=56100, 10.0382 examples/sec lr=0.000033, loss=35.549, loss_ll=6.88132, loss_ll_paf=12.7607, loss_ll_heat=1.00195, q=1000
[2018-07-13 19:12:18,227] [train] [INFO] epoch=7.00 step=56200, 10.0376 examples/sec lr=0.000033, loss=44.1501, loss_ll=8.33472, loss_ll_paf=15.5613, loss_ll_heat=1.10818, q=1000
[2018-07-13 19:14:58,826] [train] [INFO] epoch=7.00 step=56300, 10.0374 examples/sec lr=0.000033, loss=21.6167, loss_ll=4.048, loss_ll_paf=7.35041, loss_ll_heat=0.74558, q=1000
[2018-07-13 19:17:37,716] [train] [INFO] epoch=7.00 step=56400, 10.0375 examples/sec lr=0.000033, loss=15.1113, loss_ll=2.77287, loss_ll_paf=4.83813, loss_ll_heat=0.707617, q=1000
[2018-07-13 19:20:10,961] [train] [INFO] epoch=7.00 step=56500, 10.0382 examples/sec lr=0.000033, loss=17.7402, loss_ll=3.23667, loss_ll_paf=5.80511, loss_ll_heat=0.668225, q=1000
[2018-07-13 19:22:44,390] [train] [INFO] epoch=7.00 step=56600, 10.0388 examples/sec lr=0.000033, loss=18.1561, loss_ll=3.23372, loss_ll_paf=5.9903, loss_ll_heat=0.477132, q=1000
[2018-07-13 19:25:14,683] [train] [INFO] epoch=7.00 step=56700, 10.0399 examples/sec lr=0.000033, loss=34.7637, loss_ll=6.14597, loss_ll_paf=11.2264, loss_ll_heat=1.06555, q=1000
[2018-07-13 19:28:06,299] [train] [INFO] epoch=7.00 step=56800, 10.0385 examples/sec lr=0.000033, loss=23.8666, loss_ll=4.02303, loss_ll_paf=7.15032, loss_ll_heat=0.895744, q=1000
[2018-07-13 19:31:03,942] [train] [INFO] epoch=7.00 step=56900, 10.0365 examples/sec lr=0.000033, loss=28.3567, loss_ll=5.26289, loss_ll_paf=9.88979, loss_ll_heat=0.635977, q=1000
[2018-07-13 19:33:53,568] [train] [INFO] epoch=7.00 step=57000, 10.0354 examples/sec lr=0.000033, loss=30.2238, loss_ll=5.67199, loss_ll_paf=10.5389, loss_ll_heat=0.805052, q=1000
[2018-07-13 19:36:59,908] [train] [INFO] epoch=7.00 step=57100, 10.0324 examples/sec lr=0.000033, loss=16.913, loss_ll=3.10905, loss_ll_paf=5.748, loss_ll_heat=0.4701, q=1000
[2018-07-13 19:39:54,404] [train] [INFO] epoch=7.00 step=57200, 10.0307 examples/sec lr=0.000033, loss=40.5023, loss_ll=7.71801, loss_ll_paf=14.7887, loss_ll_heat=0.647322, q=1000
[2018-07-13 19:42:45,062] [train] [INFO] epoch=7.00 step=57300, 10.0295 examples/sec lr=0.000033, loss=29.4865, loss_ll=5.66144, loss_ll_paf=10.5286, loss_ll_heat=0.794291, q=1000
[2018-07-13 19:45:23,230] [train] [INFO] epoch=7.00 step=57400, 10.0297 examples/sec lr=0.000033, loss=12.4163, loss_ll=2.2789, loss_ll_paf=3.97459, loss_ll_heat=0.583204, q=1000
[2018-07-13 19:48:02,936] [train] [INFO] epoch=7.00 step=57500, 10.0296 examples/sec lr=0.000033, loss=36.2002, loss_ll=6.90404, loss_ll_paf=12.7104, loss_ll_heat=1.09766, q=1000
[2018-07-13 19:50:37,781] [train] [INFO] epoch=7.00 step=57600, 10.0302 examples/sec lr=0.000033, loss=26.0401, loss_ll=4.70003, loss_ll_paf=8.74041, loss_ll_heat=0.659648, q=1000
[2018-07-13 19:53:08,620] [train] [INFO] epoch=7.00 step=57700, 10.0311 examples/sec lr=0.000033, loss=27.3735, loss_ll=4.69253, loss_ll_paf=8.51056, loss_ll_heat=0.874506, q=1000
[2018-07-13 19:55:44,005] [train] [INFO] epoch=7.00 step=57800, 10.0315 examples/sec lr=0.000033, loss=21.487, loss_ll=3.89543, loss_ll_paf=7.0852, loss_ll_heat=0.705665, q=1000
[2018-07-13 19:58:16,120] [train] [INFO] epoch=7.00 step=57900, 10.0323 examples/sec lr=0.000033, loss=13.7206, loss_ll=2.30618, loss_ll_paf=4.0555, loss_ll_heat=0.55686, q=1000
[2018-07-13 20:00:51,251] [train] [INFO] epoch=7.00 step=58000, 10.0328 examples/sec lr=0.000033, loss=23.902, loss_ll=4.44, loss_ll_paf=7.95899, loss_ll_heat=0.921004, q=1000
[2018-07-13 20:03:39,040] [train] [INFO] epoch=7.00 step=58100, 10.0319 examples/sec lr=0.000033, loss=13.7668, loss_ll=2.51802, loss_ll_paf=4.48308, loss_ll_heat=0.552963, q=1000
[2018-07-13 20:06:15,565] [train] [INFO] epoch=7.00 step=58200, 10.0322 examples/sec lr=0.000033, loss=15.5769, loss_ll=2.37654, loss_ll_paf=4.23642, loss_ll_heat=0.516663, q=1000
[2018-07-13 20:08:52,198] [train] [INFO] epoch=7.00 step=58300, 10.0325 examples/sec lr=0.000033, loss=27.3976, loss_ll=4.93091, loss_ll_paf=9.42906, loss_ll_heat=0.432763, q=1000
[2018-07-13 20:11:26,986] [train] [INFO] epoch=7.00 step=58400, 10.0331 examples/sec lr=0.000033, loss=17.2289, loss_ll=3.06428, loss_ll_paf=5.33452, loss_ll_heat=0.794035, q=1000
[2018-07-13 20:13:59,406] [train] [INFO] epoch=7.00 step=58500, 10.0338 examples/sec lr=0.000033, loss=20.3774, loss_ll=3.69279, loss_ll_paf=6.39914, loss_ll_heat=0.986437, q=1000
[2018-07-13 20:16:36,056] [train] [INFO] epoch=7.00 step=58600, 10.0341 examples/sec lr=0.000033, loss=35.4828, loss_ll=6.47176, loss_ll_paf=12.0577, loss_ll_heat=0.885808, q=1000
[2018-07-13 20:19:12,301] [train] [INFO] epoch=7.00 step=58700, 10.0345 examples/sec lr=0.000033, loss=21.566, loss_ll=3.84146, loss_ll_paf=6.58761, loss_ll_heat=1.09531, q=1000
[2018-07-13 20:21:47,269] [train] [INFO] epoch=7.00 step=58800, 10.0349 examples/sec lr=0.000033, loss=13.5889, loss_ll=2.36447, loss_ll_paf=4.13326, loss_ll_heat=0.595681, q=1000
[2018-07-13 20:24:19,869] [train] [INFO] epoch=7.00 step=58900, 10.0357 examples/sec lr=0.000033, loss=17.2468, loss_ll=2.95446, loss_ll_paf=5.31385, loss_ll_heat=0.595075, q=1000
[2018-07-13 20:26:53,743] [train] [INFO] epoch=7.00 step=59000, 10.0363 examples/sec lr=0.000033, loss=57.7295, loss_ll=11.1991, loss_ll_paf=21.4139, loss_ll_heat=0.984285, q=1000
[2018-07-13 20:29:40,228] [train] [INFO] epoch=7.00 step=59100, 10.0355 examples/sec lr=0.000033, loss=18.876, loss_ll=3.1522, loss_ll_paf=5.6481, loss_ll_heat=0.656311, q=1000
[2018-07-13 20:32:13,865] [train] [INFO] epoch=7.00 step=59200, 10.0361 examples/sec lr=0.000033, loss=20.467, loss_ll=3.72313, loss_ll_paf=6.69649, loss_ll_heat=0.749781, q=1000
[2018-07-13 20:34:47,479] [train] [INFO] epoch=7.00 step=59300, 10.0367 examples/sec lr=0.000033, loss=19.3835, loss_ll=3.69073, loss_ll_paf=6.44135, loss_ll_heat=0.940112, q=1000
[2018-07-13 20:37:27,386] [train] [INFO] epoch=7.00 step=59400, 10.0367 examples/sec lr=0.000033, loss=12.8543, loss_ll=2.17006, loss_ll_paf=3.504, loss_ll_heat=0.836129, q=1000
[2018-07-13 20:40:03,087] [train] [INFO] epoch=7.00 step=59500, 10.0371 examples/sec lr=0.000033, loss=12.031, loss_ll=2.06261, loss_ll_paf=3.63457, loss_ll_heat=0.490654, q=1000
[2018-07-13 20:42:38,086] [train] [INFO] epoch=7.00 step=59600, 10.0376 examples/sec lr=0.000033, loss=17.2487, loss_ll=2.78845, loss_ll_paf=4.94497, loss_ll_heat=0.631941, q=1000
[2018-07-13 20:45:10,590] [train] [INFO] epoch=7.00 step=59700, 10.0383 examples/sec lr=0.000033, loss=20.1887, loss_ll=4.05607, loss_ll_paf=7.45754, loss_ll_heat=0.65459, q=1000
[2018-07-13 20:47:45,544] [train] [INFO] epoch=7.00 step=59800, 10.0387 examples/sec lr=0.000033, loss=27.2721, loss_ll=4.78832, loss_ll_paf=8.91838, loss_ll_heat=0.658253, q=1000
[2018-07-13 20:50:20,928] [train] [INFO] epoch=7.00 step=59900, 10.0392 examples/sec lr=0.000033, loss=10.0973, loss_ll=1.83012, loss_ll_paf=3.27109, loss_ll_heat=0.389144, q=1000
[2018-07-13 20:52:57,324] [train] [INFO] epoch=7.00 step=60000, 10.0395 examples/sec lr=0.000011, loss=18.7672, loss_ll=3.35479, loss_ll_paf=5.91295, loss_ll_heat=0.796621, q=1000
[2018-07-13 20:55:46,760] [train] [INFO] epoch=7.00 step=60100, 10.0384 examples/sec lr=0.000011, loss=18.8293, loss_ll=3.49099, loss_ll_paf=6.36941, loss_ll_heat=0.612573, q=1000
[2018-07-13 20:58:22,729] [train] [INFO] epoch=7.00 step=60200, 10.0388 examples/sec lr=0.000011, loss=15.5836, loss_ll=3.03236, loss_ll_paf=5.37106, loss_ll_heat=0.693664, q=1000
[2018-07-13 21:00:58,566] [train] [INFO] epoch=7.00 step=60300, 10.0392 examples/sec lr=0.000011, loss=19.2139, loss_ll=3.51778, loss_ll_paf=6.62206, loss_ll_heat=0.413496, q=1000
[2018-07-13 21:03:34,693] [train] [INFO] epoch=7.00 step=60400, 10.0395 examples/sec lr=0.000011, loss=28.3263, loss_ll=5.32163, loss_ll_paf=10.2645, loss_ll_heat=0.378732, q=1000
[2018-07-13 21:06:12,222] [train] [INFO] epoch=7.00 step=60500, 10.0397 examples/sec lr=0.000011, loss=23.6541, loss_ll=4.30532, loss_ll_paf=7.89831, loss_ll_heat=0.712331, q=1000
[2018-07-13 21:08:46,438] [train] [INFO] epoch=7.00 step=60600, 10.0402 examples/sec lr=0.000011, loss=19.119, loss_ll=3.0576, loss_ll_paf=5.62956, loss_ll_heat=0.485642, q=1000
[2018-07-13 21:11:21,686] [train] [INFO] epoch=7.00 step=60700, 10.0406 examples/sec lr=0.000011, loss=16.203, loss_ll=2.60244, loss_ll_paf=4.81168, loss_ll_heat=0.393207, q=1000
[2018-07-13 21:13:58,136] [train] [INFO] epoch=7.00 step=60800, 10.0409 examples/sec lr=0.000011, loss=22.9072, loss_ll=3.89967, loss_ll_paf=7.05105, loss_ll_heat=0.748281, q=1000
[2018-07-13 21:16:32,264] [train] [INFO] epoch=8.00 step=60900, 10.0415 examples/sec lr=0.000011, loss=21.9011, loss_ll=4.11896, loss_ll_paf=7.19878, loss_ll_heat=1.03913, q=1000
[2018-07-13 21:19:06,992] [train] [INFO] epoch=8.00 step=61000, 10.0420 examples/sec lr=0.000011, loss=14.5015, loss_ll=2.89824, loss_ll_paf=4.97899, loss_ll_heat=0.817493, q=1000
[2018-07-13 21:21:57,239] [train] [INFO] epoch=8.00 step=61100, 10.0408 examples/sec lr=0.000011, loss=20.2337, loss_ll=3.65651, loss_ll_paf=6.1808, loss_ll_heat=1.13222, q=1000
[2018-07-13 21:24:31,494] [train] [INFO] epoch=8.00 step=61200, 10.0414 examples/sec lr=0.000011, loss=15.4658, loss_ll=2.48061, loss_ll_paf=4.43777, loss_ll_heat=0.523457, q=1000
[2018-07-13 21:27:05,662] [train] [INFO] epoch=8.00 step=61300, 10.0419 examples/sec lr=0.000011, loss=22.8039, loss_ll=4.02982, loss_ll_paf=7.2748, loss_ll_heat=0.784837, q=1000
[2018-07-13 21:29:40,892] [train] [INFO] epoch=8.00 step=61400, 10.0423 examples/sec lr=0.000011, loss=23.8407, loss_ll=4.54104, loss_ll_paf=7.86222, loss_ll_heat=1.21985, q=1000
[2018-07-13 21:32:14,545] [train] [INFO] epoch=8.00 step=61500, 10.0429 examples/sec lr=0.000011, loss=25.072, loss_ll=4.40465, loss_ll_paf=8.10224, loss_ll_heat=0.707058, q=1000
[2018-07-13 21:34:50,011] [train] [INFO] epoch=8.00 step=61600, 10.0433 examples/sec lr=0.000011, loss=19.1959, loss_ll=3.52534, loss_ll_paf=6.05077, loss_ll_heat=0.999915, q=1000
[2018-07-13 21:37:24,163] [train] [INFO] epoch=8.00 step=61700, 10.0438 examples/sec lr=0.000011, loss=12.8211, loss_ll=2.29832, loss_ll_paf=3.97378, loss_ll_heat=0.622855, q=1000
[2018-07-13 21:39:59,519] [train] [INFO] epoch=8.00 step=61800, 10.0442 examples/sec lr=0.000011, loss=26.0599, loss_ll=4.73692, loss_ll_paf=9.0761, loss_ll_heat=0.397744, q=1000
[2018-07-13 21:42:35,471] [train] [INFO] epoch=8.00 step=61900, 10.0446 examples/sec lr=0.000011, loss=33.5709, loss_ll=6.18678, loss_ll_paf=11.6683, loss_ll_heat=0.705227, q=1000
[2018-07-13 21:45:10,506] [train] [INFO] epoch=8.00 step=62000, 10.0450 examples/sec lr=0.000011, loss=28.3296, loss_ll=5.04797, loss_ll_paf=9.02583, loss_ll_heat=1.07011, q=1000
[2018-07-13 21:47:58,385] [train] [INFO] epoch=8.00 step=62100, 10.0441 examples/sec lr=0.000011, loss=22.3743, loss_ll=3.78662, loss_ll_paf=6.78405, loss_ll_heat=0.789182, q=1000
[2018-07-13 21:50:33,116] [train] [INFO] epoch=8.00 step=62200, 10.0446 examples/sec lr=0.000011, loss=16.9939, loss_ll=2.98569, loss_ll_paf=5.21681, loss_ll_heat=0.754569, q=1000
[2018-07-13 21:53:10,669] [train] [INFO] epoch=8.00 step=62300, 10.0448 examples/sec lr=0.000011, loss=40.1301, loss_ll=7.92977, loss_ll_paf=14.9631, loss_ll_heat=0.896469, q=1000
[2018-07-13 21:55:44,284] [train] [INFO] epoch=8.00 step=62400, 10.0453 examples/sec lr=0.000011, loss=16.0907, loss_ll=2.92804, loss_ll_paf=4.89183, loss_ll_heat=0.964257, q=1000
[2018-07-13 21:58:20,118] [train] [INFO] epoch=8.00 step=62500, 10.0457 examples/sec lr=0.000011, loss=23.5649, loss_ll=3.64188, loss_ll_paf=6.59815, loss_ll_heat=0.685599, q=1000
[2018-07-13 22:00:56,844] [train] [INFO] epoch=8.00 step=62600, 10.0459 examples/sec lr=0.000011, loss=16.5628, loss_ll=2.94788, loss_ll_paf=5.39646, loss_ll_heat=0.499307, q=1000
[2018-07-13 22:03:30,172] [train] [INFO] epoch=8.00 step=62700, 10.0465 examples/sec lr=0.000011, loss=30.8609, loss_ll=5.83532, loss_ll_paf=10.478, loss_ll_heat=1.19263, q=1000
[2018-07-13 22:06:05,762] [train] [INFO] epoch=8.00 step=62800, 10.0469 examples/sec lr=0.000011, loss=32.6162, loss_ll=6.09999, loss_ll_paf=11.252, loss_ll_heat=0.947953, q=1000
[2018-07-13 22:08:39,960] [train] [INFO] epoch=8.00 step=62900, 10.0474 examples/sec lr=0.000011, loss=22.3592, loss_ll=4.01346, loss_ll_paf=7.40442, loss_ll_heat=0.622507, q=1000
[2018-07-13 22:11:16,080] [train] [INFO] epoch=8.00 step=63000, 10.0477 examples/sec lr=0.000011, loss=33.6759, loss_ll=6.53143, loss_ll_paf=12.2985, loss_ll_heat=0.764384, q=1000
[2018-07-13 22:14:04,369] [train] [INFO] epoch=8.00 step=63100, 10.0468 examples/sec lr=0.000011, loss=14.3976, loss_ll=2.44987, loss_ll_paf=4.12675, loss_ll_heat=0.772995, q=1000
[2018-07-13 22:16:40,419] [train] [INFO] epoch=8.00 step=63200, 10.0471 examples/sec lr=0.000011, loss=17.7121, loss_ll=3.13866, loss_ll_paf=5.91414, loss_ll_heat=0.363188, q=1000
[2018-07-13 22:19:13,716] [train] [INFO] epoch=8.00 step=63300, 10.0477 examples/sec lr=0.000011, loss=32.4535, loss_ll=5.65144, loss_ll_paf=10.0871, loss_ll_heat=1.21576, q=1000
[2018-07-13 22:21:48,355] [train] [INFO] epoch=8.00 step=63400, 10.0482 examples/sec lr=0.000011, loss=30.3979, loss_ll=4.8922, loss_ll_paf=9.06534, loss_ll_heat=0.71906, q=1000
[2018-07-13 22:24:23,852] [train] [INFO] epoch=8.00 step=63500, 10.0486 examples/sec lr=0.000011, loss=14.5843, loss_ll=2.77711, loss_ll_paf=5.06468, loss_ll_heat=0.489544, q=1000
[2018-07-13 22:26:58,000] [train] [INFO] epoch=8.00 step=63600, 10.0491 examples/sec lr=0.000011, loss=26.6228, loss_ll=4.68521, loss_ll_paf=8.58631, loss_ll_heat=0.784114, q=1000
[2018-07-13 22:29:31,325] [train] [INFO] epoch=8.00 step=63700, 10.0496 examples/sec lr=0.000011, loss=16.9647, loss_ll=2.81503, loss_ll_paf=5.3049, loss_ll_heat=0.32516, q=1000
[2018-07-13 22:32:09,358] [train] [INFO] epoch=8.00 step=63800, 10.0498 examples/sec lr=0.000011, loss=18.4576, loss_ll=3.05942, loss_ll_paf=5.63148, loss_ll_heat=0.487355, q=1000
[2018-07-13 22:34:42,559] [train] [INFO] epoch=8.00 step=63900, 10.0504 examples/sec lr=0.000011, loss=23.456, loss_ll=3.94846, loss_ll_paf=7.40807, loss_ll_heat=0.488859, q=1000
[2018-07-13 22:37:17,223] [train] [INFO] epoch=8.00 step=64000, 10.0508 examples/sec lr=0.000011, loss=22.5722, loss_ll=4.27559, loss_ll_paf=7.36895, loss_ll_heat=1.18224, q=1000
[2018-07-13 22:40:04,262] [train] [INFO] epoch=8.00 step=64100, 10.0500 examples/sec lr=0.000011, loss=34.9936, loss_ll=6.43917, loss_ll_paf=11.8916, loss_ll_heat=0.986782, q=1000
[2018-07-13 22:42:42,023] [train] [INFO] epoch=8.00 step=64200, 10.0502 examples/sec lr=0.000011, loss=22.2509, loss_ll=3.64597, loss_ll_paf=6.51877, loss_ll_heat=0.773171, q=1000
[2018-07-13 22:45:13,098] [train] [INFO] epoch=8.00 step=64300, 10.0510 examples/sec lr=0.000011, loss=15.9637, loss_ll=2.98245, loss_ll_paf=5.4499, loss_ll_heat=0.515001, q=1000
[2018-07-13 22:47:49,058] [train] [INFO] epoch=8.00 step=64400, 10.0513 examples/sec lr=0.000011, loss=16.4537, loss_ll=2.93185, loss_ll_paf=5.21463, loss_ll_heat=0.649064, q=1000
[2018-07-13 22:50:23,630] [train] [INFO] epoch=8.00 step=64500, 10.0517 examples/sec lr=0.000011, loss=21.8765, loss_ll=4.18364, loss_ll_paf=7.56354, loss_ll_heat=0.803742, q=1000
[2018-07-13 22:52:57,981] [train] [INFO] epoch=8.00 step=64600, 10.0522 examples/sec lr=0.000011, loss=25.2382, loss_ll=4.47351, loss_ll_paf=8.10693, loss_ll_heat=0.84008, q=1000
[2018-07-13 22:55:30,660] [train] [INFO] epoch=8.00 step=64700, 10.0528 examples/sec lr=0.000011, loss=37.5844, loss_ll=7.0976, loss_ll_paf=13.1992, loss_ll_heat=0.995986, q=1000
[2018-07-13 22:58:01,702] [train] [INFO] epoch=8.00 step=64800, 10.0536 examples/sec lr=0.000011, loss=24.881, loss_ll=4.92657, loss_ll_paf=8.96716, loss_ll_heat=0.885981, q=1000
[2018-07-13 23:00:34,614] [train] [INFO] epoch=8.00 step=64900, 10.0542 examples/sec lr=0.000011, loss=22.1975, loss_ll=3.5091, loss_ll_paf=6.572, loss_ll_heat=0.446207, q=1000
[2018-07-13 23:03:06,919] [train] [INFO] epoch=8.00 step=65000, 10.0549 examples/sec lr=0.000011, loss=19.6207, loss_ll=3.54198, loss_ll_paf=5.9453, loss_ll_heat=1.13865, q=1000
[2018-07-13 23:06:00,405] [train] [INFO] epoch=8.00 step=65100, 10.0535 examples/sec lr=0.000011, loss=27.2597, loss_ll=5.10389, loss_ll_paf=9.1562, loss_ll_heat=1.05158, q=1000
[2018-07-13 23:08:31,970] [train] [INFO] epoch=8.00 step=65200, 10.0542 examples/sec lr=0.000011, loss=12.1214, loss_ll=2.31069, loss_ll_paf=3.78964, loss_ll_heat=0.831742, q=1000
[2018-07-13 23:11:03,246] [train] [INFO] epoch=8.00 step=65300, 10.0550 examples/sec lr=0.000011, loss=18.853, loss_ll=3.23575, loss_ll_paf=5.87479, loss_ll_heat=0.596707, q=1000
[2018-07-13 23:13:35,922] [train] [INFO] epoch=8.00 step=65400, 10.0556 examples/sec lr=0.000011, loss=20.7492, loss_ll=3.34866, loss_ll_paf=5.60898, loss_ll_heat=1.08835, q=1000
[2018-07-13 23:16:09,166] [train] [INFO] epoch=8.00 step=65500, 10.0562 examples/sec lr=0.000011, loss=18.3894, loss_ll=3.33127, loss_ll_paf=5.94963, loss_ll_heat=0.712909, q=1000
[2018-07-13 23:18:40,665] [train] [INFO] epoch=8.00 step=65600, 10.0569 examples/sec lr=0.000011, loss=10.7365, loss_ll=1.64652, loss_ll_paf=2.96556, loss_ll_heat=0.327483, q=1000
[2018-07-13 23:21:11,609] [train] [INFO] epoch=8.00 step=65700, 10.0577 examples/sec lr=0.000011, loss=12.6363, loss_ll=2.26442, loss_ll_paf=3.99761, loss_ll_heat=0.531235, q=1000
[2018-07-13 23:23:42,273] [train] [INFO] epoch=8.00 step=65800, 10.0585 examples/sec lr=0.000011, loss=11.1773, loss_ll=1.83967, loss_ll_paf=3.16493, loss_ll_heat=0.514411, q=1000
[2018-07-13 23:26:14,764] [train] [INFO] epoch=8.00 step=65900, 10.0592 examples/sec lr=0.000011, loss=17.6772, loss_ll=3.16552, loss_ll_paf=5.77467, loss_ll_heat=0.556383, q=1000
[2018-07-13 23:28:50,809] [train] [INFO] epoch=8.00 step=66000, 10.0594 examples/sec lr=0.000011, loss=16.7518, loss_ll=2.7778, loss_ll_paf=4.9057, loss_ll_heat=0.649906, q=1000
[2018-07-13 23:31:33,872] [train] [INFO] epoch=8.00 step=66100, 10.0591 examples/sec lr=0.000011, loss=15.8893, loss_ll=2.7799, loss_ll_paf=4.90883, loss_ll_heat=0.650969, q=1000
[2018-07-13 23:34:04,557] [train] [INFO] epoch=8.00 step=66200, 10.0599 examples/sec lr=0.000011, loss=22.8996, loss_ll=4.31496, loss_ll_paf=7.7716, loss_ll_heat=0.858314, q=1000
[2018-07-13 23:36:39,851] [train] [INFO] epoch=8.00 step=66300, 10.0602 examples/sec lr=0.000011, loss=17.8816, loss_ll=2.78434, loss_ll_paf=5.09303, loss_ll_heat=0.47565, q=1000
[2018-07-13 23:39:10,454] [train] [INFO] epoch=8.00 step=66400, 10.0610 examples/sec lr=0.000011, loss=33.4336, loss_ll=6.25825, loss_ll_paf=11.6727, loss_ll_heat=0.843752, q=1000
[2018-07-13 23:41:38,098] [train] [INFO] epoch=8.00 step=66500, 10.0621 examples/sec lr=0.000011, loss=31.0915, loss_ll=5.21863, loss_ll_paf=9.42994, loss_ll_heat=1.00733, q=1000
[2018-07-13 23:44:10,441] [train] [INFO] epoch=8.00 step=66600, 10.0627 examples/sec lr=0.000011, loss=37.1836, loss_ll=6.98422, loss_ll_paf=12.9076, loss_ll_heat=1.06079, q=1000
[2018-07-13 23:46:45,658] [train] [INFO] epoch=8.00 step=66700, 10.0631 examples/sec lr=0.000011, loss=10.358, loss_ll=1.93829, loss_ll_paf=3.25496, loss_ll_heat=0.621616, q=1000
[2018-07-13 23:49:19,654] [train] [INFO] epoch=8.00 step=66800, 10.0636 examples/sec lr=0.000011, loss=18.694, loss_ll=3.59082, loss_ll_paf=6.54439, loss_ll_heat=0.637248, q=1000
[2018-07-13 23:51:53,609] [train] [INFO] epoch=8.00 step=66900, 10.0640 examples/sec lr=0.000011, loss=34.0439, loss_ll=6.45883, loss_ll_paf=12.1604, loss_ll_heat=0.757268, q=1000
[2018-07-13 23:54:27,699] [train] [INFO] epoch=8.00 step=67000, 10.0645 examples/sec lr=0.000011, loss=12.8184, loss_ll=2.4692, loss_ll_paf=4.31442, loss_ll_heat=0.62399, q=1000
[2018-07-13 23:57:17,270] [train] [INFO] epoch=8.00 step=67100, 10.0635 examples/sec lr=0.000011, loss=27.2497, loss_ll=4.93775, loss_ll_paf=9.35745, loss_ll_heat=0.518047, q=1000
[2018-07-13 23:59:49,049] [train] [INFO] epoch=8.00 step=67200, 10.0642 examples/sec lr=0.000011, loss=30.2399, loss_ll=6.07139, loss_ll_paf=11.0702, loss_ll_heat=1.07258, q=1000
[2018-07-14 00:02:24,287] [train] [INFO] epoch=8.00 step=67300, 10.0645 examples/sec lr=0.000011, loss=16.2919, loss_ll=3.00125, loss_ll_paf=5.58252, loss_ll_heat=0.419979, q=1000
[2018-07-14 00:04:58,846] [train] [INFO] epoch=8.00 step=67400, 10.0650 examples/sec lr=0.000011, loss=21.6283, loss_ll=3.56758, loss_ll_paf=6.3397, loss_ll_heat=0.795464, q=1000
[2018-07-14 00:07:28,678] [train] [INFO] epoch=8.00 step=67500, 10.0658 examples/sec lr=0.000011, loss=12.4356, loss_ll=1.93732, loss_ll_paf=3.42699, loss_ll_heat=0.447645, q=1000
[2018-07-14 00:10:04,441] [train] [INFO] epoch=8.00 step=67600, 10.0661 examples/sec lr=0.000011, loss=20.152, loss_ll=3.38841, loss_ll_paf=6.23676, loss_ll_heat=0.540066, q=1000
[2018-07-14 00:12:39,781] [train] [INFO] epoch=8.00 step=67700, 10.0665 examples/sec lr=0.000011, loss=23.1827, loss_ll=4.1294, loss_ll_paf=7.45797, loss_ll_heat=0.800833, q=1000
[2018-07-14 00:15:11,664] [train] [INFO] epoch=8.00 step=67800, 10.0671 examples/sec lr=0.000011, loss=13.9758, loss_ll=2.41545, loss_ll_paf=4.49746, loss_ll_heat=0.333437, q=1000
[2018-07-14 00:17:46,310] [train] [INFO] epoch=8.00 step=67900, 10.0675 examples/sec lr=0.000011, loss=16.5356, loss_ll=2.91313, loss_ll_paf=5.14989, loss_ll_heat=0.676369, q=1000
[2018-07-14 00:20:17,022] [train] [INFO] epoch=8.00 step=68000, 10.0683 examples/sec lr=0.000011, loss=14.3432, loss_ll=2.54264, loss_ll_paf=4.43302, loss_ll_heat=0.652262, q=1000
[2018-07-14 00:23:05,027] [train] [INFO] epoch=8.00 step=68100, 10.0674 examples/sec lr=0.000011, loss=15.3682, loss_ll=2.44213, loss_ll_paf=4.0387, loss_ll_heat=0.845551, q=1000
[2018-07-14 00:25:38,179] [train] [INFO] epoch=8.00 step=68200, 10.0680 examples/sec lr=0.000011, loss=16.7455, loss_ll=2.55193, loss_ll_paf=4.56861, loss_ll_heat=0.535247, q=1000
[2018-07-14 00:28:11,735] [train] [INFO] epoch=8.00 step=68300, 10.0685 examples/sec lr=0.000011, loss=18.037, loss_ll=2.96435, loss_ll_paf=5.32142, loss_ll_heat=0.607271, q=1000
[2018-07-14 00:30:43,635] [train] [INFO] epoch=8.00 step=68400, 10.0691 examples/sec lr=0.000011, loss=15.2976, loss_ll=2.53654, loss_ll_paf=4.5412, loss_ll_heat=0.531885, q=1000
[2018-07-14 00:33:16,044] [train] [INFO] epoch=9.00 step=68500, 10.0697 examples/sec lr=0.000011, loss=11.3351, loss_ll=2.02433, loss_ll_paf=3.59613, loss_ll_heat=0.452528, q=1000
[2018-07-14 00:35:52,929] [train] [INFO] epoch=9.00 step=68600, 10.0699 examples/sec lr=0.000011, loss=12.7721, loss_ll=2.24898, loss_ll_paf=4.19128, loss_ll_heat=0.306677, q=1000
[2018-07-14 00:38:24,926] [train] [INFO] epoch=9.00 step=68700, 10.0705 examples/sec lr=0.000011, loss=14.7955, loss_ll=2.6765, loss_ll_paf=4.47163, loss_ll_heat=0.881368, q=1000
[2018-07-14 00:40:59,990] [train] [INFO] epoch=9.00 step=68800, 10.0709 examples/sec lr=0.000011, loss=24.3494, loss_ll=4.39006, loss_ll_paf=8.13824, loss_ll_heat=0.641876, q=1000
[2018-07-14 00:43:30,151] [train] [INFO] epoch=9.00 step=68900, 10.0717 examples/sec lr=0.000011, loss=19.5995, loss_ll=3.69809, loss_ll_paf=6.44825, loss_ll_heat=0.947933, q=1000
[2018-07-14 00:46:01,589] [train] [INFO] epoch=9.00 step=69000, 10.0724 examples/sec lr=0.000011, loss=22.4508, loss_ll=4.52946, loss_ll_paf=8.14253, loss_ll_heat=0.916393, q=1000
[2018-07-14 00:48:47,689] [train] [INFO] epoch=9.00 step=69100, 10.0717 examples/sec lr=0.000011, loss=19.7331, loss_ll=3.22561, loss_ll_paf=5.89846, loss_ll_heat=0.552751, q=1000
[2018-07-14 00:51:17,736] [train] [INFO] epoch=9.00 step=69200, 10.0725 examples/sec lr=0.000011, loss=16.1588, loss_ll=2.32921, loss_ll_paf=3.82456, loss_ll_heat=0.833864, q=1000
[2018-07-14 00:53:54,224] [train] [INFO] epoch=9.00 step=69300, 10.0727 examples/sec lr=0.000011, loss=11.7414, loss_ll=2.10848, loss_ll_paf=3.66911, loss_ll_heat=0.547847, q=1000
[2018-07-14 00:56:28,776] [train] [INFO] epoch=9.00 step=69400, 10.0731 examples/sec lr=0.000011, loss=13.3462, loss_ll=1.88034, loss_ll_paf=3.21349, loss_ll_heat=0.547188, q=1000
[2018-07-14 00:58:58,844] [train] [INFO] epoch=9.00 step=69500, 10.0739 examples/sec lr=0.000011, loss=43.6257, loss_ll=8.07703, loss_ll_paf=15.1995, loss_ll_heat=0.954602, q=1000
[2018-07-14 01:01:37,111] [train] [INFO] epoch=9.00 step=69600, 10.0740 examples/sec lr=0.000011, loss=15.5818, loss_ll=2.338, loss_ll_paf=3.98584, loss_ll_heat=0.690158, q=1000
[2018-07-14 01:04:05,997] [train] [INFO] epoch=9.00 step=69700, 10.0749 examples/sec lr=0.000011, loss=17.0853, loss_ll=3.20626, loss_ll_paf=5.55744, loss_ll_heat=0.855074, q=1000
[2018-07-14 01:06:40,937] [train] [INFO] epoch=9.00 step=69800, 10.0752 examples/sec lr=0.000011, loss=12.2877, loss_ll=2.05194, loss_ll_paf=3.69378, loss_ll_heat=0.410105, q=1000
[2018-07-14 01:09:13,120] [train] [INFO] epoch=9.00 step=69900, 10.0758 examples/sec lr=0.000011, loss=29.9294, loss_ll=5.44978, loss_ll_paf=10.3765, loss_ll_heat=0.523088, q=1000
[2018-07-14 01:11:48,410] [train] [INFO] epoch=9.00 step=70000, 10.0761 examples/sec lr=0.000011, loss=27.1727, loss_ll=5.37155, loss_ll_paf=10.0035, loss_ll_heat=0.739627, q=1000
[2018-07-14 01:14:37,385] [train] [INFO] epoch=9.00 step=70100, 10.0752 examples/sec lr=0.000011, loss=29.0907, loss_ll=5.03757, loss_ll_paf=9.36086, loss_ll_heat=0.71427, q=1000
[2018-07-14 01:17:10,386] [train] [INFO] epoch=9.00 step=70200, 10.0758 examples/sec lr=0.000011, loss=34.5844, loss_ll=6.11288, loss_ll_paf=11.4877, loss_ll_heat=0.738042, q=1000
[2018-07-14 01:19:46,022] [train] [INFO] epoch=9.00 step=70300, 10.0760 examples/sec lr=0.000011, loss=17.8527, loss_ll=3.42933, loss_ll_paf=6.44149, loss_ll_heat=0.417173, q=1000
[2018-07-14 01:22:21,350] [train] [INFO] epoch=9.00 step=70400, 10.0763 examples/sec lr=0.000011, loss=16.1441, loss_ll=2.52259, loss_ll_paf=4.6185, loss_ll_heat=0.42667, q=1000
[2018-07-14 01:24:55,104] [train] [INFO] epoch=9.00 step=70500, 10.0768 examples/sec lr=0.000011, loss=19.8717, loss_ll=3.44218, loss_ll_paf=6.31248, loss_ll_heat=0.571888, q=1000
[2018-07-14 01:27:27,015] [train] [INFO] epoch=9.00 step=70600, 10.0774 examples/sec lr=0.000011, loss=18.571, loss_ll=3.07532, loss_ll_paf=5.78321, loss_ll_heat=0.367435, q=1000
[2018-07-14 01:30:01,975] [train] [INFO] epoch=9.00 step=70700, 10.0778 examples/sec lr=0.000011, loss=23.3015, loss_ll=4.41817, loss_ll_paf=8.24584, loss_ll_heat=0.590503, q=1000
[2018-07-14 01:32:34,918] [train] [INFO] epoch=9.00 step=70800, 10.0783 examples/sec lr=0.000011, loss=17.7654, loss_ll=2.79121, loss_ll_paf=4.95427, loss_ll_heat=0.62815, q=1000
[2018-07-14 01:35:09,275] [train] [INFO] epoch=9.00 step=70900, 10.0787 examples/sec lr=0.000011, loss=16.9269, loss_ll=2.80813, loss_ll_paf=5.14261, loss_ll_heat=0.473654, q=1000
[2018-07-14 01:37:43,008] [train] [INFO] epoch=9.00 step=71000, 10.0791 examples/sec lr=0.000011, loss=29.1525, loss_ll=5.88538, loss_ll_paf=10.9895, loss_ll_heat=0.7813, q=1000
[2018-07-14 01:40:32,136] [train] [INFO] epoch=9.00 step=71100, 10.0782 examples/sec lr=0.000011, loss=15.6971, loss_ll=2.82663, loss_ll_paf=5.16934, loss_ll_heat=0.483916, q=1000
[2018-07-14 01:43:07,888] [train] [INFO] epoch=9.00 step=71200, 10.0785 examples/sec lr=0.000011, loss=19.3427, loss_ll=3.56298, loss_ll_paf=6.24817, loss_ll_heat=0.877778, q=1000
[2018-07-14 01:45:39,725] [train] [INFO] epoch=9.00 step=71300, 10.0791 examples/sec lr=0.000011, loss=15.945, loss_ll=2.90694, loss_ll_paf=5.05562, loss_ll_heat=0.758258, q=1000
[2018-07-14 01:48:11,494] [train] [INFO] epoch=9.00 step=71400, 10.0797 examples/sec lr=0.000011, loss=8.08781, loss_ll=1.24073, loss_ll_paf=2.11325, loss_ll_heat=0.368212, q=1000
[2018-07-14 01:50:49,724] [train] [INFO] epoch=9.00 step=71500, 10.0797 examples/sec lr=0.000011, loss=41.2079, loss_ll=7.79194, loss_ll_paf=14.239, loss_ll_heat=1.34489, q=1000
[2018-07-14 01:53:25,675] [train] [INFO] epoch=9.00 step=71600, 10.0800 examples/sec lr=0.000011, loss=16.558, loss_ll=2.96488, loss_ll_paf=5.27189, loss_ll_heat=0.657876, q=1000
[2018-07-14 01:55:58,875] [train] [INFO] epoch=9.00 step=71700, 10.0805 examples/sec lr=0.000011, loss=22.2563, loss_ll=4.18678, loss_ll_paf=7.49432, loss_ll_heat=0.879232, q=1000
[2018-07-14 01:58:33,953] [train] [INFO] epoch=9.00 step=71800, 10.0808 examples/sec lr=0.000011, loss=15.1847, loss_ll=2.92564, loss_ll_paf=5.4943, loss_ll_heat=0.356994, q=1000
[2018-07-14 02:01:08,877] [train] [INFO] epoch=9.00 step=71900, 10.0811 examples/sec lr=0.000011, loss=16.8576, loss_ll=2.85352, loss_ll_paf=5.19343, loss_ll_heat=0.513605, q=1000
[2018-07-14 02:03:42,551] [train] [INFO] epoch=9.00 step=72000, 10.0816 examples/sec lr=0.000011, loss=17.4437, loss_ll=2.70534, loss_ll_paf=4.70274, loss_ll_heat=0.707938, q=1000
[2018-07-14 02:06:30,771] [train] [INFO] epoch=9.00 step=72100, 10.0807 examples/sec lr=0.000011, loss=15.7619, loss_ll=2.69452, loss_ll_paf=4.64078, loss_ll_heat=0.748265, q=1000
[2018-07-14 02:09:07,247] [train] [INFO] epoch=9.00 step=72200, 10.0809 examples/sec lr=0.000011, loss=14.4226, loss_ll=2.71647, loss_ll_paf=4.73429, loss_ll_heat=0.698654, q=1000
[2018-07-14 02:11:35,900] [train] [INFO] epoch=9.00 step=72300, 10.0818 examples/sec lr=0.000011, loss=11.7663, loss_ll=1.93835, loss_ll_paf=3.43229, loss_ll_heat=0.444411, q=1000
[2018-07-14 02:14:11,393] [train] [INFO] epoch=9.00 step=72400, 10.0821 examples/sec lr=0.000011, loss=27.5353, loss_ll=5.13754, loss_ll_paf=9.20021, loss_ll_heat=1.07487, q=1000
[2018-07-14 02:16:44,454] [train] [INFO] epoch=9.00 step=72500, 10.0826 examples/sec lr=0.000011, loss=25.1284, loss_ll=4.32742, loss_ll_paf=7.88466, loss_ll_heat=0.77018, q=1000
[2018-07-14 02:19:17,173] [train] [INFO] epoch=9.00 step=72600, 10.0831 examples/sec lr=0.000011, loss=15.6872, loss_ll=2.45446, loss_ll_paf=4.36316, loss_ll_heat=0.545756, q=1000
[2018-07-14 02:21:50,635] [train] [INFO] epoch=9.00 step=72700, 10.0836 examples/sec lr=0.000011, loss=12.5552, loss_ll=2.17173, loss_ll_paf=3.72511, loss_ll_heat=0.618354, q=1000
[2018-07-14 02:24:25,296] [train] [INFO] epoch=9.00 step=72800, 10.0839 examples/sec lr=0.000011, loss=19.0027, loss_ll=3.38544, loss_ll_paf=6.11408, loss_ll_heat=0.656809, q=1000
[2018-07-14 02:26:54,396] [train] [INFO] epoch=9.00 step=72900, 10.0848 examples/sec lr=0.000011, loss=29.8219, loss_ll=5.72793, loss_ll_paf=10.8726, loss_ll_heat=0.583291, q=1000
[2018-07-14 02:29:29,437] [train] [INFO] epoch=9.00 step=73000, 10.0851 examples/sec lr=0.000011, loss=24.2369, loss_ll=4.1028, loss_ll_paf=7.47536, loss_ll_heat=0.730227, q=1000
[2018-07-14 02:32:18,626] [train] [INFO] epoch=9.00 step=73100, 10.0842 examples/sec lr=0.000011, loss=21.5267, loss_ll=3.96886, loss_ll_paf=7.13087, loss_ll_heat=0.806843, q=1000
[2018-07-14 02:34:52,090] [train] [INFO] epoch=9.00 step=73200, 10.0846 examples/sec lr=0.000011, loss=15.7614, loss_ll=2.74379, loss_ll_paf=4.9964, loss_ll_heat=0.491178, q=1000
[2018-07-14 02:37:27,849] [train] [INFO] epoch=9.00 step=73300, 10.0849 examples/sec lr=0.000011, loss=16.6009, loss_ll=2.84364, loss_ll_paf=5.28665, loss_ll_heat=0.400635, q=1000
[2018-07-14 02:40:02,400] [train] [INFO] epoch=9.00 step=73400, 10.0852 examples/sec lr=0.000011, loss=22.7693, loss_ll=3.98743, loss_ll_paf=7.18523, loss_ll_heat=0.789632, q=1000
[2018-07-14 02:42:37,455] [train] [INFO] epoch=9.00 step=73500, 10.0855 examples/sec lr=0.000011, loss=21.3345, loss_ll=3.88343, loss_ll_paf=7.04972, loss_ll_heat=0.717147, q=1000
[2018-07-14 02:45:10,608] [train] [INFO] epoch=9.00 step=73600, 10.0860 examples/sec lr=0.000011, loss=11.7177, loss_ll=2.01961, loss_ll_paf=3.64375, loss_ll_heat=0.395473, q=1000
[2018-07-14 02:47:43,417] [train] [INFO] epoch=9.00 step=73700, 10.0865 examples/sec lr=0.000011, loss=10.2717, loss_ll=1.56549, loss_ll_paf=2.67311, loss_ll_heat=0.45788, q=1000
[2018-07-14 02:50:16,318] [train] [INFO] epoch=9.00 step=73800, 10.0870 examples/sec lr=0.000011, loss=14.7765, loss_ll=2.69031, loss_ll_paf=4.90861, loss_ll_heat=0.472016, q=1000
[2018-07-14 02:52:47,189] [train] [INFO] epoch=9.00 step=73900, 10.0877 examples/sec lr=0.000011, loss=31.1446, loss_ll=5.4551, loss_ll_paf=9.90676, loss_ll_heat=1.00344, q=1000
[2018-07-14 02:55:19,311] [train] [INFO] epoch=9.00 step=74000, 10.0882 examples/sec lr=0.000011, loss=18.0327, loss_ll=3.21609, loss_ll_paf=5.87589, loss_ll_heat=0.556292, q=1000
[2018-07-14 02:58:07,118] [train] [INFO] epoch=9.00 step=74100, 10.0874 examples/sec lr=0.000011, loss=22.556, loss_ll=3.92376, loss_ll_paf=7.2859, loss_ll_heat=0.561616, q=1000
[2018-07-14 03:00:42,233] [train] [INFO] epoch=9.00 step=74200, 10.0877 examples/sec lr=0.000011, loss=27.9108, loss_ll=5.26787, loss_ll_paf=9.3689, loss_ll_heat=1.16684, q=1000
[2018-07-14 03:03:13,159] [train] [INFO] epoch=9.00 step=74300, 10.0884 examples/sec lr=0.000011, loss=26.3061, loss_ll=4.77411, loss_ll_paf=8.75355, loss_ll_heat=0.794657, q=1000
[2018-07-14 03:05:45,356] [train] [INFO] epoch=9.00 step=74400, 10.0889 examples/sec lr=0.000011, loss=14.1107, loss_ll=2.47036, loss_ll_paf=4.2994, loss_ll_heat=0.641317, q=1000
[2018-07-14 03:08:19,480] [train] [INFO] epoch=9.00 step=74500, 10.0893 examples/sec lr=0.000011, loss=9.45824, loss_ll=1.52496, loss_ll_paf=2.72101, loss_ll_heat=0.328922, q=1000
[2018-07-14 03:10:57,863] [train] [INFO] epoch=9.00 step=74600, 10.0893 examples/sec lr=0.000011, loss=9.65151, loss_ll=1.6815, loss_ll_paf=2.43396, loss_ll_heat=0.929049, q=1000
[2018-07-14 03:13:32,323] [train] [INFO] epoch=9.00 step=74700, 10.0897 examples/sec lr=0.000011, loss=16.8955, loss_ll=2.97467, loss_ll_paf=5.34008, loss_ll_heat=0.609264, q=1000
[2018-07-14 03:16:09,053] [train] [INFO] epoch=9.00 step=74800, 10.0899 examples/sec lr=0.000011, loss=17.8804, loss_ll=3.36293, loss_ll_paf=5.80475, loss_ll_heat=0.921105, q=1000
[2018-07-14 03:18:46,828] [train] [INFO] epoch=9.00 step=74900, 10.0899 examples/sec lr=0.000011, loss=33.1332, loss_ll=6.11381, loss_ll_paf=11.2962, loss_ll_heat=0.931399, q=1000
[2018-07-14 03:21:21,771] [train] [INFO] epoch=9.00 step=75000, 10.0902 examples/sec lr=0.000011, loss=16.4104, loss_ll=2.83801, loss_ll_paf=5.0164, loss_ll_heat=0.659617, q=1000
[2018-07-14 03:24:13,888] [train] [INFO] epoch=9.00 step=75100, 10.0891 examples/sec lr=0.000011, loss=11.662, loss_ll=2.20644, loss_ll_paf=3.63983, loss_ll_heat=0.773047, q=1000
[2018-07-14 03:26:44,238] [train] [INFO] epoch=9.00 step=75200, 10.0898 examples/sec lr=0.000011, loss=25.0182, loss_ll=4.51401, loss_ll_paf=8.11343, loss_ll_heat=0.914596, q=1000
[2018-07-14 03:29:15,004] [train] [INFO] epoch=9.00 step=75300, 10.0904 examples/sec lr=0.000011, loss=26.2176, loss_ll=4.86706, loss_ll_paf=8.9745, loss_ll_heat=0.759617, q=1000
[2018-07-14 03:31:43,049] [train] [INFO] epoch=9.00 step=75400, 10.0913 examples/sec lr=0.000011, loss=18.2254, loss_ll=3.41017, loss_ll_paf=6.15629, loss_ll_heat=0.664047, q=1000
[2018-07-14 03:34:19,874] [train] [INFO] epoch=9.00 step=75500, 10.0915 examples/sec lr=0.000011, loss=26.9253, loss_ll=4.68078, loss_ll_paf=8.74721, loss_ll_heat=0.614343, q=1000
[2018-07-14 03:36:53,188] [train] [INFO] epoch=9.00 step=75600, 10.0919 examples/sec lr=0.000011, loss=27.2539, loss_ll=4.69067, loss_ll_paf=8.88074, loss_ll_heat=0.500601, q=1000
[2018-07-14 03:39:23,813] [train] [INFO] epoch=9.00 step=75700, 10.0926 examples/sec lr=0.000011, loss=27.5425, loss_ll=5.05429, loss_ll_paf=9.42589, loss_ll_heat=0.682695, q=1000
[2018-07-14 03:41:59,202] [train] [INFO] epoch=9.00 step=75800, 10.0928 examples/sec lr=0.000011, loss=21.3705, loss_ll=3.71697, loss_ll_paf=6.70607, loss_ll_heat=0.727869, q=1000
[2018-07-14 03:44:34,268] [train] [INFO] epoch=9.00 step=75900, 10.0931 examples/sec lr=0.000011, loss=9.91426, loss_ll=1.75364, loss_ll_paf=2.83888, loss_ll_heat=0.66839, q=1000
[2018-07-14 03:47:04,780] [train] [INFO] epoch=9.00 step=76000, 10.0938 examples/sec lr=0.000011, loss=14.692, loss_ll=2.45914, loss_ll_paf=4.21895, loss_ll_heat=0.699325, q=1000
[2018-07-14 03:49:52,465] [train] [INFO] epoch=10.00 step=76100, 10.0930 examples/sec lr=0.000011, loss=18.9419, loss_ll=3.54039, loss_ll_paf=6.75825, loss_ll_heat=0.322538, q=1000
[2018-07-14 03:52:32,324] [train] [INFO] epoch=10.00 step=76200, 10.0929 examples/sec lr=0.000011, loss=15.3827, loss_ll=2.67446, loss_ll_paf=4.7345, loss_ll_heat=0.614424, q=1000
[2018-07-14 03:55:05,550] [train] [INFO] epoch=10.00 step=76300, 10.0934 examples/sec lr=0.000011, loss=27.9912, loss_ll=4.89135, loss_ll_paf=8.92854, loss_ll_heat=0.854154, q=1000
[2018-07-14 03:57:38,308] [train] [INFO] epoch=10.00 step=76400, 10.0938 examples/sec lr=0.000011, loss=25.2577, loss_ll=4.45065, loss_ll_paf=8.11426, loss_ll_heat=0.787035, q=1000
[2018-07-14 04:00:12,047] [train] [INFO] epoch=10.00 step=76500, 10.0942 examples/sec lr=0.000011, loss=13.0352, loss_ll=2.14996, loss_ll_paf=3.55277, loss_ll_heat=0.747141, q=1000
[2018-07-14 04:02:43,176] [train] [INFO] epoch=10.00 step=76600, 10.0949 examples/sec lr=0.000011, loss=14.5495, loss_ll=2.85342, loss_ll_paf=5.21221, loss_ll_heat=0.494626, q=1000
[2018-07-14 04:05:16,464] [train] [INFO] epoch=10.00 step=76700, 10.0953 examples/sec lr=0.000011, loss=12.5463, loss_ll=2.12115, loss_ll_paf=3.81768, loss_ll_heat=0.424607, q=1000
[2018-07-14 04:07:49,737] [train] [INFO] epoch=10.00 step=76800, 10.0957 examples/sec lr=0.000011, loss=25.3364, loss_ll=4.64289, loss_ll_paf=8.3447, loss_ll_heat=0.941082, q=1000
[2018-07-14 04:10:23,086] [train] [INFO] epoch=10.00 step=76900, 10.0961 examples/sec lr=0.000011, loss=14.519, loss_ll=2.36386, loss_ll_paf=4.3836, loss_ll_heat=0.34412, q=1000
[2018-07-14 04:12:54,282] [train] [INFO] epoch=10.00 step=77000, 10.0968 examples/sec lr=0.000011, loss=23.5003, loss_ll=4.59859, loss_ll_paf=8.62001, loss_ll_heat=0.577161, q=1000
[2018-07-14 04:15:39,143] [train] [INFO] epoch=10.00 step=77100, 10.0962 examples/sec lr=0.000011, loss=17.9501, loss_ll=3.31143, loss_ll_paf=6.05223, loss_ll_heat=0.570635, q=1000
[2018-07-14 04:18:11,577] [train] [INFO] epoch=10.00 step=77200, 10.0967 examples/sec lr=0.000011, loss=12.2463, loss_ll=2.12542, loss_ll_paf=3.68516, loss_ll_heat=0.565684, q=1000
[2018-07-14 04:20:48,768] [train] [INFO] epoch=10.00 step=77300, 10.0968 examples/sec lr=0.000011, loss=29.4087, loss_ll=5.40776, loss_ll_paf=9.96571, loss_ll_heat=0.849814, q=1000
[2018-07-14 04:23:20,296] [train] [INFO] epoch=10.00 step=77400, 10.0974 examples/sec lr=0.000011, loss=14.3273, loss_ll=2.57324, loss_ll_paf=4.72588, loss_ll_heat=0.420602, q=1000
[2018-07-14 04:25:51,439] [train] [INFO] epoch=10.00 step=77500, 10.0980 examples/sec lr=0.000011, loss=13.8816, loss_ll=2.0982, loss_ll_paf=3.86535, loss_ll_heat=0.331052, q=1000
[2018-07-14 04:28:24,332] [train] [INFO] epoch=10.00 step=77600, 10.0985 examples/sec lr=0.000011, loss=16.8617, loss_ll=2.48671, loss_ll_paf=4.29439, loss_ll_heat=0.679017, q=1000
[2018-07-14 04:31:02,876] [train] [INFO] epoch=10.00 step=77700, 10.0984 examples/sec lr=0.000011, loss=29.7939, loss_ll=5.74396, loss_ll_paf=10.4885, loss_ll_heat=0.999393, q=1000
[2018-07-14 04:33:36,429] [train] [INFO] epoch=10.00 step=77800, 10.0988 examples/sec lr=0.000011, loss=28.3157, loss_ll=4.98411, loss_ll_paf=8.89107, loss_ll_heat=1.07716, q=1000
[2018-07-14 04:36:09,508] [train] [INFO] epoch=10.00 step=77900, 10.0993 examples/sec lr=0.000011, loss=16.0603, loss_ll=2.73306, loss_ll_paf=5.01045, loss_ll_heat=0.455676, q=1000
[2018-07-14 04:38:41,957] [train] [INFO] epoch=10.00 step=78000, 10.0998 examples/sec lr=0.000011, loss=16.2594, loss_ll=2.47653, loss_ll_paf=4.34233, loss_ll_heat=0.610733, q=1000
[2018-07-14 04:41:33,270] [train] [INFO] epoch=10.00 step=78100, 10.0987 examples/sec lr=0.000011, loss=26.304, loss_ll=4.81927, loss_ll_paf=8.88956, loss_ll_heat=0.748978, q=1000
[2018-07-14 04:44:05,956] [train] [INFO] epoch=10.00 step=78200, 10.0992 examples/sec lr=0.000011, loss=20.4736, loss_ll=3.80105, loss_ll_paf=6.96887, loss_ll_heat=0.633236, q=1000
[2018-07-14 04:46:39,918] [train] [INFO] epoch=10.00 step=78300, 10.0996 examples/sec lr=0.000011, loss=13.6134, loss_ll=2.26356, loss_ll_paf=3.93183, loss_ll_heat=0.595285, q=1000
[2018-07-14 04:49:13,999] [train] [INFO] epoch=10.00 step=78400, 10.0999 examples/sec lr=0.000011, loss=16.4583, loss_ll=2.92578, loss_ll_paf=5.14624, loss_ll_heat=0.705316, q=1000
[2018-07-14 04:51:45,305] [train] [INFO] epoch=10.00 step=78500, 10.1005 examples/sec lr=0.000011, loss=31.0061, loss_ll=5.84897, loss_ll_paf=10.9827, loss_ll_heat=0.715195, q=1000
[2018-07-14 04:54:15,049] [train] [INFO] epoch=10.00 step=78600, 10.1012 examples/sec lr=0.000011, loss=25.3571, loss_ll=4.17649, loss_ll_paf=7.57869, loss_ll_heat=0.774297, q=1000
[2018-07-14 04:56:46,770] [train] [INFO] epoch=10.00 step=78700, 10.1017 examples/sec lr=0.000011, loss=20.7892, loss_ll=3.82476, loss_ll_paf=6.96131, loss_ll_heat=0.688207, q=1000
[2018-07-14 04:59:23,198] [train] [INFO] epoch=10.00 step=78800, 10.1019 examples/sec lr=0.000011, loss=17.6689, loss_ll=2.97738, loss_ll_paf=5.00393, loss_ll_heat=0.950833, q=1000
[2018-07-14 05:01:54,177] [train] [INFO] epoch=10.00 step=78900, 10.1025 examples/sec lr=0.000011, loss=7.91738, loss_ll=1.39047, loss_ll_paf=2.27445, loss_ll_heat=0.506481, q=1000
[2018-07-14 05:04:29,407] [train] [INFO] epoch=10.00 step=79000, 10.1027 examples/sec lr=0.000011, loss=8.50989, loss_ll=1.54131, loss_ll_paf=2.5835, loss_ll_heat=0.499126, q=1000
[2018-07-14 05:07:16,290] [train] [INFO] epoch=10.00 step=79100, 10.1021 examples/sec lr=0.000011, loss=27.8881, loss_ll=4.56794, loss_ll_paf=8.22313, loss_ll_heat=0.91275, q=1000
[2018-07-14 05:09:48,746] [train] [INFO] epoch=10.00 step=79200, 10.1025 examples/sec lr=0.000011, loss=11.9352, loss_ll=1.87769, loss_ll_paf=3.2449, loss_ll_heat=0.510478, q=1000
[2018-07-14 05:12:23,568] [train] [INFO] epoch=10.00 step=79300, 10.1028 examples/sec lr=0.000011, loss=22.8902, loss_ll=4.00172, loss_ll_paf=7.44754, loss_ll_heat=0.555898, q=1000
[2018-07-14 05:14:59,145] [train] [INFO] epoch=10.00 step=79400, 10.1030 examples/sec lr=0.000011, loss=17.263, loss_ll=3.1661, loss_ll_paf=5.62352, loss_ll_heat=0.70868, q=1000
[2018-07-14 05:17:29,039] [train] [INFO] epoch=10.00 step=79500, 10.1037 examples/sec lr=0.000011, loss=13.5135, loss_ll=1.92633, loss_ll_paf=3.41421, loss_ll_heat=0.438457, q=1000
[2018-07-14 05:20:00,576] [train] [INFO] epoch=10.00 step=79600, 10.1043 examples/sec lr=0.000011, loss=17.1214, loss_ll=2.98267, loss_ll_paf=5.36245, loss_ll_heat=0.602896, q=1000
[2018-07-14 05:22:31,464] [train] [INFO] epoch=10.00 step=79700, 10.1049 examples/sec lr=0.000011, loss=15.4422, loss_ll=2.65026, loss_ll_paf=4.41081, loss_ll_heat=0.889715, q=1000
[2018-07-14 05:25:02,186] [train] [INFO] epoch=10.00 step=79800, 10.1055 examples/sec lr=0.000011, loss=27.833, loss_ll=5.46889, loss_ll_paf=10.1559, loss_ll_heat=0.781905, q=1000
[2018-07-14 05:27:37,642] [train] [INFO] epoch=10.00 step=79900, 10.1057 examples/sec lr=0.000011, loss=20.0802, loss_ll=3.86705, loss_ll_paf=7.27984, loss_ll_heat=0.454269, q=1000
[2018-07-14 05:30:11,805] [train] [INFO] epoch=10.00 step=80000, 10.1060 examples/sec lr=0.000011, loss=11.495, loss_ll=1.97448, loss_ll_paf=3.57905, loss_ll_heat=0.369911, q=1000
[2018-07-14 05:32:58,986] [train] [INFO] epoch=10.00 step=80100, 10.1053 examples/sec lr=0.000011, loss=9.63993, loss_ll=1.65645, loss_ll_paf=2.62162, loss_ll_heat=0.691272, q=1000
[2018-07-14 05:35:32,614] [train] [INFO] epoch=10.00 step=80200, 10.1057 examples/sec lr=0.000011, loss=16.8176, loss_ll=3.29714, loss_ll_paf=5.43468, loss_ll_heat=1.1596, q=1000
[2018-07-14 05:38:03,051] [train] [INFO] epoch=10.00 step=80300, 10.1063 examples/sec lr=0.000011, loss=10.9954, loss_ll=1.75427, loss_ll_paf=3.21167, loss_ll_heat=0.296864, q=1000
[2018-07-14 05:40:35,073] [train] [INFO] epoch=10.00 step=80400, 10.1068 examples/sec lr=0.000011, loss=14.6396, loss_ll=2.36449, loss_ll_paf=3.9299, loss_ll_heat=0.799072, q=1000
[2018-07-14 05:43:12,771] [train] [INFO] epoch=10.00 step=80500, 10.1069 examples/sec lr=0.000011, loss=14.6452, loss_ll=2.14926, loss_ll_paf=3.60646, loss_ll_heat=0.692055, q=1000
[2018-07-14 05:45:47,056] [train] [INFO] epoch=10.00 step=80600, 10.1072 examples/sec lr=0.000011, loss=8.64752, loss_ll=1.41889, loss_ll_paf=2.55519, loss_ll_heat=0.282599, q=1000
[2018-07-14 05:48:19,785] [train] [INFO] epoch=10.00 step=80700, 10.1076 examples/sec lr=0.000011, loss=17.2517, loss_ll=3.30636, loss_ll_paf=5.95126, loss_ll_heat=0.661464, q=1000
[2018-07-14 05:50:50,632] [train] [INFO] epoch=10.00 step=80800, 10.1082 examples/sec lr=0.000011, loss=38.721, loss_ll=7.2893, loss_ll_paf=13.7499, loss_ll_heat=0.828721, q=1000
[2018-07-14 05:53:27,055] [train] [INFO] epoch=10.00 step=80900, 10.1084 examples/sec lr=0.000011, loss=16.4008, loss_ll=2.8173, loss_ll_paf=4.85834, loss_ll_heat=0.77626, q=1000
[2018-07-14 05:55:59,674] [train] [INFO] epoch=10.00 step=81000, 10.1088 examples/sec lr=0.000011, loss=21.9161, loss_ll=4.11886, loss_ll_paf=7.67556, loss_ll_heat=0.56217, q=1000
[2018-07-14 05:58:52,823] [train] [INFO] epoch=10.00 step=81100, 10.1077 examples/sec lr=0.000011, loss=14.2632, loss_ll=2.74973, loss_ll_paf=4.58583, loss_ll_heat=0.913633, q=1000
[2018-07-14 06:01:23,647] [train] [INFO] epoch=10.00 step=81200, 10.1082 examples/sec lr=0.000011, loss=23.8155, loss_ll=4.56059, loss_ll_paf=8.50723, loss_ll_heat=0.613955, q=1000
[2018-07-14 06:03:58,961] [train] [INFO] epoch=10.00 step=81300, 10.1085 examples/sec lr=0.000011, loss=29.1692, loss_ll=4.96186, loss_ll_paf=9.38276, loss_ll_heat=0.540961, q=1000
[2018-07-14 06:06:36,407] [train] [INFO] epoch=10.00 step=81400, 10.1085 examples/sec lr=0.000011, loss=14.5789, loss_ll=2.4447, loss_ll_paf=4.36959, loss_ll_heat=0.519824, q=1000
[2018-07-14 06:09:11,663] [train] [INFO] epoch=10.00 step=81500, 10.1088 examples/sec lr=0.000011, loss=25.9494, loss_ll=4.47362, loss_ll_paf=8.16077, loss_ll_heat=0.786475, q=1000
[2018-07-14 06:11:41,240] [train] [INFO] epoch=10.00 step=81600, 10.1095 examples/sec lr=0.000011, loss=15.8944, loss_ll=2.86674, loss_ll_paf=5.04251, loss_ll_heat=0.690972, q=1000
[2018-07-14 06:14:16,932] [train] [INFO] epoch=10.00 step=81700, 10.1097 examples/sec lr=0.000011, loss=21.2783, loss_ll=4.01037, loss_ll_paf=7.33785, loss_ll_heat=0.682884, q=1000
[2018-07-14 06:16:53,376] [train] [INFO] epoch=10.00 step=81800, 10.1098 examples/sec lr=0.000011, loss=19.322, loss_ll=3.47703, loss_ll_paf=6.10851, loss_ll_heat=0.845552, q=1000
[2018-07-14 06:19:22,901] [train] [INFO] epoch=10.00 step=81900, 10.1105 examples/sec lr=0.000011, loss=16.4604, loss_ll=3.08055, loss_ll_paf=5.36851, loss_ll_heat=0.792593, q=1000
[2018-07-14 06:21:53,653] [train] [INFO] epoch=10.00 step=82000, 10.1111 examples/sec lr=0.000011, loss=28.1668, loss_ll=5.03864, loss_ll_paf=9.31776, loss_ll_heat=0.759528, q=1000
[2018-07-14 06:24:39,589] [train] [INFO] epoch=10.00 step=82100, 10.1105 examples/sec lr=0.000011, loss=22.1499, loss_ll=4.16703, loss_ll_paf=7.2657, loss_ll_heat=1.06836, q=1000
[2018-07-14 06:27:10,265] [train] [INFO] epoch=10.00 step=82200, 10.1111 examples/sec lr=0.000011, loss=18.2277, loss_ll=3.04565, loss_ll_paf=5.71195, loss_ll_heat=0.379341, q=1000
[2018-07-14 06:29:44,892] [train] [INFO] epoch=10.00 step=82300, 10.1113 examples/sec lr=0.000011, loss=23.381, loss_ll=4.03252, loss_ll_paf=7.73212, loss_ll_heat=0.332912, q=1000
[2018-07-14 06:32:19,354] [train] [INFO] epoch=10.00 step=82400, 10.1116 examples/sec lr=0.000011, loss=17.4666, loss_ll=2.96225, loss_ll_paf=5.47918, loss_ll_heat=0.445324, q=1000
[2018-07-14 06:34:51,963] [train] [INFO] epoch=10.00 step=82500, 10.1121 examples/sec lr=0.000011, loss=12.4847, loss_ll=1.89314, loss_ll_paf=3.26576, loss_ll_heat=0.520513, q=1000
[2018-07-14 06:37:24,277] [train] [INFO] epoch=10.00 step=82600, 10.1125 examples/sec lr=0.000011, loss=14.5194, loss_ll=2.58126, loss_ll_paf=4.6562, loss_ll_heat=0.506324, q=1000
[2018-07-14 06:40:00,724] [train] [INFO] epoch=10.00 step=82700, 10.1127 examples/sec lr=0.000011, loss=22.7482, loss_ll=3.77168, loss_ll_paf=7.01068, loss_ll_heat=0.532679, q=1000
[2018-07-14 06:42:36,728] [train] [INFO] epoch=10.00 step=82800, 10.1128 examples/sec lr=0.000011, loss=11.6738, loss_ll=2.10365, loss_ll_paf=3.74866, loss_ll_heat=0.458649, q=1000
[2018-07-14 06:45:07,571] [train] [INFO] epoch=10.00 step=82900, 10.1134 examples/sec lr=0.000011, loss=14.5269, loss_ll=2.30367, loss_ll_paf=4.0954, loss_ll_heat=0.51194, q=1000
[2018-07-14 06:47:46,628] [train] [INFO] epoch=10.00 step=83000, 10.1133 examples/sec lr=0.000011, loss=13.5418, loss_ll=2.50665, loss_ll_paf=3.98758, loss_ll_heat=1.02572, q=1000
[2018-07-14 06:50:36,706] [train] [INFO] epoch=10.00 step=83100, 10.1124 examples/sec lr=0.000011, loss=16.6319, loss_ll=2.93231, loss_ll_paf=5.30702, loss_ll_heat=0.557612, q=1000
[2018-07-14 06:53:10,066] [train] [INFO] epoch=10.00 step=83200, 10.1128 examples/sec lr=0.000011, loss=26.9426, loss_ll=4.99317, loss_ll_paf=8.99336, loss_ll_heat=0.99299, q=1000
[2018-07-14 06:55:43,303] [train] [INFO] epoch=10.00 step=83300, 10.1132 examples/sec lr=0.000011, loss=17.3906, loss_ll=2.70622, loss_ll_paf=4.79406, loss_ll_heat=0.618367, q=1000
[2018-07-14 06:58:17,323] [train] [INFO] epoch=10.00 step=83400, 10.1135 examples/sec lr=0.000011, loss=15.0787, loss_ll=2.4828, loss_ll_paf=4.58617, loss_ll_heat=0.379425, q=1000
[2018-07-14 07:00:49,091] [train] [INFO] epoch=10.00 step=83500, 10.1140 examples/sec lr=0.000011, loss=25.5956, loss_ll=4.26522, loss_ll_paf=7.7318, loss_ll_heat=0.798637, q=1000
[2018-07-14 07:03:21,738] [train] [INFO] epoch=10.00 step=83600, 10.1144 examples/sec lr=0.000011, loss=19.3742, loss_ll=3.8224, loss_ll_paf=7.28305, loss_ll_heat=0.361752, q=1000
[2018-07-14 07:05:56,618] [train] [INFO] epoch=11.00 step=83700, 10.1147 examples/sec lr=0.000011, loss=12.532, loss_ll=2.10563, loss_ll_paf=3.81955, loss_ll_heat=0.39172, q=1000
[2018-07-14 07:08:26,904] [train] [INFO] epoch=11.00 step=83800, 10.1153 examples/sec lr=0.000011, loss=23.5699, loss_ll=4.16752, loss_ll_paf=7.70135, loss_ll_heat=0.633694, q=1000
[2018-07-14 07:11:04,807] [train] [INFO] epoch=11.00 step=83900, 10.1153 examples/sec lr=0.000011, loss=19.1282, loss_ll=3.42969, loss_ll_paf=6.40835, loss_ll_heat=0.451033, q=1000
[2018-07-14 07:13:37,267] [train] [INFO] epoch=11.00 step=84000, 10.1157 examples/sec lr=0.000011, loss=23.9763, loss_ll=4.4801, loss_ll_paf=8.13197, loss_ll_heat=0.82823, q=1000
[2018-07-14 07:16:19,401] [train] [INFO] epoch=11.00 step=84100, 10.1154 examples/sec lr=0.000011, loss=17.3666, loss_ll=3.17116, loss_ll_paf=5.72172, loss_ll_heat=0.620596, q=1000
[2018-07-14 07:18:51,389] [train] [INFO] epoch=11.00 step=84200, 10.1159 examples/sec lr=0.000011, loss=26.6182, loss_ll=4.83044, loss_ll_paf=8.91688, loss_ll_heat=0.74401, q=1000
[2018-07-14 07:21:24,543] [train] [INFO] epoch=11.00 step=84300, 10.1163 examples/sec lr=0.000011, loss=20.8322, loss_ll=3.73731, loss_ll_paf=6.81358, loss_ll_heat=0.661046, q=1000
[2018-07-14 07:23:58,634] [train] [INFO] epoch=11.00 step=84400, 10.1166 examples/sec lr=0.000011, loss=15.6423, loss_ll=2.44693, loss_ll_paf=4.30444, loss_ll_heat=0.58941, q=1000
[2018-07-14 07:26:28,984] [train] [INFO] epoch=11.00 step=84500, 10.1172 examples/sec lr=0.000011, loss=21.7627, loss_ll=4.11708, loss_ll_paf=7.4549, loss_ll_heat=0.779254, q=1000
[2018-07-14 07:29:03,660] [train] [INFO] epoch=11.00 step=84600, 10.1174 examples/sec lr=0.000011, loss=19.9618, loss_ll=2.77603, loss_ll_paf=4.76629, loss_ll_heat=0.78577, q=1000
[2018-07-14 07:31:39,796] [train] [INFO] epoch=11.00 step=84700, 10.1176 examples/sec lr=0.000011, loss=11.3643, loss_ll=1.80572, loss_ll_paf=2.78991, loss_ll_heat=0.821524, q=1000
[2018-07-14 07:34:15,991] [train] [INFO] epoch=11.00 step=84800, 10.1177 examples/sec lr=0.000011, loss=7.25875, loss_ll=1.24612, loss_ll_paf=2.06966, loss_ll_heat=0.422574, q=1000
[2018-07-14 07:36:52,234] [train] [INFO] epoch=11.00 step=84900, 10.1179 examples/sec lr=0.000011, loss=16.8506, loss_ll=3.15672, loss_ll_paf=5.5439, loss_ll_heat=0.769528, q=1000
[2018-07-14 07:39:26,203] [train] [INFO] epoch=11.00 step=85000, 10.1182 examples/sec lr=0.000011, loss=19.2467, loss_ll=3.32974, loss_ll_paf=6.01965, loss_ll_heat=0.639834, q=1000
[2018-07-14 07:42:15,630] [train] [INFO] epoch=11.00 step=85100, 10.1173 examples/sec lr=0.000011, loss=42.0106, loss_ll=7.93306, loss_ll_paf=14.9364, loss_ll_heat=0.929766, q=1000
[2018-07-14 07:44:51,953] [train] [INFO] epoch=11.00 step=85200, 10.1175 examples/sec lr=0.000011, loss=32.7489, loss_ll=6.61134, loss_ll_paf=12.1869, loss_ll_heat=1.03574, q=1000
[2018-07-14 07:47:25,803] [train] [INFO] epoch=11.00 step=85300, 10.1178 examples/sec lr=0.000011, loss=32.7034, loss_ll=6.04407, loss_ll_paf=11.3862, loss_ll_heat=0.701985, q=1000
[2018-07-14 07:50:00,425] [train] [INFO] epoch=11.00 step=85400, 10.1181 examples/sec lr=0.000011, loss=29.7189, loss_ll=5.01566, loss_ll_paf=8.95578, loss_ll_heat=1.07554, q=1000
[2018-07-14 07:52:36,307] [train] [INFO] epoch=11.00 step=85500, 10.1182 examples/sec lr=0.000011, loss=17.7816, loss_ll=3.30336, loss_ll_paf=5.93574, loss_ll_heat=0.670979, q=1000
[2018-07-14 07:55:07,654] [train] [INFO] epoch=11.00 step=85600, 10.1187 examples/sec lr=0.000011, loss=5.68226, loss_ll=0.907863, loss_ll_paf=1.51177, loss_ll_heat=0.303957, q=1000
[2018-07-14 07:57:44,161] [train] [INFO] epoch=11.00 step=85700, 10.1189 examples/sec lr=0.000011, loss=11.7921, loss_ll=1.88356, loss_ll_paf=3.27962, loss_ll_heat=0.487502, q=1000
[2018-07-14 08:00:16,243] [train] [INFO] epoch=11.00 step=85800, 10.1193 examples/sec lr=0.000011, loss=15.8098, loss_ll=2.67145, loss_ll_paf=4.97234, loss_ll_heat=0.370553, q=1000
[2018-07-14 08:02:47,300] [train] [INFO] epoch=11.00 step=85900, 10.1198 examples/sec lr=0.000011, loss=14.3967, loss_ll=2.78182, loss_ll_paf=4.52383, loss_ll_heat=1.03981, q=1000
[2018-07-14 08:05:19,900] [train] [INFO] epoch=11.00 step=86000, 10.1202 examples/sec lr=0.000011, loss=18.3941, loss_ll=3.18199, loss_ll_paf=5.50415, loss_ll_heat=0.859822, q=1000
[2018-07-14 08:08:12,790] [train] [INFO] epoch=11.00 step=86100, 10.1191 examples/sec lr=0.000011, loss=17.787, loss_ll=3.67371, loss_ll_paf=6.68055, loss_ll_heat=0.666867, q=1000
[2018-07-14 08:10:43,562] [train] [INFO] epoch=11.00 step=86200, 10.1197 examples/sec lr=0.000011, loss=22.7231, loss_ll=4.18888, loss_ll_paf=7.50616, loss_ll_heat=0.871591, q=1000
[2018-07-14 08:13:17,730] [train] [INFO] epoch=11.00 step=86300, 10.1200 examples/sec lr=0.000011, loss=22.1021, loss_ll=4.16622, loss_ll_paf=7.33865, loss_ll_heat=0.993804, q=1000
[2018-07-14 08:15:51,593] [train] [INFO] epoch=11.00 step=86400, 10.1203 examples/sec lr=0.000011, loss=28.8664, loss_ll=4.86822, loss_ll_paf=8.46315, loss_ll_heat=1.27328, q=1000
[2018-07-14 08:18:29,082] [train] [INFO] epoch=11.00 step=86500, 10.1203 examples/sec lr=0.000011, loss=15.5104, loss_ll=2.86078, loss_ll_paf=4.84128, loss_ll_heat=0.880282, q=1000
[2018-07-14 08:21:04,470] [train] [INFO] epoch=11.00 step=86600, 10.1205 examples/sec lr=0.000011, loss=22.8537, loss_ll=4.26498, loss_ll_paf=7.78459, loss_ll_heat=0.745368, q=1000
[2018-07-14 08:23:38,274] [train] [INFO] epoch=11.00 step=86700, 10.1209 examples/sec lr=0.000011, loss=16.9977, loss_ll=3.06723, loss_ll_paf=5.6495, loss_ll_heat=0.484951, q=1000
[2018-07-14 08:26:12,499] [train] [INFO] epoch=11.00 step=86800, 10.1211 examples/sec lr=0.000011, loss=31.9802, loss_ll=6.00433, loss_ll_paf=11.4308, loss_ll_heat=0.577813, q=1000
[2018-07-14 08:28:47,108] [train] [INFO] epoch=11.00 step=86900, 10.1214 examples/sec lr=0.000011, loss=6.29345, loss_ll=1.0298, loss_ll_paf=1.77072, loss_ll_heat=0.288891, q=1000
[2018-07-14 08:31:22,677] [train] [INFO] epoch=11.00 step=87000, 10.1216 examples/sec lr=0.000011, loss=22.2227, loss_ll=3.82864, loss_ll_paf=7.16352, loss_ll_heat=0.493755, q=1000
[2018-07-14 08:34:10,731] [train] [INFO] epoch=11.00 step=87100, 10.1209 examples/sec lr=0.000011, loss=35.8244, loss_ll=7.25939, loss_ll_paf=13.8121, loss_ll_heat=0.706701, q=1000
[2018-07-14 08:36:46,630] [train] [INFO] epoch=11.00 step=87200, 10.1210 examples/sec lr=0.000011, loss=27.0713, loss_ll=4.81632, loss_ll_paf=9.14546, loss_ll_heat=0.487193, q=1000
[2018-07-14 08:39:20,883] [train] [INFO] epoch=11.00 step=87300, 10.1213 examples/sec lr=0.000011, loss=15.4046, loss_ll=2.71675, loss_ll_paf=5.07324, loss_ll_heat=0.360254, q=1000
[2018-07-14 08:41:58,465] [train] [INFO] epoch=11.00 step=87400, 10.1213 examples/sec lr=0.000011, loss=29.9598, loss_ll=5.76791, loss_ll_paf=11.0514, loss_ll_heat=0.484409, q=1000
[2018-07-14 08:44:35,302] [train] [INFO] epoch=11.00 step=87500, 10.1214 examples/sec lr=0.000011, loss=21.685, loss_ll=3.81294, loss_ll_paf=6.95495, loss_ll_heat=0.670941, q=1000
[2018-07-14 08:47:14,634] [train] [INFO] epoch=11.00 step=87600, 10.1213 examples/sec lr=0.000011, loss=9.42681, loss_ll=1.67341, loss_ll_paf=2.83146, loss_ll_heat=0.515361, q=1000
[2018-07-14 08:49:47,019] [train] [INFO] epoch=11.00 step=87700, 10.1217 examples/sec lr=0.000011, loss=15.3716, loss_ll=2.47286, loss_ll_paf=4.55241, loss_ll_heat=0.393305, q=1000
[2018-07-14 08:52:17,663] [train] [INFO] epoch=11.00 step=87800, 10.1223 examples/sec lr=0.000011, loss=31.9001, loss_ll=5.88536, loss_ll_paf=11.1491, loss_ll_heat=0.621655, q=1000
[2018-07-14 08:54:46,186] [train] [INFO] epoch=11.00 step=87900, 10.1230 examples/sec lr=0.000011, loss=20.2796, loss_ll=3.42542, loss_ll_paf=6.16256, loss_ll_heat=0.688283, q=1000
[2018-07-14 08:57:20,275] [train] [INFO] epoch=11.00 step=88000, 10.1233 examples/sec lr=0.000011, loss=18.347, loss_ll=3.34691, loss_ll_paf=5.83047, loss_ll_heat=0.863352, q=1000
[2018-07-14 09:00:08,441] [train] [INFO] epoch=11.00 step=88100, 10.1225 examples/sec lr=0.000011, loss=10.8691, loss_ll=1.84957, loss_ll_paf=3.32966, loss_ll_heat=0.369485, q=1000
[2018-07-14 09:02:40,199] [train] [INFO] epoch=11.00 step=88200, 10.1230 examples/sec lr=0.000011, loss=24.0243, loss_ll=4.32727, loss_ll_paf=7.72528, loss_ll_heat=0.929249, q=1000
[2018-07-14 09:05:14,739] [train] [INFO] epoch=11.00 step=88300, 10.1233 examples/sec lr=0.000011, loss=22.0068, loss_ll=3.60016, loss_ll_paf=6.73612, loss_ll_heat=0.464193, q=1000
[2018-07-14 09:07:48,128] [train] [INFO] epoch=11.00 step=88400, 10.1236 examples/sec lr=0.000011, loss=28.698, loss_ll=5.11478, loss_ll_paf=9.50217, loss_ll_heat=0.727391, q=1000
[2018-07-14 09:10:21,637] [train] [INFO] epoch=11.00 step=88500, 10.1239 examples/sec lr=0.000011, loss=18.7892, loss_ll=3.17735, loss_ll_paf=5.85575, loss_ll_heat=0.49894, q=1000
[2018-07-14 09:12:56,746] [train] [INFO] epoch=11.00 step=88600, 10.1241 examples/sec lr=0.000011, loss=15.8985, loss_ll=2.86027, loss_ll_paf=4.93704, loss_ll_heat=0.783509, q=1000
[2018-07-14 09:15:33,514] [train] [INFO] epoch=11.00 step=88700, 10.1242 examples/sec lr=0.000011, loss=20.6093, loss_ll=3.90188, loss_ll_paf=6.80783, loss_ll_heat=0.995933, q=1000
[2018-07-14 09:18:05,522] [train] [INFO] epoch=11.00 step=88800, 10.1247 examples/sec lr=0.000011, loss=17.9347, loss_ll=3.03282, loss_ll_paf=5.5942, loss_ll_heat=0.471445, q=1000
[2018-07-14 09:20:40,361] [train] [INFO] epoch=11.00 step=88900, 10.1249 examples/sec lr=0.000011, loss=21.4154, loss_ll=4.00994, loss_ll_paf=7.60033, loss_ll_heat=0.419558, q=1000
[2018-07-14 09:23:11,234] [train] [INFO] epoch=11.00 step=89000, 10.1254 examples/sec lr=0.000011, loss=13.402, loss_ll=2.34198, loss_ll_paf=4.29806, loss_ll_heat=0.38589, q=1000
[2018-07-14 09:25:59,640] [train] [INFO] epoch=11.00 step=89100, 10.1247 examples/sec lr=0.000011, loss=29.3234, loss_ll=5.32479, loss_ll_paf=9.87048, loss_ll_heat=0.779103, q=1000
[2018-07-14 09:28:40,381] [train] [INFO] epoch=11.00 step=89200, 10.1245 examples/sec lr=0.000011, loss=9.98986, loss_ll=1.84193, loss_ll_paf=3.08999, loss_ll_heat=0.593866, q=1000
[2018-07-14 09:31:12,779] [train] [INFO] epoch=11.00 step=89300, 10.1249 examples/sec lr=0.000011, loss=16.4359, loss_ll=3.20022, loss_ll_paf=5.87866, loss_ll_heat=0.521791, q=1000
[2018-07-14 09:33:46,000] [train] [INFO] epoch=11.00 step=89400, 10.1252 examples/sec lr=0.000011, loss=36.9357, loss_ll=6.66007, loss_ll_paf=12.3476, loss_ll_heat=0.972501, q=1000
[2018-07-14 09:36:20,366] [train] [INFO] epoch=11.00 step=89500, 10.1255 examples/sec lr=0.000011, loss=19.4589, loss_ll=3.22818, loss_ll_paf=5.88994, loss_ll_heat=0.566414, q=1000
[2018-07-14 09:38:53,787] [train] [INFO] epoch=11.00 step=89600, 10.1258 examples/sec lr=0.000011, loss=21.9881, loss_ll=4.19044, loss_ll_paf=7.50645, loss_ll_heat=0.874423, q=1000
[2018-07-14 09:41:26,262] [train] [INFO] epoch=11.00 step=89700, 10.1262 examples/sec lr=0.000011, loss=36.6242, loss_ll=6.88996, loss_ll_paf=12.7732, loss_ll_heat=1.00675, q=1000
[2018-07-14 09:43:57,199] [train] [INFO] epoch=11.00 step=89800, 10.1267 examples/sec lr=0.000011, loss=16.0412, loss_ll=2.91617, loss_ll_paf=5.17282, loss_ll_heat=0.659519, q=1000
[2018-07-14 09:46:28,500] [train] [INFO] epoch=11.00 step=89900, 10.1272 examples/sec lr=0.000011, loss=23.3931, loss_ll=3.81083, loss_ll_paf=6.95517, loss_ll_heat=0.666498, q=1000
[2018-07-14 09:49:00,491] [train] [INFO] epoch=11.00 step=90000, 10.1276 examples/sec lr=0.000004, loss=26.4718, loss_ll=4.45378, loss_ll_paf=8.0781, loss_ll_heat=0.829457, q=1000
[2018-07-14 09:51:50,931] [train] [INFO] epoch=11.00 step=90100, 10.1267 examples/sec lr=0.000004, loss=12.3423, loss_ll=2.12413, loss_ll_paf=3.53338, loss_ll_heat=0.714878, q=1000
[2018-07-14 09:54:26,424] [train] [INFO] epoch=11.00 step=90200, 10.1269 examples/sec lr=0.000004, loss=25.4966, loss_ll=4.70848, loss_ll_paf=8.6068, loss_ll_heat=0.810154, q=1000
[2018-07-14 09:56:59,968] [train] [INFO] epoch=11.00 step=90300, 10.1272 examples/sec lr=0.000004, loss=20.657, loss_ll=3.09996, loss_ll_paf=5.65708, loss_ll_heat=0.542847, q=1000
[2018-07-14 09:59:36,406] [train] [INFO] epoch=11.00 step=90400, 10.1273 examples/sec lr=0.000004, loss=24.5522, loss_ll=4.33018, loss_ll_paf=8.0543, loss_ll_heat=0.606067, q=1000
[2018-07-14 10:02:07,682] [train] [INFO] epoch=11.00 step=90500, 10.1278 examples/sec lr=0.000004, loss=21.1306, loss_ll=3.72393, loss_ll_paf=6.96874, loss_ll_heat=0.479125, q=1000
[2018-07-14 10:04:41,705] [train] [INFO] epoch=11.00 step=90600, 10.1281 examples/sec lr=0.000004, loss=13.253, loss_ll=2.58028, loss_ll_paf=4.53076, loss_ll_heat=0.6298, q=1000
[2018-07-14 10:07:21,044] [train] [INFO] epoch=11.00 step=90700, 10.1280 examples/sec lr=0.000004, loss=13.7783, loss_ll=2.42846, loss_ll_paf=4.3188, loss_ll_heat=0.538121, q=1000
[2018-07-14 10:09:54,946] [train] [INFO] epoch=11.00 step=90800, 10.1283 examples/sec lr=0.000004, loss=23.8289, loss_ll=4.34371, loss_ll_paf=8.01001, loss_ll_heat=0.677402, q=1000
[2018-07-14 10:12:29,645] [train] [INFO] epoch=11.00 step=90900, 10.1285 examples/sec lr=0.000004, loss=15.7927, loss_ll=2.77745, loss_ll_paf=4.60422, loss_ll_heat=0.950682, q=1000
[2018-07-14 10:15:04,209] [train] [INFO] epoch=11.00 step=91000, 10.1287 examples/sec lr=0.000004, loss=24.1254, loss_ll=4.24254, loss_ll_paf=7.95601, loss_ll_heat=0.529085, q=1000
[2018-07-14 10:17:53,145] [train] [INFO] epoch=11.00 step=91100, 10.1280 examples/sec lr=0.000004, loss=16.8843, loss_ll=2.28011, loss_ll_paf=3.97371, loss_ll_heat=0.586515, q=1000
[2018-07-14 10:20:25,646] [train] [INFO] epoch=11.00 step=91200, 10.1284 examples/sec lr=0.000004, loss=15.3455, loss_ll=2.86233, loss_ll_paf=5.1365, loss_ll_heat=0.588154, q=1000
[2018-07-14 10:23:01,492] [train] [INFO] epoch=11.00 step=91300, 10.1285 examples/sec lr=0.000004, loss=35.4671, loss_ll=6.66145, loss_ll_paf=12.4877, loss_ll_heat=0.83522, q=1000
[2018-07-14 10:25:33,909] [train] [INFO] epoch=12.00 step=91400, 10.1289 examples/sec lr=0.000004, loss=22.5448, loss_ll=4.0876, loss_ll_paf=7.52961, loss_ll_heat=0.645599, q=1000
[2018-07-14 10:28:09,536] [train] [INFO] epoch=12.00 step=91500, 10.1291 examples/sec lr=0.000004, loss=12.0783, loss_ll=2.15753, loss_ll_paf=3.74698, loss_ll_heat=0.568072, q=1000
[2018-07-14 10:30:40,678] [train] [INFO] epoch=12.00 step=91600, 10.1295 examples/sec lr=0.000004, loss=17.4867, loss_ll=3.16937, loss_ll_paf=5.46995, loss_ll_heat=0.868789, q=1000
[2018-07-14 10:33:17,795] [train] [INFO] epoch=12.00 step=91700, 10.1296 examples/sec lr=0.000004, loss=14.8806, loss_ll=2.13288, loss_ll_paf=3.53903, loss_ll_heat=0.726722, q=1000
[2018-07-14 10:35:48,487] [train] [INFO] epoch=12.00 step=91800, 10.1301 examples/sec lr=0.000004, loss=18.884, loss_ll=3.18547, loss_ll_paf=5.88694, loss_ll_heat=0.483994, q=1000
[2018-07-14 10:38:21,415] [train] [INFO] epoch=12.00 step=91900, 10.1304 examples/sec lr=0.000004, loss=13.035, loss_ll=2.3061, loss_ll_paf=4.12025, loss_ll_heat=0.491963, q=1000
[2018-07-14 10:40:56,441] [train] [INFO] epoch=12.00 step=92000, 10.1307 examples/sec lr=0.000004, loss=14.6046, loss_ll=2.51051, loss_ll_paf=4.34877, loss_ll_heat=0.672254, q=1000
[2018-07-14 10:43:37,839] [train] [INFO] epoch=12.00 step=92100, 10.1304 examples/sec lr=0.000004, loss=23.8468, loss_ll=4.2924, loss_ll_paf=8.11609, loss_ll_heat=0.468723, q=1000
[2018-07-14 10:46:11,661] [train] [INFO] epoch=12.00 step=92200, 10.1307 examples/sec lr=0.000004, loss=13.5642, loss_ll=1.98986, loss_ll_paf=3.44521, loss_ll_heat=0.534503, q=1000
[2018-07-14 10:48:43,020] [train] [INFO] epoch=12.00 step=92300, 10.1312 examples/sec lr=0.000004, loss=31.7442, loss_ll=5.81052, loss_ll_paf=10.6941, loss_ll_heat=0.926899, q=1000
[2018-07-14 10:51:15,850] [train] [INFO] epoch=12.00 step=92400, 10.1315 examples/sec lr=0.000004, loss=22.9057, loss_ll=3.63486, loss_ll_paf=6.83225, loss_ll_heat=0.437457, q=1000
[2018-07-14 10:53:51,852] [train] [INFO] epoch=12.00 step=92500, 10.1316 examples/sec lr=0.000004, loss=11.2357, loss_ll=2.04791, loss_ll_paf=3.60716, loss_ll_heat=0.488663, q=1000
[2018-07-14 10:56:28,138] [train] [INFO] epoch=12.00 step=92600, 10.1318 examples/sec lr=0.000004, loss=18.6245, loss_ll=3.44756, loss_ll_paf=6.01841, loss_ll_heat=0.876699, q=1000
[2018-07-14 10:59:01,178] [train] [INFO] epoch=12.00 step=92700, 10.1321 examples/sec lr=0.000004, loss=17.7457, loss_ll=3.1956, loss_ll_paf=5.43823, loss_ll_heat=0.952978, q=1000
[2018-07-14 11:01:38,937] [train] [INFO] epoch=12.00 step=92800, 10.1321 examples/sec lr=0.000004, loss=23.8508, loss_ll=4.43328, loss_ll_paf=7.88279, loss_ll_heat=0.983775, q=1000
[2018-07-14 11:04:14,214] [train] [INFO] epoch=12.00 step=92900, 10.1323 examples/sec lr=0.000004, loss=15.1939, loss_ll=2.51957, loss_ll_paf=4.50237, loss_ll_heat=0.53676, q=1000
[2018-07-14 11:06:51,319] [train] [INFO] epoch=12.00 step=93000, 10.1323 examples/sec lr=0.000004, loss=15.0564, loss_ll=2.81652, loss_ll_paf=5.34096, loss_ll_heat=0.292081, q=1000
[2018-07-14 11:09:35,244] [train] [INFO] epoch=12.00 step=93100, 10.1319 examples/sec lr=0.000004, loss=17.7137, loss_ll=2.95835, loss_ll_paf=5.12185, loss_ll_heat=0.794841, q=1000
[2018-07-14 11:12:11,320] [train] [INFO] epoch=12.00 step=93200, 10.1321 examples/sec lr=0.000004, loss=34.7377, loss_ll=6.31199, loss_ll_paf=12.265, loss_ll_heat=0.358932, q=1000
[2018-07-14 11:14:46,169] [train] [INFO] epoch=12.00 step=93300, 10.1323 examples/sec lr=0.000004, loss=13.4262, loss_ll=2.2588, loss_ll_paf=4.14394, loss_ll_heat=0.373656, q=1000
[2018-07-14 11:17:17,454] [train] [INFO] epoch=12.00 step=93400, 10.1327 examples/sec lr=0.000004, loss=25.8047, loss_ll=4.78705, loss_ll_paf=9.04834, loss_ll_heat=0.525773, q=1000
[2018-07-14 11:19:48,019] [train] [INFO] epoch=12.00 step=93500, 10.1332 examples/sec lr=0.000004, loss=15.7232, loss_ll=2.89104, loss_ll_paf=5.32833, loss_ll_heat=0.453743, q=1000
[2018-07-14 11:22:20,784] [train] [INFO] epoch=12.00 step=93600, 10.1336 examples/sec lr=0.000004, loss=30.8697, loss_ll=5.68633, loss_ll_paf=10.6993, loss_ll_heat=0.673326, q=1000
[2018-07-14 11:24:58,458] [train] [INFO] epoch=12.00 step=93700, 10.1336 examples/sec lr=0.000004, loss=26.3263, loss_ll=4.91484, loss_ll_paf=9.27393, loss_ll_heat=0.555756, q=1000
[2018-07-14 11:27:35,505] [train] [INFO] epoch=12.00 step=93800, 10.1336 examples/sec lr=0.000004, loss=26.2796, loss_ll=4.64338, loss_ll_paf=8.62983, loss_ll_heat=0.65692, q=1000
[2018-07-14 11:30:11,031] [train] [INFO] epoch=12.00 step=93900, 10.1338 examples/sec lr=0.000004, loss=16.5108, loss_ll=2.27313, loss_ll_paf=3.99737, loss_ll_heat=0.548892, q=1000
[2018-07-14 11:32:46,801] [train] [INFO] epoch=12.00 step=94000, 10.1340 examples/sec lr=0.000004, loss=12.0432, loss_ll=1.88448, loss_ll_paf=2.96985, loss_ll_heat=0.799117, q=1000
[2018-07-14 11:35:37,645] [train] [INFO] epoch=12.00 step=94100, 10.1331 examples/sec lr=0.000004, loss=22.4329, loss_ll=4.04991, loss_ll_paf=7.37068, loss_ll_heat=0.729148, q=1000
[2018-07-14 11:38:14,144] [train] [INFO] epoch=12.00 step=94200, 10.1332 examples/sec lr=0.000004, loss=14.8056, loss_ll=2.54889, loss_ll_paf=4.59353, loss_ll_heat=0.504246, q=1000
[2018-07-14 11:40:46,072] [train] [INFO] epoch=12.00 step=94300, 10.1336 examples/sec lr=0.000004, loss=16.2524, loss_ll=2.43196, loss_ll_paf=4.10404, loss_ll_heat=0.759884, q=1000
[2018-07-14 11:43:23,126] [train] [INFO] epoch=12.00 step=94400, 10.1336 examples/sec lr=0.000004, loss=18.0504, loss_ll=3.19661, loss_ll_paf=6.01563, loss_ll_heat=0.377589, q=1000
[2018-07-14 11:45:58,777] [train] [INFO] epoch=12.00 step=94500, 10.1338 examples/sec lr=0.000004, loss=21.7062, loss_ll=3.94777, loss_ll_paf=6.89219, loss_ll_heat=1.00335, q=1000
[2018-07-14 11:48:35,118] [train] [INFO] epoch=12.00 step=94600, 10.1339 examples/sec lr=0.000004, loss=16.6529, loss_ll=2.84134, loss_ll_paf=5.15069, loss_ll_heat=0.531995, q=1000
[2018-07-14 11:51:08,290] [train] [INFO] epoch=12.00 step=94700, 10.1342 examples/sec lr=0.000004, loss=16.0267, loss_ll=2.96661, loss_ll_paf=5.29717, loss_ll_heat=0.636053, q=1000
[2018-07-14 11:53:42,387] [train] [INFO] epoch=12.00 step=94800, 10.1345 examples/sec lr=0.000004, loss=8.98793, loss_ll=1.40489, loss_ll_paf=2.34023, loss_ll_heat=0.469553, q=1000
[2018-07-14 11:56:17,048] [train] [INFO] epoch=12.00 step=94900, 10.1347 examples/sec lr=0.000004, loss=10.6398, loss_ll=2.08043, loss_ll_paf=3.65421, loss_ll_heat=0.506646, q=1000
[2018-07-14 11:58:50,226] [train] [INFO] epoch=12.00 step=95000, 10.1350 examples/sec lr=0.000004, loss=13.2809, loss_ll=2.27339, loss_ll_paf=4.07257, loss_ll_heat=0.474201, q=1000
[2018-07-14 12:01:39,976] [train] [INFO] epoch=12.00 step=95100, 10.1342 examples/sec lr=0.000004, loss=28.2884, loss_ll=5.35113, loss_ll_paf=9.69052, loss_ll_heat=1.01174, q=1000
[2018-07-14 12:04:09,880] [train] [INFO] epoch=12.00 step=95200, 10.1347 examples/sec lr=0.000004, loss=11.6142, loss_ll=1.91153, loss_ll_paf=3.5675, loss_ll_heat=0.255552, q=1000
[2018-07-14 12:06:43,974] [train] [INFO] epoch=12.00 step=95300, 10.1350 examples/sec lr=0.000004, loss=20.1655, loss_ll=3.5823, loss_ll_paf=6.56766, loss_ll_heat=0.596935, q=1000
[2018-07-14 12:09:18,195] [train] [INFO] epoch=12.00 step=95400, 10.1352 examples/sec lr=0.000004, loss=27.1884, loss_ll=4.98406, loss_ll_paf=9.43361, loss_ll_heat=0.534508, q=1000
[2018-07-14 12:11:48,592] [train] [INFO] epoch=12.00 step=95500, 10.1357 examples/sec lr=0.000004, loss=26.4835, loss_ll=4.87195, loss_ll_paf=8.91829, loss_ll_heat=0.825603, q=1000
[2018-07-14 12:14:18,502] [train] [INFO] epoch=12.00 step=95600, 10.1363 examples/sec lr=0.000004, loss=13.9471, loss_ll=2.2983, loss_ll_paf=3.96522, loss_ll_heat=0.631388, q=1000
[2018-07-14 12:16:52,467] [train] [INFO] epoch=12.00 step=95700, 10.1365 examples/sec lr=0.000004, loss=13.6324, loss_ll=2.4578, loss_ll_paf=4.4936, loss_ll_heat=0.421996, q=1000
[2018-07-14 12:19:28,052] [train] [INFO] epoch=12.00 step=95800, 10.1367 examples/sec lr=0.000004, loss=11.2858, loss_ll=1.78413, loss_ll_paf=3.25677, loss_ll_heat=0.311494, q=1000
[2018-07-14 12:22:01,677] [train] [INFO] epoch=12.00 step=95900, 10.1370 examples/sec lr=0.000004, loss=17.6, loss_ll=2.83712, loss_ll_paf=5.17515, loss_ll_heat=0.499084, q=1000
[2018-07-14 12:24:33,650] [train] [INFO] epoch=12.00 step=96000, 10.1374 examples/sec lr=0.000004, loss=13.2514, loss_ll=2.23771, loss_ll_paf=4.04001, loss_ll_heat=0.435419, q=1000
[2018-07-14 12:27:18,128] [train] [INFO] epoch=12.00 step=96100, 10.1369 examples/sec lr=0.000004, loss=11.5586, loss_ll=1.91156, loss_ll_paf=3.52993, loss_ll_heat=0.293187, q=1000
[2018-07-14 12:29:52,085] [train] [INFO] epoch=12.00 step=96200, 10.1372 examples/sec lr=0.000004, loss=16.6223, loss_ll=2.83698, loss_ll_paf=5.25406, loss_ll_heat=0.4199, q=1000
[2018-07-14 12:32:27,866] [train] [INFO] epoch=12.00 step=96300, 10.1373 examples/sec lr=0.000004, loss=13.7874, loss_ll=2.26513, loss_ll_paf=3.97322, loss_ll_heat=0.557027, q=1000
[2018-07-14 12:35:00,602] [train] [INFO] epoch=12.00 step=96400, 10.1376 examples/sec lr=0.000004, loss=22.5373, loss_ll=4.42071, loss_ll_paf=8.01023, loss_ll_heat=0.831181, q=1000
[2018-07-14 12:37:36,929] [train] [INFO] epoch=12.00 step=96500, 10.1377 examples/sec lr=0.000004, loss=11.4742, loss_ll=2.01581, loss_ll_paf=3.70309, loss_ll_heat=0.328538, q=1000
[2018-07-14 12:40:12,327] [train] [INFO] epoch=12.00 step=96600, 10.1379 examples/sec lr=0.000004, loss=29.9713, loss_ll=5.48376, loss_ll_paf=10.3091, loss_ll_heat=0.658403, q=1000
[2018-07-14 12:42:43,632] [train] [INFO] epoch=12.00 step=96700, 10.1383 examples/sec lr=0.000004, loss=13.1301, loss_ll=2.34286, loss_ll_paf=4.0849, loss_ll_heat=0.600818, q=1000
[2018-07-14 12:45:16,296] [train] [INFO] epoch=12.00 step=96800, 10.1387 examples/sec lr=0.000004, loss=9.06577, loss_ll=1.62425, loss_ll_paf=2.64405, loss_ll_heat=0.60446, q=1000
[2018-07-14 12:47:49,617] [train] [INFO] epoch=12.00 step=96900, 10.1390 examples/sec lr=0.000004, loss=21.6391, loss_ll=3.76263, loss_ll_paf=7.07292, loss_ll_heat=0.452352, q=1000
[2018-07-14 12:50:21,669] [train] [INFO] epoch=12.00 step=97000, 10.1394 examples/sec lr=0.000004, loss=19.8011, loss_ll=3.42769, loss_ll_paf=6.30548, loss_ll_heat=0.549909, q=1000
[2018-07-14 12:53:08,037] [train] [INFO] epoch=12.00 step=97100, 10.1388 examples/sec lr=0.000004, loss=18.6307, loss_ll=3.01251, loss_ll_paf=5.61823, loss_ll_heat=0.406791, q=1000
[2018-07-14 12:55:41,011] [train] [INFO] epoch=12.00 step=97200, 10.1391 examples/sec lr=0.000004, loss=29.8875, loss_ll=5.05067, loss_ll_paf=9.13618, loss_ll_heat=0.965147, q=1000
[2018-07-14 12:58:19,257] [train] [INFO] epoch=12.00 step=97300, 10.1391 examples/sec lr=0.000004, loss=14.6193, loss_ll=2.57563, loss_ll_paf=4.47675, loss_ll_heat=0.674524, q=1000
[2018-07-14 13:00:49,162] [train] [INFO] epoch=12.00 step=97400, 10.1396 examples/sec lr=0.000004, loss=7.62905, loss_ll=1.36026, loss_ll_paf=2.44246, loss_ll_heat=0.278063, q=1000
[2018-07-14 13:03:22,037] [train] [INFO] epoch=12.00 step=97500, 10.1399 examples/sec lr=0.000004, loss=13.8771, loss_ll=2.35393, loss_ll_paf=4.39679, loss_ll_heat=0.311061, q=1000
[2018-07-14 13:05:56,268] [train] [INFO] epoch=12.00 step=97600, 10.1402 examples/sec lr=0.000004, loss=12.5801, loss_ll=1.96412, loss_ll_paf=3.41608, loss_ll_heat=0.512163, q=1000
[2018-07-14 13:08:29,600] [train] [INFO] epoch=12.00 step=97700, 10.1405 examples/sec lr=0.000004, loss=15.6575, loss_ll=2.87475, loss_ll_paf=5.41555, loss_ll_heat=0.333957, q=1000
[2018-07-14 13:11:03,282] [train] [INFO] epoch=12.00 step=97800, 10.1407 examples/sec lr=0.000004, loss=29.3388, loss_ll=5.18032, loss_ll_paf=9.4998, loss_ll_heat=0.860842, q=1000
[2018-07-14 13:13:39,574] [train] [INFO] epoch=12.00 step=97900, 10.1408 examples/sec lr=0.000004, loss=17.4882, loss_ll=3.07944, loss_ll_paf=5.57333, loss_ll_heat=0.58556, q=1000
[2018-07-14 13:16:13,025] [train] [INFO] epoch=12.00 step=98000, 10.1411 examples/sec lr=0.000004, loss=16.9579, loss_ll=3.1096, loss_ll_paf=5.61337, loss_ll_heat=0.605838, q=1000
[2018-07-14 13:19:05,971] [train] [INFO] epoch=12.00 step=98100, 10.1401 examples/sec lr=0.000004, loss=18.8183, loss_ll=3.13265, loss_ll_paf=5.7587, loss_ll_heat=0.50661, q=1000
[2018-07-14 13:21:39,535] [train] [INFO] epoch=12.00 step=98200, 10.1404 examples/sec lr=0.000004, loss=9.81519, loss_ll=1.5678, loss_ll_paf=2.77503, loss_ll_heat=0.36056, q=1000
[2018-07-14 13:24:12,835] [train] [INFO] epoch=12.00 step=98300, 10.1407 examples/sec lr=0.000004, loss=35.8107, loss_ll=6.47884, loss_ll_paf=12.3146, loss_ll_heat=0.643024, q=1000
[2018-07-14 13:26:46,567] [train] [INFO] epoch=12.00 step=98400, 10.1409 examples/sec lr=0.000004, loss=16.2173, loss_ll=2.99034, loss_ll_paf=5.41847, loss_ll_heat=0.562208, q=1000
[2018-07-14 13:29:24,967] [train] [INFO] epoch=12.00 step=98500, 10.1409 examples/sec lr=0.000004, loss=21.749, loss_ll=3.92809, loss_ll_paf=6.88561, loss_ll_heat=0.970571, q=1000
[2018-07-14 13:31:59,181] [train] [INFO] epoch=12.00 step=98600, 10.1411 examples/sec lr=0.000004, loss=31.8045, loss_ll=6.14415, loss_ll_paf=11.5853, loss_ll_heat=0.70305, q=1000
[2018-07-14 13:34:30,813] [train] [INFO] epoch=12.00 step=98700, 10.1415 examples/sec lr=0.000004, loss=10.6503, loss_ll=2.03016, loss_ll_paf=3.5275, loss_ll_heat=0.532825, q=1000
[2018-07-14 13:37:06,821] [train] [INFO] epoch=12.00 step=98800, 10.1417 examples/sec lr=0.000004, loss=15.2802, loss_ll=2.706, loss_ll_paf=4.79764, loss_ll_heat=0.61436, q=1000
[2018-07-14 13:39:38,794] [train] [INFO] epoch=12.00 step=98900, 10.1420 examples/sec lr=0.000004, loss=10.7905, loss_ll=1.64885, loss_ll_paf=2.89638, loss_ll_heat=0.401316, q=1000
[2018-07-14 13:42:13,960] [train] [INFO] epoch=13.00 step=99000, 10.1422 examples/sec lr=0.000004, loss=20.7597, loss_ll=3.50385, loss_ll_paf=6.37167, loss_ll_heat=0.636028, q=1000
[2018-07-14 13:45:05,880] [train] [INFO] epoch=13.00 step=99100, 10.1413 examples/sec lr=0.000004, loss=12.917, loss_ll=1.97209, loss_ll_paf=3.64595, loss_ll_heat=0.298225, q=1000
[2018-07-14 13:47:40,120] [train] [INFO] epoch=13.00 step=99200, 10.1415 examples/sec lr=0.000004, loss=22.2427, loss_ll=3.73214, loss_ll_paf=6.7254, loss_ll_heat=0.738868, q=1000
[2018-07-14 13:50:13,876] [train] [INFO] epoch=13.00 step=99300, 10.1418 examples/sec lr=0.000004, loss=37.1638, loss_ll=7.16037, loss_ll_paf=13.1551, loss_ll_heat=1.16566, q=1000
[2018-07-14 13:52:46,702] [train] [INFO] epoch=13.00 step=99400, 10.1421 examples/sec lr=0.000004, loss=25.8471, loss_ll=4.74526, loss_ll_paf=8.89375, loss_ll_heat=0.596776, q=1000
[2018-07-14 13:55:21,535] [train] [INFO] epoch=13.00 step=99500, 10.1423 examples/sec lr=0.000004, loss=21.3277, loss_ll=3.38307, loss_ll_paf=6.24534, loss_ll_heat=0.520794, q=1000
[2018-07-14 13:57:56,564] [train] [INFO] epoch=13.00 step=99600, 10.1425 examples/sec lr=0.000004, loss=17.0924, loss_ll=3.31532, loss_ll_paf=5.96846, loss_ll_heat=0.66218, q=1000
[2018-07-14 14:00:26,262] [train] [INFO] epoch=13.00 step=99700, 10.1430 examples/sec lr=0.000004, loss=19.8241, loss_ll=3.82723, loss_ll_paf=6.73459, loss_ll_heat=0.919868, q=1000
[2018-07-14 14:02:59,020] [train] [INFO] epoch=13.00 step=99800, 10.1433 examples/sec lr=0.000004, loss=41.1697, loss_ll=7.85902, loss_ll_paf=14.6561, loss_ll_heat=1.06197, q=1000
[2018-07-14 14:05:32,688] [train] [INFO] epoch=13.00 step=99900, 10.1436 examples/sec lr=0.000004, loss=8.23069, loss_ll=1.37566, loss_ll_paf=2.18895, loss_ll_heat=0.562375, q=1000
[2018-07-14 14:08:08,654] [train] [INFO] epoch=13.00 step=100000, 10.1437 examples/sec lr=0.000004, loss=24.7332, loss_ll=4.4112, loss_ll_paf=8.08301, loss_ll_heat=0.739379, q=1000
[2018-07-14 14:10:55,772] [train] [INFO] epoch=13.00 step=100100, 10.1431 examples/sec lr=0.000004, loss=17.369, loss_ll=3.2371, loss_ll_paf=6.08094, loss_ll_heat=0.393268, q=1000
[2018-07-14 14:13:30,784] [train] [INFO] epoch=13.00 step=100200, 10.1432 examples/sec lr=0.000004, loss=15.1601, loss_ll=2.50532, loss_ll_paf=4.56581, loss_ll_heat=0.444819, q=1000
[2018-07-14 14:16:02,369] [train] [INFO] epoch=13.00 step=100300, 10.1436 examples/sec lr=0.000004, loss=23.7622, loss_ll=4.11441, loss_ll_paf=7.48917, loss_ll_heat=0.739656, q=1000
[2018-07-14 14:18:34,134] [train] [INFO] epoch=13.00 step=100400, 10.1440 examples/sec lr=0.000004, loss=10.35, loss_ll=1.69928, loss_ll_paf=3.05945, loss_ll_heat=0.33912, q=1000
[2018-07-14 14:21:08,310] [train] [INFO] epoch=13.00 step=100500, 10.1442 examples/sec lr=0.000004, loss=12.0301, loss_ll=2.11491, loss_ll_paf=3.39901, loss_ll_heat=0.830803, q=1000
[2018-07-14 14:23:40,244] [train] [INFO] epoch=13.00 step=100600, 10.1446 examples/sec lr=0.000004, loss=13.0393, loss_ll=2.07283, loss_ll_paf=3.84967, loss_ll_heat=0.295991, q=1000
[2018-07-14 14:26:12,959] [train] [INFO] epoch=13.00 step=100700, 10.1449 examples/sec lr=0.000004, loss=9.73686, loss_ll=1.41967, loss_ll_paf=2.65312, loss_ll_heat=0.186215, q=1000
[2018-07-14 14:28:45,940] [train] [INFO] epoch=13.00 step=100800, 10.1452 examples/sec lr=0.000004, loss=16.0426, loss_ll=2.72981, loss_ll_paf=4.93703, loss_ll_heat=0.522587, q=1000
[2018-07-14 14:31:17,707] [train] [INFO] epoch=13.00 step=100900, 10.1456 examples/sec lr=0.000004, loss=23.9345, loss_ll=4.39462, loss_ll_paf=8.23765, loss_ll_heat=0.551597, q=1000
[2018-07-14 14:33:50,962] [train] [INFO] epoch=13.00 step=101000, 10.1459 examples/sec lr=0.000004, loss=30.2735, loss_ll=5.59085, loss_ll_paf=10.2635, loss_ll_heat=0.918173, q=1000
[2018-07-14 14:36:43,531] [train] [INFO] epoch=13.00 step=101100, 10.1450 examples/sec lr=0.000004, loss=17.4355, loss_ll=2.99469, loss_ll_paf=5.28583, loss_ll_heat=0.703554, q=1000
[2018-07-14 14:39:14,535] [train] [INFO] epoch=13.00 step=101200, 10.1454 examples/sec lr=0.000004, loss=23.3348, loss_ll=4.22236, loss_ll_paf=7.6877, loss_ll_heat=0.757017, q=1000
[2018-07-14 14:41:49,041] [train] [INFO] epoch=13.00 step=101300, 10.1456 examples/sec lr=0.000004, loss=10.959, loss_ll=1.9433, loss_ll_paf=3.4905, loss_ll_heat=0.396108, q=1000
[2018-07-14 14:44:17,051] [train] [INFO] epoch=13.00 step=101400, 10.1462 examples/sec lr=0.000004, loss=14.0226, loss_ll=2.6343, loss_ll_paf=4.73742, loss_ll_heat=0.531175, q=1000
[2018-07-14 14:46:53,133] [train] [INFO] epoch=13.00 step=101500, 10.1463 examples/sec lr=0.000004, loss=9.21059, loss_ll=1.52594, loss_ll_paf=2.63716, loss_ll_heat=0.414731, q=1000
[2018-07-14 14:49:28,736] [train] [INFO] epoch=13.00 step=101600, 10.1464 examples/sec lr=0.000004, loss=17.9901, loss_ll=3.19738, loss_ll_paf=5.92345, loss_ll_heat=0.471299, q=1000
[2018-07-14 14:52:04,619] [train] [INFO] epoch=13.00 step=101700, 10.1465 examples/sec lr=0.000004, loss=15.865, loss_ll=2.67009, loss_ll_paf=4.95249, loss_ll_heat=0.387698, q=1000
[2018-07-14 14:54:33,856] [train] [INFO] epoch=13.00 step=101800, 10.1471 examples/sec lr=0.000004, loss=32.2686, loss_ll=5.98899, loss_ll_paf=11.2736, loss_ll_heat=0.704411, q=1000
[2018-07-14 14:57:08,453] [train] [INFO] epoch=13.00 step=101900, 10.1473 examples/sec lr=0.000004, loss=11.9095, loss_ll=1.93713, loss_ll_paf=3.33804, loss_ll_heat=0.536216, q=1000
[2018-07-14 14:59:46,128] [train] [INFO] epoch=13.00 step=102000, 10.1473 examples/sec lr=0.000004, loss=32.956, loss_ll=6.11932, loss_ll_paf=11.7273, loss_ll_heat=0.51133, q=1000
[2018-07-14 15:02:36,647] [train] [INFO] epoch=13.00 step=102100, 10.1465 examples/sec lr=0.000004, loss=14.8781, loss_ll=2.14971, loss_ll_paf=3.78601, loss_ll_heat=0.513419, q=1000
[2018-07-14 15:05:13,773] [train] [INFO] epoch=13.00 step=102200, 10.1465 examples/sec lr=0.000004, loss=24.6131, loss_ll=4.37758, loss_ll_paf=8.11109, loss_ll_heat=0.644065, q=1000
[2018-07-14 15:07:51,636] [train] [INFO] epoch=13.00 step=102300, 10.1465 examples/sec lr=0.000004, loss=5.32487, loss_ll=0.99805, loss_ll_paf=1.62718, loss_ll_heat=0.368924, q=1000
[2018-07-14 15:10:30,153] [train] [INFO] epoch=13.00 step=102400, 10.1464 examples/sec lr=0.000004, loss=24.0673, loss_ll=4.21328, loss_ll_paf=7.73665, loss_ll_heat=0.689913, q=1000
[2018-07-14 15:13:04,997] [train] [INFO] epoch=13.00 step=102500, 10.1466 examples/sec lr=0.000004, loss=25.1986, loss_ll=4.53837, loss_ll_paf=8.4922, loss_ll_heat=0.584536, q=1000
[2018-07-14 15:15:40,344] [train] [INFO] epoch=13.00 step=102600, 10.1468 examples/sec lr=0.000004, loss=18.5755, loss_ll=3.22089, loss_ll_paf=5.73279, loss_ll_heat=0.708987, q=1000
[2018-07-14 15:18:19,901] [train] [INFO] epoch=13.00 step=102700, 10.1467 examples/sec lr=0.000004, loss=7.53869, loss_ll=1.3269, loss_ll_paf=2.1422, loss_ll_heat=0.511592, q=1000
[2018-07-14 15:20:50,435] [train] [INFO] epoch=13.00 step=102800, 10.1471 examples/sec lr=0.000004, loss=13.8541, loss_ll=2.48222, loss_ll_paf=3.97488, loss_ll_heat=0.989562, q=1000
[2018-07-14 15:23:28,273] [train] [INFO] epoch=13.00 step=102900, 10.1471 examples/sec lr=0.000004, loss=16.4221, loss_ll=2.62219, loss_ll_paf=4.71796, loss_ll_heat=0.526411, q=1000
[2018-07-14 15:26:02,743] [train] [INFO] epoch=13.00 step=103000, 10.1473 examples/sec lr=0.000004, loss=20.626, loss_ll=3.0718, loss_ll_paf=5.42812, loss_ll_heat=0.715476, q=1000
[2018-07-14 15:28:53,198] [train] [INFO] epoch=13.00 step=103100, 10.1465 examples/sec lr=0.000004, loss=10.6545, loss_ll=1.94903, loss_ll_paf=3.65555, loss_ll_heat=0.242504, q=1000
[2018-07-14 15:31:25,985] [train] [INFO] epoch=13.00 step=103200, 10.1468 examples/sec lr=0.000004, loss=11.2635, loss_ll=1.94297, loss_ll_paf=3.62263, loss_ll_heat=0.263305, q=1000
[2018-07-14 15:34:04,057] [train] [INFO] epoch=13.00 step=103300, 10.1468 examples/sec lr=0.000004, loss=21.1668, loss_ll=3.46429, loss_ll_paf=6.00512, loss_ll_heat=0.923459, q=1000
[2018-07-14 15:36:39,529] [train] [INFO] epoch=13.00 step=103400, 10.1469 examples/sec lr=0.000004, loss=9.23822, loss_ll=1.6161, loss_ll_paf=2.84069, loss_ll_heat=0.391509, q=1000
[2018-07-14 15:39:15,202] [train] [INFO] epoch=13.00 step=103500, 10.1470 examples/sec lr=0.000004, loss=12.3613, loss_ll=1.53887, loss_ll_paf=2.65216, loss_ll_heat=0.425581, q=1000
[2018-07-14 15:41:49,250] [train] [INFO] epoch=13.00 step=103600, 10.1473 examples/sec lr=0.000004, loss=13.899, loss_ll=2.57689, loss_ll_paf=4.36673, loss_ll_heat=0.787062, q=1000
[2018-07-14 15:44:24,416] [train] [INFO] epoch=13.00 step=103700, 10.1474 examples/sec lr=0.000004, loss=13.2578, loss_ll=2.25994, loss_ll_paf=4.02609, loss_ll_heat=0.493789, q=1000
[2018-07-14 15:46:59,336] [train] [INFO] epoch=13.00 step=103800, 10.1476 examples/sec lr=0.000004, loss=9.9138, loss_ll=1.68015, loss_ll_paf=3.08684, loss_ll_heat=0.273456, q=1000
[2018-07-14 15:49:30,065] [train] [INFO] epoch=13.00 step=103900, 10.1480 examples/sec lr=0.000004, loss=18.1294, loss_ll=3.45479, loss_ll_paf=5.88422, loss_ll_heat=1.02537, q=1000
[2018-07-14 15:52:00,168] [train] [INFO] epoch=13.00 step=104000, 10.1485 examples/sec lr=0.000004, loss=17.5956, loss_ll=3.08347, loss_ll_paf=5.27298, loss_ll_heat=0.893969, q=1000
[2018-07-14 15:54:47,780] [train] [INFO] epoch=13.00 step=104100, 10.1479 examples/sec lr=0.000004, loss=19.4933, loss_ll=3.11998, loss_ll_paf=5.43912, loss_ll_heat=0.800853, q=1000
[2018-07-14 15:57:23,815] [train] [INFO] epoch=13.00 step=104200, 10.1480 examples/sec lr=0.000004, loss=18.5524, loss_ll=3.32075, loss_ll_paf=5.91321, loss_ll_heat=0.728288, q=1000
[2018-07-14 15:59:55,409] [train] [INFO] epoch=13.00 step=104300, 10.1483 examples/sec lr=0.000004, loss=15.7251, loss_ll=2.82198, loss_ll_paf=5.0949, loss_ll_heat=0.54906, q=1000
[2018-07-14 16:02:25,652] [train] [INFO] epoch=13.00 step=104400, 10.1488 examples/sec lr=0.000004, loss=25.6435, loss_ll=4.83781, loss_ll_paf=9.03384, loss_ll_heat=0.641781, q=1000
[2018-07-14 16:05:01,002] [train] [INFO] epoch=13.00 step=104500, 10.1489 examples/sec lr=0.000004, loss=27.7189, loss_ll=4.62801, loss_ll_paf=8.75954, loss_ll_heat=0.496477, q=1000
[2018-07-14 16:07:33,209] [train] [INFO] epoch=13.00 step=104600, 10.1493 examples/sec lr=0.000004, loss=24.977, loss_ll=4.65062, loss_ll_paf=8.78606, loss_ll_heat=0.515184, q=1000
[2018-07-14 16:10:06,196] [train] [INFO] epoch=13.00 step=104700, 10.1496 examples/sec lr=0.000004, loss=23.5511, loss_ll=4.50261, loss_ll_paf=8.48299, loss_ll_heat=0.52223, q=1000
[2018-07-14 16:12:43,697] [train] [INFO] epoch=13.00 step=104800, 10.1496 examples/sec lr=0.000004, loss=11.6804, loss_ll=1.87834, loss_ll_paf=3.2874, loss_ll_heat=0.469281, q=1000
[2018-07-14 16:15:15,445] [train] [INFO] epoch=13.00 step=104900, 10.1499 examples/sec lr=0.000004, loss=16.8867, loss_ll=3.23353, loss_ll_paf=5.60967, loss_ll_heat=0.85739, q=1000
[2018-07-14 16:17:48,030] [train] [INFO] epoch=13.00 step=105000, 10.1502 examples/sec lr=0.000004, loss=11.099, loss_ll=1.85469, loss_ll_paf=2.93458, loss_ll_heat=0.774811, q=1000
[2018-07-14 16:20:37,450] [train] [INFO] epoch=13.00 step=105100, 10.1495 examples/sec lr=0.000004, loss=16.2675, loss_ll=2.89987, loss_ll_paf=5.01364, loss_ll_heat=0.786099, q=1000
[2018-07-14 16:23:07,187] [train] [INFO] epoch=13.00 step=105200, 10.1500 examples/sec lr=0.000004, loss=14.1514, loss_ll=2.61388, loss_ll_paf=4.18357, loss_ll_heat=1.0442, q=1000
[2018-07-14 16:25:40,699] [train] [INFO] epoch=13.00 step=105300, 10.1503 examples/sec lr=0.000004, loss=9.30547, loss_ll=1.5342, loss_ll_paf=2.7706, loss_ll_heat=0.297807, q=1000
[2018-07-14 16:28:14,515] [train] [INFO] epoch=13.00 step=105400, 10.1505 examples/sec lr=0.000004, loss=12.5366, loss_ll=2.07547, loss_ll_paf=3.15414, loss_ll_heat=0.996796, q=1000
[2018-07-14 16:30:51,585] [train] [INFO] epoch=13.00 step=105500, 10.1505 examples/sec lr=0.000004, loss=15.0873, loss_ll=2.63192, loss_ll_paf=4.52808, loss_ll_heat=0.735759, q=1000
[2018-07-14 16:33:22,581] [train] [INFO] epoch=13.00 step=105600, 10.1509 examples/sec lr=0.000004, loss=9.25522, loss_ll=1.55183, loss_ll_paf=2.74282, loss_ll_heat=0.360838, q=1000
[2018-07-14 16:35:57,041] [train] [INFO] epoch=13.00 step=105700, 10.1511 examples/sec lr=0.000004, loss=15.917, loss_ll=2.70384, loss_ll_paf=4.51384, loss_ll_heat=0.893846, q=1000
[2018-07-14 16:38:31,558] [train] [INFO] epoch=13.00 step=105800, 10.1513 examples/sec lr=0.000004, loss=16.1495, loss_ll=2.91387, loss_ll_paf=5.02889, loss_ll_heat=0.798859, q=1000
[2018-07-14 16:41:04,983] [train] [INFO] epoch=13.00 step=105900, 10.1516 examples/sec lr=0.000004, loss=20.5889, loss_ll=3.57386, loss_ll_paf=6.47553, loss_ll_heat=0.672191, q=1000
[2018-07-14 16:43:36,868] [train] [INFO] epoch=13.00 step=106000, 10.1519 examples/sec lr=0.000004, loss=18.1297, loss_ll=3.40392, loss_ll_paf=6.26733, loss_ll_heat=0.540519, q=1000
[2018-07-14 16:46:22,138] [train] [INFO] epoch=13.00 step=106100, 10.1515 examples/sec lr=0.000004, loss=27.7662, loss_ll=4.89073, loss_ll_paf=9.19038, loss_ll_heat=0.591073, q=1000
[2018-07-14 16:48:56,053] [train] [INFO] epoch=13.00 step=106200, 10.1517 examples/sec lr=0.000004, loss=19.3922, loss_ll=3.62726, loss_ll_paf=6.56698, loss_ll_heat=0.687548, q=1000
[2018-07-14 16:51:29,376] [train] [INFO] epoch=13.00 step=106300, 10.1519 examples/sec lr=0.000004, loss=20.3054, loss_ll=3.02467, loss_ll_paf=5.57776, loss_ll_heat=0.47157, q=1000
[2018-07-14 16:54:04,298] [train] [INFO] epoch=13.00 step=106400, 10.1521 examples/sec lr=0.000004, loss=24.1057, loss_ll=4.30513, loss_ll_paf=7.89401, loss_ll_heat=0.716246, q=1000
[2018-07-14 16:56:35,347] [train] [INFO] epoch=13.00 step=106500, 10.1525 examples/sec lr=0.000004, loss=12.1755, loss_ll=1.98544, loss_ll_paf=3.3571, loss_ll_heat=0.613769, q=1000
[2018-07-14 16:59:10,724] [train] [INFO] epoch=14.00 step=106600, 10.1526 examples/sec lr=0.000004, loss=22.5711, loss_ll=4.38363, loss_ll_paf=8.03249, loss_ll_heat=0.734773, q=1000
[2018-07-14 17:01:41,201] [train] [INFO] epoch=14.00 step=106700, 10.1531 examples/sec lr=0.000004, loss=20.8972, loss_ll=3.83425, loss_ll_paf=7.26023, loss_ll_heat=0.408269, q=1000
[2018-07-14 17:04:18,384] [train] [INFO] epoch=14.00 step=106800, 10.1531 examples/sec lr=0.000004, loss=16.8807, loss_ll=3.10658, loss_ll_paf=5.53475, loss_ll_heat=0.678413, q=1000
[2018-07-14 17:06:49,012] [train] [INFO] epoch=14.00 step=106900, 10.1535 examples/sec lr=0.000004, loss=17.1224, loss_ll=3.25454, loss_ll_paf=6.08979, loss_ll_heat=0.41929, q=1000
[2018-07-14 17:09:19,607] [train] [INFO] epoch=14.00 step=107000, 10.1539 examples/sec lr=0.000004, loss=16.1459, loss_ll=2.92578, loss_ll_paf=5.15185, loss_ll_heat=0.699713, q=1000
[2018-07-14 17:12:08,642] [train] [INFO] epoch=14.00 step=107100, 10.1532 examples/sec lr=0.000004, loss=13.9703, loss_ll=2.48723, loss_ll_paf=4.31185, loss_ll_heat=0.662614, q=1000
[2018-07-14 17:14:42,078] [train] [INFO] epoch=14.00 step=107200, 10.1535 examples/sec lr=0.000004, loss=11.0336, loss_ll=1.95753, loss_ll_paf=3.26445, loss_ll_heat=0.650609, q=1000
[2018-07-14 17:17:17,959] [train] [INFO] epoch=14.00 step=107300, 10.1536 examples/sec lr=0.000004, loss=15.8197, loss_ll=2.75382, loss_ll_paf=4.81471, loss_ll_heat=0.692928, q=1000
[2018-07-14 17:19:49,383] [train] [INFO] epoch=14.00 step=107400, 10.1540 examples/sec lr=0.000004, loss=17.2246, loss_ll=3.03028, loss_ll_paf=5.48092, loss_ll_heat=0.579646, q=1000
[2018-07-14 17:22:21,027] [train] [INFO] epoch=14.00 step=107500, 10.1543 examples/sec lr=0.000004, loss=10.8228, loss_ll=1.79535, loss_ll_paf=3.07238, loss_ll_heat=0.518328, q=1000
[2018-07-14 17:24:55,497] [train] [INFO] epoch=14.00 step=107600, 10.1545 examples/sec lr=0.000004, loss=26.8621, loss_ll=4.39956, loss_ll_paf=8.13673, loss_ll_heat=0.662392, q=1000
[2018-07-14 17:27:27,014] [train] [INFO] epoch=14.00 step=107700, 10.1549 examples/sec lr=0.000004, loss=25.2771, loss_ll=4.26751, loss_ll_paf=7.72145, loss_ll_heat=0.813583, q=1000
[2018-07-14 17:30:01,049] [train] [INFO] epoch=14.00 step=107800, 10.1551 examples/sec lr=0.000004, loss=13.9039, loss_ll=2.10094, loss_ll_paf=3.59678, loss_ll_heat=0.60511, q=1000
[2018-07-14 17:32:34,939] [train] [INFO] epoch=14.00 step=107900, 10.1553 examples/sec lr=0.000004, loss=8.72717, loss_ll=1.57711, loss_ll_paf=2.55478, loss_ll_heat=0.599451, q=1000
[2018-07-14 17:35:10,597] [train] [INFO] epoch=14.00 step=108000, 10.1554 examples/sec lr=0.000004, loss=11.0136, loss_ll=1.79139, loss_ll_paf=3.20229, loss_ll_heat=0.380482, q=1000
[2018-07-14 17:37:54,061] [train] [INFO] epoch=14.00 step=108100, 10.1550 examples/sec lr=0.000004, loss=12.6116, loss_ll=2.13096, loss_ll_paf=3.59853, loss_ll_heat=0.663389, q=1000
[2018-07-14 17:40:27,214] [train] [INFO] epoch=14.00 step=108200, 10.1553 examples/sec lr=0.000004, loss=15.505, loss_ll=2.5881, loss_ll_paf=4.87066, loss_ll_heat=0.305547, q=1000
[2018-07-14 17:42:58,884] [train] [INFO] epoch=14.00 step=108300, 10.1557 examples/sec lr=0.000004, loss=32.5504, loss_ll=6.02093, loss_ll_paf=11.4442, loss_ll_heat=0.597631, q=1000
[2018-07-14 17:45:30,820] [train] [INFO] epoch=14.00 step=108400, 10.1560 examples/sec lr=0.000004, loss=15.883, loss_ll=2.99333, loss_ll_paf=5.48721, loss_ll_heat=0.499453, q=1000
[2018-07-14 17:48:04,679] [train] [INFO] epoch=14.00 step=108500, 10.1562 examples/sec lr=0.000004, loss=17.48, loss_ll=3.15679, loss_ll_paf=5.61382, loss_ll_heat=0.699767, q=1000
[2018-07-14 17:50:37,942] [train] [INFO] epoch=14.00 step=108600, 10.1565 examples/sec lr=0.000004, loss=12.9162, loss_ll=2.15237, loss_ll_paf=3.88067, loss_ll_heat=0.424069, q=1000
[2018-07-14 17:53:08,554] [train] [INFO] epoch=14.00 step=108700, 10.1569 examples/sec lr=0.000004, loss=17.9269, loss_ll=2.89476, loss_ll_paf=5.27286, loss_ll_heat=0.516657, q=1000
[2018-07-14 17:55:42,809] [train] [INFO] epoch=14.00 step=108800, 10.1571 examples/sec lr=0.000004, loss=12.7066, loss_ll=1.91302, loss_ll_paf=3.24741, loss_ll_heat=0.578628, q=1000
[2018-07-14 17:58:15,897] [train] [INFO] epoch=14.00 step=108900, 10.1573 examples/sec lr=0.000004, loss=41.7802, loss_ll=7.55999, loss_ll_paf=14.1811, loss_ll_heat=0.938881, q=1000
[2018-07-14 18:00:47,926] [train] [INFO] epoch=14.00 step=109000, 10.1577 examples/sec lr=0.000004, loss=11.8371, loss_ll=2.03541, loss_ll_paf=3.80919, loss_ll_heat=0.261632, q=1000
[2018-07-14 18:03:35,487] [train] [INFO] epoch=14.00 step=109100, 10.1571 examples/sec lr=0.000004, loss=36.3508, loss_ll=6.90727, loss_ll_paf=12.994, loss_ll_heat=0.820553, q=1000
[2018-07-14 18:06:09,039] [train] [INFO] epoch=14.00 step=109200, 10.1573 examples/sec lr=0.000004, loss=30.1196, loss_ll=5.81389, loss_ll_paf=10.4403, loss_ll_heat=1.1875, q=1000
[2018-07-14 18:08:41,860] [train] [INFO] epoch=14.00 step=109300, 10.1576 examples/sec lr=0.000004, loss=10.7361, loss_ll=1.67891, loss_ll_paf=2.75103, loss_ll_heat=0.606787, q=1000
[2018-07-14 18:11:18,440] [train] [INFO] epoch=14.00 step=109400, 10.1576 examples/sec lr=0.000004, loss=21.434, loss_ll=3.63957, loss_ll_paf=6.66458, loss_ll_heat=0.614549, q=1000
[2018-07-14 18:13:54,936] [train] [INFO] epoch=14.00 step=109500, 10.1577 examples/sec lr=0.000004, loss=16.038, loss_ll=2.92778, loss_ll_paf=5.17039, loss_ll_heat=0.685167, q=1000
[2018-07-14 18:16:32,114] [train] [INFO] epoch=14.00 step=109600, 10.1577 examples/sec lr=0.000004, loss=16.0985, loss_ll=2.84697, loss_ll_paf=5.10605, loss_ll_heat=0.587892, q=1000
[2018-07-14 18:19:07,789] [train] [INFO] epoch=14.00 step=109700, 10.1578 examples/sec lr=0.000004, loss=12.7477, loss_ll=2.27697, loss_ll_paf=4.13928, loss_ll_heat=0.414662, q=1000
[2018-07-14 18:21:44,441] [train] [INFO] epoch=14.00 step=109800, 10.1579 examples/sec lr=0.000004, loss=19.3136, loss_ll=3.67231, loss_ll_paf=6.6487, loss_ll_heat=0.695916, q=1000
[2018-07-14 18:24:19,743] [train] [INFO] epoch=14.00 step=109900, 10.1580 examples/sec lr=0.000004, loss=24.7069, loss_ll=4.87968, loss_ll_paf=8.79117, loss_ll_heat=0.968197, q=1000
[2018-07-14 18:26:51,069] [train] [INFO] epoch=14.00 step=110000, 10.1584 examples/sec lr=0.000004, loss=16.7239, loss_ll=3.10294, loss_ll_paf=5.73266, loss_ll_heat=0.47322, q=1000
[2018-07-14 18:29:44,797] [train] [INFO] epoch=14.00 step=110100, 10.1574 examples/sec lr=0.000004, loss=20.5084, loss_ll=3.85266, loss_ll_paf=7.13862, loss_ll_heat=0.566705, q=1000
[2018-07-14 18:32:17,501] [train] [INFO] epoch=14.00 step=110200, 10.1577 examples/sec lr=0.000004, loss=16.9963, loss_ll=2.88003, loss_ll_paf=5.16637, loss_ll_heat=0.593691, q=1000
[2018-07-14 18:34:53,427] [train] [INFO] epoch=14.00 step=110300, 10.1578 examples/sec lr=0.000004, loss=11.4851, loss_ll=2.02436, loss_ll_paf=3.38396, loss_ll_heat=0.664757, q=1000
[2018-07-14 18:37:24,492] [train] [INFO] epoch=14.00 step=110400, 10.1582 examples/sec lr=0.000004, loss=30.4184, loss_ll=5.52477, loss_ll_paf=10.379, loss_ll_heat=0.670534, q=1000
[2018-07-14 18:40:01,205] [train] [INFO] epoch=14.00 step=110500, 10.1582 examples/sec lr=0.000004, loss=14.721, loss_ll=2.64631, loss_ll_paf=4.83688, loss_ll_heat=0.455733, q=1000
[2018-07-14 18:42:32,165] [train] [INFO] epoch=14.00 step=110600, 10.1586 examples/sec lr=0.000004, loss=20.4506, loss_ll=3.90417, loss_ll_paf=7.50383, loss_ll_heat=0.304511, q=1000
[2018-07-14 18:45:07,394] [train] [INFO] epoch=14.00 step=110700, 10.1587 examples/sec lr=0.000004, loss=13.7406, loss_ll=2.44444, loss_ll_paf=4.32224, loss_ll_heat=0.566652, q=1000
[2018-07-14 18:47:40,379] [train] [INFO] epoch=14.00 step=110800, 10.1590 examples/sec lr=0.000004, loss=28.6394, loss_ll=5.37844, loss_ll_paf=10.4542, loss_ll_heat=0.302697, q=1000
[2018-07-14 18:50:16,427] [train] [INFO] epoch=14.00 step=110900, 10.1591 examples/sec lr=0.000004, loss=12.9148, loss_ll=2.02129, loss_ll_paf=3.54912, loss_ll_heat=0.493469, q=1000
[2018-07-14 18:52:49,070] [train] [INFO] epoch=14.00 step=111000, 10.1594 examples/sec lr=0.000004, loss=41.2136, loss_ll=7.32142, loss_ll_paf=13.6813, loss_ll_heat=0.961578, q=1000
[2018-07-14 18:55:38,509] [train] [INFO] epoch=14.00 step=111100, 10.1587 examples/sec lr=0.000004, loss=14.652, loss_ll=2.38902, loss_ll_paf=4.2371, loss_ll_heat=0.540946, q=1000
[2018-07-14 18:58:15,095] [train] [INFO] epoch=14.00 step=111200, 10.1587 examples/sec lr=0.000004, loss=18.2686, loss_ll=3.4007, loss_ll_paf=6.21305, loss_ll_heat=0.588344, q=1000
[2018-07-14 19:00:45,794] [train] [INFO] epoch=14.00 step=111300, 10.1591 examples/sec lr=0.000004, loss=17.4819, loss_ll=3.18347, loss_ll_paf=5.89331, loss_ll_heat=0.473637, q=1000
[2018-07-14 19:03:23,325] [train] [INFO] epoch=14.00 step=111400, 10.1591 examples/sec lr=0.000004, loss=11.4975, loss_ll=1.94806, loss_ll_paf=3.46429, loss_ll_heat=0.431828, q=1000
[2018-07-14 19:05:54,256] [train] [INFO] epoch=14.00 step=111500, 10.1595 examples/sec lr=0.000004, loss=11.701, loss_ll=2.10614, loss_ll_paf=3.5025, loss_ll_heat=0.709776, q=1000
[2018-07-14 19:08:30,209] [train] [INFO] epoch=14.00 step=111600, 10.1596 examples/sec lr=0.000004, loss=16.9569, loss_ll=2.88602, loss_ll_paf=5.34202, loss_ll_heat=0.430033, q=1000
[2018-07-14 19:11:00,383] [train] [INFO] epoch=14.00 step=111700, 10.1600 examples/sec lr=0.000004, loss=9.53665, loss_ll=1.38734, loss_ll_paf=2.28803, loss_ll_heat=0.48665, q=1000
[2018-07-14 19:13:37,012] [train] [INFO] epoch=14.00 step=111800, 10.1600 examples/sec lr=0.000004, loss=12.8485, loss_ll=2.20393, loss_ll_paf=3.8089, loss_ll_heat=0.598967, q=1000
[2018-07-14 19:16:10,667] [train] [INFO] epoch=14.00 step=111900, 10.1603 examples/sec lr=0.000004, loss=20.4726, loss_ll=3.95385, loss_ll_paf=7.34907, loss_ll_heat=0.558634, q=1000
[2018-07-14 19:18:43,797] [train] [INFO] epoch=14.00 step=112000, 10.1605 examples/sec lr=0.000004, loss=10.137, loss_ll=1.7777, loss_ll_paf=3.17899, loss_ll_heat=0.376414, q=1000
[2018-07-14 19:21:36,147] [train] [INFO] epoch=14.00 step=112100, 10.1597 examples/sec lr=0.000004, loss=17.8987, loss_ll=3.26681, loss_ll_paf=5.90478, loss_ll_heat=0.62884, q=1000
[2018-07-14 19:24:07,364] [train] [INFO] epoch=14.00 step=112200, 10.1600 examples/sec lr=0.000004, loss=52.0021, loss_ll=10.4262, loss_ll_paf=19.7746, loss_ll_heat=1.07782, q=1000
[2018-07-14 19:26:38,874] [train] [INFO] epoch=14.00 step=112300, 10.1604 examples/sec lr=0.000004, loss=8.96726, loss_ll=1.68793, loss_ll_paf=3.09346, loss_ll_heat=0.282397, q=1000
[2018-07-14 19:29:12,850] [train] [INFO] epoch=14.00 step=112400, 10.1606 examples/sec lr=0.000004, loss=11.4301, loss_ll=2.15921, loss_ll_paf=4.02098, loss_ll_heat=0.297454, q=1000
[2018-07-14 19:31:49,648] [train] [INFO] epoch=14.00 step=112500, 10.1606 examples/sec lr=0.000004, loss=16.3998, loss_ll=2.99216, loss_ll_paf=5.39629, loss_ll_heat=0.588024, q=1000
[2018-07-14 19:34:20,208] [train] [INFO] epoch=14.00 step=112600, 10.1610 examples/sec lr=0.000004, loss=11.1838, loss_ll=2.16729, loss_ll_paf=3.49151, loss_ll_heat=0.843071, q=1000
[2018-07-14 19:36:52,542] [train] [INFO] epoch=14.00 step=112700, 10.1613 examples/sec lr=0.000004, loss=17.5176, loss_ll=2.96134, loss_ll_paf=5.50704, loss_ll_heat=0.415648, q=1000
[2018-07-14 19:39:26,364] [train] [INFO] epoch=14.00 step=112800, 10.1615 examples/sec lr=0.000004, loss=17.4817, loss_ll=2.71841, loss_ll_paf=5.1191, loss_ll_heat=0.31771, q=1000
[2018-07-14 19:42:02,746] [train] [INFO] epoch=14.00 step=112900, 10.1616 examples/sec lr=0.000004, loss=23.0517, loss_ll=4.0853, loss_ll_paf=7.35634, loss_ll_heat=0.81426, q=1000
[2018-07-14 19:44:38,441] [train] [INFO] epoch=14.00 step=113000, 10.1617 examples/sec lr=0.000004, loss=20.7347, loss_ll=2.92135, loss_ll_paf=5.41763, loss_ll_heat=0.425075, q=1000
[2018-07-14 19:47:28,728] [train] [INFO] epoch=14.00 step=113100, 10.1609 examples/sec lr=0.000004, loss=25.5922, loss_ll=4.67401, loss_ll_paf=8.65788, loss_ll_heat=0.690142, q=1000
[2018-07-14 19:50:00,765] [train] [INFO] epoch=14.00 step=113200, 10.1612 examples/sec lr=0.000004, loss=20.6462, loss_ll=3.52655, loss_ll_paf=6.44918, loss_ll_heat=0.603925, q=1000
[2018-07-14 19:52:34,084] [train] [INFO] epoch=14.00 step=113300, 10.1615 examples/sec lr=0.000004, loss=19.5665, loss_ll=3.31427, loss_ll_paf=6.20872, loss_ll_heat=0.419817, q=1000
[2018-07-14 19:55:07,060] [train] [INFO] epoch=14.00 step=113400, 10.1617 examples/sec lr=0.000004, loss=15.0723, loss_ll=2.41712, loss_ll_paf=4.45341, loss_ll_heat=0.380822, q=1000
[2018-07-14 19:57:42,548] [train] [INFO] epoch=14.00 step=113500, 10.1618 examples/sec lr=0.000004, loss=5.0868, loss_ll=0.913775, loss_ll_paf=1.51433, loss_ll_heat=0.313222, q=1000
[2018-07-14 20:00:17,161] [train] [INFO] epoch=14.00 step=113600, 10.1620 examples/sec lr=0.000004, loss=13.0247, loss_ll=2.25605, loss_ll_paf=4.20863, loss_ll_heat=0.303463, q=1000
[2018-07-14 20:02:48,527] [train] [INFO] epoch=14.00 step=113700, 10.1624 examples/sec lr=0.000004, loss=16.7458, loss_ll=3.1284, loss_ll_paf=5.54047, loss_ll_heat=0.716319, q=1000
[2018-07-14 20:05:20,801] [train] [INFO] epoch=14.00 step=113800, 10.1626 examples/sec lr=0.000004, loss=12.831, loss_ll=2.31644, loss_ll_paf=3.62735, loss_ll_heat=1.00552, q=1000
[2018-07-14 20:07:55,689] [train] [INFO] epoch=14.00 step=113900, 10.1628 examples/sec lr=0.000004, loss=19.8257, loss_ll=3.68748, loss_ll_paf=7.10971, loss_ll_heat=0.26525, q=1000
[2018-07-14 20:10:30,561] [train] [INFO] epoch=14.00 step=114000, 10.1629 examples/sec lr=0.000004, loss=26.4807, loss_ll=4.87979, loss_ll_paf=9.1489, loss_ll_heat=0.610674, q=1000
[2018-07-14 20:13:24,483] [train] [INFO] epoch=14.00 step=114100, 10.1620 examples/sec lr=0.000004, loss=10.1666, loss_ll=1.85159, loss_ll_paf=3.02012, loss_ll_heat=0.683066, q=1000
[2018-07-14 20:16:03,471] [train] [INFO] epoch=15.00 step=114200, 10.1619 examples/sec lr=0.000004, loss=8.18785, loss_ll=1.42062, loss_ll_paf=2.32861, loss_ll_heat=0.512621, q=1000
[2018-07-14 20:18:42,152] [train] [INFO] epoch=15.00 step=114300, 10.1618 examples/sec lr=0.000004, loss=14.5219, loss_ll=2.66292, loss_ll_paf=4.87846, loss_ll_heat=0.447376, q=1000
[2018-07-14 20:21:14,773] [train] [INFO] epoch=15.00 step=114400, 10.1621 examples/sec lr=0.000004, loss=11.9068, loss_ll=1.84345, loss_ll_paf=3.22801, loss_ll_heat=0.458892, q=1000
[2018-07-14 20:23:51,191] [train] [INFO] epoch=15.00 step=114500, 10.1622 examples/sec lr=0.000004, loss=9.25285, loss_ll=1.47145, loss_ll_paf=2.33271, loss_ll_heat=0.610181, q=1000
[2018-07-14 20:26:25,961] [train] [INFO] epoch=15.00 step=114600, 10.1623 examples/sec lr=0.000004, loss=9.69781, loss_ll=1.83871, loss_ll_paf=3.25323, loss_ll_heat=0.424199, q=1000
[2018-07-14 20:29:00,369] [train] [INFO] epoch=15.00 step=114700, 10.1625 examples/sec lr=0.000004, loss=23.9699, loss_ll=4.28307, loss_ll_paf=7.88606, loss_ll_heat=0.680079, q=1000
[2018-07-14 20:31:33,796] [train] [INFO] epoch=15.00 step=114800, 10.1627 examples/sec lr=0.000004, loss=13.3129, loss_ll=2.23188, loss_ll_paf=3.99082, loss_ll_heat=0.472942, q=1000
[2018-07-14 20:34:10,977] [train] [INFO] epoch=15.00 step=114900, 10.1627 examples/sec lr=0.000004, loss=15.0105, loss_ll=2.66324, loss_ll_paf=4.7884, loss_ll_heat=0.538081, q=1000
[2018-07-14 20:36:43,594] [train] [INFO] epoch=15.00 step=115000, 10.1630 examples/sec lr=0.000004, loss=15.5073, loss_ll=2.20908, loss_ll_paf=3.86343, loss_ll_heat=0.554736, q=1000
[2018-07-14 20:39:36,699] [train] [INFO] epoch=15.00 step=115100, 10.1621 examples/sec lr=0.000004, loss=14.7163, loss_ll=2.41974, loss_ll_paf=4.0644, loss_ll_heat=0.775083, q=1000
[2018-07-14 20:42:09,601] [train] [INFO] epoch=15.00 step=115200, 10.1624 examples/sec lr=0.000004, loss=7.61215, loss_ll=1.31901, loss_ll_paf=2.27211, loss_ll_heat=0.365908, q=1000
[2018-07-14 20:44:43,270] [train] [INFO] epoch=15.00 step=115300, 10.1626 examples/sec lr=0.000004, loss=13.0431, loss_ll=2.51543, loss_ll_paf=4.48038, loss_ll_heat=0.550483, q=1000
[2018-07-14 20:47:17,378] [train] [INFO] epoch=15.00 step=115400, 10.1628 examples/sec lr=0.000004, loss=23.8922, loss_ll=4.3337, loss_ll_paf=8.12501, loss_ll_heat=0.542382, q=1000
[2018-07-14 20:49:51,256] [train] [INFO] epoch=15.00 step=115500, 10.1630 examples/sec lr=0.000004, loss=17.1031, loss_ll=3.10615, loss_ll_paf=5.47509, loss_ll_heat=0.7372, q=1000
[2018-07-14 20:52:25,639] [train] [INFO] epoch=15.00 step=115600, 10.1632 examples/sec lr=0.000004, loss=8.20319, loss_ll=1.3473, loss_ll_paf=2.31841, loss_ll_heat=0.37619, q=1000
[2018-07-14 20:55:00,980] [train] [INFO] epoch=15.00 step=115700, 10.1633 examples/sec lr=0.000004, loss=16.7662, loss_ll=2.91086, loss_ll_paf=5.4451, loss_ll_heat=0.376623, q=1000
[2018-07-14 20:57:32,465] [train] [INFO] epoch=15.00 step=115800, 10.1636 examples/sec lr=0.000004, loss=9.45232, loss_ll=1.538, loss_ll_paf=2.75103, loss_ll_heat=0.324972, q=1000
[2018-07-14 21:00:04,552] [train] [INFO] epoch=15.00 step=115900, 10.1639 examples/sec lr=0.000004, loss=11.1078, loss_ll=1.91364, loss_ll_paf=3.25878, loss_ll_heat=0.568502, q=1000
[2018-07-14 21:02:38,849] [train] [INFO] epoch=15.00 step=116000, 10.1641 examples/sec lr=0.000004, loss=11.5705, loss_ll=1.94004, loss_ll_paf=3.64329, loss_ll_heat=0.236798, q=1000
[2018-07-14 21:05:29,785] [train] [INFO] epoch=15.00 step=116100, 10.1633 examples/sec lr=0.000004, loss=22.5561, loss_ll=4.35945, loss_ll_paf=8.29456, loss_ll_heat=0.424329, q=1000
[2018-07-14 21:08:03,413] [train] [INFO] epoch=15.00 step=116200, 10.1635 examples/sec lr=0.000004, loss=23.3964, loss_ll=4.1344, loss_ll_paf=7.75633, loss_ll_heat=0.512476, q=1000
[2018-07-14 21:10:39,254] [train] [INFO] epoch=15.00 step=116300, 10.1636 examples/sec lr=0.000004, loss=22.738, loss_ll=3.94556, loss_ll_paf=7.37656, loss_ll_heat=0.514571, q=1000
[2018-07-14 21:13:13,205] [train] [INFO] epoch=15.00 step=116400, 10.1638 examples/sec lr=0.000004, loss=21.3323, loss_ll=3.93103, loss_ll_paf=7.04606, loss_ll_heat=0.816, q=1000
[2018-07-14 21:15:49,437] [train] [INFO] epoch=15.00 step=116500, 10.1639 examples/sec lr=0.000004, loss=27.1144, loss_ll=5.30742, loss_ll_paf=9.5122, loss_ll_heat=1.10263, q=1000
[2018-07-14 21:18:27,225] [train] [INFO] epoch=15.00 step=116600, 10.1639 examples/sec lr=0.000004, loss=11.7344, loss_ll=1.90687, loss_ll_paf=3.21332, loss_ll_heat=0.600427, q=1000
[2018-07-14 21:21:02,113] [train] [INFO] epoch=15.00 step=116700, 10.1640 examples/sec lr=0.000004, loss=15.2829, loss_ll=2.71109, loss_ll_paf=4.92798, loss_ll_heat=0.494205, q=1000
[2018-07-14 21:23:37,025] [train] [INFO] epoch=15.00 step=116800, 10.1641 examples/sec lr=0.000004, loss=26.3496, loss_ll=5.3217, loss_ll_paf=9.90705, loss_ll_heat=0.736344, q=1000
[2018-07-14 21:26:13,636] [train] [INFO] epoch=15.00 step=116900, 10.1642 examples/sec lr=0.000004, loss=18.6118, loss_ll=3.34813, loss_ll_paf=6.14084, loss_ll_heat=0.555425, q=1000
[2018-07-14 21:28:47,228] [train] [INFO] epoch=15.00 step=117000, 10.1644 examples/sec lr=0.000004, loss=13.2087, loss_ll=2.30044, loss_ll_paf=4.02624, loss_ll_heat=0.574627, q=1000
[2018-07-14 21:31:41,937] [train] [INFO] epoch=15.00 step=117100, 10.1634 examples/sec lr=0.000004, loss=16.3045, loss_ll=3.07165, loss_ll_paf=5.313, loss_ll_heat=0.830287, q=1000
[2018-07-14 21:34:16,558] [train] [INFO] epoch=15.00 step=117200, 10.1636 examples/sec lr=0.000004, loss=5.82247, loss_ll=0.943448, loss_ll_paf=1.54569, loss_ll_heat=0.341202, q=1000
[2018-07-14 21:36:51,589] [train] [INFO] epoch=15.00 step=117300, 10.1637 examples/sec lr=0.000004, loss=30.6808, loss_ll=5.78913, loss_ll_paf=10.8886, loss_ll_heat=0.68967, q=1000
[2018-07-14 21:39:27,906] [train] [INFO] epoch=15.00 step=117400, 10.1638 examples/sec lr=0.000004, loss=14.5837, loss_ll=2.67576, loss_ll_paf=4.87503, loss_ll_heat=0.476492, q=1000
[2018-07-14 21:42:00,815] [train] [INFO] epoch=15.00 step=117500, 10.1640 examples/sec lr=0.000004, loss=8.9791, loss_ll=1.74833, loss_ll_paf=2.8497, loss_ll_heat=0.646966, q=1000
[2018-07-14 21:44:36,991] [train] [INFO] epoch=15.00 step=117600, 10.1641 examples/sec lr=0.000004, loss=18.4164, loss_ll=3.26038, loss_ll_paf=6.06464, loss_ll_heat=0.456115, q=1000
[2018-07-14 21:47:10,430] [train] [INFO] epoch=15.00 step=117700, 10.1643 examples/sec lr=0.000004, loss=12.0093, loss_ll=2.09809, loss_ll_paf=3.70476, loss_ll_heat=0.491416, q=1000
[2018-07-14 21:49:43,948] [train] [INFO] epoch=15.00 step=117800, 10.1645 examples/sec lr=0.000004, loss=8.13913, loss_ll=1.43537, loss_ll_paf=2.49944, loss_ll_heat=0.37131, q=1000
[2018-07-14 21:52:16,151] [train] [INFO] epoch=15.00 step=117900, 10.1648 examples/sec lr=0.000004, loss=12.7108, loss_ll=2.1736, loss_ll_paf=3.88273, loss_ll_heat=0.464466, q=1000
[2018-07-14 21:54:49,360] [train] [INFO] epoch=15.00 step=118000, 10.1650 examples/sec lr=0.000004, loss=15.7405, loss_ll=2.96846, loss_ll_paf=5.40046, loss_ll_heat=0.536469, q=1000
[2018-07-14 21:57:41,840] [train] [INFO] epoch=15.00 step=118100, 10.1642 examples/sec lr=0.000004, loss=7.76564, loss_ll=1.3396, loss_ll_paf=2.30814, loss_ll_heat=0.371059, q=1000
[2018-07-14 22:00:16,419] [train] [INFO] epoch=15.00 step=118200, 10.1644 examples/sec lr=0.000004, loss=16.8341, loss_ll=3.26054, loss_ll_paf=6.14996, loss_ll_heat=0.371119, q=1000
[2018-07-14 22:02:52,436] [train] [INFO] epoch=15.00 step=118300, 10.1645 examples/sec lr=0.000004, loss=17.4368, loss_ll=3.18819, loss_ll_paf=5.82817, loss_ll_heat=0.548215, q=1000
[2018-07-14 22:05:25,568] [train] [INFO] epoch=15.00 step=118400, 10.1647 examples/sec lr=0.000004, loss=20.3284, loss_ll=3.55392, loss_ll_paf=6.69708, loss_ll_heat=0.410757, q=1000
[2018-07-14 22:08:03,163] [train] [INFO] epoch=15.00 step=118500, 10.1647 examples/sec lr=0.000004, loss=25.3678, loss_ll=4.71479, loss_ll_paf=8.66871, loss_ll_heat=0.760873, q=1000
[2018-07-14 22:10:37,017] [train] [INFO] epoch=15.00 step=118600, 10.1649 examples/sec lr=0.000004, loss=11.5113, loss_ll=1.59204, loss_ll_paf=2.71783, loss_ll_heat=0.466252, q=1000
[2018-07-14 22:13:10,123] [train] [INFO] epoch=15.00 step=118700, 10.1651 examples/sec lr=0.000004, loss=7.30582, loss_ll=1.15797, loss_ll_paf=1.80845, loss_ll_heat=0.507492, q=1000
[2018-07-14 22:15:46,381] [train] [INFO] epoch=15.00 step=118800, 10.1652 examples/sec lr=0.000004, loss=17.5568, loss_ll=2.97944, loss_ll_paf=5.53325, loss_ll_heat=0.425643, q=1000
[2018-07-14 22:18:17,614] [train] [INFO] epoch=15.00 step=118900, 10.1655 examples/sec lr=0.000004, loss=22.9421, loss_ll=4.17175, loss_ll_paf=7.46853, loss_ll_heat=0.874974, q=1000
[2018-07-14 22:20:53,154] [train] [INFO] epoch=15.00 step=119000, 10.1656 examples/sec lr=0.000004, loss=13.0324, loss_ll=2.30873, loss_ll_paf=4.11778, loss_ll_heat=0.49969, q=1000
[2018-07-14 22:23:43,443] [train] [INFO] epoch=15.00 step=119100, 10.1649 examples/sec lr=0.000004, loss=26.3125, loss_ll=4.42349, loss_ll_paf=8.34099, loss_ll_heat=0.505981, q=1000
[2018-07-14 22:26:17,939] [train] [INFO] epoch=15.00 step=119200, 10.1651 examples/sec lr=0.000004, loss=11.3829, loss_ll=2.13086, loss_ll_paf=3.77818, loss_ll_heat=0.483545, q=1000
[2018-07-14 22:28:47,607] [train] [INFO] epoch=15.00 step=119300, 10.1655 examples/sec lr=0.000004, loss=15.3468, loss_ll=2.74081, loss_ll_paf=5.03432, loss_ll_heat=0.447293, q=1000
[2018-07-14 22:31:24,921] [train] [INFO] epoch=15.00 step=119400, 10.1655 examples/sec lr=0.000004, loss=20.4308, loss_ll=3.91307, loss_ll_paf=6.94921, loss_ll_heat=0.876923, q=1000
[2018-07-14 22:33:56,732] [train] [INFO] epoch=15.00 step=119500, 10.1658 examples/sec lr=0.000004, loss=11.2956, loss_ll=1.93199, loss_ll_paf=3.35566, loss_ll_heat=0.508308, q=1000
[2018-07-14 22:36:29,424] [train] [INFO] epoch=15.00 step=119600, 10.1660 examples/sec lr=0.000004, loss=10.0981, loss_ll=1.76711, loss_ll_paf=3.04711, loss_ll_heat=0.487104, q=1000
[2018-07-14 22:39:02,627] [train] [INFO] epoch=15.00 step=119700, 10.1663 examples/sec lr=0.000004, loss=18.5925, loss_ll=3.43267, loss_ll_paf=6.34254, loss_ll_heat=0.522801, q=1000
[2018-07-14 22:41:35,735] [train] [INFO] epoch=15.00 step=119800, 10.1665 examples/sec lr=0.000004, loss=29.7349, loss_ll=5.34926, loss_ll_paf=10.0768, loss_ll_heat=0.621768, q=1000
[2018-07-14 22:44:14,626] [train] [INFO] epoch=15.00 step=119900, 10.1664 examples/sec lr=0.000004, loss=13.9209, loss_ll=2.47517, loss_ll_paf=4.35883, loss_ll_heat=0.591501, q=1000
[2018-07-14 22:46:52,848] [train] [INFO] epoch=15.00 step=120000, 10.1664 examples/sec lr=0.000001, loss=7.63242, loss_ll=1.11717, loss_ll_paf=1.85109, loss_ll_heat=0.383254, q=1000
[2018-07-14 22:49:47,740] [train] [INFO] epoch=15.00 step=120100, 10.1654 examples/sec lr=0.000001, loss=16.9217, loss_ll=3.07088, loss_ll_paf=5.57395, loss_ll_heat=0.5678, q=1000
[2018-07-14 22:52:23,522] [train] [INFO] epoch=15.00 step=120200, 10.1655 examples/sec lr=0.000001, loss=15.1999, loss_ll=2.71669, loss_ll_paf=5.06073, loss_ll_heat=0.372655, q=1000
[2018-07-14 22:54:58,014] [train] [INFO] epoch=15.00 step=120300, 10.1657 examples/sec lr=0.000001, loss=14.2432, loss_ll=2.71098, loss_ll_paf=4.98118, loss_ll_heat=0.440791, q=1000
[2018-07-14 22:57:33,384] [train] [INFO] epoch=15.00 step=120400, 10.1658 examples/sec lr=0.000001, loss=21.2809, loss_ll=4.08536, loss_ll_paf=7.83159, loss_ll_heat=0.339121, q=1000
[2018-07-14 23:00:11,073] [train] [INFO] epoch=15.00 step=120500, 10.1658 examples/sec lr=0.000001, loss=28.4963, loss_ll=5.06169, loss_ll_paf=9.34531, loss_ll_heat=0.778069, q=1000
[2018-07-14 23:02:46,294] [train] [INFO] epoch=15.00 step=120600, 10.1659 examples/sec lr=0.000001, loss=14.0285, loss_ll=2.5183, loss_ll_paf=4.75684, loss_ll_heat=0.279753, q=1000
[2018-07-14 23:05:20,341] [train] [INFO] epoch=15.00 step=120700, 10.1661 examples/sec lr=0.000001, loss=16.167, loss_ll=2.25478, loss_ll_paf=4.14203, loss_ll_heat=0.367543, q=1000
[2018-07-14 23:07:55,528] [train] [INFO] epoch=15.00 step=120800, 10.1662 examples/sec lr=0.000001, loss=10.6179, loss_ll=1.59982, loss_ll_paf=2.91119, loss_ll_heat=0.288453, q=1000
[2018-07-14 23:10:33,818] [train] [INFO] epoch=15.00 step=120900, 10.1661 examples/sec lr=0.000001, loss=15.5847, loss_ll=2.6214, loss_ll_paf=4.73607, loss_ll_heat=0.50674, q=1000
[2018-07-14 23:13:04,071] [train] [INFO] epoch=15.00 step=121000, 10.1665 examples/sec lr=0.000001, loss=23.683, loss_ll=3.64155, loss_ll_paf=6.56686, loss_ll_heat=0.716242, q=1000
[2018-07-14 23:15:53,953] [train] [INFO] epoch=15.00 step=121100, 10.1658 examples/sec lr=0.000001, loss=31.7067, loss_ll=5.79964, loss_ll_paf=11.0908, loss_ll_heat=0.508525, q=1000
[2018-07-14 23:18:31,048] [train] [INFO] epoch=15.00 step=121200, 10.1659 examples/sec lr=0.000001, loss=16.386, loss_ll=2.59886, loss_ll_paf=4.84593, loss_ll_heat=0.351798, q=1000
[2018-07-14 23:21:03,252] [train] [INFO] epoch=15.00 step=121300, 10.1661 examples/sec lr=0.000001, loss=10.1604, loss_ll=1.88382, loss_ll_paf=3.34761, loss_ll_heat=0.420022, q=1000
[2018-07-14 23:23:39,279] [train] [INFO] epoch=15.00 step=121400, 10.1662 examples/sec lr=0.000001, loss=31.9614, loss_ll=5.8126, loss_ll_paf=10.9917, loss_ll_heat=0.633472, q=1000
[2018-07-14 23:26:16,250] [train] [INFO] epoch=15.00 step=121500, 10.1662 examples/sec lr=0.000001, loss=20.7466, loss_ll=3.76035, loss_ll_paf=6.7358, loss_ll_heat=0.784901, q=1000
[2018-07-14 23:28:49,556] [train] [INFO] epoch=15.00 step=121600, 10.1664 examples/sec lr=0.000001, loss=28.3719, loss_ll=5.0427, loss_ll_paf=9.35561, loss_ll_heat=0.729786, q=1000
[2018-07-14 23:31:21,945] [train] [INFO] epoch=15.00 step=121700, 10.1667 examples/sec lr=0.000001, loss=9.80663, loss_ll=1.80423, loss_ll_paf=3.18947, loss_ll_heat=0.418984, q=1000
[2018-07-14 23:33:57,334] [train] [INFO] epoch=16.00 step=121800, 10.1668 examples/sec lr=0.000001, loss=23.7055, loss_ll=4.36123, loss_ll_paf=8.32508, loss_ll_heat=0.397377, q=1000
[2018-07-14 23:36:34,391] [train] [INFO] epoch=16.00 step=121900, 10.1668 examples/sec lr=0.000001, loss=25.0798, loss_ll=4.57804, loss_ll_paf=8.96634, loss_ll_heat=0.189742, q=1000
[2018-07-14 23:39:06,158] [train] [INFO] epoch=16.00 step=122000, 10.1671 examples/sec lr=0.000001, loss=21.0584, loss_ll=3.39327, loss_ll_paf=6.1195, loss_ll_heat=0.667038, q=1000
[2018-07-14 23:42:02,598] [train] [INFO] epoch=16.00 step=122100, 10.1661 examples/sec lr=0.000001, loss=19.599, loss_ll=3.73868, loss_ll_paf=6.28179, loss_ll_heat=1.19556, q=1000
[2018-07-14 23:44:37,133] [train] [INFO] epoch=16.00 step=122200, 10.1663 examples/sec lr=0.000001, loss=19.695, loss_ll=3.26308, loss_ll_paf=6.0728, loss_ll_heat=0.453365, q=1000
[2018-07-14 23:47:12,724] [train] [INFO] epoch=16.00 step=122300, 10.1664 examples/sec lr=0.000001, loss=12.1248, loss_ll=1.92089, loss_ll_paf=3.25027, loss_ll_heat=0.591513, q=1000
[2018-07-14 23:49:51,329] [train] [INFO] epoch=16.00 step=122400, 10.1663 examples/sec lr=0.000001, loss=17.7903, loss_ll=3.38043, loss_ll_paf=5.81283, loss_ll_heat=0.948027, q=1000
[2018-07-14 23:52:26,626] [train] [INFO] epoch=16.00 step=122500, 10.1664 examples/sec lr=0.000001, loss=19.1721, loss_ll=3.35805, loss_ll_paf=6.12118, loss_ll_heat=0.594913, q=1000
[2018-07-14 23:55:01,080] [train] [INFO] epoch=16.00 step=122600, 10.1666 examples/sec lr=0.000001, loss=10.5319, loss_ll=1.73847, loss_ll_paf=2.866, loss_ll_heat=0.610939, q=1000
[2018-07-14 23:57:36,187] [train] [INFO] epoch=16.00 step=122700, 10.1667 examples/sec lr=0.000001, loss=13.9156, loss_ll=2.24161, loss_ll_paf=4.21972, loss_ll_heat=0.263505, q=1000
[2018-07-15 00:00:10,409] [train] [INFO] epoch=16.00 step=122800, 10.1668 examples/sec lr=0.000001, loss=6.54571, loss_ll=1.09922, loss_ll_paf=1.91158, loss_ll_heat=0.28685, q=1000
[2018-07-15 00:02:42,436] [train] [INFO] epoch=16.00 step=122900, 10.1671 examples/sec lr=0.000001, loss=19.5686, loss_ll=3.25537, loss_ll_paf=5.93237, loss_ll_heat=0.578379, q=1000
[2018-07-15 00:05:18,151] [train] [INFO] epoch=16.00 step=123000, 10.1672 examples/sec lr=0.000001, loss=21.6141, loss_ll=3.61063, loss_ll_paf=6.50985, loss_ll_heat=0.711401, q=1000
[2018-07-15 00:08:11,276] [train] [INFO] epoch=16.00 step=123100, 10.1664 examples/sec lr=0.000001, loss=17.6488, loss_ll=3.12655, loss_ll_paf=5.70968, loss_ll_heat=0.543419, q=1000
[2018-07-15 00:10:44,188] [train] [INFO] epoch=16.00 step=123200, 10.1666 examples/sec lr=0.000001, loss=21.317, loss_ll=3.91162, loss_ll_paf=7.32373, loss_ll_heat=0.499498, q=1000
[2018-07-15 00:13:20,068] [train] [INFO] epoch=16.00 step=123300, 10.1667 examples/sec lr=0.000001, loss=17.1391, loss_ll=3.02901, loss_ll_paf=5.25035, loss_ll_heat=0.807664, q=1000
[2018-07-15 00:15:53,239] [train] [INFO] epoch=16.00 step=123400, 10.1669 examples/sec lr=0.000001, loss=11.6753, loss_ll=1.90996, loss_ll_paf=3.41984, loss_ll_heat=0.400072, q=1000
[2018-07-15 00:18:27,505] [train] [INFO] epoch=16.00 step=123500, 10.1671 examples/sec lr=0.000001, loss=8.9084, loss_ll=1.51083, loss_ll_paf=2.73417, loss_ll_heat=0.28748, q=1000
[2018-07-15 00:21:02,698] [train] [INFO] epoch=16.00 step=123600, 10.1672 examples/sec lr=0.000001, loss=15.9834, loss_ll=3.1206, loss_ll_paf=5.7929, loss_ll_heat=0.448289, q=1000
[2018-07-15 00:23:35,292] [train] [INFO] epoch=16.00 step=123700, 10.1674 examples/sec lr=0.000001, loss=27.9722, loss_ll=5.30919, loss_ll_paf=9.6657, loss_ll_heat=0.95269, q=1000
[2018-07-15 00:26:10,869] [train] [INFO] epoch=16.00 step=123800, 10.1675 examples/sec lr=0.000001, loss=8.0443, loss_ll=1.34561, loss_ll_paf=2.3849, loss_ll_heat=0.30632, q=1000
[2018-07-15 00:28:42,237] [train] [INFO] epoch=16.00 step=123900, 10.1679 examples/sec lr=0.000001, loss=22.6116, loss_ll=4.13955, loss_ll_paf=7.57576, loss_ll_heat=0.703336, q=1000
[2018-07-15 00:31:17,246] [train] [INFO] epoch=16.00 step=124000, 10.1680 examples/sec lr=0.000001, loss=36.9757, loss_ll=6.98319, loss_ll_paf=12.9442, loss_ll_heat=1.02214, q=1000
[2018-07-15 00:34:09,354] [train] [INFO] epoch=16.00 step=124100, 10.1672 examples/sec lr=0.000001, loss=27.046, loss_ll=5.07905, loss_ll_paf=9.51025, loss_ll_heat=0.647848, q=1000
[2018-07-15 00:36:43,342] [train] [INFO] epoch=16.00 step=124200, 10.1674 examples/sec lr=0.000001, loss=27.2502, loss_ll=4.63245, loss_ll_paf=8.20976, loss_ll_heat=1.05515, q=1000
[2018-07-15 00:39:20,689] [train] [INFO] epoch=16.00 step=124300, 10.1674 examples/sec lr=0.000001, loss=33.6773, loss_ll=6.28434, loss_ll_paf=11.6633, loss_ll_heat=0.905343, q=1000
[2018-07-15 00:41:57,199] [train] [INFO] epoch=16.00 step=124400, 10.1674 examples/sec lr=0.000001, loss=9.86853, loss_ll=1.77287, loss_ll_paf=3.12761, loss_ll_heat=0.418125, q=1000
[2018-07-15 00:44:33,680] [train] [INFO] epoch=16.00 step=124500, 10.1675 examples/sec lr=0.000001, loss=23.2436, loss_ll=3.98958, loss_ll_paf=7.61509, loss_ll_heat=0.364066, q=1000
[2018-07-15 00:47:08,906] [train] [INFO] epoch=16.00 step=124600, 10.1676 examples/sec lr=0.000001, loss=20.2989, loss_ll=3.68577, loss_ll_paf=6.59912, loss_ll_heat=0.772422, q=1000
[2018-07-15 00:49:45,865] [train] [INFO] epoch=16.00 step=124700, 10.1676 examples/sec lr=0.000001, loss=18.2041, loss_ll=3.14471, loss_ll_paf=5.86879, loss_ll_heat=0.420628, q=1000
[2018-07-15 00:52:22,841] [train] [INFO] epoch=16.00 step=124800, 10.1676 examples/sec lr=0.000001, loss=8.21212, loss_ll=1.45891, loss_ll_paf=2.5534, loss_ll_heat=0.364418, q=1000
[2018-07-15 00:54:56,606] [train] [INFO] epoch=16.00 step=124900, 10.1678 examples/sec lr=0.000001, loss=16.5664, loss_ll=3.08671, loss_ll_paf=5.77252, loss_ll_heat=0.400896, q=1000
[2018-07-15 00:57:33,184] [train] [INFO] epoch=16.00 step=125000, 10.1679 examples/sec lr=0.000001, loss=19.9307, loss_ll=3.48831, loss_ll_paf=5.95086, loss_ll_heat=1.02576, q=1000
[2018-07-15 01:00:27,377] [train] [INFO] epoch=16.00 step=125100, 10.1670 examples/sec lr=0.000001, loss=16.4157, loss_ll=3.01385, loss_ll_paf=5.04967, loss_ll_heat=0.978037, q=1000
[2018-07-15 01:03:00,415] [train] [INFO] epoch=16.00 step=125200, 10.1672 examples/sec lr=0.000001, loss=15.1569, loss_ll=2.60755, loss_ll_paf=4.68639, loss_ll_heat=0.528713, q=1000
[2018-07-15 01:05:35,812] [train] [INFO] epoch=16.00 step=125300, 10.1673 examples/sec lr=0.000001, loss=10.7888, loss_ll=1.97201, loss_ll_paf=3.71847, loss_ll_heat=0.225556, q=1000
[2018-07-15 01:08:10,156] [train] [INFO] epoch=16.00 step=125400, 10.1675 examples/sec lr=0.000001, loss=31.5924, loss_ll=6.19629, loss_ll_paf=11.6251, loss_ll_heat=0.767425, q=1000
[2018-07-15 01:10:44,141] [train] [INFO] epoch=16.00 step=125500, 10.1676 examples/sec lr=0.000001, loss=16.0529, loss_ll=2.89129, loss_ll_paf=5.27758, loss_ll_heat=0.505007, q=1000
[2018-07-15 01:13:19,503] [train] [INFO] epoch=16.00 step=125600, 10.1677 examples/sec lr=0.000001, loss=18.3048, loss_ll=2.97724, loss_ll_paf=5.2468, loss_ll_heat=0.70768, q=1000
[2018-07-15 01:15:55,911] [train] [INFO] epoch=16.00 step=125700, 10.1678 examples/sec lr=0.000001, loss=23.2076, loss_ll=4.19278, loss_ll_paf=7.32205, loss_ll_heat=1.06352, q=1000
[2018-07-15 01:18:32,006] [train] [INFO] epoch=16.00 step=125800, 10.1679 examples/sec lr=0.000001, loss=29.074, loss_ll=5.7982, loss_ll_paf=10.9547, loss_ll_heat=0.641654, q=1000
[2018-07-15 01:21:06,470] [train] [INFO] epoch=16.00 step=125900, 10.1680 examples/sec lr=0.000001, loss=11.2047, loss_ll=1.8674, loss_ll_paf=3.18967, loss_ll_heat=0.54513, q=1000
[2018-07-15 01:23:40,382] [train] [INFO] epoch=16.00 step=126000, 10.1682 examples/sec lr=0.000001, loss=17.2138, loss_ll=2.78131, loss_ll_paf=5.10403, loss_ll_heat=0.458586, q=1000
[2018-07-15 01:26:30,284] [train] [INFO] epoch=16.00 step=126100, 10.1675 examples/sec lr=0.000001, loss=15.791, loss_ll=2.84224, loss_ll_paf=5.04772, loss_ll_heat=0.636759, q=1000
[2018-07-15 01:29:04,993] [train] [INFO] epoch=16.00 step=126200, 10.1677 examples/sec lr=0.000001, loss=11.2825, loss_ll=2.08643, loss_ll_paf=3.59918, loss_ll_heat=0.573667, q=1000
[2018-07-15 01:31:41,084] [train] [INFO] epoch=16.00 step=126300, 10.1677 examples/sec lr=0.000001, loss=12.742, loss_ll=2.05675, loss_ll_paf=3.65904, loss_ll_heat=0.45446, q=1000
[2018-07-15 01:34:15,738] [train] [INFO] epoch=16.00 step=126400, 10.1679 examples/sec lr=0.000001, loss=10.4382, loss_ll=1.75158, loss_ll_paf=3.07788, loss_ll_heat=0.425283, q=1000
[2018-07-15 01:36:51,961] [train] [INFO] epoch=16.00 step=126500, 10.1679 examples/sec lr=0.000001, loss=34.8291, loss_ll=6.88676, loss_ll_paf=12.9895, loss_ll_heat=0.783979, q=1000
[2018-07-15 01:39:29,718] [train] [INFO] epoch=16.00 step=126600, 10.1679 examples/sec lr=0.000001, loss=30.9501, loss_ll=5.72112, loss_ll_paf=10.9613, loss_ll_heat=0.480949, q=1000
[2018-07-15 01:42:03,558] [train] [INFO] epoch=16.00 step=126700, 10.1681 examples/sec lr=0.000001, loss=22.02, loss_ll=4.16802, loss_ll_paf=7.6489, loss_ll_heat=0.687141, q=1000
[2018-07-15 01:44:37,338] [train] [INFO] epoch=16.00 step=126800, 10.1683 examples/sec lr=0.000001, loss=16.5646, loss_ll=2.95869, loss_ll_paf=5.68724, loss_ll_heat=0.23014, q=1000
[2018-07-15 01:47:14,556] [train] [INFO] epoch=16.00 step=126900, 10.1683 examples/sec lr=0.000001, loss=24.6744, loss_ll=4.56137, loss_ll_paf=8.65597, loss_ll_heat=0.466765, q=1000
[2018-07-15 01:49:53,196] [train] [INFO] epoch=16.00 step=127000, 10.1682 examples/sec lr=0.000001, loss=12.5384, loss_ll=1.83065, loss_ll_paf=3.1887, loss_ll_heat=0.472605, q=1000
[2018-07-15 01:52:48,346] [train] [INFO] epoch=16.00 step=127100, 10.1673 examples/sec lr=0.000001, loss=11.2153, loss_ll=1.69969, loss_ll_paf=2.99555, loss_ll_heat=0.403821, q=1000
[2018-07-15 01:55:25,503] [train] [INFO] epoch=16.00 step=127200, 10.1673 examples/sec lr=0.000001, loss=10.5703, loss_ll=1.79024, loss_ll_paf=3.13574, loss_ll_heat=0.444748, q=1000
[2018-07-15 01:58:02,123] [train] [INFO] epoch=16.00 step=127300, 10.1674 examples/sec lr=0.000001, loss=11.4987, loss_ll=2.10228, loss_ll_paf=3.86089, loss_ll_heat=0.343665, q=1000
[2018-07-15 02:00:39,376] [train] [INFO] epoch=16.00 step=127400, 10.1674 examples/sec lr=0.000001, loss=8.16492, loss_ll=1.38862, loss_ll_paf=2.15967, loss_ll_heat=0.61758, q=1000
[2018-07-15 02:03:13,682] [train] [INFO] epoch=16.00 step=127500, 10.1675 examples/sec lr=0.000001, loss=18.9206, loss_ll=3.43326, loss_ll_paf=6.00196, loss_ll_heat=0.864548, q=1000
[2018-07-15 02:05:48,442] [train] [INFO] epoch=16.00 step=127600, 10.1677 examples/sec lr=0.000001, loss=17.9713, loss_ll=3.53771, loss_ll_paf=6.58112, loss_ll_heat=0.494308, q=1000
[2018-07-15 02:08:22,341] [train] [INFO] epoch=16.00 step=127700, 10.1678 examples/sec lr=0.000001, loss=14.3997, loss_ll=2.60088, loss_ll_paf=4.60175, loss_ll_heat=0.600008, q=1000
[2018-07-15 02:10:58,947] [train] [INFO] epoch=16.00 step=127800, 10.1679 examples/sec lr=0.000001, loss=16.283, loss_ll=3.00564, loss_ll_paf=5.26266, loss_ll_heat=0.748623, q=1000
[2018-07-15 02:13:34,057] [train] [INFO] epoch=16.00 step=127900, 10.1680 examples/sec lr=0.000001, loss=13.0149, loss_ll=2.10382, loss_ll_paf=3.79129, loss_ll_heat=0.416346, q=1000
[2018-07-15 02:16:10,815] [train] [INFO] epoch=16.00 step=128000, 10.1680 examples/sec lr=0.000001, loss=15.4653, loss_ll=2.51361, loss_ll_paf=4.55366, loss_ll_heat=0.473553, q=1000
[2018-07-15 02:19:00,641] [train] [INFO] epoch=16.00 step=128100, 10.1674 examples/sec lr=0.000001, loss=16.065, loss_ll=2.71927, loss_ll_paf=4.98762, loss_ll_heat=0.45091, q=1000
[2018-07-15 02:21:35,462] [train] [INFO] epoch=16.00 step=128200, 10.1675 examples/sec lr=0.000001, loss=12.3162, loss_ll=2.25314, loss_ll_paf=4.23233, loss_ll_heat=0.273941, q=1000
[2018-07-15 02:24:11,679] [train] [INFO] epoch=16.00 step=128300, 10.1676 examples/sec lr=0.000001, loss=13.2619, loss_ll=2.30785, loss_ll_paf=3.9481, loss_ll_heat=0.667609, q=1000
[2018-07-15 02:26:47,641] [train] [INFO] epoch=16.00 step=128400, 10.1676 examples/sec lr=0.000001, loss=19.6852, loss_ll=3.62436, loss_ll_paf=6.96106, loss_ll_heat=0.287661, q=1000
[2018-07-15 02:29:24,341] [train] [INFO] epoch=16.00 step=128500, 10.1677 examples/sec lr=0.000001, loss=31.6606, loss_ll=5.80899, loss_ll_paf=10.9297, loss_ll_heat=0.688317, q=1000
[2018-07-15 02:32:02,135] [train] [INFO] epoch=16.00 step=128600, 10.1677 examples/sec lr=0.000001, loss=17.5326, loss_ll=3.31327, loss_ll_paf=6.09142, loss_ll_heat=0.535119, q=1000
[2018-07-15 02:34:38,381] [train] [INFO] epoch=16.00 step=128700, 10.1677 examples/sec lr=0.000001, loss=12.8304, loss_ll=2.34072, loss_ll_paf=4.09024, loss_ll_heat=0.591199, q=1000
[2018-07-15 02:37:14,687] [train] [INFO] epoch=16.00 step=128800, 10.1678 examples/sec lr=0.000001, loss=42.7118, loss_ll=7.52313, loss_ll_paf=13.86, loss_ll_heat=1.18626, q=1000
[2018-07-15 02:39:50,007] [train] [INFO] epoch=16.00 step=128900, 10.1679 examples/sec lr=0.000001, loss=9.95627, loss_ll=1.76447, loss_ll_paf=3.08986, loss_ll_heat=0.439084, q=1000
[2018-07-15 02:42:25,101] [train] [INFO] epoch=16.00 step=129000, 10.1680 examples/sec lr=0.000001, loss=16.6198, loss_ll=3.05206, loss_ll_paf=5.37658, loss_ll_heat=0.72755, q=1000
[2018-07-15 02:45:17,526] [train] [INFO] epoch=16.00 step=129100, 10.1672 examples/sec lr=0.000001, loss=15.3093, loss_ll=2.40463, loss_ll_paf=4.29564, loss_ll_heat=0.513615, q=1000
[2018-07-15 02:47:55,827] [train] [INFO] epoch=16.00 step=129200, 10.1672 examples/sec lr=0.000001, loss=6.87722, loss_ll=1.17531, loss_ll_paf=1.91012, loss_ll_heat=0.4405, q=1000
[2018-07-15 02:50:28,678] [train] [INFO] epoch=16.00 step=129300, 10.1674 examples/sec lr=0.000001, loss=23.7432, loss_ll=4.22704, loss_ll_paf=7.62991, loss_ll_heat=0.824176, q=1000
[2018-07-15 02:53:04,299] [train] [INFO] epoch=17.00 step=129400, 10.1675 examples/sec lr=0.000001, loss=6.57016, loss_ll=1.13472, loss_ll_paf=1.8717, loss_ll_heat=0.39774, q=1000
[2018-07-15 02:55:41,826] [train] [INFO] epoch=17.00 step=129500, 10.1675 examples/sec lr=0.000001, loss=17.8148, loss_ll=3.06271, loss_ll_paf=5.73752, loss_ll_heat=0.387908, q=1000
[2018-07-15 02:58:16,299] [train] [INFO] epoch=17.00 step=129600, 10.1676 examples/sec lr=0.000001, loss=20.0223, loss_ll=3.73722, loss_ll_paf=6.25925, loss_ll_heat=1.21519, q=1000
[2018-07-15 03:00:50,656] [train] [INFO] epoch=17.00 step=129700, 10.1678 examples/sec lr=0.000001, loss=30.6279, loss_ll=5.73278, loss_ll_paf=10.9251, loss_ll_heat=0.540468, q=1000
[2018-07-15 03:03:28,384] [train] [INFO] epoch=17.00 step=129800, 10.1678 examples/sec lr=0.000001, loss=11.4395, loss_ll=1.96309, loss_ll_paf=3.51234, loss_ll_heat=0.413834, q=1000
[2018-07-15 03:06:22,399] [train] [INFO] epoch=17.00 step=129900, 10.1669 examples/sec lr=0.000001, loss=28.765, loss_ll=5.1465, loss_ll_paf=9.53216, loss_ll_heat=0.760844, q=1000
[2018-07-15 03:08:57,673] [train] [INFO] epoch=17.00 step=130000, 10.1670 examples/sec lr=0.000001, loss=22.9914, loss_ll=3.98698, loss_ll_paf=7.61613, loss_ll_heat=0.357828, q=1000
[2018-07-15 03:11:52,493] [train] [INFO] epoch=17.00 step=130100, 10.1662 examples/sec lr=0.000001, loss=14.6824, loss_ll=2.50346, loss_ll_paf=4.61256, loss_ll_heat=0.394353, q=1000
[2018-07-15 03:14:28,893] [train] [INFO] epoch=17.00 step=130200, 10.1662 examples/sec lr=0.000001, loss=15.9733, loss_ll=2.75737, loss_ll_paf=5.06735, loss_ll_heat=0.447383, q=1000
[2018-07-15 03:17:07,385] [train] [INFO] epoch=17.00 step=130300, 10.1662 examples/sec lr=0.000001, loss=10.4133, loss_ll=1.81722, loss_ll_paf=3.17513, loss_ll_heat=0.459303, q=1000
[2018-07-15 03:19:40,547] [train] [INFO] epoch=17.00 step=130400, 10.1664 examples/sec lr=0.000001, loss=14.3239, loss_ll=2.28207, loss_ll_paf=4.00479, loss_ll_heat=0.55935, q=1000
[2018-07-15 03:22:15,262] [train] [INFO] epoch=17.00 step=130500, 10.1665 examples/sec lr=0.000001, loss=15.4415, loss_ll=2.85276, loss_ll_paf=5.25086, loss_ll_heat=0.454656, q=1000
[2018-07-15 03:24:52,204] [train] [INFO] epoch=17.00 step=130600, 10.1665 examples/sec lr=0.000001, loss=12.4632, loss_ll=2.36762, loss_ll_paf=4.36015, loss_ll_heat=0.375092, q=1000
[2018-07-15 03:27:28,165] [train] [INFO] epoch=17.00 step=130700, 10.1666 examples/sec lr=0.000001, loss=10.043, loss_ll=1.82816, loss_ll_paf=3.30525, loss_ll_heat=0.351073, q=1000
[2018-07-15 03:30:05,019] [train] [INFO] epoch=17.00 step=130800, 10.1666 examples/sec lr=0.000001, loss=28.2522, loss_ll=5.15937, loss_ll_paf=9.71287, loss_ll_heat=0.605869, q=1000
[2018-07-15 03:32:42,090] [train] [INFO] epoch=17.00 step=130900, 10.1666 examples/sec lr=0.000001, loss=14.2831, loss_ll=2.40903, loss_ll_paf=4.38278, loss_ll_heat=0.435292, q=1000
[2018-07-15 03:35:17,298] [train] [INFO] epoch=17.00 step=131000, 10.1667 examples/sec lr=0.000001, loss=16.9725, loss_ll=2.61279, loss_ll_paf=4.59963, loss_ll_heat=0.625946, q=1000
[2018-07-15 03:38:12,736] [train] [INFO] epoch=17.00 step=131100, 10.1658 examples/sec lr=0.000001, loss=12.4948, loss_ll=2.30334, loss_ll_paf=4.02478, loss_ll_heat=0.581906, q=1000
[2018-07-15 03:40:48,091] [train] [INFO] epoch=17.00 step=131200, 10.1659 examples/sec lr=0.000001, loss=22.9185, loss_ll=3.85409, loss_ll_paf=7.35642, loss_ll_heat=0.351759, q=1000
[2018-07-15 03:43:25,960] [train] [INFO] epoch=17.00 step=131300, 10.1659 examples/sec lr=0.000001, loss=31.6917, loss_ll=6.36786, loss_ll_paf=11.9024, loss_ll_heat=0.833347, q=1000
[2018-07-15 03:46:04,333] [train] [INFO] epoch=17.00 step=131400, 10.1659 examples/sec lr=0.000001, loss=17.8676, loss_ll=3.03561, loss_ll_paf=5.5339, loss_ll_heat=0.537324, q=1000
[2018-07-15 03:48:38,463] [train] [INFO] epoch=17.00 step=131500, 10.1660 examples/sec lr=0.000001, loss=15.8368, loss_ll=2.78148, loss_ll_paf=5.20487, loss_ll_heat=0.358086, q=1000
[2018-07-15 03:51:14,463] [train] [INFO] epoch=17.00 step=131600, 10.1661 examples/sec lr=0.000001, loss=13.9666, loss_ll=2.10612, loss_ll_paf=3.87894, loss_ll_heat=0.333308, q=1000
[2018-07-15 03:53:51,237] [train] [INFO] epoch=17.00 step=131700, 10.1661 examples/sec lr=0.000001, loss=8.46707, loss_ll=1.36767, loss_ll_paf=2.26904, loss_ll_heat=0.466292, q=1000
[2018-07-15 03:56:28,793] [train] [INFO] epoch=17.00 step=131800, 10.1661 examples/sec lr=0.000001, loss=20.574, loss_ll=3.90312, loss_ll_paf=7.3077, loss_ll_heat=0.498526, q=1000
[2018-07-15 03:59:03,823] [train] [INFO] epoch=17.00 step=131900, 10.1662 examples/sec lr=0.000001, loss=12.2917, loss_ll=2.33313, loss_ll_paf=4.24133, loss_ll_heat=0.424933, q=1000
[2018-07-15 04:01:40,843] [train] [INFO] epoch=17.00 step=132000, 10.1663 examples/sec lr=0.000001, loss=25.855, loss_ll=4.87489, loss_ll_paf=9.0828, loss_ll_heat=0.666971, q=1000
[2018-07-15 04:04:35,910] [train] [INFO] epoch=17.00 step=132100, 10.1654 examples/sec lr=0.000001, loss=14.3855, loss_ll=2.78891, loss_ll_paf=4.64822, loss_ll_heat=0.929594, q=1000
[2018-07-15 04:07:12,863] [train] [INFO] epoch=17.00 step=132200, 10.1654 examples/sec lr=0.000001, loss=20.0126, loss_ll=3.61434, loss_ll_paf=6.76402, loss_ll_heat=0.464669, q=1000
[2018-07-15 04:09:49,233] [train] [INFO] epoch=17.00 step=132300, 10.1655 examples/sec lr=0.000001, loss=28.2198, loss_ll=5.64091, loss_ll_paf=10.6238, loss_ll_heat=0.658011, q=1000
[2018-07-15 04:12:27,718] [train] [INFO] epoch=17.00 step=132400, 10.1654 examples/sec lr=0.000001, loss=12.4217, loss_ll=2.00842, loss_ll_paf=3.36487, loss_ll_heat=0.651971, q=1000
[2018-07-15 04:15:02,849] [train] [INFO] epoch=17.00 step=132500, 10.1655 examples/sec lr=0.000001, loss=16.1427, loss_ll=2.81727, loss_ll_paf=5.23109, loss_ll_heat=0.403439, q=1000
[2018-07-15 04:17:38,902] [train] [INFO] epoch=17.00 step=132600, 10.1656 examples/sec lr=0.000001, loss=22.9467, loss_ll=4.23363, loss_ll_paf=7.98918, loss_ll_heat=0.47809, q=1000
[2018-07-15 04:20:15,955] [train] [INFO] epoch=17.00 step=132700, 10.1656 examples/sec lr=0.000001, loss=22.1486, loss_ll=3.74691, loss_ll_paf=7.14762, loss_ll_heat=0.346211, q=1000
[2018-07-15 04:22:53,472] [train] [INFO] epoch=17.00 step=132800, 10.1656 examples/sec lr=0.000001, loss=37.5985, loss_ll=7.33152, loss_ll_paf=13.472, loss_ll_heat=1.191, q=1000
[2018-07-15 04:25:30,172] [train] [INFO] epoch=17.00 step=132900, 10.1656 examples/sec lr=0.000001, loss=20.7502, loss_ll=3.34654, loss_ll_paf=5.85723, loss_ll_heat=0.835862, q=1000
[2018-07-15 04:28:06,113] [train] [INFO] epoch=17.00 step=133000, 10.1657 examples/sec lr=0.000001, loss=22.9105, loss_ll=4.51294, loss_ll_paf=8.14836, loss_ll_heat=0.877516, q=1000
[2018-07-15 04:31:02,252] [train] [INFO] epoch=17.00 step=133100, 10.1648 examples/sec lr=0.000001, loss=8.98567, loss_ll=1.52607, loss_ll_paf=2.57476, loss_ll_heat=0.477384, q=1000
[2018-07-15 04:33:39,760] [train] [INFO] epoch=17.00 step=133200, 10.1648 examples/sec lr=0.000001, loss=10.6552, loss_ll=1.56456, loss_ll_paf=2.83591, loss_ll_heat=0.293199, q=1000
[2018-07-15 04:36:15,882] [train] [INFO] epoch=17.00 step=133300, 10.1649 examples/sec lr=0.000001, loss=16.2502, loss_ll=2.74359, loss_ll_paf=4.90652, loss_ll_heat=0.580653, q=1000
[2018-07-15 04:38:52,159] [train] [INFO] epoch=17.00 step=133400, 10.1649 examples/sec lr=0.000001, loss=29.2347, loss_ll=5.53799, loss_ll_paf=9.95481, loss_ll_heat=1.12117, q=1000
[2018-07-15 04:41:28,727] [train] [INFO] epoch=17.00 step=133500, 10.1649 examples/sec lr=0.000001, loss=17.996, loss_ll=3.35712, loss_ll_paf=6.09308, loss_ll_heat=0.621154, q=1000
[2018-07-15 04:44:02,555] [train] [INFO] epoch=17.00 step=133600, 10.1651 examples/sec lr=0.000001, loss=17.9274, loss_ll=2.92399, loss_ll_paf=5.24355, loss_ll_heat=0.604429, q=1000
[2018-07-15 04:46:37,976] [train] [INFO] epoch=17.00 step=133700, 10.1652 examples/sec lr=0.000001, loss=13.3684, loss_ll=2.50597, loss_ll_paf=4.27161, loss_ll_heat=0.74034, q=1000
[2018-07-15 04:49:12,573] [train] [INFO] epoch=17.00 step=133800, 10.1654 examples/sec lr=0.000001, loss=26.7777, loss_ll=4.76723, loss_ll_paf=8.88457, loss_ll_heat=0.649883, q=1000
[2018-07-15 04:51:50,194] [train] [INFO] epoch=17.00 step=133900, 10.1653 examples/sec lr=0.000001, loss=12.4443, loss_ll=2.28217, loss_ll_paf=4.26561, loss_ll_heat=0.298734, q=1000
[2018-07-15 04:54:25,103] [train] [INFO] epoch=17.00 step=134000, 10.1655 examples/sec lr=0.000001, loss=16.3602, loss_ll=2.78455, loss_ll_paf=5.23412, loss_ll_heat=0.334974, q=1000
[2018-07-15 04:57:17,051] [train] [INFO] epoch=17.00 step=134100, 10.1648 examples/sec lr=0.000001, loss=15.0302, loss_ll=2.79765, loss_ll_paf=4.81818, loss_ll_heat=0.777123, q=1000
[2018-07-15 04:59:53,045] [train] [INFO] epoch=17.00 step=134200, 10.1648 examples/sec lr=0.000001, loss=23.1154, loss_ll=4.36345, loss_ll_paf=8.18611, loss_ll_heat=0.540795, q=1000
[2018-07-15 05:02:26,709] [train] [INFO] epoch=17.00 step=134300, 10.1650 examples/sec lr=0.000001, loss=15.9665, loss_ll=3.065, loss_ll_paf=5.67222, loss_ll_heat=0.457779, q=1000
[2018-07-15 05:05:01,184] [train] [INFO] epoch=17.00 step=134400, 10.1651 examples/sec lr=0.000001, loss=22.8796, loss_ll=3.68494, loss_ll_paf=6.94074, loss_ll_heat=0.429134, q=1000
[2018-07-15 05:07:33,242] [train] [INFO] epoch=17.00 step=134500, 10.1654 examples/sec lr=0.000001, loss=10.0864, loss_ll=1.59536, loss_ll_paf=2.67101, loss_ll_heat=0.519701, q=1000
[2018-07-15 05:10:07,170] [train] [INFO] epoch=17.00 step=134600, 10.1656 examples/sec lr=0.000001, loss=23.5923, loss_ll=4.32683, loss_ll_paf=8.1394, loss_ll_heat=0.514264, q=1000
[2018-07-15 05:12:42,521] [train] [INFO] epoch=17.00 step=134700, 10.1657 examples/sec lr=0.000001, loss=14.8826, loss_ll=2.28857, loss_ll_paf=3.75872, loss_ll_heat=0.81842, q=1000
[2018-07-15 05:15:16,754] [train] [INFO] epoch=17.00 step=134800, 10.1658 examples/sec lr=0.000001, loss=19.5131, loss_ll=3.69904, loss_ll_paf=6.77697, loss_ll_heat=0.621104, q=1000
[2018-07-15 05:17:48,729] [train] [INFO] epoch=17.00 step=134900, 10.1661 examples/sec lr=0.000001, loss=25.7573, loss_ll=4.61659, loss_ll_paf=8.72623, loss_ll_heat=0.506951, q=1000
[2018-07-15 05:20:21,885] [train] [INFO] epoch=17.00 step=135000, 10.1663 examples/sec lr=0.000001, loss=28.0307, loss_ll=5.23641, loss_ll_paf=9.78013, loss_ll_heat=0.692689, q=1000
[2018-07-15 05:23:13,059] [train] [INFO] epoch=17.00 step=135100, 10.1656 examples/sec lr=0.000001, loss=14.1525, loss_ll=2.44885, loss_ll_paf=4.36821, loss_ll_heat=0.529496, q=1000
[2018-07-15 05:25:50,185] [train] [INFO] epoch=17.00 step=135200, 10.1656 examples/sec lr=0.000001, loss=14.0771, loss_ll=2.52362, loss_ll_paf=4.44244, loss_ll_heat=0.604808, q=1000
[2018-07-15 05:28:24,212] [train] [INFO] epoch=17.00 step=135300, 10.1658 examples/sec lr=0.000001, loss=22.2112, loss_ll=3.82856, loss_ll_paf=7.03628, loss_ll_heat=0.620833, q=1000
[2018-07-15 05:30:58,935] [train] [INFO] epoch=17.00 step=135400, 10.1659 examples/sec lr=0.000001, loss=10.7942, loss_ll=1.88363, loss_ll_paf=3.10552, loss_ll_heat=0.661742, q=1000
[2018-07-15 05:33:31,300] [train] [INFO] epoch=17.00 step=135500, 10.1662 examples/sec lr=0.000001, loss=20.3862, loss_ll=3.41413, loss_ll_paf=6.32989, loss_ll_heat=0.498375, q=1000
[2018-07-15 05:36:06,405] [train] [INFO] epoch=17.00 step=135600, 10.1663 examples/sec lr=0.000001, loss=18.8529, loss_ll=3.14861, loss_ll_paf=5.84897, loss_ll_heat=0.448244, q=1000
[2018-07-15 05:38:45,631] [train] [INFO] epoch=17.00 step=135700, 10.1662 examples/sec lr=0.000001, loss=13.5674, loss_ll=2.32013, loss_ll_paf=4.17827, loss_ll_heat=0.462002, q=1000
[2018-07-15 05:41:19,212] [train] [INFO] epoch=17.00 step=135800, 10.1664 examples/sec lr=0.000001, loss=8.95333, loss_ll=1.43659, loss_ll_paf=2.29659, loss_ll_heat=0.576579, q=1000
[2018-07-15 05:43:52,700] [train] [INFO] epoch=17.00 step=135900, 10.1665 examples/sec lr=0.000001, loss=22.5602, loss_ll=3.13439, loss_ll_paf=5.41883, loss_ll_heat=0.849943, q=1000
[2018-07-15 05:46:26,915] [train] [INFO] epoch=17.00 step=136000, 10.1667 examples/sec lr=0.000001, loss=28.6784, loss_ll=4.59499, loss_ll_paf=8.40436, loss_ll_heat=0.785626, q=1000
[2018-07-15 05:49:15,505] [train] [INFO] epoch=17.00 step=136100, 10.1662 examples/sec lr=0.000001, loss=14.9212, loss_ll=2.5975, loss_ll_paf=4.65833, loss_ll_heat=0.536668, q=1000
[2018-07-15 05:51:48,802] [train] [INFO] epoch=17.00 step=136200, 10.1664 examples/sec lr=0.000001, loss=7.41068, loss_ll=1.28278, loss_ll_paf=2.16388, loss_ll_heat=0.401677, q=1000
[2018-07-15 05:54:22,453] [train] [INFO] epoch=17.00 step=136300, 10.1665 examples/sec lr=0.000001, loss=14.6541, loss_ll=2.60891, loss_ll_paf=4.26893, loss_ll_heat=0.948892, q=1000
[2018-07-15 05:56:56,660] [train] [INFO] epoch=17.00 step=136400, 10.1667 examples/sec lr=0.000001, loss=12.2952, loss_ll=1.8789, loss_ll_paf=3.42627, loss_ll_heat=0.331524, q=1000
[2018-07-15 05:59:33,595] [train] [INFO] epoch=17.00 step=136500, 10.1667 examples/sec lr=0.000001, loss=32.6535, loss_ll=6.28179, loss_ll_paf=11.8671, loss_ll_heat=0.696528, q=1000
[2018-07-15 06:02:09,378] [train] [INFO] epoch=17.00 step=136600, 10.1668 examples/sec lr=0.000001, loss=9.91304, loss_ll=1.74195, loss_ll_paf=2.80024, loss_ll_heat=0.683648, q=1000
[2018-07-15 06:04:46,135] [train] [INFO] epoch=17.00 step=136700, 10.1668 examples/sec lr=0.000001, loss=16.0511, loss_ll=2.79343, loss_ll_paf=4.81601, loss_ll_heat=0.770852, q=1000
[2018-07-15 06:07:23,535] [train] [INFO] epoch=17.00 step=136800, 10.1668 examples/sec lr=0.000001, loss=20.4919, loss_ll=3.58632, loss_ll_paf=6.47251, loss_ll_heat=0.70012, q=1000
[2018-07-15 06:09:56,755] [train] [INFO] epoch=17.00 step=136900, 10.1670 examples/sec lr=0.000001, loss=24.0108, loss_ll=4.51425, loss_ll_paf=8.56126, loss_ll_heat=0.467234, q=1000
[2018-07-15 06:12:33,301] [train] [INFO] epoch=18.00 step=137000, 10.1670 examples/sec lr=0.000001, loss=23.1241, loss_ll=4.00867, loss_ll_paf=7.16596, loss_ll_heat=0.851376, q=1000
[2018-07-15 06:15:23,728] [train] [INFO] epoch=18.00 step=137100, 10.1664 examples/sec lr=0.000001, loss=12.031, loss_ll=2.03949, loss_ll_paf=3.55439, loss_ll_heat=0.524588, q=1000
[2018-07-15 06:17:57,442] [train] [INFO] epoch=18.00 step=137200, 10.1666 examples/sec lr=0.000001, loss=15.3867, loss_ll=2.85654, loss_ll_paf=5.24788, loss_ll_heat=0.46519, q=1000
[2018-07-15 06:20:32,799] [train] [INFO] epoch=18.00 step=137300, 10.1667 examples/sec lr=0.000001, loss=19.8018, loss_ll=2.95366, loss_ll_paf=4.94432, loss_ll_heat=0.962993, q=1000
[2018-07-15 06:23:12,637] [train] [INFO] epoch=18.00 step=137400, 10.1666 examples/sec lr=0.000001, loss=19.6498, loss_ll=3.27508, loss_ll_paf=5.59376, loss_ll_heat=0.956412, q=1000
[2018-07-15 06:25:48,002] [train] [INFO] epoch=18.00 step=137500, 10.1667 examples/sec lr=0.000001, loss=17.8671, loss_ll=3.10603, loss_ll_paf=5.67613, loss_ll_heat=0.535927, q=1000
[2018-07-15 06:28:19,911] [train] [INFO] epoch=18.00 step=137600, 10.1669 examples/sec lr=0.000001, loss=17.6864, loss_ll=3.24521, loss_ll_paf=5.84354, loss_ll_heat=0.646876, q=1000
[2018-07-15 06:30:56,453] [train] [INFO] epoch=18.00 step=137700, 10.1670 examples/sec lr=0.000001, loss=33.5159, loss_ll=5.77192, loss_ll_paf=10.7126, loss_ll_heat=0.831295, q=1000
[2018-07-15 06:33:32,025] [train] [INFO] epoch=18.00 step=137800, 10.1671 examples/sec lr=0.000001, loss=18.6512, loss_ll=3.00432, loss_ll_paf=5.30684, loss_ll_heat=0.701801, q=1000
[2018-07-15 06:36:05,507] [train] [INFO] epoch=18.00 step=137900, 10.1672 examples/sec lr=0.000001, loss=19.3802, loss_ll=3.20714, loss_ll_paf=6.11219, loss_ll_heat=0.302089, q=1000
[2018-07-15 06:38:37,644] [train] [INFO] epoch=18.00 step=138000, 10.1675 examples/sec lr=0.000001, loss=18.7456, loss_ll=3.3642, loss_ll_paf=6.16487, loss_ll_heat=0.563526, q=1000
[2018-07-15 06:41:29,679] [train] [INFO] epoch=18.00 step=138100, 10.1668 examples/sec lr=0.000001, loss=19.2187, loss_ll=3.44351, loss_ll_paf=5.93905, loss_ll_heat=0.947963, q=1000
[2018-07-15 06:44:04,472] [train] [INFO] epoch=18.00 step=138200, 10.1669 examples/sec lr=0.000001, loss=15.1713, loss_ll=2.40351, loss_ll_paf=4.18625, loss_ll_heat=0.620778, q=1000
[2018-07-15 06:46:36,340] [train] [INFO] epoch=18.00 step=138300, 10.1672 examples/sec lr=0.000001, loss=18.1302, loss_ll=3.12583, loss_ll_paf=5.46984, loss_ll_heat=0.781821, q=1000
[2018-07-15 06:49:15,119] [train] [INFO] epoch=18.00 step=138400, 10.1671 examples/sec lr=0.000001, loss=15.4043, loss_ll=2.51784, loss_ll_paf=4.48797, loss_ll_heat=0.547712, q=1000
[2018-07-15 06:51:47,054] [train] [INFO] epoch=18.00 step=138500, 10.1674 examples/sec lr=0.000001, loss=15.6161, loss_ll=2.6955, loss_ll_paf=4.92759, loss_ll_heat=0.463408, q=1000
[2018-07-15 06:54:19,706] [train] [INFO] epoch=18.00 step=138600, 10.1676 examples/sec lr=0.000001, loss=37.5684, loss_ll=7.00826, loss_ll_paf=13.1868, loss_ll_heat=0.829735, q=1000
[2018-07-15 06:56:55,144] [train] [INFO] epoch=18.00 step=138700, 10.1677 examples/sec lr=0.000001, loss=13.7928, loss_ll=2.40289, loss_ll_paf=4.33567, loss_ll_heat=0.470102, q=1000
[2018-07-15 06:59:28,453] [train] [INFO] epoch=18.00 step=138800, 10.1679 examples/sec lr=0.000001, loss=14.9479, loss_ll=2.48216, loss_ll_paf=4.51174, loss_ll_heat=0.452569, q=1000
[2018-07-15 07:02:03,606] [train] [INFO] epoch=18.00 step=138900, 10.1680 examples/sec lr=0.000001, loss=32.6329, loss_ll=5.97421, loss_ll_paf=10.8611, loss_ll_heat=1.08729, q=1000
[2018-07-15 07:04:41,390] [train] [INFO] epoch=18.00 step=139000, 10.1679 examples/sec lr=0.000001, loss=9.56942, loss_ll=1.80485, loss_ll_paf=3.15059, loss_ll_heat=0.459119, q=1000
[2018-07-15 07:07:34,913] [train] [INFO] epoch=18.00 step=139100, 10.1672 examples/sec lr=0.000001, loss=22.8582, loss_ll=3.55167, loss_ll_paf=6.65518, loss_ll_heat=0.448154, q=1000
[2018-07-15 07:10:08,072] [train] [INFO] epoch=18.00 step=139200, 10.1674 examples/sec lr=0.000001, loss=14.3483, loss_ll=2.50177, loss_ll_paf=4.62465, loss_ll_heat=0.378896, q=1000
[2018-07-15 07:12:41,843] [train] [INFO] epoch=18.00 step=139300, 10.1676 examples/sec lr=0.000001, loss=18.6947, loss_ll=3.35263, loss_ll_paf=6.01742, loss_ll_heat=0.687827, q=1000
[2018-07-15 07:15:14,465] [train] [INFO] epoch=18.00 step=139400, 10.1678 examples/sec lr=0.000001, loss=29.9782, loss_ll=5.67795, loss_ll_paf=10.7209, loss_ll_heat=0.63503, q=1000
[2018-07-15 07:17:51,698] [train] [INFO] epoch=18.00 step=139500, 10.1678 examples/sec lr=0.000001, loss=8.69936, loss_ll=1.60073, loss_ll_paf=2.8208, loss_ll_heat=0.380658, q=1000
[2018-07-15 07:20:27,192] [train] [INFO] epoch=18.00 step=139600, 10.1679 examples/sec lr=0.000001, loss=10.544, loss_ll=1.87469, loss_ll_paf=3.19049, loss_ll_heat=0.558891, q=1000
[2018-07-15 07:23:01,118] [train] [INFO] epoch=18.00 step=139700, 10.1680 examples/sec lr=0.000001, loss=24.1412, loss_ll=4.4366, loss_ll_paf=8.19862, loss_ll_heat=0.674581, q=1000
[2018-07-15 07:25:36,505] [train] [INFO] epoch=18.00 step=139800, 10.1681 examples/sec lr=0.000001, loss=8.41745, loss_ll=1.31397, loss_ll_paf=2.20122, loss_ll_heat=0.426712, q=1000
[2018-07-15 07:28:11,523] [train] [INFO] epoch=18.00 step=139900, 10.1682 examples/sec lr=0.000001, loss=9.44483, loss_ll=1.5147, loss_ll_paf=2.62032, loss_ll_heat=0.409073, q=1000
[2018-07-15 07:30:44,350] [train] [INFO] epoch=18.00 step=140000, 10.1684 examples/sec lr=0.000001, loss=9.88781, loss_ll=1.62839, loss_ll_paf=2.7865, loss_ll_heat=0.470288, q=1000
[2018-07-15 07:33:38,895] [train] [INFO] epoch=18.00 step=140100, 10.1676 examples/sec lr=0.000001, loss=11.8272, loss_ll=2.17645, loss_ll_paf=3.86122, loss_ll_heat=0.491692, q=1000
[2018-07-15 07:36:18,601] [train] [INFO] epoch=18.00 step=140200, 10.1675 examples/sec lr=0.000001, loss=7.51964, loss_ll=1.33661, loss_ll_paf=2.33356, loss_ll_heat=0.339667, q=1000
[2018-07-15 07:38:51,787] [train] [INFO] epoch=18.00 step=140300, 10.1677 examples/sec lr=0.000001, loss=20.1166, loss_ll=3.63932, loss_ll_paf=6.69569, loss_ll_heat=0.582938, q=1000
[2018-07-15 07:41:29,369] [train] [INFO] epoch=18.00 step=140400, 10.1677 examples/sec lr=0.000001, loss=14.8454, loss_ll=2.61003, loss_ll_paf=4.38007, loss_ll_heat=0.83998, q=1000
[2018-07-15 07:44:03,513] [train] [INFO] epoch=18.00 step=140500, 10.1679 examples/sec lr=0.000001, loss=17.657, loss_ll=3.54443, loss_ll_paf=6.50818, loss_ll_heat=0.580675, q=1000
[2018-07-15 07:46:39,946] [train] [INFO] epoch=18.00 step=140600, 10.1679 examples/sec lr=0.000001, loss=22.5921, loss_ll=3.77291, loss_ll_paf=7.12561, loss_ll_heat=0.42021, q=1000
[2018-07-15 07:49:14,886] [train] [INFO] epoch=18.00 step=140700, 10.1680 examples/sec lr=0.000001, loss=18.9097, loss_ll=3.61726, loss_ll_paf=6.65663, loss_ll_heat=0.577903, q=1000
[2018-07-15 07:51:49,411] [train] [INFO] epoch=18.00 step=140800, 10.1681 examples/sec lr=0.000001, loss=10.0688, loss_ll=1.55217, loss_ll_paf=2.73697, loss_ll_heat=0.367371, q=1000
[2018-07-15 07:54:21,476] [train] [INFO] epoch=18.00 step=140900, 10.1684 examples/sec lr=0.000001, loss=24.8567, loss_ll=4.73691, loss_ll_paf=8.79944, loss_ll_heat=0.674368, q=1000
[2018-07-15 07:56:55,189] [train] [INFO] epoch=18.00 step=141000, 10.1686 examples/sec lr=0.000001, loss=9.9132, loss_ll=1.64266, loss_ll_paf=3.08965, loss_ll_heat=0.195659, q=1000
[2018-07-15 07:59:42,550] [train] [INFO] epoch=18.00 step=141100, 10.1681 examples/sec lr=0.000001, loss=16.5132, loss_ll=2.76828, loss_ll_paf=5.15442, loss_ll_heat=0.382136, q=1000
[2018-07-15 08:02:19,729] [train] [INFO] epoch=18.00 step=141200, 10.1681 examples/sec lr=0.000001, loss=21.9353, loss_ll=4.14013, loss_ll_paf=7.60205, loss_ll_heat=0.678202, q=1000
[2018-07-15 08:04:55,554] [train] [INFO] epoch=18.00 step=141300, 10.1682 examples/sec lr=0.000001, loss=30.0191, loss_ll=5.38743, loss_ll_paf=9.50777, loss_ll_heat=1.26708, q=1000
[2018-07-15 08:07:29,522] [train] [INFO] epoch=18.00 step=141400, 10.1683 examples/sec lr=0.000001, loss=9.05315, loss_ll=1.60654, loss_ll_paf=2.57921, loss_ll_heat=0.63386, q=1000
[2018-07-15 08:10:04,416] [train] [INFO] epoch=18.00 step=141500, 10.1684 examples/sec lr=0.000001, loss=22.9321, loss_ll=3.61834, loss_ll_paf=6.37238, loss_ll_heat=0.864298, q=1000
[2018-07-15 08:12:37,244] [train] [INFO] epoch=18.00 step=141600, 10.1687 examples/sec lr=0.000001, loss=16.4179, loss_ll=2.66991, loss_ll_paf=4.91647, loss_ll_heat=0.423346, q=1000
[2018-07-15 08:15:15,440] [train] [INFO] epoch=18.00 step=141700, 10.1686 examples/sec lr=0.000001, loss=20.1703, loss_ll=3.70975, loss_ll_paf=6.81358, loss_ll_heat=0.605929, q=1000
[2018-07-15 08:17:53,645] [train] [INFO] epoch=18.00 step=141800, 10.1686 examples/sec lr=0.000001, loss=25.5568, loss_ll=4.7568, loss_ll_paf=8.84679, loss_ll_heat=0.666823, q=1000
[2018-07-15 08:20:28,138] [train] [INFO] epoch=18.00 step=141900, 10.1687 examples/sec lr=0.000001, loss=9.34301, loss_ll=1.49111, loss_ll_paf=2.5447, loss_ll_heat=0.437521, q=1000
[2018-07-15 08:23:03,865] [train] [INFO] epoch=18.00 step=142000, 10.1688 examples/sec lr=0.000001, loss=17.8923, loss_ll=3.15809, loss_ll_paf=5.54956, loss_ll_heat=0.766624, q=1000
[2018-07-15 08:25:58,521] [train] [INFO] epoch=18.00 step=142100, 10.1680 examples/sec lr=0.000001, loss=8.29318, loss_ll=1.42567, loss_ll_paf=2.5339, loss_ll_heat=0.31743, q=1000
[2018-07-15 08:28:35,160] [train] [INFO] epoch=18.00 step=142200, 10.1680 examples/sec lr=0.000001, loss=12.629, loss_ll=2.13211, loss_ll_paf=3.85729, loss_ll_heat=0.406929, q=1000
[2018-07-15 08:31:08,703] [train] [INFO] epoch=18.00 step=142300, 10.1682 examples/sec lr=0.000001, loss=14.676, loss_ll=2.71616, loss_ll_paf=5.01982, loss_ll_heat=0.4125, q=1000
[2018-07-15 08:33:41,443] [train] [INFO] epoch=18.00 step=142400, 10.1684 examples/sec lr=0.000001, loss=22.5402, loss_ll=3.79511, loss_ll_paf=6.7701, loss_ll_heat=0.820115, q=1000
[2018-07-15 08:36:13,902] [train] [INFO] epoch=18.00 step=142500, 10.1686 examples/sec lr=0.000001, loss=10.2402, loss_ll=1.62336, loss_ll_paf=2.91119, loss_ll_heat=0.335541, q=1000
[2018-07-15 08:38:45,184] [train] [INFO] epoch=18.00 step=142600, 10.1689 examples/sec lr=0.000001, loss=19.2096, loss_ll=3.14457, loss_ll_paf=5.47083, loss_ll_heat=0.818309, q=1000
[2018-07-15 08:41:14,652] [train] [INFO] epoch=18.00 step=142700, 10.1693 examples/sec lr=0.000001, loss=22.8031, loss_ll=4.20942, loss_ll_paf=8.07567, loss_ll_heat=0.34318, q=1000
[2018-07-15 08:43:49,028] [train] [INFO] epoch=18.00 step=142800, 10.1694 examples/sec lr=0.000001, loss=8.40302, loss_ll=1.35851, loss_ll_paf=2.53218, loss_ll_heat=0.184845, q=1000
[2018-07-15 08:46:24,852] [train] [INFO] epoch=18.00 step=142900, 10.1695 examples/sec lr=0.000001, loss=15.0043, loss_ll=2.62574, loss_ll_paf=4.78542, loss_ll_heat=0.466055, q=1000
[2018-07-15 08:48:58,145] [train] [INFO] epoch=18.00 step=143000, 10.1696 examples/sec lr=0.000001, loss=10.447, loss_ll=2.00731, loss_ll_paf=3.55354, loss_ll_heat=0.461083, q=1000
[2018-07-15 08:51:45,533] [train] [INFO] epoch=18.00 step=143100, 10.1692 examples/sec lr=0.000001, loss=5.88586, loss_ll=1.07673, loss_ll_paf=1.83713, loss_ll_heat=0.31632, q=1000
[2018-07-15 08:54:20,801] [train] [INFO] epoch=18.00 step=143200, 10.1693 examples/sec lr=0.000001, loss=8.28401, loss_ll=1.56292, loss_ll_paf=2.63162, loss_ll_heat=0.494216, q=1000
[2018-07-15 08:56:58,937] [train] [INFO] epoch=18.00 step=143300, 10.1692 examples/sec lr=0.000001, loss=8.48386, loss_ll=1.44964, loss_ll_paf=2.44689, loss_ll_heat=0.452388, q=1000
[2018-07-15 08:59:33,341] [train] [INFO] epoch=18.00 step=143400, 10.1694 examples/sec lr=0.000001, loss=21.4832, loss_ll=4.21719, loss_ll_paf=7.58536, loss_ll_heat=0.849019, q=1000
[2018-07-15 09:02:06,609] [train] [INFO] epoch=18.00 step=143500, 10.1696 examples/sec lr=0.000001, loss=50.4977, loss_ll=9.71618, loss_ll_paf=18.3445, loss_ll_heat=1.08782, q=1000
[2018-07-15 09:04:37,817] [train] [INFO] epoch=18.00 step=143600, 10.1698 examples/sec lr=0.000001, loss=16.0444, loss_ll=2.75214, loss_ll_paf=5.07762, loss_ll_heat=0.426661, q=1000
[2018-07-15 09:07:08,949] [train] [INFO] epoch=18.00 step=143700, 10.1701 examples/sec lr=0.000001, loss=14.1316, loss_ll=2.24612, loss_ll_paf=3.96441, loss_ll_heat=0.527826, q=1000
[2018-07-15 09:09:40,659] [train] [INFO] epoch=18.00 step=143800, 10.1704 examples/sec lr=0.000001, loss=14.9539, loss_ll=2.71748, loss_ll_paf=4.86361, loss_ll_heat=0.571351, q=1000
[2018-07-15 09:12:13,948] [train] [INFO] epoch=18.00 step=143900, 10.1705 examples/sec lr=0.000001, loss=32.7997, loss_ll=6.06412, loss_ll_paf=10.8906, loss_ll_heat=1.2376, q=1000
[2018-07-15 09:14:48,217] [train] [INFO] epoch=18.00 step=144000, 10.1707 examples/sec lr=0.000001, loss=22.3477, loss_ll=3.99641, loss_ll_paf=7.43349, loss_ll_heat=0.559324, q=1000
[2018-07-15 09:17:41,495] [train] [INFO] epoch=18.00 step=144100, 10.1700 examples/sec lr=0.000001, loss=27.5213, loss_ll=5.37719, loss_ll_paf=9.93936, loss_ll_heat=0.815022, q=1000
[2018-07-15 09:20:20,509] [train] [INFO] epoch=18.00 step=144200, 10.1699 examples/sec lr=0.000001, loss=12.0541, loss_ll=2.13271, loss_ll_paf=3.98835, loss_ll_heat=0.277074, q=1000
[2018-07-15 09:22:56,886] [train] [INFO] epoch=18.00 step=144300, 10.1699 examples/sec lr=0.000001, loss=21.2644, loss_ll=3.73668, loss_ll_paf=6.78589, loss_ll_heat=0.687472, q=1000
[2018-07-15 09:25:30,389] [train] [INFO] epoch=18.00 step=144400, 10.1701 examples/sec lr=0.000001, loss=12.7491, loss_ll=2.35073, loss_ll_paf=4.04529, loss_ll_heat=0.656173, q=1000
[2018-07-15 09:28:07,992] [train] [INFO] epoch=18.00 step=144500, 10.1701 examples/sec lr=0.000001, loss=17.5723, loss_ll=3.14579, loss_ll_paf=5.58057, loss_ll_heat=0.711022, q=1000
[2018-07-15 09:30:45,511] [train] [INFO] epoch=19.00 step=144600, 10.1701 examples/sec lr=0.000001, loss=12.2371, loss_ll=2.2195, loss_ll_paf=3.90574, loss_ll_heat=0.533266, q=1000
[2018-07-15 09:33:21,161] [train] [INFO] epoch=19.00 step=144700, 10.1702 examples/sec lr=0.000001, loss=22.567, loss_ll=3.9258, loss_ll_paf=7.18243, loss_ll_heat=0.669169, q=1000
[2018-07-15 09:35:58,693] [train] [INFO] epoch=19.00 step=144800, 10.1702 examples/sec lr=0.000001, loss=33.4348, loss_ll=6.26413, loss_ll_paf=11.9734, loss_ll_heat=0.554891, q=1000
[2018-07-15 09:38:35,751] [train] [INFO] epoch=19.00 step=144900, 10.1702 examples/sec lr=0.000001, loss=18.2392, loss_ll=2.93922, loss_ll_paf=5.13649, loss_ll_heat=0.741952, q=1000
[2018-07-15 09:41:13,724] [train] [INFO] epoch=19.00 step=145000, 10.1701 examples/sec lr=0.000001, loss=17.8423, loss_ll=3.29815, loss_ll_paf=6.28139, loss_ll_heat=0.314916, q=1000
[2018-07-15 09:44:10,067] [train] [INFO] epoch=19.00 step=145100, 10.1693 examples/sec lr=0.000001, loss=9.9404, loss_ll=1.8003, loss_ll_paf=3.09986, loss_ll_heat=0.500734, q=1000
[2018-07-15 09:46:47,752] [train] [INFO] epoch=19.00 step=145200, 10.1693 examples/sec lr=0.000001, loss=14.9118, loss_ll=2.52978, loss_ll_paf=4.73708, loss_ll_heat=0.322488, q=1000
[2018-07-15 09:49:23,049] [train] [INFO] epoch=19.00 step=145300, 10.1694 examples/sec lr=0.000001, loss=14.315, loss_ll=2.36434, loss_ll_paf=4.17959, loss_ll_heat=0.549095, q=1000
[2018-07-15 09:51:57,013] [train] [INFO] epoch=19.00 step=145400, 10.1695 examples/sec lr=0.000001, loss=40.5572, loss_ll=7.27802, loss_ll_paf=13.9833, loss_ll_heat=0.572722, q=1000
[2018-07-15 09:54:32,328] [train] [INFO] epoch=19.00 step=145500, 10.1696 examples/sec lr=0.000001, loss=6.80992, loss_ll=1.18423, loss_ll_paf=1.9251, loss_ll_heat=0.443362, q=1000
[2018-07-15 09:57:03,962] [train] [INFO] epoch=19.00 step=145600, 10.1699 examples/sec lr=0.000001, loss=17.1768, loss_ll=3.06595, loss_ll_paf=5.4483, loss_ll_heat=0.683602, q=1000
[2018-07-15 09:59:33,482] [train] [INFO] epoch=19.00 step=145700, 10.1702 examples/sec lr=0.000001, loss=9.62127, loss_ll=1.69945, loss_ll_paf=2.73326, loss_ll_heat=0.665642, q=1000
[2018-07-15 10:02:10,709] [train] [INFO] epoch=19.00 step=145800, 10.1702 examples/sec lr=0.000001, loss=26.311, loss_ll=4.99436, loss_ll_paf=9.52129, loss_ll_heat=0.467434, q=1000
[2018-07-15 10:04:41,784] [train] [INFO] epoch=19.00 step=145900, 10.1705 examples/sec lr=0.000001, loss=9.42258, loss_ll=1.60868, loss_ll_paf=2.86715, loss_ll_heat=0.350204, q=1000
[2018-07-15 10:07:13,080] [train] [INFO] epoch=19.00 step=146000, 10.1707 examples/sec lr=0.000001, loss=22.6608, loss_ll=3.91577, loss_ll_paf=7.09527, loss_ll_heat=0.736263, q=1000
[2018-07-15 10:10:04,664] [train] [INFO] epoch=19.00 step=146100, 10.1701 examples/sec lr=0.000001, loss=23.2264, loss_ll=3.99913, loss_ll_paf=7.67777, loss_ll_heat=0.320489, q=1000
[2018-07-15 10:12:35,583] [train] [INFO] epoch=19.00 step=146200, 10.1704 examples/sec lr=0.000001, loss=19.1871, loss_ll=3.42525, loss_ll_paf=6.43971, loss_ll_heat=0.41078, q=1000
[2018-07-15 10:15:08,971] [train] [INFO] epoch=19.00 step=146300, 10.1706 examples/sec lr=0.000001, loss=11.6723, loss_ll=2.06827, loss_ll_paf=3.77, loss_ll_heat=0.366534, q=1000
[2018-07-15 10:17:44,366] [train] [INFO] epoch=19.00 step=146400, 10.1707 examples/sec lr=0.000001, loss=26.3977, loss_ll=5.0622, loss_ll_paf=9.33456, loss_ll_heat=0.78984, q=1000
[2018-07-15 10:20:18,680] [train] [INFO] epoch=19.00 step=146500, 10.1708 examples/sec lr=0.000001, loss=8.06917, loss_ll=1.1906, loss_ll_paf=1.98625, loss_ll_heat=0.394946, q=1000
[2018-07-15 10:22:55,874] [train] [INFO] epoch=19.00 step=146600, 10.1708 examples/sec lr=0.000001, loss=16.763, loss_ll=2.90006, loss_ll_paf=5.17845, loss_ll_heat=0.621666, q=1000
[2018-07-15 10:25:27,776] [train] [INFO] epoch=19.00 step=146700, 10.1710 examples/sec lr=0.000001, loss=7.57688, loss_ll=1.30787, loss_ll_paf=2.27266, loss_ll_heat=0.343084, q=1000
[2018-07-15 10:28:04,124] [train] [INFO] epoch=19.00 step=146800, 10.1711 examples/sec lr=0.000001, loss=22.589, loss_ll=4.11849, loss_ll_paf=7.86973, loss_ll_heat=0.367256, q=1000
[2018-07-15 10:30:41,159] [train] [INFO] epoch=19.00 step=146900, 10.1711 examples/sec lr=0.000001, loss=20.6153, loss_ll=3.73828, loss_ll_paf=6.65964, loss_ll_heat=0.816912, q=1000
[2018-07-15 10:33:13,076] [train] [INFO] epoch=19.00 step=147000, 10.1713 examples/sec lr=0.000001, loss=21.2844, loss_ll=3.71891, loss_ll_paf=6.91262, loss_ll_heat=0.525214, q=1000
[2018-07-15 10:36:04,698] [train] [INFO] epoch=19.00 step=147100, 10.1707 examples/sec lr=0.000001, loss=32.071, loss_ll=6.4242, loss_ll_paf=11.8629, loss_ll_heat=0.985499, q=1000
[2018-07-15 10:38:39,327] [train] [INFO] epoch=19.00 step=147200, 10.1708 examples/sec lr=0.000001, loss=18.197, loss_ll=3.33426, loss_ll_paf=5.80788, loss_ll_heat=0.860642, q=1000
[2018-07-15 10:41:16,820] [train] [INFO] epoch=19.00 step=147300, 10.1708 examples/sec lr=0.000001, loss=10.0029, loss_ll=1.7045, loss_ll_paf=3.07436, loss_ll_heat=0.334644, q=1000
[2018-07-15 10:43:51,307] [train] [INFO] epoch=19.00 step=147400, 10.1709 examples/sec lr=0.000001, loss=14.834, loss_ll=2.44046, loss_ll_paf=4.57071, loss_ll_heat=0.310202, q=1000
[2018-07-15 10:46:27,453] [train] [INFO] epoch=19.00 step=147500, 10.1710 examples/sec lr=0.000001, loss=32.3207, loss_ll=6.11184, loss_ll_paf=11.8143, loss_ll_heat=0.409384, q=1000
[2018-07-15 10:49:00,341] [train] [INFO] epoch=19.00 step=147600, 10.1712 examples/sec lr=0.000001, loss=16.3182, loss_ll=3.04689, loss_ll_paf=5.55578, loss_ll_heat=0.538011, q=1000
[2018-07-15 10:51:37,439] [train] [INFO] epoch=19.00 step=147700, 10.1712 examples/sec lr=0.000001, loss=13.8923, loss_ll=2.10814, loss_ll_paf=3.70017, loss_ll_heat=0.516111, q=1000
[2018-07-15 10:54:16,556] [train] [INFO] epoch=19.00 step=147800, 10.1711 examples/sec lr=0.000001, loss=16.1784, loss_ll=2.98903, loss_ll_paf=5.41619, loss_ll_heat=0.561876, q=1000
[2018-07-15 10:56:53,131] [train] [INFO] epoch=19.00 step=147900, 10.1711 examples/sec lr=0.000001, loss=20.4803, loss_ll=3.68452, loss_ll_paf=6.54057, loss_ll_heat=0.82847, q=1000
[2018-07-15 10:59:27,594] [train] [INFO] epoch=19.00 step=148000, 10.1713 examples/sec lr=0.000001, loss=19.3089, loss_ll=3.77715, loss_ll_paf=7.13527, loss_ll_heat=0.419029, q=1000
[2018-07-15 11:02:12,842] [train] [INFO] epoch=19.00 step=148100, 10.1709 examples/sec lr=0.000001, loss=25.799, loss_ll=4.36596, loss_ll_paf=8.15986, loss_ll_heat=0.572049, q=901
[2018-07-15 11:04:48,417] [train] [INFO] epoch=19.00 step=148200, 10.1710 examples/sec lr=0.000001, loss=16.7035, loss_ll=2.79055, loss_ll_paf=5.12108, loss_ll_heat=0.460024, q=800
[2018-07-15 11:07:20,887] [train] [INFO] epoch=19.00 step=148300, 10.1712 examples/sec lr=0.000001, loss=13.4758, loss_ll=2.42858, loss_ll_paf=4.2803, loss_ll_heat=0.576861, q=698
[2018-07-15 11:09:51,902] [train] [INFO] epoch=19.00 step=148400, 10.1715 examples/sec lr=0.000001, loss=11.0461, loss_ll=1.77946, loss_ll_paf=3.13695, loss_ll_heat=0.421978, q=598
[2018-07-15 11:12:23,450] [train] [INFO] epoch=19.00 step=148500, 10.1717 examples/sec lr=0.000001, loss=19.5165, loss_ll=3.50452, loss_ll_paf=6.27408, loss_ll_heat=0.734965, q=497
[2018-07-15 11:15:00,096] [train] [INFO] epoch=19.00 step=148600, 10.1718 examples/sec lr=0.000001, loss=20.8558, loss_ll=3.85432, loss_ll_paf=7.03003, loss_ll_heat=0.678617, q=395
[2018-07-15 11:17:33,153] [train] [INFO] epoch=19.00 step=148700, 10.1719 examples/sec lr=0.000001, loss=13.5174, loss_ll=2.58989, loss_ll_paf=4.64551, loss_ll_heat=0.534269, q=295
[2018-07-15 11:20:08,321] [train] [INFO] epoch=19.00 step=148800, 10.1720 examples/sec lr=0.000001, loss=13.4452, loss_ll=2.15552, loss_ll_paf=3.79635, loss_ll_heat=0.514696, q=194
[2018-07-15 11:22:39,473] [train] [INFO] epoch=19.00 step=148900, 10.1723 examples/sec lr=0.000001, loss=35.867, loss_ll=6.66445, loss_ll_paf=12.6133, loss_ll_heat=0.715605, q=93
./begin_to_train.sh: line 19:  1087 Terminated              python train.py --model 'seresnet50' --datapath '/home/shy/projects/tf-openpose/data/' --imgpath '/home/shy/projects/tf-openpose/data/' --batchsize 16 --gpus 8 --max-epoch 23 --lr '0.0001' --pretrain_basepath '/home/shy/projects/tf-openpose/models/' --pretrain_path 'numpy/se_resnet50.npy' --modelpath '/home/shy/projects/tf-openpose/models/trained/skirt' --logpath '/home/shy/projects/tf-openpose/models/trained/skirt/' --checkpoint '' --tag 'skirt' --input-width 368 --input-height 368
