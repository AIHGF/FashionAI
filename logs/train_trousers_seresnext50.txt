[2018-06-27 23:07:58,670] [train] [INFO] epoch=0.00 step=100, 2.0602 examples/sec lr=0.000100, loss=209.962, loss_ll=41.7882, loss_ll_paf=69.8435, loss_ll_heat=13.733, q=1000
[2018-06-27 23:12:09,873] [train] [INFO] epoch=0.00 step=200, 3.1133 examples/sec lr=0.000100, loss=216.375, loss_ll=43.1721, loss_ll_paf=73.0256, loss_ll_heat=13.3186, q=1000
[2018-06-27 23:16:25,775] [train] [INFO] epoch=0.00 step=300, 3.7391 examples/sec lr=0.000100, loss=185.398, loss_ll=36.4109, loss_ll_paf=61.5637, loss_ll_heat=11.258, q=1000
[2018-06-27 23:20:42,773] [train] [INFO] epoch=0.00 step=400, 4.1539 examples/sec lr=0.000100, loss=216.37, loss_ll=42.6268, loss_ll_paf=70.2581, loss_ll_heat=14.9954, q=1000
[2018-06-27 23:24:57,842] [train] [INFO] epoch=0.00 step=500, 4.4548 examples/sec lr=0.000100, loss=192.712, loss_ll=37.4745, loss_ll_paf=64.4193, loss_ll_heat=10.5298, q=1000
[2018-06-27 23:29:13,393] [train] [INFO] epoch=0.00 step=600, 4.6798 examples/sec lr=0.000100, loss=143.95, loss_ll=28.118, loss_ll_paf=48.0421, loss_ll_heat=8.19394, q=1000
[2018-06-27 23:33:25,368] [train] [INFO] epoch=0.00 step=700, 4.8625 examples/sec lr=0.000100, loss=214.454, loss_ll=41.4323, loss_ll_paf=70.6271, loss_ll_heat=12.2375, q=1000
[2018-06-27 23:37:40,745] [train] [INFO] epoch=0.00 step=800, 5.0025 examples/sec lr=0.000100, loss=136.86, loss_ll=26.2986, loss_ll_paf=44.6893, loss_ll_heat=7.90786, q=1000
[2018-06-27 23:41:57,862] [train] [INFO] epoch=0.00 step=900, 5.1140 examples/sec lr=0.000100, loss=142.065, loss_ll=27.1065, loss_ll_paf=45.8366, loss_ll_heat=8.37643, q=1000
[2018-06-27 23:46:14,985] [train] [INFO] epoch=0.00 step=1000, 5.2067 examples/sec lr=0.000100, loss=188.029, loss_ll=35.4648, loss_ll_paf=60.2748, loss_ll_heat=10.6549, q=1000
[2018-06-27 23:52:07,074] [train] [INFO] epoch=0.00 step=1100, 5.1386 examples/sec lr=0.000100, loss=131.122, loss_ll=24.6838, loss_ll_paf=40.8788, loss_ll_heat=8.48888, q=1000
[2018-06-27 23:56:19,833] [train] [INFO] epoch=0.00 step=1200, 5.2205 examples/sec lr=0.000100, loss=144.438, loss_ll=27.4977, loss_ll_paf=46.0056, loss_ll_heat=8.98972, q=1000
[2018-06-28 00:00:34,992] [train] [INFO] epoch=0.00 step=1300, 5.2886 examples/sec lr=0.000100, loss=133.924, loss_ll=25.9064, loss_ll_paf=43.4041, loss_ll_heat=8.4088, q=1000
^[[2018-06-28 00:04:54,405] [train] [INFO] epoch=0.00 step=1400, 5.3430 examples/sec lr=0.000100, loss=127.56, loss_ll=23.6553, loss_ll_paf=39.8156, loss_ll_heat=7.49495, q=1000
[2018-06-28 00:09:08,620] [train] [INFO] epoch=0.00 step=1500, 5.3974 examples/sec lr=0.000100, loss=134.212, loss_ll=24.5133, loss_ll_paf=41.3941, loss_ll_heat=7.63242, q=1000
[2018-06-28 00:13:24,055] [train] [INFO] epoch=0.00 step=1600, 5.4445 examples/sec lr=0.000100, loss=149.683, loss_ll=27.4186, loss_ll_paf=46.7794, loss_ll_heat=8.05766, q=1000
[2018-06-28 00:17:41,706] [train] [INFO] epoch=0.00 step=1700, 5.4842 examples/sec lr=0.000100, loss=119.225, loss_ll=22.4333, loss_ll_paf=38.0387, loss_ll_heat=6.8278, q=1000
[2018-06-28 00:21:57,349] [train] [INFO] epoch=0.00 step=1800, 5.5222 examples/sec lr=0.000100, loss=116.251, loss_ll=20.3609, loss_ll_paf=34.4541, loss_ll_heat=6.26776, q=1000
[2018-06-28 00:26:07,710] [train] [INFO] epoch=0.00 step=1900, 5.5620 examples/sec lr=0.000100, loss=122.173, loss_ll=22.831, loss_ll_paf=39.7211, loss_ll_heat=5.94091, q=1000
[2018-06-28 00:30:22,389] [train] [INFO] epoch=0.00 step=2000, 5.5941 examples/sec lr=0.000100, loss=128.309, loss_ll=23.7554, loss_ll_paf=41.0716, loss_ll_heat=6.43922, q=1000
[2018-06-28 00:35:08,951] [train] [INFO] epoch=0.00 step=2100, 5.5936 examples/sec lr=0.000100, loss=144.566, loss_ll=26.8431, loss_ll_paf=46.0882, loss_ll_heat=7.59796, q=1000
[2018-06-28 00:39:23,164] [train] [INFO] epoch=0.00 step=2200, 5.6220 examples/sec lr=0.000100, loss=129.135, loss_ll=23.9456, loss_ll_paf=42.0683, loss_ll_heat=5.82289, q=1000
[2018-06-28 00:43:40,807] [train] [INFO] epoch=0.00 step=2300, 5.6452 examples/sec lr=0.000100, loss=129.829, loss_ll=24.5301, loss_ll_paf=41.3997, loss_ll_heat=7.66041, q=1000
[2018-06-28 00:47:56,363] [train] [INFO] epoch=0.00 step=2400, 5.6685 examples/sec lr=0.000100, loss=139.621, loss_ll=25.9886, loss_ll_paf=45.9065, loss_ll_heat=6.07072, q=1000
[2018-06-28 00:52:08,934] [train] [INFO] epoch=0.00 step=2500, 5.6924 examples/sec lr=0.000100, loss=101.342, loss_ll=19.0936, loss_ll_paf=32.9493, loss_ll_heat=5.23779, q=1000
[2018-06-28 00:56:22,879] [train] [INFO] epoch=0.00 step=2600, 5.7136 examples/sec lr=0.000100, loss=95.7259, loss_ll=17.6908, loss_ll_paf=30.4833, loss_ll_heat=4.8984, q=1000
[2018-06-28 01:00:35,179] [train] [INFO] epoch=0.00 step=2700, 5.7347 examples/sec lr=0.000100, loss=123.837, loss_ll=23.1716, loss_ll_paf=39.1865, loss_ll_heat=7.15662, q=1000
[2018-06-28 01:04:51,440] [train] [INFO] epoch=0.00 step=2800, 5.7514 examples/sec lr=0.000100, loss=83.1353, loss_ll=15.4604, loss_ll_paf=26.8636, loss_ll_heat=4.05711, q=1000
[2018-06-28 01:09:06,143] [train] [INFO] epoch=0.00 step=2900, 5.7682 examples/sec lr=0.000100, loss=95.4905, loss_ll=16.768, loss_ll_paf=27.4087, loss_ll_heat=6.12731, q=1000
[2018-06-28 01:13:22,584] [train] [INFO] epoch=0.00 step=3000, 5.7827 examples/sec lr=0.000100, loss=88.5888, loss_ll=16.2206, loss_ll_paf=27.3927, loss_ll_heat=5.04839, q=1000
[2018-06-28 01:18:04,866] [train] [INFO] epoch=0.00 step=3100, 5.7790 examples/sec lr=0.000100, loss=105.257, loss_ll=19.5362, loss_ll_paf=34.7166, loss_ll_heat=4.35589, q=1000
[2018-06-28 01:22:23,676] [train] [INFO] epoch=0.00 step=3200, 5.7908 examples/sec lr=0.000100, loss=80.7354, loss_ll=14.7783, loss_ll_paf=25.2184, loss_ll_heat=4.33821, q=1000
[2018-06-28 01:26:40,832] [train] [INFO] epoch=0.00 step=3300, 5.8030 examples/sec lr=0.000100, loss=83.3618, loss_ll=14.7237, loss_ll_paf=24.5681, loss_ll_heat=4.87924, q=1000
[2018-06-28 01:30:55,331] [train] [INFO] epoch=0.00 step=3400, 5.8161 examples/sec lr=0.000100, loss=96.0385, loss_ll=17.9444, loss_ll_paf=30.0548, loss_ll_heat=5.83392, q=1000
[2018-06-28 01:35:08,662] [train] [INFO] epoch=0.00 step=3500, 5.8293 examples/sec lr=0.000100, loss=85.2062, loss_ll=15.2583, loss_ll_paf=25.353, loss_ll_heat=5.16368, q=1000
[2018-06-28 01:39:21,576] [train] [INFO] epoch=0.00 step=3600, 5.8421 examples/sec lr=0.000100, loss=78.0221, loss_ll=14.5192, loss_ll_paf=23.5458, loss_ll_heat=5.4926, q=1000
[2018-06-28 01:43:43,848] [train] [INFO] epoch=0.00 step=3700, 5.8488 examples/sec lr=0.000100, loss=114.182, loss_ll=20.7349, loss_ll_paf=37.253, loss_ll_heat=4.21676, q=1000
[2018-06-28 01:48:02,266] [train] [INFO] epoch=0.00 step=3800, 5.8573 examples/sec lr=0.000100, loss=80.8103, loss_ll=13.1372, loss_ll_paf=22.8712, loss_ll_heat=3.40315, q=1000
[2018-06-28 01:52:26,128] [train] [INFO] epoch=0.00 step=3900, 5.8624 examples/sec lr=0.000100, loss=99.7094, loss_ll=18.6003, loss_ll_paf=32.2956, loss_ll_heat=4.90504, q=1000
[2018-06-28 01:56:47,793] [train] [INFO] epoch=0.00 step=4000, 5.8685 examples/sec lr=0.000100, loss=76.6889, loss_ll=13.6741, loss_ll_paf=22.9274, loss_ll_heat=4.42086, q=1000
[2018-06-28 02:01:26,792] [train] [INFO] epoch=0.00 step=4100, 5.8651 examples/sec lr=0.000100, loss=66.3894, loss_ll=11.8946, loss_ll_paf=19.6081, loss_ll_heat=4.18119, q=1000
[2018-06-28 02:05:42,190] [train] [INFO] epoch=0.00 step=4200, 5.8740 examples/sec lr=0.000100, loss=96.6553, loss_ll=17.9915, loss_ll_paf=31.2448, loss_ll_heat=4.7383, q=1000
[2018-06-28 02:09:57,658] [train] [INFO] epoch=0.00 step=4300, 5.8825 examples/sec lr=0.000100, loss=73.9111, loss_ll=12.6207, loss_ll_paf=20.4871, loss_ll_heat=4.75435, q=1000
[2018-06-28 02:14:13,646] [train] [INFO] epoch=0.00 step=4400, 5.8904 examples/sec lr=0.000100, loss=85.2651, loss_ll=14.8476, loss_ll_paf=24.8276, loss_ll_heat=4.86756, q=1000
[2018-06-28 02:18:32,168] [train] [INFO] epoch=0.00 step=4500, 5.8967 examples/sec lr=0.000100, loss=82.7586, loss_ll=13.9422, loss_ll_paf=23.9201, loss_ll_heat=3.96437, q=1000
[2018-06-28 02:22:42,325] [train] [INFO] epoch=0.00 step=4600, 5.9068 examples/sec lr=0.000100, loss=107.041, loss_ll=20.4994, loss_ll_paf=34.5951, loss_ll_heat=6.40364, q=1000
[2018-06-28 02:26:55,339] [train] [INFO] epoch=0.00 step=4700, 5.9151 examples/sec lr=0.000100, loss=73.0006, loss_ll=12.9557, loss_ll_paf=21.4268, loss_ll_heat=4.48461, q=1000
[2018-06-28 02:31:09,211] [train] [INFO] epoch=0.00 step=4800, 5.9226 examples/sec lr=0.000100, loss=82.4664, loss_ll=14.8308, loss_ll_paf=25.0952, loss_ll_heat=4.56631, q=1000
[2018-06-28 02:35:21,276] [train] [INFO] epoch=0.00 step=4900, 5.9307 examples/sec lr=0.000100, loss=111.661, loss_ll=20.0782, loss_ll_paf=34.6278, loss_ll_heat=5.52859, q=1000
[2018-06-28 02:39:39,816] [train] [INFO] epoch=0.00 step=5000, 5.9357 examples/sec lr=0.000100, loss=76.0649, loss_ll=13.6064, loss_ll_paf=22.5388, loss_ll_heat=4.67393, q=1000
[2018-06-28 02:44:23,461] [train] [INFO] epoch=0.00 step=5100, 5.9296 examples/sec lr=0.000100, loss=92.5585, loss_ll=15.5455, loss_ll_paf=26.5465, loss_ll_heat=4.54448, q=1000
[2018-06-28 02:48:36,426] [train] [INFO] epoch=0.00 step=5200, 5.9368 examples/sec lr=0.000100, loss=73.3451, loss_ll=12.8883, loss_ll_paf=21.1054, loss_ll_heat=4.67123, q=1000
[2018-06-28 02:52:57,046] [train] [INFO] epoch=0.00 step=5300, 5.9405 examples/sec lr=0.000100, loss=95.6494, loss_ll=17.7998, loss_ll_paf=30.8244, loss_ll_heat=4.77509, q=1000
[2018-06-28 02:57:11,362] [train] [INFO] epoch=0.00 step=5400, 5.9466 examples/sec lr=0.000100, loss=85.929, loss_ll=15.6232, loss_ll_paf=26.8202, loss_ll_heat=4.4262, q=1000
[2018-06-28 03:01:32,849] [train] [INFO] epoch=0.00 step=5500, 5.9496 examples/sec lr=0.000100, loss=88.0899, loss_ll=15.9801, loss_ll_paf=26.188, loss_ll_heat=5.77227, q=1000
[2018-06-28 03:05:51,055] [train] [INFO] epoch=0.00 step=5600, 5.9539 examples/sec lr=0.000100, loss=88.3844, loss_ll=15.8648, loss_ll_paf=26.1741, loss_ll_heat=5.55558, q=1000
[2018-06-28 03:10:06,256] [train] [INFO] epoch=0.00 step=5700, 5.9591 examples/sec lr=0.000100, loss=68.8364, loss_ll=12.3703, loss_ll_paf=21.9259, loss_ll_heat=2.81472, q=1000
[2018-06-28 03:14:26,687] [train] [INFO] epoch=0.00 step=5800, 5.9622 examples/sec lr=0.000100, loss=80.6623, loss_ll=14.5254, loss_ll_paf=25.3037, loss_ll_heat=3.74713, q=1000
[2018-06-28 03:18:37,640] [train] [INFO] epoch=0.00 step=5900, 5.9688 examples/sec lr=0.000100, loss=96.8619, loss_ll=17.5365, loss_ll_paf=29.2886, loss_ll_heat=5.78444, q=1000
[2018-06-28 03:22:48,642] [train] [INFO] epoch=0.00 step=6000, 5.9751 examples/sec lr=0.000100, loss=69.0203, loss_ll=12.1132, loss_ll_paf=19.1852, loss_ll_heat=5.04115, q=1000
[2018-06-28 03:27:25,479] [train] [INFO] epoch=0.00 step=6100, 5.9718 examples/sec lr=0.000100, loss=96.7694, loss_ll=17.74, loss_ll_paf=30.7709, loss_ll_heat=4.70907, q=1000
[2018-06-28 03:31:42,375] [train] [INFO] epoch=0.00 step=6200, 5.9758 examples/sec lr=0.000100, loss=67.0961, loss_ll=11.6192, loss_ll_paf=19.3494, loss_ll_heat=3.88894, q=1000
[2018-06-28 03:35:53,621] [train] [INFO] epoch=0.00 step=6300, 5.9816 examples/sec lr=0.000100, loss=78.7103, loss_ll=14.8554, loss_ll_paf=25.5087, loss_ll_heat=4.20215, q=1000
[2018-06-28 03:40:07,175] [train] [INFO] epoch=0.00 step=6400, 5.9865 examples/sec lr=0.000100, loss=65.021, loss_ll=11.3815, loss_ll_paf=17.5264, loss_ll_heat=5.23658, q=1000
[2018-06-28 03:44:18,062] [train] [INFO] epoch=0.00 step=6500, 5.9922 examples/sec lr=0.000100, loss=86.6069, loss_ll=15.476, loss_ll_paf=25.5615, loss_ll_heat=5.39045, q=1000
[2018-06-28 03:48:34,892] [train] [INFO] epoch=0.00 step=6600, 5.9956 examples/sec lr=0.000100, loss=76.5424, loss_ll=14.171, loss_ll_paf=24.0724, loss_ll_heat=4.26964, q=1000
[2018-06-28 03:52:49,848] [train] [INFO] epoch=0.00 step=6700, 5.9996 examples/sec lr=0.000100, loss=66.2989, loss_ll=11.9862, loss_ll_paf=19.8124, loss_ll_heat=4.16, q=1000
[2018-06-28 03:57:04,482] [train] [INFO] epoch=0.00 step=6800, 6.0036 examples/sec lr=0.000100, loss=79.3171, loss_ll=13.9666, loss_ll_paf=24.398, loss_ll_heat=3.53529, q=1000
[2018-06-28 04:01:17,014] [train] [INFO] epoch=0.00 step=6900, 6.0082 examples/sec lr=0.000100, loss=78.3883, loss_ll=14.7635, loss_ll_paf=25.2937, loss_ll_heat=4.23328, q=1000
[2018-06-28 04:05:31,273] [train] [INFO] epoch=0.00 step=7000, 6.0121 examples/sec lr=0.000100, loss=59.1337, loss_ll=9.92535, loss_ll_paf=16.2656, loss_ll_heat=3.58507, q=1000
[2018-06-28 04:10:13,446] [train] [INFO] epoch=0.00 step=7100, 6.0070 examples/sec lr=0.000100, loss=72.9739, loss_ll=12.9779, loss_ll_paf=22.2609, loss_ll_heat=3.69495, q=1000
[2018-06-28 04:14:24,670] [train] [INFO] epoch=0.00 step=7200, 6.0117 examples/sec lr=0.000100, loss=97.305, loss_ll=18.5075, loss_ll_paf=30.8917, loss_ll_heat=6.12326, q=1000
[2018-06-28 04:18:38,884] [train] [INFO] epoch=0.00 step=7300, 6.0154 examples/sec lr=0.000100, loss=72.3405, loss_ll=12.6712, loss_ll_paf=21.4495, loss_ll_heat=3.89298, q=1000
[2018-06-28 04:22:51,581] [train] [INFO] epoch=0.00 step=7400, 6.0195 examples/sec lr=0.000100, loss=79.1489, loss_ll=13.9433, loss_ll_paf=24.0138, loss_ll_heat=3.87287, q=1000
[2018-06-28 04:27:05,485] [train] [INFO] epoch=0.00 step=7500, 6.0231 examples/sec lr=0.000100, loss=93.5041, loss_ll=17.4159, loss_ll_paf=30.8146, loss_ll_heat=4.01726, q=1000
[2018-06-28 04:31:20,430] [train] [INFO] epoch=0.00 step=7600, 6.0262 examples/sec lr=0.000100, loss=73.4868, loss_ll=12.544, loss_ll_paf=20.4209, loss_ll_heat=4.667, q=1000
[2018-06-28 04:35:33,369] [train] [INFO] epoch=1.00 step=7700, 6.0300 examples/sec lr=0.000100, loss=78.0917, loss_ll=13.085, loss_ll_paf=20.9599, loss_ll_heat=5.21012, q=1000
[2018-06-28 04:39:45,236] [train] [INFO] epoch=1.00 step=7800, 6.0339 examples/sec lr=0.000100, loss=84.9474, loss_ll=14.3706, loss_ll_paf=24.0842, loss_ll_heat=4.65689, q=1000
[2018-06-28 04:44:01,596] [train] [INFO] epoch=1.00 step=7900, 6.0364 examples/sec lr=0.000100, loss=74.7438, loss_ll=12.6596, loss_ll_paf=22.1439, loss_ll_heat=3.17534, q=1000
[2018-06-28 04:48:14,016] [train] [INFO] epoch=1.00 step=8000, 6.0400 examples/sec lr=0.000100, loss=76.7617, loss_ll=13.6481, loss_ll_paf=23.4987, loss_ll_heat=3.79755, q=1000
[2018-06-28 04:52:53,989] [train] [INFO] epoch=1.00 step=8100, 6.0358 examples/sec lr=0.000100, loss=70.7578, loss_ll=12.4099, loss_ll_paf=20.3122, loss_ll_heat=4.50771, q=1000
[2018-06-28 04:57:10,164] [train] [INFO] epoch=1.00 step=8200, 6.0383 examples/sec lr=0.000100, loss=69.3504, loss_ll=11.8069, loss_ll_paf=20.7442, loss_ll_heat=2.86955, q=1000
[2018-06-28 05:01:20,096] [train] [INFO] epoch=1.00 step=8300, 6.0424 examples/sec lr=0.000100, loss=60.4368, loss_ll=10.4306, loss_ll_paf=17.6395, loss_ll_heat=3.2218, q=1000
[2018-06-28 05:05:34,223] [train] [INFO] epoch=1.00 step=8400, 6.0453 examples/sec lr=0.000100, loss=68.8453, loss_ll=12.202, loss_ll_paf=20.0187, loss_ll_heat=4.3854, q=1000
[2018-06-28 05:09:47,854] [train] [INFO] epoch=1.00 step=8500, 6.0483 examples/sec lr=0.000100, loss=54.2501, loss_ll=8.86351, loss_ll_paf=13.6932, loss_ll_heat=4.03384, q=1000
[2018-06-28 05:13:57,634] [train] [INFO] epoch=1.00 step=8600, 6.0522 examples/sec lr=0.000100, loss=76.1427, loss_ll=13.5688, loss_ll_paf=22.7213, loss_ll_heat=4.41632, q=1000
[2018-06-28 05:18:11,729] [train] [INFO] epoch=1.00 step=8700, 6.0549 examples/sec lr=0.000100, loss=60.8956, loss_ll=10.2736, loss_ll_paf=17.0413, loss_ll_heat=3.50588, q=1000
[2018-06-28 05:22:21,598] [train] [INFO] epoch=1.00 step=8800, 6.0586 examples/sec lr=0.000100, loss=66.4234, loss_ll=11.8114, loss_ll_paf=20.1238, loss_ll_heat=3.49899, q=1000
[2018-06-28 05:26:29,597] [train] [INFO] epoch=1.00 step=8900, 6.0628 examples/sec lr=0.000100, loss=87.6218, loss_ll=16.1844, loss_ll_paf=28.3602, loss_ll_heat=4.00853, q=1000
[2018-06-28 05:30:42,488] [train] [INFO] epoch=1.00 step=9000, 6.0656 examples/sec lr=0.000100, loss=78.4552, loss_ll=14.1183, loss_ll_paf=24.5452, loss_ll_heat=3.69128, q=1000
[2018-06-28 05:35:24,149] [train] [INFO] epoch=1.00 step=9100, 6.0611 examples/sec lr=0.000100, loss=58.2939, loss_ll=10.7264, loss_ll_paf=17.669, loss_ll_heat=3.78379, q=1000
[2018-06-28 05:39:33,451] [train] [INFO] epoch=1.00 step=9200, 6.0647 examples/sec lr=0.000100, loss=70.1838, loss_ll=11.3989, loss_ll_paf=19.3185, loss_ll_heat=3.47938, q=1000
[2018-06-28 05:43:43,579] [train] [INFO] epoch=1.00 step=9300, 6.0681 examples/sec lr=0.000100, loss=70.27, loss_ll=13.3515, loss_ll_paf=22.8536, loss_ll_heat=3.84935, q=1000
[2018-06-28 05:47:54,341] [train] [INFO] epoch=1.00 step=9400, 6.0713 examples/sec lr=0.000100, loss=73.0737, loss_ll=12.7457, loss_ll_paf=20.8356, loss_ll_heat=4.6557, q=1000
[2018-06-28 05:52:10,288] [train] [INFO] epoch=1.00 step=9500, 6.0731 examples/sec lr=0.000100, loss=69.9246, loss_ll=12.6771, loss_ll_paf=21.1674, loss_ll_heat=4.18676, q=1000
[2018-06-28 05:56:21,432] [train] [INFO] epoch=1.00 step=9600, 6.0761 examples/sec lr=0.000100, loss=62.6511, loss_ll=11.6597, loss_ll_paf=18.7088, loss_ll_heat=4.61046, q=1000
[2018-06-28 06:00:31,492] [train] [INFO] epoch=1.00 step=9700, 6.0793 examples/sec lr=0.000100, loss=63.9223, loss_ll=11.6248, loss_ll_paf=19.0665, loss_ll_heat=4.18316, q=1000
[2018-06-28 06:04:45,859] [train] [INFO] epoch=1.00 step=9800, 6.0813 examples/sec lr=0.000100, loss=36.5774, loss_ll=6.39986, loss_ll_paf=9.85843, loss_ll_heat=2.94128, q=1000
[2018-06-28 06:09:02,198] [train] [INFO] epoch=1.00 step=9900, 6.0829 examples/sec lr=0.000100, loss=73.1559, loss_ll=13.0612, loss_ll_paf=22.9126, loss_ll_heat=3.20988, q=1000
[2018-06-28 06:13:15,957] [train] [INFO] epoch=1.00 step=10000, 6.0851 examples/sec lr=0.000100, loss=46.693, loss_ll=7.71962, loss_ll_paf=12.7362, loss_ll_heat=2.703, q=1000
[2018-06-28 06:17:56,546] [train] [INFO] epoch=1.00 step=10100, 6.0810 examples/sec lr=0.000100, loss=60.7023, loss_ll=10.5842, loss_ll_paf=18.1802, loss_ll_heat=2.98823, q=1000
[2018-06-28 06:22:08,198] [train] [INFO] epoch=1.00 step=10200, 6.0836 examples/sec lr=0.000100, loss=64.418, loss_ll=11.7833, loss_ll_paf=19.2743, loss_ll_heat=4.29233, q=1000
[2018-06-28 06:26:24,887] [train] [INFO] epoch=1.00 step=10300, 6.0850 examples/sec lr=0.000100, loss=80.0293, loss_ll=14.2329, loss_ll_paf=25.2679, loss_ll_heat=3.19786, q=1000
[2018-06-28 06:30:38,053] [train] [INFO] epoch=1.00 step=10400, 6.0872 examples/sec lr=0.000100, loss=49.1364, loss_ll=8.67869, loss_ll_paf=14.3172, loss_ll_heat=3.04021, q=1000
[2018-06-28 06:34:54,537] [train] [INFO] epoch=1.00 step=10500, 6.0886 examples/sec lr=0.000100, loss=79.6332, loss_ll=15.0459, loss_ll_paf=26.1784, loss_ll_heat=3.91337, q=1000
[2018-06-28 06:39:08,688] [train] [INFO] epoch=1.00 step=10600, 6.0905 examples/sec lr=0.000100, loss=77.6464, loss_ll=14.6253, loss_ll_paf=24.6659, loss_ll_heat=4.58471, q=1000
[2018-06-28 06:43:22,791] [train] [INFO] epoch=1.00 step=10700, 6.0924 examples/sec lr=0.000100, loss=64.1866, loss_ll=12.3262, loss_ll_paf=21.8408, loss_ll_heat=2.81167, q=1000
[2018-06-28 06:47:42,623] [train] [INFO] epoch=1.00 step=10800, 6.0930 examples/sec lr=0.000100, loss=92.2282, loss_ll=17.5215, loss_ll_paf=30.0078, loss_ll_heat=5.03513, q=1000
[2018-06-28 06:51:56,282] [train] [INFO] epoch=1.00 step=10900, 6.0949 examples/sec lr=0.000100, loss=59.797, loss_ll=10.5605, loss_ll_paf=18.283, loss_ll_heat=2.83804, q=1000
[2018-06-28 06:56:09,929] [train] [INFO] epoch=1.00 step=11000, 6.0967 examples/sec lr=0.000100, loss=84.356, loss_ll=15.2852, loss_ll_paf=25.692, loss_ll_heat=4.87836, q=1000
[2018-06-28 07:00:55,845] [train] [INFO] epoch=1.00 step=11100, 6.0918 examples/sec lr=0.000100, loss=65.4151, loss_ll=10.9464, loss_ll_paf=18.656, loss_ll_heat=3.23677, q=1000
[2018-06-28 07:05:12,162] [train] [INFO] epoch=1.00 step=11200, 6.0931 examples/sec lr=0.000100, loss=67.2384, loss_ll=12.1989, loss_ll_paf=21.3145, loss_ll_heat=3.08329, q=1000
[2018-06-28 07:09:26,002] [train] [INFO] epoch=1.00 step=11300, 6.0949 examples/sec lr=0.000100, loss=79.711, loss_ll=14.2527, loss_ll_paf=24.7566, loss_ll_heat=3.74881, q=1000
[2018-06-28 07:13:41,774] [train] [INFO] epoch=1.00 step=11400, 6.0963 examples/sec lr=0.000100, loss=51.4868, loss_ll=8.76262, loss_ll_paf=14.2583, loss_ll_heat=3.26698, q=1000
[2018-06-28 07:17:52,836] [train] [INFO] epoch=1.00 step=11500, 6.0986 examples/sec lr=0.000100, loss=67.0825, loss_ll=12.0991, loss_ll_paf=20.8933, loss_ll_heat=3.30492, q=1000
[2018-06-28 07:22:04,445] [train] [INFO] epoch=1.00 step=11600, 6.1008 examples/sec lr=0.000100, loss=58.0773, loss_ll=9.74143, loss_ll_paf=15.9578, loss_ll_heat=3.52502, q=1000
[2018-06-28 07:26:18,354] [train] [INFO] epoch=1.00 step=11700, 6.1024 examples/sec lr=0.000100, loss=109.879, loss_ll=20.225, loss_ll_paf=35.5649, loss_ll_heat=4.88507, q=1000
[2018-06-28 07:30:30,565] [train] [INFO] epoch=1.00 step=11800, 6.1044 examples/sec lr=0.000100, loss=69.2733, loss_ll=12.3177, loss_ll_paf=20.2096, loss_ll_heat=4.42582, q=1000
[2018-06-28 07:34:41,044] [train] [INFO] epoch=1.00 step=11900, 6.1067 examples/sec lr=0.000100, loss=48.4255, loss_ll=7.76042, loss_ll_paf=12.1975, loss_ll_heat=3.3233, q=1000
[2018-06-28 07:38:47,826] [train] [INFO] epoch=1.00 step=12000, 6.1096 examples/sec lr=0.000100, loss=42.6968, loss_ll=6.68684, loss_ll_paf=10.8958, loss_ll_heat=2.4779, q=1000
[2018-06-28 07:43:30,940] [train] [INFO] epoch=1.00 step=12100, 6.1055 examples/sec lr=0.000100, loss=53.9034, loss_ll=9.28884, loss_ll_paf=15.8548, loss_ll_heat=2.7229, q=1000
[2018-06-28 07:47:46,299] [train] [INFO] epoch=1.00 step=12200, 6.1068 examples/sec lr=0.000100, loss=82.7072, loss_ll=15.2957, loss_ll_paf=26.4123, loss_ll_heat=4.17911, q=1000
[2018-06-28 07:51:57,625] [train] [INFO] epoch=1.00 step=12300, 6.1088 examples/sec lr=0.000100, loss=49.3719, loss_ll=8.70906, loss_ll_paf=15.2728, loss_ll_heat=2.1453, q=1000
[2018-06-28 07:56:11,642] [train] [INFO] epoch=1.00 step=12400, 6.1103 examples/sec lr=0.000100, loss=56.4032, loss_ll=9.91238, loss_ll_paf=17.025, loss_ll_heat=2.79976, q=1000
[2018-06-28 08:00:29,283] [train] [INFO] epoch=1.00 step=12500, 6.1111 examples/sec lr=0.000100, loss=64.6657, loss_ll=10.5788, loss_ll_paf=18.9614, loss_ll_heat=2.19612, q=1000
[2018-06-28 08:04:41,904] [train] [INFO] epoch=1.00 step=12600, 6.1128 examples/sec lr=0.000100, loss=61.4558, loss_ll=10.9087, loss_ll_paf=18.4639, loss_ll_heat=3.35358, q=1000
[2018-06-28 08:08:55,525] [train] [INFO] epoch=1.00 step=12700, 6.1143 examples/sec lr=0.000100, loss=34.5972, loss_ll=5.89131, loss_ll_paf=9.52558, loss_ll_heat=2.25704, q=1000
[2018-06-28 08:13:11,188] [train] [INFO] epoch=1.00 step=12800, 6.1154 examples/sec lr=0.000100, loss=46.4482, loss_ll=8.61622, loss_ll_paf=14.3498, loss_ll_heat=2.88267, q=1000
[2018-06-28 08:17:24,743] [train] [INFO] epoch=1.00 step=12900, 6.1169 examples/sec lr=0.000100, loss=52.5242, loss_ll=8.88068, loss_ll_paf=15.0547, loss_ll_heat=2.70663, q=1000
[2018-06-28 08:21:39,364] [train] [INFO] epoch=1.00 step=13000, 6.1181 examples/sec lr=0.000100, loss=68.1926, loss_ll=11.7944, loss_ll_paf=19.9977, loss_ll_heat=3.59121, q=1000
[2018-06-28 08:26:19,653] [train] [INFO] epoch=1.00 step=13100, 6.1148 examples/sec lr=0.000100, loss=66.8793, loss_ll=11.8246, loss_ll_paf=20.4505, loss_ll_heat=3.19874, q=1000
[2018-06-28 08:30:29,600] [train] [INFO] epoch=1.00 step=13200, 6.1169 examples/sec lr=0.000100, loss=63.9077, loss_ll=11.3262, loss_ll_paf=19.2203, loss_ll_heat=3.432, q=1000
[2018-06-28 08:34:44,012] [train] [INFO] epoch=1.00 step=13300, 6.1181 examples/sec lr=0.000100, loss=40.5738, loss_ll=6.39706, loss_ll_paf=10.2875, loss_ll_heat=2.50659, q=1000
[2018-06-28 08:38:54,555] [train] [INFO] epoch=1.00 step=13400, 6.1200 examples/sec lr=0.000100, loss=67.3749, loss_ll=11.7562, loss_ll_paf=20.7669, loss_ll_heat=2.74559, q=1000
[2018-06-28 08:43:09,342] [train] [INFO] epoch=1.00 step=13500, 6.1212 examples/sec lr=0.000100, loss=55.8771, loss_ll=9.44853, loss_ll_paf=16.0085, loss_ll_heat=2.8886, q=1000
[2018-06-28 08:47:23,060] [train] [INFO] epoch=1.00 step=13600, 6.1225 examples/sec lr=0.000100, loss=45.7343, loss_ll=7.42582, loss_ll_paf=12.7131, loss_ll_heat=2.13856, q=1000
[2018-06-28 08:51:36,431] [train] [INFO] epoch=1.00 step=13700, 6.1239 examples/sec lr=0.000100, loss=60.9129, loss_ll=10.5402, loss_ll_paf=17.5752, loss_ll_heat=3.5051, q=1000
[2018-06-28 08:55:52,708] [train] [INFO] epoch=1.00 step=13800, 6.1247 examples/sec lr=0.000100, loss=50.4848, loss_ll=8.7602, loss_ll_paf=13.9665, loss_ll_heat=3.55394, q=1000
[2018-06-28 09:00:10,122] [train] [INFO] epoch=1.00 step=13900, 6.1254 examples/sec lr=0.000100, loss=36.1367, loss_ll=6.38096, loss_ll_paf=10.6203, loss_ll_heat=2.14167, q=1000
[2018-06-28 09:04:22,883] [train] [INFO] epoch=1.00 step=14000, 6.1268 examples/sec lr=0.000100, loss=54.2882, loss_ll=9.17184, loss_ll_paf=15.4166, loss_ll_heat=2.92713, q=1000
[2018-06-28 09:09:03,921] [train] [INFO] epoch=1.00 step=14100, 6.1235 examples/sec lr=0.000100, loss=71.6906, loss_ll=12.525, loss_ll_paf=20.9806, loss_ll_heat=4.06946, q=1000
[2018-06-28 09:13:18,152] [train] [INFO] epoch=1.00 step=14200, 6.1246 examples/sec lr=0.000100, loss=32.7521, loss_ll=5.3671, loss_ll_paf=7.91626, loss_ll_heat=2.81793, q=1000
[2018-06-28 09:17:30,872] [train] [INFO] epoch=1.00 step=14300, 6.1260 examples/sec lr=0.000100, loss=56.6215, loss_ll=10.0439, loss_ll_paf=16.9065, loss_ll_heat=3.18134, q=1000
[2018-06-28 09:21:45,263] [train] [INFO] epoch=1.00 step=14400, 6.1271 examples/sec lr=0.000100, loss=66.7659, loss_ll=11.3759, loss_ll_paf=19.3122, loss_ll_heat=3.43953, q=1000
[2018-06-28 09:25:59,065] [train] [INFO] epoch=1.00 step=14500, 6.1283 examples/sec lr=0.000100, loss=48.9851, loss_ll=8.51281, loss_ll_paf=13.706, loss_ll_heat=3.31962, q=1000
[2018-06-28 09:30:12,715] [train] [INFO] epoch=1.00 step=14600, 6.1295 examples/sec lr=0.000100, loss=48.7738, loss_ll=7.95914, loss_ll_paf=12.7641, loss_ll_heat=3.1542, q=1000
[2018-06-28 09:34:26,999] [train] [INFO] epoch=1.00 step=14700, 6.1306 examples/sec lr=0.000100, loss=48.5183, loss_ll=8.98243, loss_ll_paf=14.447, loss_ll_heat=3.51791, q=1000
[2018-06-28 09:38:41,186] [train] [INFO] epoch=1.00 step=14800, 6.1317 examples/sec lr=0.000100, loss=91.7606, loss_ll=16.0909, loss_ll_paf=28.9863, loss_ll_heat=3.19546, q=1000
[2018-06-28 09:42:56,595] [train] [INFO] epoch=1.00 step=14900, 6.1325 examples/sec lr=0.000100, loss=56.4834, loss_ll=9.56551, loss_ll_paf=16.4103, loss_ll_heat=2.7207, q=1000
[2018-06-28 09:47:13,592] [train] [INFO] epoch=1.00 step=15000, 6.1332 examples/sec lr=0.000100, loss=56.5994, loss_ll=9.33785, loss_ll_paf=16.1699, loss_ll_heat=2.50581, q=1000
[2018-06-28 09:51:53,973] [train] [INFO] epoch=1.00 step=15100, 6.1301 examples/sec lr=0.000100, loss=37.6805, loss_ll=6.07026, loss_ll_paf=10.1928, loss_ll_heat=1.94775, q=1000
[2018-06-28 09:56:06,885] [train] [INFO] epoch=1.00 step=15200, 6.1314 examples/sec lr=0.000100, loss=46.555, loss_ll=7.55462, loss_ll_paf=12.2516, loss_ll_heat=2.85761, q=1000
[2018-06-28 10:00:15,339] [train] [INFO] epoch=2.00 step=15300, 6.1333 examples/sec lr=0.000100, loss=45.3583, loss_ll=8.03571, loss_ll_paf=13.3598, loss_ll_heat=2.71158, q=1000
[2018-06-28 10:04:31,781] [train] [INFO] epoch=2.00 step=15400, 6.1340 examples/sec lr=0.000100, loss=55.4429, loss_ll=10.0894, loss_ll_paf=17.5048, loss_ll_heat=2.67411, q=1000
[2018-06-28 10:08:45,919] [train] [INFO] epoch=2.00 step=15500, 6.1350 examples/sec lr=0.000100, loss=41.5688, loss_ll=7.57361, loss_ll_paf=12.7543, loss_ll_heat=2.39291, q=1000
[2018-06-28 10:12:58,484] [train] [INFO] epoch=2.00 step=15600, 6.1362 examples/sec lr=0.000100, loss=48.8248, loss_ll=8.03212, loss_ll_paf=13.068, loss_ll_heat=2.99623, q=1000
[2018-06-28 10:17:11,048] [train] [INFO] epoch=2.00 step=15700, 6.1375 examples/sec lr=0.000100, loss=71.1322, loss_ll=13.0807, loss_ll_paf=22.4262, loss_ll_heat=3.73517, q=1000
[2018-06-28 10:21:25,032] [train] [INFO] epoch=2.00 step=15800, 6.1385 examples/sec lr=0.000100, loss=56.7962, loss_ll=10.2459, loss_ll_paf=17.335, loss_ll_heat=3.15682, q=1000
[2018-06-28 10:25:37,196] [train] [INFO] epoch=2.00 step=15900, 6.1397 examples/sec lr=0.000100, loss=54.3983, loss_ll=9.1483, loss_ll_paf=14.8954, loss_ll_heat=3.40118, q=1000
[2018-06-28 10:29:47,835] [train] [INFO] epoch=2.00 step=16000, 6.1412 examples/sec lr=0.000100, loss=52.9626, loss_ll=9.44693, loss_ll_paf=15.9423, loss_ll_heat=2.95156, q=1000
[2018-06-28 10:34:28,880] [train] [INFO] epoch=2.00 step=16100, 6.1382 examples/sec lr=0.000100, loss=56.4579, loss_ll=9.77587, loss_ll_paf=16.7358, loss_ll_heat=2.81598, q=1000
[2018-06-28 10:38:38,542] [train] [INFO] epoch=2.00 step=16200, 6.1398 examples/sec lr=0.000100, loss=51.038, loss_ll=9.07988, loss_ll_paf=15.3303, loss_ll_heat=2.8295, q=1000
[2018-06-28 10:42:50,117] [train] [INFO] epoch=2.00 step=16300, 6.1411 examples/sec lr=0.000100, loss=47.4548, loss_ll=8.21237, loss_ll_paf=13.5873, loss_ll_heat=2.83747, q=1000
[2018-06-28 10:47:04,088] [train] [INFO] epoch=2.00 step=16400, 6.1420 examples/sec lr=0.000100, loss=35.8396, loss_ll=6.08034, loss_ll_paf=9.51172, loss_ll_heat=2.64895, q=1000
[2018-06-28 10:51:16,321] [train] [INFO] epoch=2.00 step=16500, 6.1432 examples/sec lr=0.000100, loss=54.8781, loss_ll=9.77162, loss_ll_paf=16.6226, loss_ll_heat=2.92067, q=1000
[2018-06-28 10:55:28,413] [train] [INFO] epoch=2.00 step=16600, 6.1444 examples/sec lr=0.000100, loss=48.2564, loss_ll=8.07219, loss_ll_paf=13.8651, loss_ll_heat=2.2793, q=1000
[2018-06-28 10:59:41,182] [train] [INFO] epoch=2.00 step=16700, 6.1455 examples/sec lr=0.000100, loss=61.8959, loss_ll=11.1624, loss_ll_paf=18.7669, loss_ll_heat=3.5579, q=1000
[2018-06-28 11:03:55,479] [train] [INFO] epoch=2.00 step=16800, 6.1463 examples/sec lr=0.000100, loss=41.1852, loss_ll=7.34959, loss_ll_paf=11.6812, loss_ll_heat=3.01801, q=1000
[2018-06-28 11:08:06,079] [train] [INFO] epoch=2.00 step=16900, 6.1477 examples/sec lr=0.000100, loss=41.209, loss_ll=6.81694, loss_ll_paf=11.3215, loss_ll_heat=2.31234, q=1000
[2018-06-28 11:12:13,698] [train] [INFO] epoch=2.00 step=17000, 6.1494 examples/sec lr=0.000100, loss=46.9181, loss_ll=7.35247, loss_ll_paf=11.4804, loss_ll_heat=3.22452, q=1000
[2018-06-28 11:16:58,137] [train] [INFO] epoch=2.00 step=17100, 6.1461 examples/sec lr=0.000100, loss=39.1564, loss_ll=7.22405, loss_ll_paf=12.2374, loss_ll_heat=2.21073, q=1000
[2018-06-28 11:21:08,704] [train] [INFO] epoch=2.00 step=17200, 6.1474 examples/sec lr=0.000100, loss=44.9253, loss_ll=7.78686, loss_ll_paf=13.0237, loss_ll_heat=2.54998, q=1000
[2018-06-28 11:25:25,315] [train] [INFO] epoch=2.00 step=17300, 6.1479 examples/sec lr=0.000100, loss=58.7147, loss_ll=10.0779, loss_ll_paf=17.0588, loss_ll_heat=3.09698, q=1000
[2018-06-28 11:29:41,498] [train] [INFO] epoch=2.00 step=17400, 6.1485 examples/sec lr=0.000100, loss=55.9698, loss_ll=10.7121, loss_ll_paf=19.3458, loss_ll_heat=2.07842, q=1000
[2018-06-28 11:33:50,748] [train] [INFO] epoch=2.00 step=17500, 6.1500 examples/sec lr=0.000100, loss=34.4034, loss_ll=5.73253, loss_ll_paf=8.87425, loss_ll_heat=2.5908, q=1000
[2018-06-28 11:38:06,141] [train] [INFO] epoch=2.00 step=17600, 6.1506 examples/sec lr=0.000100, loss=49.9022, loss_ll=8.16867, loss_ll_paf=13.5744, loss_ll_heat=2.76296, q=1000

